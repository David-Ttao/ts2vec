Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='etth1_etth2', random_seed=2021, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/etth1_etth2_epochs_250_seed_2021
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=5.080698524202619
Epoch #1: loss=2.718505484717233
Epoch #2: loss=2.3282486711229597
Epoch #3: loss=2.043904866491045
Epoch #4: loss=1.9648444993155343
Epoch #5: loss=1.852328053542546
Epoch #6: loss=1.7351522530828203
Epoch #7: loss=1.7029000861304147
Epoch #8: loss=1.5276555163519723
Epoch #9: loss=1.4344984292984009
Epoch #10: loss=1.3508724910872323
Epoch #11: loss=1.337577794279371
Epoch #12: loss=1.2817190374646867
Epoch #13: loss=1.2250910231045313
Epoch #14: loss=1.1753148777144296
Epoch #15: loss=1.1056038013526372
Epoch #16: loss=1.0565550455025263
Epoch #17: loss=1.0266104255403792
Epoch #18: loss=1.0792265491826194
Epoch #19: loss=0.9487440500940595
Epoch #20: loss=0.9040963649749756
Epoch #21: loss=0.9436520125184741
Epoch #22: loss=0.7954932068075452
Epoch #23: loss=0.8448275242533002
Epoch #24: loss=0.8314051968710763
Epoch #25: loss=0.8875381776264736
Epoch #26: loss=0.8571560042245048
Epoch #27: loss=0.6785628582750048
Epoch #28: loss=0.7409255504608154
Epoch #29: loss=0.7157911700861794
Epoch #30: loss=0.6963481605052948
Epoch #31: loss=0.9038836402552468
Epoch #32: loss=0.6621159378971372
Epoch #33: loss=0.6674987205437252
Epoch #34: loss=0.6940826411758151
Epoch #35: loss=0.5781969768660409
Epoch #36: loss=0.5726205557584763
Epoch #37: loss=0.5329600636448178
Epoch #38: loss=0.5455525496176311
Epoch #39: loss=0.5302049432482038
Epoch #40: loss=0.5350044731582914
Epoch #41: loss=0.6230170662914004
Epoch #42: loss=0.5231561256306512
Epoch #43: loss=0.5479754017932075
Epoch #44: loss=0.4625006211655481
Epoch #45: loss=0.5863912808043616
Epoch #46: loss=0.48864997923374176
Epoch #47: loss=0.5138788861887795
Epoch #48: loss=0.5296225036893573
Epoch #49: loss=0.6585824276719775
Epoch #50: loss=0.5106298540319715
Epoch #51: loss=0.5418128030640739
Epoch #52: loss=0.6127773714917046
Epoch #53: loss=0.4204495506627219
Epoch #54: loss=0.41854239787374226
Epoch #55: loss=0.5074841465268817
Epoch #56: loss=0.39134298477854046
Epoch #57: loss=0.4455406644514629
Epoch #58: loss=0.38962193046297344
Epoch #59: loss=0.3813253811427525
Epoch #60: loss=0.5396079186882291
Epoch #61: loss=0.4545043408870697
Epoch #62: loss=0.4350118913820812
Epoch #63: loss=0.4088066411869867
Epoch #64: loss=0.3371439299413136
Epoch #65: loss=0.3086269593664578
Epoch #66: loss=0.47026440288339344
Epoch #67: loss=0.38926173001527786
Epoch #68: loss=0.43030613341501783
Epoch #69: loss=0.4050804717200143
Epoch #70: loss=0.44262187608650755
Epoch #71: loss=0.35898943671158384
Epoch #72: loss=0.4694150388240814
Epoch #73: loss=0.4357159637979099
Epoch #74: loss=0.41405172433171955
Epoch #75: loss=0.4406012658561979
Epoch #76: loss=0.3592663598912103
Epoch #77: loss=0.3774988204240799
Epoch #78: loss=0.30496036048446384
Epoch #79: loss=0.44452498640332905
Epoch #80: loss=0.3573808946779796
Epoch #81: loss=0.44238018883126123
Epoch #82: loss=0.43212869337626864
Epoch #83: loss=0.3236049935221672
Epoch #84: loss=0.3886424068893705
Epoch #85: loss=0.2957690932921001
Epoch #86: loss=0.33457829696791513
Epoch #87: loss=0.2716449179819652
Epoch #88: loss=0.31450422001736505
Epoch #89: loss=0.49502256725515636
Epoch #90: loss=0.3745903553707259
Epoch #91: loss=0.35015203058719635
Epoch #92: loss=0.3870960207922118
Epoch #93: loss=0.2898104584642819
Epoch #94: loss=0.28706391900777817
Epoch #95: loss=0.27972431906632017
Epoch #96: loss=0.24301091049398696
Epoch #97: loss=0.26457748668534414
Epoch #98: loss=0.28186379798821043
Epoch #99: loss=0.25217507247413906
Epoch #100: loss=0.2594082419361387
Epoch #101: loss=0.28243991839034216
Epoch #102: loss=0.30080731213092804
Epoch #103: loss=0.3565681885395731
Epoch #104: loss=0.33352024640355793
Epoch #105: loss=0.25884182431868147
Epoch #106: loss=0.2816162641559328
Epoch #107: loss=0.28098975334848675
Epoch #108: loss=0.37025337134088787
Epoch #109: loss=0.35854330233165194
Epoch #110: loss=0.3194152531879289
Epoch #111: loss=0.24711227949176515
Epoch #112: loss=0.24261387011834554
Epoch #113: loss=0.3722431808710098
Epoch #114: loss=0.3599101166640009
Epoch #115: loss=0.28174157972846714
Epoch #116: loss=0.23205003993851797
Epoch #117: loss=0.3139375022479466
Epoch #118: loss=0.2730090341397694
Epoch #119: loss=0.25978792884520124
Epoch #120: loss=0.27738956681319643
Epoch #121: loss=0.353663227387837
Epoch #122: loss=0.3587897228343146
Epoch #123: loss=0.2597770243883133
Epoch #124: loss=0.4437498929245131
Epoch #125: loss=0.28786235621997286
Epoch #126: loss=0.24606567514794214
Epoch #127: loss=0.2486633967076029
Epoch #128: loss=0.2928501144051552
Epoch #129: loss=0.25738428639514105
Epoch #130: loss=0.38729401571410044
Epoch #131: loss=0.25547246422086445
Epoch #132: loss=0.24545275845697948
Epoch #133: loss=0.2845732048153877
Epoch #134: loss=0.26163820709500996
Epoch #135: loss=0.2716650462576321
Epoch #136: loss=0.22881924893174851
Epoch #137: loss=0.28462805705411093
Epoch #138: loss=0.26955720888716833
Epoch #139: loss=0.40107384962694986
Epoch #140: loss=0.239217004605702
Epoch #141: loss=0.24707079572337015
Epoch #142: loss=0.2706161450062479
Epoch #143: loss=0.29613533722502844
Epoch #144: loss=0.2223566547036171
Epoch #145: loss=0.21802818988050734
Epoch #146: loss=0.26798735771860394
Epoch #147: loss=0.20325838455132075
Epoch #148: loss=0.22267688385077886
Epoch #149: loss=0.21454965536083495
Epoch #150: loss=0.21733012050390244
Epoch #151: loss=0.20778958392994745
Epoch #152: loss=0.26426950629268375
Epoch #153: loss=0.21432885208300181
Epoch #154: loss=0.25378959093775066
Epoch #155: loss=0.21435427985021047
Epoch #156: loss=0.23461575380393437
Epoch #157: loss=0.18950181028672627
Epoch #158: loss=0.2752125992306641
Epoch #159: loss=0.20994053142411367
Epoch #160: loss=0.19657582151038305
Epoch #161: loss=0.23858214701925004
Epoch #162: loss=0.2275113867861884
Epoch #163: loss=0.20251894103629248
Epoch #164: loss=0.1861157449228423
Epoch #165: loss=0.21066851860710553
Epoch #166: loss=0.20196167486054556
Epoch #167: loss=0.20200113207101822
Epoch #168: loss=0.1765764696257455
Epoch #169: loss=0.2144333262528692
Epoch #170: loss=0.3049688456313951
Epoch #171: loss=0.24513401782938413
Epoch #172: loss=0.2236745080777577
Epoch #173: loss=0.2901407373803003
Epoch #174: loss=0.22231710702180862
Epoch #175: loss=0.2016008815595082
Epoch #176: loss=0.22883106182728494
Epoch #177: loss=0.17469357486282075
Epoch #178: loss=0.22944600507616997
Epoch #179: loss=0.19832077303103038
Epoch #180: loss=0.17885793798736163
Epoch #181: loss=0.24575114675930568
Epoch #182: loss=0.19478689719523704
Epoch #183: loss=0.16902993779097283
Epoch #184: loss=0.19255270063877106
Epoch #185: loss=0.32141685113310814
Epoch #186: loss=0.23081757766859873
Epoch #187: loss=0.19334728377205984
Epoch #188: loss=0.15539915221078054
Epoch #189: loss=0.18208083510398865
Epoch #190: loss=0.16407151467033795
Epoch #191: loss=0.16604343535644667
Epoch #192: loss=0.14567501417228154
Epoch #193: loss=0.14909726647394045
Epoch #194: loss=0.13599873653479985
Epoch #195: loss=0.16963164135813713
Epoch #196: loss=0.24010947878871644
Epoch #197: loss=0.1606661794441087
Epoch #198: loss=0.22840262789811408
Epoch #199: loss=0.21549942823393003
Epoch #200: loss=0.1737112296479089
Epoch #201: loss=0.15966779525790895
Epoch #202: loss=0.15405411273241043
Epoch #203: loss=0.1910920233598777
Epoch #204: loss=0.2085462702172143
Epoch #205: loss=0.1591695642897061
Epoch #206: loss=0.18676814224038804
Epoch #207: loss=0.1789275567446436
Epoch #208: loss=0.15726409639630998
Epoch #209: loss=0.14228982212288038
Epoch #210: loss=0.1574600674211979
Epoch #211: loss=0.13722748256155423
Epoch #212: loss=0.13288925481694086
Epoch #213: loss=0.14053144146289145
Epoch #214: loss=0.16145205710615432
Epoch #215: loss=0.17059580449547088
Epoch #216: loss=0.12423971774322647
Epoch #217: loss=0.16509847342967987
Epoch #218: loss=0.1511933941926275
Epoch #219: loss=0.20623239928058215
Epoch #220: loss=0.1486442339207445
Epoch #221: loss=0.15221336483955383
Epoch #222: loss=0.16849278498973166
Epoch #223: loss=0.1512380497796195
Epoch #224: loss=0.2718523432101522
Epoch #225: loss=0.22628175893000194
Epoch #226: loss=0.19053297862410545
Epoch #227: loss=0.154568719544581
Epoch #228: loss=0.16506347539169447
Epoch #229: loss=0.13594587945512362
Epoch #230: loss=0.11974117585590907
Epoch #231: loss=0.10144134238362312
Epoch #232: loss=0.16905623993703298
Epoch #233: loss=0.15468183213046618
Epoch #234: loss=0.14780642677630698
Epoch #235: loss=0.1623352595738002
Epoch #236: loss=0.16627034757818496
Epoch #237: loss=0.13867905469877378
Epoch #238: loss=0.16084244368331774
Epoch #239: loss=0.20340874365397862
Epoch #240: loss=0.1449924877711705
Epoch #241: loss=0.18120982497930527
Epoch #242: loss=0.15631176105567388
Epoch #243: loss=0.1611622396324362
Epoch #244: loss=0.16849639053855622
Epoch #245: loss=0.1301808724445956
Epoch #246: loss=0.10233937629631587
Epoch #247: loss=0.10536976584366389
Epoch #248: loss=0.1435134488024882
Epoch #249: loss=0.1159248213682856

Training time: 0:08:05.768901

Finished.
n2one setting etth1_etth2 -> electricity
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth1_etth2_epochs_250_seed_2021/model.pkl', muti_dataset='etth1_etth2', random_seed=2021, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='electricity')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.16772e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.3621e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=5.7416e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.16772e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.35498098415625406, 'MAE': 0.4208167838495795}
Finished.
------------------------- record done -------------------------
n2one setting etth1_etth2 -> weather
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth1_etth2_epochs_250_seed_2021/model.pkl', muti_dataset='etth1_etth2', random_seed=2021, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='weather')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.3515e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.43029e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.3515e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.45094728509882936, 'MAE': 0.4802567974503512}
Finished.
------------------------- record done -------------------------
n2one setting etth1_etth2 -> exchange
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth1_etth2_epochs_250_seed_2021/model.pkl', muti_dataset='etth1_etth2', random_seed=2021, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='exchange')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=4.29712e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.21010815065242563, 'MAE': 0.31145405232925427}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='etth1_ettm1', random_seed=2021, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/etth1_ettm1_epochs_250_seed_2021
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=5.404422971937391
Epoch #1: loss=2.8177378045188055
Epoch #2: loss=2.391717712084452
Epoch #3: loss=2.1896077394485474
Epoch #4: loss=2.05168749888738
Epoch #5: loss=1.9862435460090637
Epoch #6: loss=1.8317808045281305
Epoch #7: loss=1.7517604231834412
Epoch #8: loss=1.6483428743150499
Epoch #9: loss=1.5177442100312974
Epoch #10: loss=1.4753165178828769
Epoch #11: loss=1.420159684287177
Epoch #12: loss=1.3478929267989264
Epoch #13: loss=1.228371222813924
Epoch #14: loss=1.2055085235171847
Epoch #15: loss=1.1010948684480455
Epoch #16: loss=1.1280052661895752
Epoch #17: loss=1.1368503305647109
Epoch #18: loss=1.0520161224736109
Epoch #19: loss=1.0556636485788558
Epoch #20: loss=1.0698973668946161
Epoch #21: loss=1.0568695498837366
Epoch #22: loss=0.9226915803220537
Epoch #23: loss=0.9440517425537109
Epoch #24: loss=0.8988081647290124
Epoch #25: loss=0.8699471652507782
Epoch #26: loss=0.8779608044359419
Epoch #27: loss=0.8461618853939904
Epoch #28: loss=0.8703666428724924
Epoch #29: loss=0.7827014459504021
Epoch #30: loss=0.7968202332655588
Epoch #31: loss=0.8626543912622664
Epoch #32: loss=0.774913145436181
Epoch #33: loss=0.6709777348571353
Epoch #34: loss=0.7682666348086463
Epoch #35: loss=0.6740230123202006
Epoch #36: loss=0.6882472038269043
Epoch #37: loss=0.7061761087841458
Epoch #38: loss=0.7056751052538554
Epoch #39: loss=0.6850752433141073
Epoch #40: loss=0.6114900790982776
Epoch #41: loss=0.7383225841654671
Epoch #42: loss=0.7622071388694975
Epoch #43: loss=0.8460175328784518
Epoch #44: loss=0.6900624732176462
Epoch #45: loss=0.6619289186265733
Epoch #46: loss=0.5714183201392492
Epoch #47: loss=0.5431365420420965
Epoch #48: loss=0.6219895482063293
Epoch #49: loss=0.5315381967359118
Epoch #50: loss=0.6305845545397865
Epoch #51: loss=0.5453539921177758
Epoch #52: loss=0.6243831680880653
Epoch #53: loss=0.551579174068239
Epoch #54: loss=0.5436700036128362
Epoch #55: loss=0.4876357913017273
Epoch #56: loss=0.5442087865538068
Epoch #57: loss=0.6135010139809715
Epoch #58: loss=0.4779840177959866
Epoch #59: loss=0.5307372129625745
Epoch #60: loss=0.43125999470551807
Epoch #61: loss=0.44513747096061707
Epoch #62: loss=0.5193755163086785
Epoch #63: loss=0.6046323147084978
Epoch #64: loss=0.5492455777194765
Epoch #65: loss=0.45746298631032306
Epoch #66: loss=0.5116698427332772
Epoch #67: loss=0.522347104218271
Epoch #68: loss=0.4714393334256278
Epoch #69: loss=0.510949949423472
Epoch #70: loss=0.47987229459815556
Epoch #71: loss=0.4524623403946559
Epoch #72: loss=0.43719204763571423
Epoch #73: loss=0.48187097741497886
Epoch #74: loss=0.4247586876153946
Epoch #75: loss=0.5359878738721212
Epoch #76: loss=0.464783627125952
Epoch #77: loss=0.42929478817515904
Epoch #78: loss=0.4441523179411888
Epoch #79: loss=0.5481455094284482
Epoch #80: loss=0.4770875424146652
Epoch #81: loss=0.4762824873129527
Epoch #82: loss=0.5029005739423964
Epoch #83: loss=0.42069386276933884
Epoch #84: loss=0.3441538181569841
Epoch #85: loss=0.3988337574733628
Epoch #86: loss=0.38214803238709766
Epoch #87: loss=0.43000073896514046
Epoch #88: loss=0.42294226090113324
Epoch #89: loss=0.395412888791826
Epoch #90: loss=0.36646157337559593
Epoch #91: loss=0.35582080152299667
Epoch #92: loss=0.4203126480182012
Epoch #93: loss=0.423440835542149
Epoch #94: loss=0.3885138928890228
Epoch #95: loss=0.3896600777904193
Epoch #96: loss=0.3908061914973789
Epoch #97: loss=0.3848971666561233
Epoch #98: loss=0.37631961206595105
Epoch #99: loss=0.39092349592182374
Epoch #100: loss=0.31668007870515186
Epoch #101: loss=0.45320721301767564
Epoch #102: loss=0.4100445542070601
Epoch #103: loss=0.40492885145876145
Epoch #104: loss=0.41326702468925053
Epoch #105: loss=0.34698804053995347
Epoch #106: loss=0.38511693725983304
Epoch #107: loss=0.3256355747580528
Epoch #108: loss=0.536894277566009
Epoch #109: loss=0.33240264654159546
Epoch #110: loss=0.3078383240434859
Epoch #111: loss=0.35162364774280125
Epoch #112: loss=0.28247279425462085
Epoch #113: loss=0.28910820765627754
Epoch #114: loss=0.35664460476901794
Epoch #115: loss=0.31712986363304985
Epoch #116: loss=0.28499681999286014
Epoch #117: loss=0.2531840362482601
Epoch #118: loss=0.24807942244741651
Epoch #119: loss=0.27589236862129635
Epoch #120: loss=0.33912500076823765
Epoch #121: loss=0.3461768279472987
Epoch #122: loss=0.3561176624563005
Epoch #123: loss=0.3415018560157882
Epoch #124: loss=0.2761850870317883
Epoch #125: loss=0.28407139745023513
Epoch #126: loss=0.2491511031985283
Epoch #127: loss=0.2538283293445905
Epoch #128: loss=0.3057296913531091
Epoch #129: loss=0.28226426657703185
Epoch #130: loss=0.3002243844999207
Epoch #131: loss=0.2611854804886712
Epoch #132: loss=0.22562583535909653
Epoch #133: loss=0.24263722946246466
Epoch #134: loss=0.23142111177245775
Epoch #135: loss=0.2601262463463677
Epoch #136: loss=0.22878767301638922
Epoch #137: loss=0.2614574241969321
Epoch #138: loss=0.22165918226043382
Epoch #139: loss=0.22373764134115642
Epoch #140: loss=0.1921370132929749
Epoch #141: loss=0.24654621134201685
Epoch #142: loss=0.32684289664030075
Epoch #143: loss=0.24724752290381324
Epoch #144: loss=0.23565779833330047
Epoch #145: loss=0.3046339228749275
Epoch #146: loss=0.2964932430121634
Epoch #147: loss=0.19789578310317463
Epoch #148: loss=0.25018374539083904
Epoch #149: loss=0.20755368429753515
Epoch #150: loss=0.17855727796753249
Epoch #151: loss=0.18839243178566298
Epoch #152: loss=0.2221097383234236
Epoch #153: loss=0.21569308390220007
Epoch #154: loss=0.17529872101214197
Epoch #155: loss=0.1757185599870152
Epoch #156: loss=0.13658763799402449
Epoch #157: loss=0.1472320511109299
Epoch #158: loss=0.21643566795521313
Epoch #159: loss=0.2224788140091631
Epoch #160: loss=0.1907756986717383
Epoch #161: loss=0.19602430529064602
Epoch #162: loss=0.13907422580652767
Epoch #163: loss=0.16977961444192463
Epoch #164: loss=0.19376411164800325
Epoch #165: loss=0.16847208908034694
Epoch #166: loss=0.170455535252889
Epoch #167: loss=0.13608290751775107
Epoch #168: loss=0.1614154213004642
Epoch #169: loss=0.16638639445106188
Epoch #170: loss=0.15726253762841225
Epoch #171: loss=0.21765382380949128
Epoch #172: loss=0.21624227778779137
Epoch #173: loss=0.21456133243110445
Epoch #174: loss=0.20470940487252343
Epoch #175: loss=0.17235758693681824
Epoch #176: loss=0.15206873623861206
Epoch #177: loss=0.1937180906534195
Epoch #178: loss=0.1534981338514222
Epoch #179: loss=0.11015002284612921
Epoch #180: loss=0.16912004724144936
Epoch #181: loss=0.1564810520244969
Epoch #182: loss=0.12949411571025848
Epoch #183: loss=0.12107469244963592
Epoch #184: loss=0.10571172564393944
Epoch #185: loss=0.1949502925078074
Epoch #186: loss=0.22435571460260284
Epoch #187: loss=0.2216790479918321
Epoch #188: loss=0.18039694759580824
Epoch #189: loss=0.13159950512150922
Epoch #190: loss=0.3355216723349359
Epoch #191: loss=0.278316708902518
Epoch #192: loss=0.1883462361163563
Epoch #193: loss=0.14459072632922065
Epoch #194: loss=0.1612426849702994
Epoch #195: loss=0.1072168027361234
Epoch #196: loss=0.13713423452443546
Epoch #197: loss=0.12722631647355026
Epoch #198: loss=0.15012777286271253
Epoch #199: loss=0.17581175288392437
Epoch #200: loss=0.11172103778355652
Epoch #201: loss=0.12495254435473019
Epoch #202: loss=0.1386103466567066
Epoch #203: loss=0.11752253998484877
Epoch #204: loss=0.1039713207218382
Epoch #205: loss=0.15438412937025228
Epoch #206: loss=0.16062135414944756
Epoch #207: loss=0.15170182122124565
Epoch #208: loss=0.13348896015021536
Epoch #209: loss=0.1031670905649662
Epoch #210: loss=0.09313524959401952
Epoch #211: loss=0.22810792229655716
Epoch #212: loss=0.11340170602003734
Epoch #213: loss=0.17114398028287622
Epoch #214: loss=0.1559446027709378
Epoch #215: loss=0.09007601398560736
Epoch #216: loss=0.11663661421173149
Epoch #217: loss=0.10383527312013838
Epoch #218: loss=0.09343649819493294
Epoch #219: loss=0.10380568603674571
Epoch #220: loss=0.08970627074854241
Epoch #221: loss=0.09033096881790294
Epoch #222: loss=0.09322670785089333
Epoch #223: loss=0.08986010981930627
Epoch #224: loss=0.19079679364545477
Epoch #225: loss=0.11707223744855987
Epoch #226: loss=0.12870635237130854
Epoch #227: loss=0.14192595415645176
Epoch #228: loss=0.10296659958031443
Epoch #229: loss=0.11242377323408921
Epoch #230: loss=0.11532671904812257
Epoch #231: loss=0.10507204052474764
Epoch #232: loss=0.12220026014579667
Epoch #233: loss=0.15370311143083704
Epoch #234: loss=0.10419351990438169
Epoch #235: loss=0.13211673208408886
Epoch #236: loss=0.16468215795854727
Epoch #237: loss=0.10360521367854542
Epoch #238: loss=0.1293269528283013
Epoch #239: loss=0.11680772859189245
Epoch #240: loss=0.20694276856051552
Epoch #241: loss=0.13476147544052866
Epoch #242: loss=0.11408729809853765
Epoch #243: loss=0.11630439199507236
Epoch #244: loss=0.12085346029036576
Epoch #245: loss=0.1749028301694327
Epoch #246: loss=0.14693920148743522
Epoch #247: loss=0.12313710059970617
Epoch #248: loss=0.07934357122414643
Epoch #249: loss=0.0907944922025005

Training time: 0:16:34.172511

Finished.
n2one setting etth1_ettm1 -> electricity
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth1_ettm1_epochs_250_seed_2021/model.pkl', muti_dataset='etth1_ettm1', random_seed=2021, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='electricity')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.36963e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.73213e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.36963e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.35084132226664766, 'MAE': 0.4176420083812755}
Finished.
------------------------- record done -------------------------
n2one setting etth1_ettm1 -> weather
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth1_ettm1_epochs_250_seed_2021/model.pkl', muti_dataset='etth1_ettm1', random_seed=2021, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='weather')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.57361e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.84501e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.57361e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.710677843064398, 'MAE': 0.6762635081893793}
Finished.
------------------------- record done -------------------------
n2one setting etth1_ettm1 -> exchange
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth1_ettm1_epochs_250_seed_2021/model.pkl', muti_dataset='etth1_ettm1', random_seed=2021, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='exchange')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=3.10704e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.23399392726680868, 'MAE': 0.33150279875933686}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='etth1_ettm2', random_seed=2021, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/etth1_ettm2_epochs_250_seed_2021
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=5.4422287411159935
Epoch #1: loss=2.8983510202831693
Epoch #2: loss=2.4215150475502014
Epoch #3: loss=2.230413834253947
Epoch #4: loss=2.0755391584502325
Epoch #5: loss=2.031514604886373
Epoch #6: loss=1.8714924454689026
Epoch #7: loss=1.791293329662747
Epoch #8: loss=1.7149523893992107
Epoch #9: loss=1.5626833372645907
Epoch #10: loss=1.4948170516226027
Epoch #11: loss=1.438791036605835
Epoch #12: loss=1.367972281244066
Epoch #13: loss=1.2636787758933172
Epoch #14: loss=1.2404078145821889
Epoch #15: loss=1.1123521758450403
Epoch #16: loss=1.134634365638097
Epoch #17: loss=1.1344400114483304
Epoch #18: loss=1.0697379046016269
Epoch #19: loss=1.0763382746113672
Epoch #20: loss=1.0724520683288574
Epoch #21: loss=1.112298948897256
Epoch #22: loss=0.9236355225245158
Epoch #23: loss=0.9672659469975365
Epoch #24: loss=0.9059360093540616
Epoch #25: loss=0.8427155680126615
Epoch #26: loss=0.8763361672560374
Epoch #27: loss=0.7825991478231218
Epoch #28: loss=0.8125755455758836
Epoch #29: loss=0.7829851243231032
Epoch #30: loss=0.8150968253612518
Epoch #31: loss=0.8902224202950796
Epoch #32: loss=0.7843772570292155
Epoch #33: loss=0.7076298130883111
Epoch #34: loss=0.7843875818782382
Epoch #35: loss=0.6759992274973128
Epoch #36: loss=0.6930411325560676
Epoch #37: loss=0.7618046601613363
Epoch #38: loss=0.7556673255231645
Epoch #39: loss=0.715439572930336
Epoch #40: loss=0.6413849790891012
Epoch #41: loss=0.760484367609024
Epoch #42: loss=0.8158324476745393
Epoch #43: loss=0.8806184132893881
Epoch #44: loss=0.6584263079696231
Epoch #45: loss=0.6868930591477288
Epoch #46: loss=0.591032452053494
Epoch #47: loss=0.5513268576727973
Epoch #48: loss=0.6182212266657088
Epoch #49: loss=0.5337026682164934
Epoch #50: loss=0.6356786108679242
Epoch #51: loss=0.5420560489098231
Epoch #52: loss=0.6233788381020228
Epoch #53: loss=0.5582587288485633
Epoch #54: loss=0.565627795126703
Epoch #55: loss=0.5148112177848816
Epoch #56: loss=0.559196843041314
Epoch #57: loss=0.6175450632969538
Epoch #58: loss=0.4957173847489887
Epoch #59: loss=0.515782536731826
Epoch #60: loss=0.43792371948560077
Epoch #61: loss=0.44371375772688126
Epoch #62: loss=0.5134021970960829
Epoch #63: loss=0.5884661310248904
Epoch #64: loss=0.5611910786893632
Epoch #65: loss=0.46455849210421246
Epoch #66: loss=0.501052169336213
Epoch #67: loss=0.5073742336697049
Epoch #68: loss=0.47887885404957664
Epoch #69: loss=0.5216055346859826
Epoch #70: loss=0.4693016856908798
Epoch #71: loss=0.43394072353839874
Epoch #72: loss=0.4440217051241133
Epoch #73: loss=0.5783274852567248
Epoch #74: loss=0.4843120359712177
Epoch #75: loss=0.5803825308879217
Epoch #76: loss=0.45482341117329067
Epoch #77: loss=0.4025783985853195
Epoch #78: loss=0.38260243667496574
Epoch #79: loss=0.46764205230606926
Epoch #80: loss=0.4581381479899089
Epoch #81: loss=0.43235908614264595
Epoch #82: loss=0.4708261489868164
Epoch #83: loss=0.39165978878736496
Epoch #84: loss=0.33817198127508163
Epoch #85: loss=0.40631844931178623
Epoch #86: loss=0.3950614647732841
Epoch #87: loss=0.43773805018928313
Epoch #88: loss=0.43341628462076187
Epoch #89: loss=0.40458490285608506
Epoch #90: loss=0.3813515139950646
Epoch #91: loss=0.35728005154265297
Epoch #92: loss=0.42363491819964516
Epoch #93: loss=0.4658714963330163
Epoch #94: loss=0.3947548617919286
Epoch #95: loss=0.39554281698332894
Epoch #96: loss=0.40800003624624676
Epoch #97: loss=0.4144677602582508
Epoch #98: loss=0.3925387180513806
Epoch #99: loss=0.39724068178070915
Epoch #100: loss=0.3249843559331364
Epoch #101: loss=0.43203607863850063
Epoch #102: loss=0.3926231803165542
Epoch #103: loss=0.38427699026134277
Epoch #104: loss=0.3480931404564116
Epoch #105: loss=0.3160722860031658
Epoch #106: loss=0.3785439787639512
Epoch #107: loss=0.3694710921910074
Epoch #108: loss=0.6026208450396856
Epoch #109: loss=0.36671396758821273
Epoch #110: loss=0.324683106607861
Epoch #111: loss=0.34147579471270245
Epoch #112: loss=0.2897772184676594
Epoch #113: loss=0.28963734292321736
Epoch #114: loss=0.3682621113128132
Epoch #115: loss=0.3384225633409288
Epoch #116: loss=0.2923672455880377
Epoch #117: loss=0.25490086608462864
Epoch #118: loss=0.26234614931874806
Epoch #119: loss=0.27304429478115505
Epoch #120: loss=0.33217352794276345
Epoch #121: loss=0.3478848685820897
Epoch #122: loss=0.3694368484947417
Epoch #123: loss=0.356945497294267
Epoch #124: loss=0.27751309672991437
Epoch #125: loss=0.26803841524653965
Epoch #126: loss=0.2485463809635904
Epoch #127: loss=0.258964694208569
Epoch #128: loss=0.29527904176049763
Epoch #129: loss=0.27919107178846997
Epoch #130: loss=0.33546458681424457
Epoch #131: loss=0.285391290154722
Epoch #132: loss=0.24560294134749305
Epoch #133: loss=0.3818427059385512
Epoch #134: loss=0.3714915273918046
Epoch #135: loss=0.36947111702627605
Epoch #136: loss=0.25710684806108475
Epoch #137: loss=0.2948601022362709
Epoch #138: loss=0.22021363716986445
Epoch #139: loss=0.24991387377182642
Epoch #140: loss=0.2096646966205703
Epoch #141: loss=0.2602647129032347
Epoch #142: loss=0.28534526088171536
Epoch #143: loss=0.2771667130291462
Epoch #144: loss=0.22168355103996065
Epoch #145: loss=0.2614925238821242
Epoch #146: loss=0.1995990313589573
Epoch #147: loss=0.180645319322745
Epoch #148: loss=0.24867036193609238
Epoch #149: loss=0.21013411382834116
Epoch #150: loss=0.20822832112511
Epoch #151: loss=0.20346030675702625
Epoch #152: loss=0.22374822033776176
Epoch #153: loss=0.24711863903535736
Epoch #154: loss=0.18155400910311276
Epoch #155: loss=0.18528791475627157
Epoch #156: loss=0.14986138294140497
Epoch #157: loss=0.16632948319117227
Epoch #158: loss=0.21694667264819145
Epoch #159: loss=0.2162412732011742
Epoch #160: loss=0.213082667440176
Epoch #161: loss=0.19004711943368116
Epoch #162: loss=0.17012258991599083
Epoch #163: loss=0.1771734394133091
Epoch #164: loss=0.19117266477810013
Epoch #165: loss=0.17990011204448011
Epoch #166: loss=0.17861437093880442
Epoch #167: loss=0.1347079620593124
Epoch #168: loss=0.16392773100071484
Epoch #169: loss=0.1738064760963122
Epoch #170: loss=0.18873024442129666
Epoch #171: loss=0.1980984513130453
Epoch #172: loss=0.19855395042233998
Epoch #173: loss=0.21652096282276842
Epoch #174: loss=0.22101297395096886
Epoch #175: loss=0.18045005781782997
Epoch #176: loss=0.14161660625702804
Epoch #177: loss=0.16182060829467243
Epoch #178: loss=0.13833682922025523
Epoch #179: loss=0.1159953081773387
Epoch #180: loss=0.18305146342350376
Epoch #181: loss=0.16265562404361036
Epoch #182: loss=0.13534431842466196
Epoch #183: loss=0.1366055520872275
Epoch #184: loss=0.11738248955872324
Epoch #185: loss=0.2104479012389978
Epoch #186: loss=0.21200997941195965
Epoch #187: loss=0.2633606489333842
Epoch #188: loss=0.1905397940427065
Epoch #189: loss=0.14737333067589337
Epoch #190: loss=0.3579271199802558
Epoch #191: loss=0.2870191000401974
Epoch #192: loss=0.19337472278210852
Epoch #193: loss=0.17606524171100724
Epoch #194: loss=0.16313561921318373
Epoch #195: loss=0.1233164467331436
Epoch #196: loss=0.13930335847867859
Epoch #197: loss=0.16006552303830782
Epoch #198: loss=0.16140375555389458
Epoch #199: loss=0.20006576635771328
Epoch #200: loss=0.1309345532208681
Epoch #201: loss=0.14267732203006744
Epoch #202: loss=0.14600293214122453
Epoch #203: loss=0.12131083611812857
Epoch #204: loss=0.09849153190023369
Epoch #205: loss=0.16613418236374855
Epoch #206: loss=0.1616920137570964
Epoch #207: loss=0.16570270020100805
Epoch #208: loss=0.14692530553374025
Epoch #209: loss=0.1292809457000759
Epoch #210: loss=0.10017792094084951
Epoch #211: loss=0.2786828734808498
Epoch #212: loss=0.13227332673139042
Epoch #213: loss=0.20400944207277563
Epoch #214: loss=0.17090069730248716
Epoch #215: loss=0.09421918023791578
Epoch #216: loss=0.12440494986044036
Epoch #217: loss=0.1347756971501642
Epoch #218: loss=0.12173879167271985
Epoch #219: loss=0.122367265013357
Epoch #220: loss=0.1125116538670328
Epoch #221: loss=0.10672772510184182
Epoch #222: loss=0.11381101784192854
Epoch #223: loss=0.10474636426402463
Epoch #224: loss=0.2191027839564615
Epoch #225: loss=0.10939485248592165
Epoch #226: loss=0.12666129734781054
Epoch #227: loss=0.14113399531278345
Epoch #228: loss=0.11761738898025619
Epoch #229: loss=0.10759586685647567
Epoch #230: loss=0.10224124830630091
Epoch #231: loss=0.09958139869074027
Epoch #232: loss=0.12403556228511864
Epoch #233: loss=0.13128794077783823
Epoch #234: loss=0.10871934704482555
Epoch #235: loss=0.13130069979363018
Epoch #236: loss=0.162978854858213
Epoch #237: loss=0.1048799515184429
Epoch #238: loss=0.11946162715968159
Epoch #239: loss=0.11393891523281734
Epoch #240: loss=0.2126015881076455
Epoch #241: loss=0.13524150972565016
Epoch #242: loss=0.13255922723975447
Epoch #243: loss=0.1239014236877362
Epoch #244: loss=0.1490599208821853
Epoch #245: loss=0.15698340121242735
Epoch #246: loss=0.11949668534927899
Epoch #247: loss=0.12882793260117373
Epoch #248: loss=0.10131209974901544
Epoch #249: loss=0.10678427593989505

Training time: 0:17:08.922413

Finished.
n2one setting etth1_ettm2 -> electricity
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth1_ettm2_epochs_250_seed_2021/model.pkl', muti_dataset='etth1_ettm2', random_seed=2021, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='electricity')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.47988e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.91103e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.47988e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.36466864237432844, 'MAE': 0.42686823532539175}
Finished.
------------------------- record done -------------------------
n2one setting etth1_ettm2 -> weather
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth1_ettm2_epochs_250_seed_2021/model.pkl', muti_dataset='etth1_ettm2', random_seed=2021, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='weather')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.59224e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=3.04891e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.59224e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.6115454256953258, 'MAE': 0.6111591832206442}
Finished.
------------------------- record done -------------------------
n2one setting etth1_ettm2 -> exchange
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth1_ettm2_epochs_250_seed_2021/model.pkl', muti_dataset='etth1_ettm2', random_seed=2021, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='exchange')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=3.66511e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.21531986078663928, 'MAE': 0.32159841805585715}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='etth1_electricity', random_seed=2021, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/etth1_electricity_epochs_250_seed_2021
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=1.702728907509548
Epoch #1: loss=0.7007242676506682
Epoch #2: loss=0.4473555558701841
Epoch #3: loss=0.3578468403529103
Epoch #4: loss=0.3211595025367853
Epoch #5: loss=0.27248293006928953
Epoch #6: loss=0.2465523474781615
Epoch #7: loss=0.20314487839889964
Epoch #8: loss=0.18218375099595727
Epoch #9: loss=0.15121031856936654
Epoch #10: loss=0.14659857343337157
Epoch #11: loss=0.13240260629710265
Epoch #12: loss=0.13175325678298022
Epoch #13: loss=0.12246668677231888
Epoch #14: loss=0.11059102687484972
Epoch #15: loss=0.09404907980365906
Epoch #16: loss=0.09348170074248095
Epoch #17: loss=0.10305117444339686
Epoch #18: loss=0.07083082389336352
Epoch #19: loss=0.0713047515203404
Epoch #20: loss=0.06977197425099226
Epoch #21: loss=0.06811178997215792
Epoch #22: loss=0.05195627497259255
Epoch #23: loss=0.06651612695465546
Epoch #24: loss=0.05296370868355308
Epoch #25: loss=0.061052751256062125
Epoch #26: loss=0.04405716151175112
Epoch #27: loss=0.0365410358517249
Epoch #28: loss=0.05645663130222025
Epoch #29: loss=0.030464426116739603
Epoch #30: loss=0.047258077395182646
Epoch #31: loss=0.032404974392592545
Epoch #32: loss=0.041114385725317025
Epoch #33: loss=0.048861327190690376
Epoch #34: loss=0.03411401267285549
Epoch #35: loss=0.02421524170033702
Epoch #36: loss=0.034840439314948304
Epoch #37: loss=0.037561937086937225
Epoch #38: loss=0.03937203922839949
Epoch #39: loss=0.033547635561440185
Epoch #40: loss=0.042580800849331045
Epoch #41: loss=0.028013702294584816
Epoch #42: loss=0.031231178371122152
Epoch #43: loss=0.025182756339057873
Epoch #44: loss=0.05344408919620214
Epoch #45: loss=0.030359486137834808
Epoch #46: loss=0.024613313102191767
Epoch #47: loss=0.02574944011398546
Epoch #48: loss=0.03880591909954802
Epoch #49: loss=0.024430093508804353
Epoch #50: loss=0.021113491629920447
Epoch #51: loss=0.025357292256624687
Epoch #52: loss=0.029806486690110278
Epoch #53: loss=0.019264395424681603
Epoch #54: loss=0.025382436491071997
Epoch #55: loss=0.0308791503577107
Epoch #56: loss=0.025869823956759873
Epoch #57: loss=0.01689750549281243
Epoch #58: loss=0.01592387537232021
Epoch #59: loss=0.03225906558261003
Epoch #60: loss=0.016384909602759474
Epoch #61: loss=0.020047963354821747
Epoch #62: loss=0.02596550055590697
Epoch #63: loss=0.015519344279288439
Epoch #64: loss=0.022590988986449678
Epoch #65: loss=0.01964931647263456
Epoch #66: loss=0.01731074789060541
Epoch #67: loss=0.022755578300836157
Epoch #68: loss=0.04377284377320704
Epoch #69: loss=0.017440561516062787
Epoch #70: loss=0.01836249730482317
Epoch #71: loss=0.027686525047501187
Epoch #72: loss=0.014999290123097768
Epoch #73: loss=0.01839795726083117
Epoch #74: loss=0.021830923860273663
Epoch #75: loss=0.013998598529724404
Epoch #76: loss=0.016816875121304937
Epoch #77: loss=0.018023742028872664
Epoch #78: loss=0.016514451326035736
Epoch #79: loss=0.018116631343529175
Epoch #80: loss=0.022640079881345695
Epoch #81: loss=0.01653776965040157
Epoch #82: loss=0.013490429169728375
Epoch #83: loss=0.027802900103113908
Epoch #84: loss=0.02135180438323537
Epoch #85: loss=0.01395273062974788
Epoch #86: loss=0.01841135066738721
Epoch #87: loss=0.01169747116453037
Epoch #88: loss=0.017503455069367548
Epoch #89: loss=0.014673720407455864
Epoch #90: loss=0.022706047353184235
Epoch #91: loss=0.011675609389806973
Epoch #92: loss=0.017696500677408127
Epoch #93: loss=0.03305623646919252
Epoch #94: loss=0.012696197042426253
Epoch #95: loss=0.009199394712972785
Epoch #96: loss=0.014990913871925598
Epoch #97: loss=0.016157691440439964
Epoch #98: loss=0.01455522472803373
Epoch #99: loss=0.02146179975179399
Epoch #100: loss=0.009086098322657606
Epoch #101: loss=0.025365202669224634
Epoch #102: loss=0.01576907626014606
Epoch #103: loss=0.02023874310650238
Epoch #104: loss=0.022401486556812348
Epoch #105: loss=0.01498808401562955
Epoch #106: loss=0.01654230678654929
Epoch #107: loss=0.02161199637020018
Epoch #108: loss=0.010176394112044719
Epoch #109: loss=0.013278716128452954
Epoch #110: loss=0.03482045266506115
Epoch #111: loss=0.012846703010237855
Epoch #112: loss=0.012460343403778629
Epoch #113: loss=0.014264116142507324
Epoch #114: loss=0.011041914846939064
Epoch #115: loss=0.010228061994428394
Epoch #116: loss=0.01875323073376998
Epoch #117: loss=0.018834004542086183
Epoch #118: loss=0.013234411330702685
Epoch #119: loss=0.011032873056914158
Epoch #120: loss=0.017680490009341868
Epoch #121: loss=0.011505482891729249
Epoch #122: loss=0.020637531156286814
Epoch #123: loss=0.010176488853176124
Epoch #124: loss=0.011169520212428235
Epoch #125: loss=0.017198611252796384
Epoch #126: loss=0.017705535995113812
Epoch #127: loss=0.01587534005570555
Epoch #128: loss=0.012880775168901536
Epoch #129: loss=0.012446877788489673
Epoch #130: loss=0.013270130887465755
Epoch #131: loss=0.015805432232588006
Epoch #132: loss=0.010725628859335102
Epoch #133: loss=0.020836647116528006
Epoch #134: loss=0.0219732233374266
Epoch #135: loss=0.012511788651619742
Epoch #136: loss=0.008822554011514024
Epoch #137: loss=0.008635641953488387
Epoch #138: loss=0.01337441856647762
Epoch #139: loss=0.00911384006953662
Epoch #140: loss=0.010879398182069275
Epoch #141: loss=0.016254973790016333
Epoch #142: loss=0.016354579796869786
Epoch #143: loss=0.010864749079834238
Epoch #144: loss=0.01566607111613493
Epoch #145: loss=0.015170453518072552
Epoch #146: loss=0.012977473788274305
Epoch #147: loss=0.010717565972832167
Epoch #148: loss=0.013524260197211733
Epoch #149: loss=0.022304517668444754
Epoch #150: loss=0.015543153947474799
Epoch #151: loss=0.013453144831678733
Epoch #152: loss=0.014350329524349057
Epoch #153: loss=0.02901283690756295
Epoch #154: loss=0.009727354884955453
Epoch #155: loss=0.012118342282852477
Epoch #156: loss=0.011558805228205307
Epoch #157: loss=0.00887290958976004
Epoch #158: loss=0.010385932766750068
Epoch #159: loss=0.012296127151785186
Epoch #160: loss=0.00852259565584884
Epoch #161: loss=0.00825762380377344
Epoch #162: loss=0.018736933756416677
Epoch #163: loss=0.019568273341198022
Epoch #164: loss=0.009705605393519806
Epoch #165: loss=0.010314173412419822
Epoch #166: loss=0.012876515944521
Epoch #167: loss=0.006411862921906016
Epoch #168: loss=0.0068960050560169135
Epoch #169: loss=0.00988399108308197
Epoch #170: loss=0.015568802364613497
Epoch #171: loss=0.015441710894243087
Epoch #172: loss=0.0097732635612618
Epoch #173: loss=0.009169749175049308
Epoch #174: loss=0.012915988645634171
Epoch #175: loss=0.02370605745286079
Epoch #176: loss=0.009185739185074742
Epoch #177: loss=0.010702587845493904
Epoch #178: loss=0.012341423797252213
Epoch #179: loss=0.009885448989956696
Epoch #180: loss=0.011388701014966057
Epoch #181: loss=0.009587390523825533
Epoch #182: loss=0.010305916718363466
Epoch #183: loss=0.01762320658670477
Epoch #184: loss=0.013853569782307803
Epoch #185: loss=0.009907037627272075
Epoch #186: loss=0.009618884250546505
Epoch #187: loss=0.03339338529267258
Epoch #188: loss=0.011082423900760903
Epoch #189: loss=0.008854868289412612
Epoch #190: loss=0.015236153129080776
Epoch #191: loss=0.008785401744961103
Epoch #192: loss=0.011387058002583217
Epoch #193: loss=0.008356634627178698
Epoch #194: loss=0.005963306192936171
Epoch #195: loss=0.009134030933814867
Epoch #196: loss=0.010900289023638612
Epoch #197: loss=0.032359160218109584
Epoch #198: loss=0.007214352640957947
Epoch #199: loss=0.007606443272379693
Epoch #200: loss=0.008802562590639487
Epoch #201: loss=0.00790118980840794
Epoch #202: loss=0.009091309781779716
Epoch #203: loss=0.00926899535225835
Epoch #204: loss=0.0086428998454322
Epoch #205: loss=0.010790929704210323
Epoch #206: loss=0.012819925405737438
Epoch #207: loss=0.008463195673113393
Epoch #208: loss=0.00772755566663918
Epoch #209: loss=0.013835993876078069
Epoch #210: loss=0.005614680140996181
Epoch #211: loss=0.009119987123995088
Epoch #212: loss=0.009212109110305348
Epoch #213: loss=0.009138525076595237
Epoch #214: loss=0.006962654001853945
Epoch #215: loss=0.0074282189676390465
Epoch #216: loss=0.01175344903361004
Epoch #217: loss=0.00666217162444543
Epoch #218: loss=0.012581422832271328
Epoch #219: loss=0.013131269856275161
Epoch #220: loss=0.010670553494181638
Epoch #221: loss=0.008670309579096416
Epoch #222: loss=0.01145033064712174
Epoch #223: loss=0.012658793753561019
Epoch #224: loss=0.010084586577804436
Epoch #225: loss=0.010124897490264052
Epoch #226: loss=0.009349314474803111
Epoch #227: loss=0.011470830403218647
Epoch #228: loss=0.011724316978205291
Epoch #229: loss=0.009412840710688104
Epoch #230: loss=0.011105711155368026
Epoch #231: loss=0.00961376334045071
Epoch #232: loss=0.009845113409693755
Epoch #233: loss=0.008698328535878895
Epoch #234: loss=0.013504617848567007
Epoch #235: loss=0.013744699635270348
Epoch #236: loss=0.005836200776551467
Epoch #237: loss=0.003689279197005373
Epoch #238: loss=0.00647100681524579
Epoch #239: loss=0.006244426380318345
Epoch #240: loss=0.008699146655733164
Epoch #241: loss=0.008185099001222136
Epoch #242: loss=0.009497458478751757
Epoch #243: loss=0.008769638906658923
Epoch #244: loss=0.013756527458232396
Epoch #245: loss=0.006167851399723298
Epoch #246: loss=0.008284367613003536
Epoch #247: loss=0.00601855087272891
Epoch #248: loss=0.009639653310864006
Epoch #249: loss=0.01098007238148811

Training time: 4:39:36.109370

Finished.
n2one setting etth1_electricity -> weather
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth1_electricity_epochs_250_seed_2021/model.pkl', muti_dataset='etth1_electricity', random_seed=2021, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='weather')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.43303e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.78801e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.43303e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.5063754642387207, 'MAE': 0.5367601525196396}
Finished.
------------------------- record done -------------------------
n2one setting etth1_electricity -> exchange
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth1_electricity_epochs_250_seed_2021/model.pkl', muti_dataset='etth1_electricity', random_seed=2021, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='exchange')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=4.46806e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.22184102645006343, 'MAE': 0.3277268923459553}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='etth1_traffic', random_seed=2021, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/etth1_traffic_epochs_250_seed_2021
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=1.0970411527005932
Epoch #1: loss=0.38453558200522314
Epoch #2: loss=0.26315755914935307
Epoch #3: loss=0.20932488821626943
Epoch #4: loss=0.14576825631570212
Epoch #5: loss=0.12500426877971624
Epoch #6: loss=0.11126633169144495
Epoch #7: loss=0.09375545164941027
Epoch #8: loss=0.08150710323578983
Epoch #9: loss=0.07338817297696415
Epoch #10: loss=0.06167078052912126
Epoch #11: loss=0.05623382988511247
Epoch #12: loss=0.061728074068527226
Epoch #13: loss=0.05354289446718417
Epoch #14: loss=0.044774205617872424
Epoch #15: loss=0.04675543749484183
Epoch #16: loss=0.041597942448455855
Epoch #17: loss=0.040926351287789105
Epoch #18: loss=0.038619093599346635
Epoch #19: loss=0.0391824192523429
Epoch #20: loss=0.039091835882229635
Epoch #21: loss=0.0322156302973583
Epoch #22: loss=0.03348205520228083
Epoch #23: loss=0.03432014581192203
Epoch #24: loss=0.02766758404721445
Epoch #25: loss=0.02797071011675365
Epoch #26: loss=0.024541509513886832
Epoch #27: loss=0.030508303730260963
Epoch #28: loss=0.023222217475218427
Epoch #29: loss=0.0282347440281272
Epoch #30: loss=0.02494719095171244
Epoch #31: loss=0.026354385801924285
Epoch #32: loss=0.023459068538221318
Epoch #33: loss=0.022904864434550448
Epoch #34: loss=0.054809362082932384
Epoch #35: loss=0.029986165132763677
Epoch #36: loss=0.019645099805775702
Epoch #37: loss=0.023651984681750962
Epoch #38: loss=0.019064371443476304
Epoch #39: loss=0.025687153740642406
Epoch #40: loss=0.01760785503818528
Epoch #41: loss=0.02391663909543057
Epoch #42: loss=0.023298661563541715
Epoch #43: loss=0.022255772235207163
Epoch #44: loss=0.019180509885660533
Epoch #45: loss=0.019238648338201034
Epoch #46: loss=0.024516250212758273
Epoch #47: loss=0.016821338749309557
Epoch #48: loss=0.015164200121919186
Epoch #49: loss=0.024089748577375
Epoch #50: loss=0.020950893927349466
Epoch #51: loss=0.018891450384084105
Epoch #52: loss=0.0258207230249841
Epoch #53: loss=0.015388165273857963
Epoch #54: loss=0.03199012614186463
Epoch #55: loss=0.014428893591559163
Epoch #56: loss=0.014446717992055758
Epoch #57: loss=0.021506269272444693
Epoch #58: loss=0.022901975280823352
Epoch #59: loss=0.016577921712383096
Epoch #60: loss=0.014508724138798446
Epoch #61: loss=0.018287283945930488
Epoch #62: loss=0.02035452874275021
Epoch #63: loss=0.018785275596392773
Epoch #64: loss=0.013102144519113897
Epoch #65: loss=0.018079736694812526
Epoch #66: loss=0.018588802057471914
Epoch #67: loss=0.012838558339594484
Epoch #68: loss=0.015131747795723988
Epoch #69: loss=0.01293315751702065
Epoch #70: loss=0.019545030177429413
Epoch #71: loss=0.013241275629317774
Epoch #72: loss=0.01714858401804397
Epoch #73: loss=0.014937622665452543
Epoch #74: loss=0.019223429586966625
Epoch #75: loss=0.01037472698270714
Epoch #76: loss=0.013961626026472366
Epoch #77: loss=0.01705950132673108
Epoch #78: loss=0.022201151835012572
Epoch #79: loss=0.011246538706858223
Epoch #80: loss=0.019250636617840624
Epoch #81: loss=0.02166180611602529
Epoch #82: loss=0.014340879905741701
Epoch #83: loss=0.01482222274026723
Epoch #84: loss=0.016378389227351922
Epoch #85: loss=0.014644839369795866
Epoch #86: loss=0.013693893195282116
Epoch #87: loss=0.015666919449206024
Epoch #88: loss=0.016450387593611948
Epoch #89: loss=0.010005688433522269
Epoch #90: loss=0.0201034503080641
Epoch #91: loss=0.016006557054665492
Epoch #92: loss=0.017171916847615057
Epoch #93: loss=0.01795074835562863
Epoch #94: loss=0.017928696907528914
Epoch #95: loss=0.011556670517438028
Epoch #96: loss=0.011038418367358516
Epoch #97: loss=0.018223701383680336
Epoch #98: loss=0.011531698166714795
Epoch #99: loss=0.013629518117710371
Epoch #100: loss=0.016373470188086367
Epoch #101: loss=0.011524442198341476
Epoch #102: loss=0.017501312888549454
Epoch #103: loss=0.012760318979747932
Epoch #104: loss=0.024322038289131136
Epoch #105: loss=0.014533392742388187
Epoch #106: loss=0.020674368976406385
Epoch #107: loss=0.013561326522828628
Epoch #108: loss=0.008977004612760641
Epoch #109: loss=0.012920314897720137
Epoch #110: loss=0.017581914315878716
Epoch #111: loss=0.015213645164617975
Epoch #112: loss=0.009575376488250692
Epoch #113: loss=0.012705178253427126
Epoch #114: loss=0.012550501172029991
Epoch #115: loss=0.014684192851690578
Epoch #116: loss=0.013507551080236475
Epoch #117: loss=0.014802465463160046
Epoch #118: loss=0.016490592959996577
Epoch #119: loss=0.015728508261287202
Epoch #120: loss=0.010685004225630105
Epoch #121: loss=0.011526778383643748
Epoch #122: loss=0.026287920119817464
Epoch #123: loss=0.01304871447264325
Epoch #124: loss=0.01297444389053
Epoch #125: loss=0.00950773950276084
Epoch #126: loss=0.014912202610203974
Epoch #127: loss=0.012638774172407474
Epoch #128: loss=0.015160108743613517
Epoch #129: loss=0.014494861648971284
Epoch #130: loss=0.011366124282646238
Epoch #131: loss=0.011093153433254127
Epoch #132: loss=0.013363970182978059
Epoch #133: loss=0.013830215589521652
Epoch #134: loss=0.012565469733274342
Epoch #135: loss=0.014822335333810686
Epoch #136: loss=0.01625015124704333
Epoch #137: loss=0.010433179313896741
Epoch #138: loss=0.010498091710405317
Epoch #139: loss=0.012142379880287728
Epoch #140: loss=0.013670258619072359
Epoch #141: loss=0.008469234834811238
Epoch #142: loss=0.00862522972939588
Epoch #143: loss=0.013017954395642403
Epoch #144: loss=0.013446803120183528
Epoch #145: loss=0.012649816185743972
Epoch #146: loss=0.010703796977581836
Epoch #147: loss=0.01049534518793678
Epoch #148: loss=0.009053427695687346
Epoch #149: loss=0.009851237151296745
Epoch #150: loss=0.011954012828928893
Epoch #151: loss=0.012609173813824068
Epoch #152: loss=0.009371581984852264
Epoch #153: loss=0.012127607263780335
Epoch #154: loss=0.012113874324681011
Epoch #155: loss=0.014183290122231833
Epoch #156: loss=0.012563501040548888
Epoch #157: loss=0.03017325123093628
Epoch #158: loss=0.011834794358375521
Epoch #159: loss=0.009258663331968143
Epoch #160: loss=0.011706818412053786
Epoch #161: loss=0.014343573818716324
Epoch #162: loss=0.01051152052596384
Epoch #163: loss=0.009922956992180786
Epoch #164: loss=0.008716823280352502
Epoch #165: loss=0.014471314322495174
Epoch #166: loss=0.012074819442158848
Epoch #167: loss=0.010058253759709031
Epoch #168: loss=0.008506612564894983
Epoch #169: loss=0.010919411694321523
Epoch #170: loss=0.016826881339448804
Epoch #171: loss=0.007242585528448762
Epoch #172: loss=0.010112811815086324
Epoch #173: loss=0.011043104175258518
Epoch #174: loss=0.013328179968113746
Epoch #175: loss=0.007902823060429982
Epoch #176: loss=0.007575613096818838
Epoch #177: loss=0.019168965226598996
Epoch #178: loss=0.01297521709758677
Epoch #179: loss=0.011995341966967034
Epoch #180: loss=0.015521905860850631
Epoch #181: loss=0.00785675580620185
Epoch #182: loss=0.010844326377635976
Epoch #183: loss=0.0131278853167933
Epoch #184: loss=0.009307576848464076
Epoch #185: loss=0.009911901950335823
Epoch #186: loss=0.007240893933611254
Epoch #187: loss=0.011288583421680065
Epoch #188: loss=0.012478881362407395
Epoch #189: loss=0.010099165211579043
Epoch #190: loss=0.013175885998914543
Epoch #191: loss=0.010095229551794385
Epoch #192: loss=0.010581784464250897
Epoch #193: loss=0.006971855886325398
Epoch #194: loss=0.0098504928040035
Epoch #195: loss=0.0151700886141646
Epoch #196: loss=0.009591197156810116
Epoch #197: loss=0.01065006281090276
Epoch #198: loss=0.013357061313685475
Epoch #199: loss=0.010815493488836748
Epoch #200: loss=0.013913118400942311
Epoch #201: loss=0.0104539836937411
Epoch #202: loss=0.008761407477581301
Epoch #203: loss=0.009543575179147973
Epoch #204: loss=0.008106387966908343
Epoch #205: loss=0.011938605036175118
Epoch #206: loss=0.010841224269791543
Epoch #207: loss=0.007502914383434487
Epoch #208: loss=0.009190179677100068
Epoch #209: loss=0.012128016609155709
Epoch #210: loss=0.008930947123368949
Epoch #211: loss=0.006827094671310417
Epoch #212: loss=0.011546598793590448
Epoch #213: loss=0.007161155391831434
Epoch #214: loss=0.012120444753567095
Epoch #215: loss=0.007982832763113094
Epoch #216: loss=0.012369914528364138
Epoch #217: loss=0.007696389500105323
Epoch #218: loss=0.012052644392611393
Epoch #219: loss=0.010791770445824075
Epoch #220: loss=0.014247749010399702
Epoch #221: loss=0.017898528134458087
Epoch #222: loss=0.008517940724128563
Epoch #223: loss=0.01076402093090387
Epoch #224: loss=0.011565669073473546
Epoch #225: loss=0.01454381640792719
Epoch #226: loss=0.008860308127602353
Epoch #227: loss=0.007086758123209991
Epoch #228: loss=0.010048863441867955
Epoch #229: loss=0.011831913253230684
Epoch #230: loss=0.008909820347601844
Epoch #231: loss=0.009004112386276786
Epoch #232: loss=0.010376024472981213
Epoch #233: loss=0.008151347999449205
Epoch #234: loss=0.009200642452723565
Epoch #235: loss=0.014145943358292968
Epoch #236: loss=0.010414402989432981
Epoch #237: loss=0.007906096586185267
Epoch #238: loss=0.007480756401575545
Epoch #239: loss=0.011072688152605262
Epoch #240: loss=0.01320703091673629
Epoch #241: loss=0.00805929990866976
Epoch #242: loss=0.008667321831198388
Epoch #243: loss=0.010357514053420513
Epoch #244: loss=0.007103011282635783
Epoch #245: loss=0.008717084731783405
Epoch #246: loss=0.007362183396502476
Epoch #247: loss=0.008682289493489384
Epoch #248: loss=0.01129749939978306
Epoch #249: loss=0.015624448823754893

Training time: 10:22:42.471915

Finished.
n2one setting etth1_traffic -> electricity
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth1_traffic_epochs_250_seed_2021/model.pkl', muti_dataset='etth1_traffic', random_seed=2021, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='electricity')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=9.89834e-09): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.14845e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=5.07313e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=9.89834e-09): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.4118688166526333, 'MAE': 0.45783344525287534}
Finished.
------------------------- record done -------------------------
n2one setting etth1_traffic -> weather
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth1_traffic_epochs_250_seed_2021/model.pkl', muti_dataset='etth1_traffic', random_seed=2021, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='weather')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.21342e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.66962e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.21342e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.6453947071182523, 'MAE': 0.6068197376642933}
Finished.
------------------------- record done -------------------------
n2one setting etth1_traffic -> exchange
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth1_traffic_epochs_250_seed_2021/model.pkl', muti_dataset='etth1_traffic', random_seed=2021, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='exchange')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=5.33975e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.38322992334894845, 'MAE': 0.39653887693201495}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='etth1_weather', random_seed=2021, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/etth1_weather_epochs_250_seed_2021
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=7.331005298730099
Epoch #1: loss=3.0610211834763037
Epoch #2: loss=2.6463055863524927
Epoch #3: loss=2.374542174917279
Epoch #4: loss=2.4421466083237617
Epoch #5: loss=2.2228461973594897
Epoch #6: loss=1.9999136635751436
Epoch #7: loss=1.8820351109360203
Epoch #8: loss=1.830088821324435
Epoch #9: loss=1.7345733498082017
Epoch #10: loss=1.6319703152685454
Epoch #11: loss=1.5468618869781494
Epoch #12: loss=1.4728427836389253
Epoch #13: loss=1.435317039489746
Epoch #14: loss=1.380235108462247
Epoch #15: loss=1.2166794321753762
Epoch #16: loss=1.3521098241661533
Epoch #17: loss=1.1464387557723306
Epoch #18: loss=1.1505006208564297
Epoch #19: loss=1.149441533016436
Epoch #20: loss=1.0502180962851553
Epoch #21: loss=1.0541273554166157
Epoch #22: loss=1.1102261073661572
Epoch #23: loss=0.9498215967958624
Epoch #24: loss=1.00785087816643
Epoch #25: loss=0.8712666666868961
Epoch #26: loss=0.8890439849911314
Epoch #27: loss=0.8177506995923591
Epoch #28: loss=0.7997881137963497
Epoch #29: loss=0.8594799421050332
Epoch #30: loss=0.834234994469267
Epoch #31: loss=0.7698943651083744
Epoch #32: loss=0.7503318967241229
Epoch #33: loss=0.7405176298184828
Epoch #34: loss=0.7347628528421576
Epoch #35: loss=0.6672359515320171
Epoch #36: loss=0.7016318115321073
Epoch #37: loss=0.6295914126164985
Epoch #38: loss=0.68007160226504
Epoch #39: loss=0.7011746592593916
Epoch #40: loss=0.8595457347956571
Epoch #41: loss=0.6945269758051092
Epoch #42: loss=0.8840762178103129
Epoch #43: loss=0.869700626893477
Epoch #44: loss=0.7412520881855127
Epoch #45: loss=0.5882515780853502
Epoch #46: loss=0.6775522340427745
Epoch #47: loss=0.6271650709889152
Epoch #48: loss=0.5383188715486815
Epoch #49: loss=0.6058806063550891
Epoch #50: loss=0.5715803476897153
Epoch #51: loss=0.5017344373645205
Epoch #52: loss=0.5144906278812524
Epoch #53: loss=0.45761627861947723
Epoch #54: loss=0.46691759698318713
Epoch #55: loss=0.5035421730894031
Epoch #56: loss=0.45008461132194055
Epoch #57: loss=0.5246225586443236
Epoch #58: loss=0.5610353007461085
Epoch #59: loss=0.5354419612523281
Epoch #60: loss=0.571857059543783
Epoch #61: loss=0.4817968462452744
Epoch #62: loss=0.4740430586265795
Epoch #63: loss=0.4925898223212271
Epoch #64: loss=0.4169751965638363
Epoch #65: loss=0.40479693629524927
Epoch #66: loss=0.5033114859552095
Epoch #67: loss=0.418223809112202
Epoch #68: loss=0.35662603513761
Epoch #69: loss=0.3652760300672416
Epoch #70: loss=0.3590549939509594
Epoch #71: loss=0.3544255346059799
Epoch #72: loss=0.4089602380990982
Epoch #73: loss=0.3404005838162971
Epoch #74: loss=0.3514059068578662
Epoch #75: loss=0.33682377139727276
Epoch #76: loss=0.40352132239124994
Epoch #77: loss=0.33676429544434405
Epoch #78: loss=0.40222231140642456
Epoch #79: loss=0.3298596518509316
Epoch #80: loss=0.29204626245932147
Epoch #81: loss=0.3073814728043296
Epoch #82: loss=0.275822483680465
Epoch #83: loss=0.39525445121707337
Epoch #84: loss=0.2588203856439302
Epoch #85: loss=0.35343703763051465
Epoch #86: loss=0.31949108839035034
Epoch #87: loss=0.3649325853947437
Epoch #88: loss=0.3231599461851698
Epoch #89: loss=0.3485882688652385
Epoch #90: loss=0.2638763313492139
Epoch #91: loss=0.26381640971610043
Epoch #92: loss=0.30945080744497705
Epoch #93: loss=0.23779324013175387
Epoch #94: loss=0.24423632635311646
Epoch #95: loss=0.2744239622896368
Epoch #96: loss=0.20792295490250443
Epoch #97: loss=0.23142494870857758
Epoch #98: loss=0.28280654617331247
Epoch #99: loss=0.23629324928377615
Epoch #100: loss=0.24066375557220343
Epoch #101: loss=0.18558674441142517
Epoch #102: loss=0.23306428189530518
Epoch #103: loss=0.22034959269292426
Epoch #104: loss=0.15174303621505247
Epoch #105: loss=0.19425914775241504
Epoch #106: loss=0.22030422678499512
Epoch #107: loss=0.18944912686040907
Epoch #108: loss=0.25698183386614826
Epoch #109: loss=0.31002249749320926
Epoch #110: loss=0.18379685508482385
Epoch #111: loss=0.1863234546599966
Epoch #112: loss=0.19588654623790222
Epoch #113: loss=0.23450885272838853
Epoch #114: loss=0.20866092813737463
Epoch #115: loss=0.1696041909356912
Epoch #116: loss=0.19873914822484506
Epoch #117: loss=0.13492003534779404
Epoch #118: loss=0.22583607471350467
Epoch #119: loss=0.2080237933180549
Epoch #120: loss=0.14779251949353653
Epoch #121: loss=0.17460311057441164
Epoch #122: loss=0.159095616051645
Epoch #123: loss=0.14599215160265114
Epoch #124: loss=0.17205393675601843
Epoch #125: loss=0.20562127763123222
Epoch #126: loss=0.3731464594602585
Epoch #127: loss=0.2883710621884375
Epoch #128: loss=0.19918373464183373
Epoch #129: loss=0.21052592834739975
Epoch #130: loss=0.19101182100447742
Epoch #131: loss=0.17850984531370076
Epoch #132: loss=0.18704645482428145
Epoch #133: loss=0.21745108406652103
Epoch #134: loss=0.2533422527891217
Epoch #135: loss=0.21790353073315186
Epoch #136: loss=0.1419799244313529
Epoch #137: loss=0.11627528500376326
Epoch #138: loss=0.12293950231237845
Epoch #139: loss=0.12964742890361583
Epoch #140: loss=0.18491305822901655
Epoch #141: loss=0.2165467847477306
Epoch #142: loss=0.13699861773938843
Epoch #143: loss=0.1325374799921657
Epoch #144: loss=0.1421417457362016
Epoch #145: loss=0.1636894881499536
Epoch #146: loss=0.13387936319817195
Epoch #147: loss=0.14370609255451144
Epoch #148: loss=0.1615876347729654
Epoch #149: loss=0.10314857852504108
Epoch #150: loss=0.13043734798151435
Epoch #151: loss=0.12650978943389474
Epoch #152: loss=0.12883025351347346
Epoch #153: loss=0.13677111228532862
Epoch #154: loss=0.14412297376177527
Epoch #155: loss=0.1638762993794499
Epoch #156: loss=0.1328399684844595
Epoch #157: loss=0.10660855167291382
Epoch #158: loss=0.15737310768754192
Epoch #159: loss=0.17155070094899696
Epoch #160: loss=0.13768683351350552
Epoch #161: loss=0.1704186402035482
Epoch #162: loss=0.10038574542285818
Epoch #163: loss=0.10616602901030671
Epoch #164: loss=0.10787562235738292
Epoch #165: loss=0.07664812548142491
Epoch #166: loss=0.12586850356875043
Epoch #167: loss=0.12994409888756997
Epoch #168: loss=0.09109811360637347
Epoch #169: loss=0.1413909909174298
Epoch #170: loss=0.13709372011097995
Epoch #171: loss=0.13007736008501414
Epoch #172: loss=0.176711292655179
Epoch #173: loss=0.17877688061333064
Epoch #174: loss=0.16098128676866041
Epoch #175: loss=0.12259782591100896
Epoch #176: loss=0.13114160316234286
Epoch #177: loss=0.16112995175926975
Epoch #178: loss=0.11660358179925066
Epoch #179: loss=0.14389595135369085
Epoch #180: loss=0.0952702716670253
Epoch #181: loss=0.09955103582505023
Epoch #182: loss=0.0789355913346464
Epoch #183: loss=0.08750670922525002
Epoch #184: loss=0.0958098600420988
Epoch #185: loss=0.11080007768715873
Epoch #186: loss=0.0965299585439039
Epoch #187: loss=0.07019305212253873
Epoch #188: loss=0.08275372156816901
Epoch #189: loss=0.10512901718417804
Epoch #190: loss=0.07786341773515398
Epoch #191: loss=0.05800080525152611
Epoch #192: loss=0.09453978127037937
Epoch #193: loss=0.10633574036711996
Epoch #194: loss=0.1203224944571654
Epoch #195: loss=0.13294767718197723
Epoch #196: loss=0.08388371680947868
Epoch #197: loss=0.08064128859250834
Epoch #198: loss=0.13151431159878318
Epoch #199: loss=0.13435338691554286
Epoch #200: loss=0.10688878516807701
Epoch #201: loss=0.08734310892495242
Epoch #202: loss=0.09495155338310834
Epoch #203: loss=0.15367938843414639
Epoch #204: loss=0.11475197592693748
Epoch #205: loss=0.14576692032543095
Epoch #206: loss=0.0900342720137401
Epoch #207: loss=0.13183525289324197
Epoch #208: loss=0.10027604329992425
Epoch #209: loss=0.080199361976349
Epoch #210: loss=0.12493892680063393
Epoch #211: loss=0.09278188978858067
Epoch #212: loss=0.0722592245561607
Epoch #213: loss=0.22552770342339168
Epoch #214: loss=0.21799730690139713
Epoch #215: loss=0.1555751890621402
Epoch #216: loss=0.1288400458341295
Epoch #217: loss=0.1590149380605329
Epoch #218: loss=0.17344945144246926
Epoch #219: loss=0.1583244443278421
Epoch #220: loss=0.1709788774676395
Epoch #221: loss=0.17754686239993933
Epoch #222: loss=0.12387668996146231
Epoch #223: loss=0.14032887086046464
Epoch #224: loss=0.08723256516863
Epoch #225: loss=0.18567389989215316
Epoch #226: loss=0.10806721877871138
Epoch #227: loss=0.08642155400505572
Epoch #228: loss=0.10397609503883304
Epoch #229: loss=0.08927150149688576
Epoch #230: loss=0.08789101587326238
Epoch #231: loss=0.11751284345871571
Epoch #232: loss=0.087076627338926
Epoch #233: loss=0.12899782265903373
Epoch #234: loss=0.07358612571701859
Epoch #235: loss=0.0662543328874039
Epoch #236: loss=0.09035633292726496
Epoch #237: loss=0.0850460533278458
Epoch #238: loss=0.1040766363342603
Epoch #239: loss=0.06876155214779305
Epoch #240: loss=0.07046999610170271
Epoch #241: loss=0.08315326146442782
Epoch #242: loss=0.07643195758150383
Epoch #243: loss=0.07473193173945854
Epoch #244: loss=0.060634590770033275
Epoch #245: loss=0.06448887965895912
Epoch #246: loss=0.05568174611438404
Epoch #247: loss=0.058910935326959145
Epoch #248: loss=0.1128924431448633
Epoch #249: loss=0.08639844760976055

Training time: 0:32:39.921457

Finished.
n2one setting etth1_weather -> electricity
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth1_weather_epochs_250_seed_2021/model.pkl', muti_dataset='etth1_weather', random_seed=2021, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='electricity')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.52132e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=3.00207e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.52132e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.3669047068944407, 'MAE': 0.42890217829820015}
Finished.
------------------------- record done -------------------------
n2one setting etth1_weather -> exchange
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth1_weather_epochs_250_seed_2021/model.pkl', muti_dataset='etth1_weather', random_seed=2021, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='exchange')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=3.05476e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.18701755313728402, 'MAE': 0.30092026315117015}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='etth1_exchange', random_seed=2021, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/etth1_exchange_epochs_250_seed_2021
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=4.68854787349701
Epoch #1: loss=2.251226270198822
Epoch #2: loss=2.05611138343811
Epoch #3: loss=1.9030083179473878
Epoch #4: loss=1.8624148766199748
Epoch #5: loss=1.8882937908172608
Epoch #6: loss=1.809850025177002
Epoch #7: loss=1.6926610390345256
Epoch #8: loss=1.635644006729126
Epoch #9: loss=1.5289223114649455
Epoch #10: loss=1.5778375705083212
Epoch #11: loss=1.4249775012334187
Epoch #12: loss=1.4596051375071208
Epoch #13: loss=1.3832558711369833
Epoch #14: loss=1.396814235051473
Epoch #15: loss=1.2648577372233072
Epoch #16: loss=1.273733909924825
Epoch #17: loss=1.1896360079447428
Epoch #18: loss=1.2197621504465739
Epoch #19: loss=1.2261049151420593
Epoch #20: loss=1.189454702536265
Epoch #21: loss=1.1124034921328227
Epoch #22: loss=1.0043247540791829
Epoch #23: loss=1.0275505860646565
Epoch #24: loss=1.0531015157699586
Epoch #25: loss=1.0938000400861105
Epoch #26: loss=1.0485273559888204
Epoch #27: loss=1.0886428912480672
Epoch #28: loss=1.0668513496716818
Epoch #29: loss=1.0114309032758078
Epoch #30: loss=1.0624183416366577
Epoch #31: loss=0.9461383978525798
Epoch #32: loss=0.9898582577705384
Epoch #33: loss=0.9040261507034302
Epoch #34: loss=0.8360379775365193
Epoch #35: loss=0.8182313879330952
Epoch #36: loss=0.8275846560796102
Epoch #37: loss=0.8402160962422689
Epoch #38: loss=0.8795445124308269
Epoch #39: loss=0.8720346927642822
Epoch #40: loss=0.7267287532488506
Epoch #41: loss=0.8477031807104747
Epoch #42: loss=0.8007697383562724
Epoch #43: loss=0.9036698460578918
Epoch #44: loss=0.6547312200069427
Epoch #45: loss=0.6888723492622375
Epoch #46: loss=0.7323523799578349
Epoch #47: loss=0.609612242380778
Epoch #48: loss=0.700174460808436
Epoch #49: loss=0.7941785832246144
Epoch #50: loss=0.6254168391227722
Epoch #51: loss=0.6894839247067769
Epoch #52: loss=0.6982690294583639
Epoch #53: loss=0.5973528087139129
Epoch #54: loss=0.5864102164904277
Epoch #55: loss=0.7209079881509145
Epoch #56: loss=0.7383133927981059
Epoch #57: loss=0.672849730650584
Epoch #58: loss=0.6658600449562073
Epoch #59: loss=0.5625618020693461
Epoch #60: loss=0.6828816453615825
Epoch #61: loss=0.6860630134741466
Epoch #62: loss=0.6232878029346466
Epoch #63: loss=0.5310518761475881
Epoch #64: loss=0.5419176816940308
Epoch #65: loss=0.568291570742925
Epoch #66: loss=0.47822112441062925
Epoch #67: loss=0.5487065593401591
Epoch #68: loss=0.48507580161094666
Epoch #69: loss=0.5078982452551524
Epoch #70: loss=0.6370966871579488
Epoch #71: loss=0.4467590262492498
Epoch #72: loss=0.4962340533733368
Epoch #73: loss=0.5589098701874415
Epoch #74: loss=0.5357788840929667
Epoch #75: loss=0.5522229870160421
Epoch #76: loss=0.4730182111263275
Epoch #77: loss=0.4997818241516749
Epoch #78: loss=0.4554007242123286
Epoch #79: loss=0.4559334377447764
Epoch #80: loss=0.5424478868643443
Epoch #81: loss=0.41630233228206637
Epoch #82: loss=0.42030514578024547
Epoch #83: loss=0.4842771391073863
Epoch #84: loss=0.3554465800523758
Epoch #85: loss=0.40785138805707294
Epoch #86: loss=0.4740632563829422
Epoch #87: loss=0.3732555548350016
Epoch #88: loss=0.3260142236948013
Epoch #89: loss=0.48800765375296273
Epoch #90: loss=0.533597989877065
Epoch #91: loss=0.45014729102452594
Epoch #92: loss=0.4434809943040212
Epoch #93: loss=0.5143688241640727
Epoch #94: loss=0.45481038888295494
Epoch #95: loss=0.4117830008268356
Epoch #96: loss=0.3670978893836339
Epoch #97: loss=0.4187060624361038
Epoch #98: loss=0.48065616289774576
Epoch #99: loss=0.3586643065015475
Epoch #100: loss=0.3808419456084569
Epoch #101: loss=0.3575700561205546
Epoch #102: loss=0.39453830718994143
Epoch #103: loss=0.4591929038365682
Epoch #104: loss=0.3478048155705134
Epoch #105: loss=0.43850064774354297
Epoch #106: loss=0.4538805941740672
Epoch #107: loss=0.41997029483318327
Epoch #108: loss=0.39816467265288036
Epoch #109: loss=0.39011384844779967
Epoch #110: loss=0.427441535393397
Epoch #111: loss=0.3826733410358429
Epoch #112: loss=0.45873083571592965
Epoch #113: loss=0.42429569363594055
Epoch #114: loss=0.404137193163236
Epoch #115: loss=0.35224378407001494
Epoch #116: loss=0.3135591367880503
Epoch #117: loss=0.2883856991926829
Epoch #118: loss=0.37344941794872283
Epoch #119: loss=0.3663240720828374
Epoch #120: loss=0.35160631040732065
Epoch #121: loss=0.3645699292421341
Epoch #122: loss=0.39932911346356076
Epoch #123: loss=0.46386219263076783
Epoch #124: loss=0.35853957533836367
Epoch #125: loss=0.34143096655607225
Epoch #126: loss=0.43212900360425316
Epoch #127: loss=0.4234444389740626
Epoch #128: loss=0.3237892250219981
Epoch #129: loss=0.3602636307477951
Epoch #130: loss=0.34226338962713876
Epoch #131: loss=0.39361307621002195
Epoch #132: loss=0.3307497004667918
Epoch #133: loss=0.27693280080954236
Epoch #134: loss=0.38533863226572673
Epoch #135: loss=0.3542250523964564
Epoch #136: loss=0.2872478033105532
Epoch #137: loss=0.2808207362890244
Epoch #138: loss=0.2620697299639384
Epoch #139: loss=0.3643954262137413
Epoch #140: loss=0.2878229339917501
Epoch #141: loss=0.3530866156021754
Epoch #142: loss=0.337692724665006
Epoch #143: loss=0.4284553388754527
Epoch #144: loss=0.3142985165119171
Epoch #145: loss=0.3057648191849391
Epoch #146: loss=0.5297613441944122
Epoch #147: loss=0.4794189075628916
Epoch #148: loss=0.3656226575374603
Epoch #149: loss=0.2933508366346359
Epoch #150: loss=0.3057213991880417
Epoch #151: loss=0.27040339360634486
Epoch #152: loss=0.2641432429353396
Epoch #153: loss=0.2500478612879912
Epoch #154: loss=0.25406636744737626
Epoch #155: loss=0.35796316464742023
Epoch #156: loss=0.25221288204193115
Epoch #157: loss=0.38157159785429634
Epoch #158: loss=0.27121316492557523
Epoch #159: loss=0.23557713826497395
Epoch #160: loss=0.24236448953549067
Epoch #161: loss=0.25482337226470314
Epoch #162: loss=0.23876938025156658
Epoch #163: loss=0.21135751431186994
Epoch #164: loss=0.32552036717534066
Epoch #165: loss=0.26281680166721344
Epoch #166: loss=0.317218878865242
Epoch #167: loss=0.2949947034319242
Epoch #168: loss=0.3036364952723185
Epoch #169: loss=0.23240346262852352
Epoch #170: loss=0.33408589909474057
Epoch #171: loss=0.26923160801331203
Epoch #172: loss=0.20742610146601995
Epoch #173: loss=0.2779668132464091
Epoch #174: loss=0.2476625328262647
Epoch #175: loss=0.26929389784733454
Epoch #176: loss=0.26019848783810934
Epoch #177: loss=0.3002379839619001
Epoch #178: loss=0.21894088834524156
Epoch #179: loss=0.22359751636783282
Epoch #180: loss=0.24750416924556096
Epoch #181: loss=0.30303194175163906
Epoch #182: loss=0.26436838110287986
Epoch #183: loss=0.2095247636238734
Epoch #184: loss=0.3043058062593142
Epoch #185: loss=0.29643236647049587
Epoch #186: loss=0.2469502126177152
Epoch #187: loss=0.31895456115404763
Epoch #188: loss=0.3107545584440231
Epoch #189: loss=0.35594364752372104
Epoch #190: loss=0.3015370900432269
Epoch #191: loss=0.2497888371348381
Epoch #192: loss=0.2397475376725197
Epoch #193: loss=0.2370174040397008
Epoch #194: loss=0.2288833404580752
Epoch #195: loss=0.2806081290046374
Epoch #196: loss=0.2536091913779577
Epoch #197: loss=0.2915883034467697
Epoch #198: loss=0.3059644485513369
Epoch #199: loss=0.24996225113670031
Epoch #200: loss=0.24666923781236014
Epoch #201: loss=0.21997339477141697
Epoch #202: loss=0.21558103462060293
Epoch #203: loss=0.24333024273316065
Epoch #204: loss=0.21993982940912246
Epoch #205: loss=0.23745792657136916
Epoch #206: loss=0.24534575045108795
Epoch #207: loss=0.26113819926977155
Epoch #208: loss=0.18790140797694524
Epoch #209: loss=0.17103606462478638
Epoch #210: loss=0.27692455674211186
Epoch #211: loss=0.2653338740269343
Epoch #212: loss=0.21213628153006236
Epoch #213: loss=0.2054020474354426
Epoch #214: loss=0.20487007622917494
Epoch #215: loss=0.2143628257016341
Epoch #216: loss=0.211980402469635
Epoch #217: loss=0.23236116021871567
Epoch #218: loss=0.41463984698057177
Epoch #219: loss=0.5965925087531407
Epoch #220: loss=0.3776848554611206
Epoch #221: loss=0.24075534492731093
Epoch #222: loss=0.226174263159434
Epoch #223: loss=0.23455362568298976
Epoch #224: loss=0.1814501258234183
Epoch #225: loss=0.20335833330949146
Epoch #226: loss=0.18888229926427205
Epoch #227: loss=0.2394509548942248
Epoch #228: loss=0.283029405772686
Epoch #229: loss=0.22915145456790925
Epoch #230: loss=0.22732376406590143
Epoch #231: loss=0.20692488277951876
Epoch #232: loss=0.18528279860814412
Epoch #233: loss=0.2297947645187378
Epoch #234: loss=0.2141296016673247
Epoch #235: loss=0.217045105745395
Epoch #236: loss=0.17439137399196625
Epoch #237: loss=0.23125321070353191
Epoch #238: loss=0.24124667197465896
Epoch #239: loss=0.1852359525859356
Epoch #240: loss=0.20152384862303735
Epoch #241: loss=0.2265474369128545
Epoch #242: loss=0.17376732279857
Epoch #243: loss=0.1963324266175429
Epoch #244: loss=0.2045629635453224
Epoch #245: loss=0.15874817123015722
Epoch #246: loss=0.18759771461288136
Epoch #247: loss=0.17291947628060977
Epoch #248: loss=0.19569568037986756
Epoch #249: loss=0.19396420444051424

Training time: 0:10:13.884020

Finished.
n2one setting etth1_exchange -> electricity
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth1_exchange_epochs_250_seed_2021/model.pkl', muti_dataset='etth1_exchange', random_seed=2021, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='electricity')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.25569e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.28329e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=5.61922e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.25569e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.3556584793666866, 'MAE': 0.41836952871206545}
Finished.
------------------------- record done -------------------------
n2one setting etth1_exchange -> weather
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth1_exchange_epochs_250_seed_2021/model.pkl', muti_dataset='etth1_exchange', random_seed=2021, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='weather')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.04555e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.01125e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=4.79269e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.6043082768179838, 'MAE': 0.5945395507206399}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='etth2_ettm1', random_seed=2021, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/etth2_ettm1_epochs_250_seed_2021
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=5.334633045726353
Epoch #1: loss=2.767171621322632
Epoch #2: loss=2.3737131887012057
Epoch #3: loss=2.1843006345960827
Epoch #4: loss=2.0356560548146567
Epoch #5: loss=1.9596925775210063
Epoch #6: loss=1.830189757876926
Epoch #7: loss=1.7436964180734422
Epoch #8: loss=1.6263401442103915
Epoch #9: loss=1.514516896671719
Epoch #10: loss=1.4614587028821309
Epoch #11: loss=1.4002955357233684
Epoch #12: loss=1.3326170245806377
Epoch #13: loss=1.2018758720821805
Epoch #14: loss=1.1828845474455092
Epoch #15: loss=1.1128552887174818
Epoch #16: loss=1.1254887746440039
Epoch #17: loss=1.122594462500678
Epoch #18: loss=1.085164984067281
Epoch #19: loss=1.0703191227383084
Epoch #20: loss=1.067112045155631
Epoch #21: loss=1.0607095062732697
Epoch #22: loss=0.9439280264907413
Epoch #23: loss=1.0184202790260315
Epoch #24: loss=0.9394545257091522
Epoch #25: loss=0.901465955707762
Epoch #26: loss=0.901950779888365
Epoch #27: loss=0.7910165091355642
Epoch #28: loss=0.8025447626908621
Epoch #29: loss=0.7311530941062503
Epoch #30: loss=0.7739688025580512
Epoch #31: loss=0.8603108525276184
Epoch #32: loss=0.754997968673706
Epoch #33: loss=0.678364548418257
Epoch #34: loss=0.7768039074209001
Epoch #35: loss=0.6697462995847067
Epoch #36: loss=0.680689103073544
Epoch #37: loss=0.6941652860906389
Epoch #38: loss=0.6912017514308294
Epoch #39: loss=0.6918771084811952
Epoch #40: loss=0.6126852333545685
Epoch #41: loss=0.7399720168775983
Epoch #42: loss=0.7476869142717786
Epoch #43: loss=0.7392817984024683
Epoch #44: loss=0.6119676695929633
Epoch #45: loss=0.6320096734497282
Epoch #46: loss=0.5859630422459708
Epoch #47: loss=0.5653584053119024
Epoch #48: loss=0.6312094430128733
Epoch #49: loss=0.5554103934102588
Epoch #50: loss=0.665882537762324
Epoch #51: loss=0.5592815064721637
Epoch #52: loss=0.6180800795555115
Epoch #53: loss=0.5460551811589135
Epoch #54: loss=0.5473027543889152
Epoch #55: loss=0.5376508782307307
Epoch #56: loss=0.5772946725289027
Epoch #57: loss=0.6568028380473455
Epoch #58: loss=0.514181430141131
Epoch #59: loss=0.5262516438961029
Epoch #60: loss=0.4481234351793925
Epoch #61: loss=0.4243385609653261
Epoch #62: loss=0.4579399476448695
Epoch #63: loss=0.5562709073225657
Epoch #64: loss=0.5704114735126495
Epoch #65: loss=0.4461511837111579
Epoch #66: loss=0.5119728479120467
Epoch #67: loss=0.4871932549609078
Epoch #68: loss=0.48515622980064815
Epoch #69: loss=0.5038450260957082
Epoch #70: loss=0.44197792808214825
Epoch #71: loss=0.4179377721415626
Epoch #72: loss=0.43156511502133477
Epoch #73: loss=0.4956357263856464
Epoch #74: loss=0.44517306652334
Epoch #75: loss=0.5159079498714871
Epoch #76: loss=0.41039219001928967
Epoch #77: loss=0.3792126468486256
Epoch #78: loss=0.35701987478468156
Epoch #79: loss=0.43386272258228725
Epoch #80: loss=0.4558831635448668
Epoch #81: loss=0.46623875035179985
Epoch #82: loss=0.509864835275544
Epoch #83: loss=0.4481525967518489
Epoch #84: loss=0.39823150965902543
Epoch #85: loss=0.4604221102264192
Epoch #86: loss=0.41960324347019196
Epoch #87: loss=0.43125295059548485
Epoch #88: loss=0.39482785513003665
Epoch #89: loss=0.3759259639514817
Epoch #90: loss=0.35218870060311425
Epoch #91: loss=0.3356347100602256
Epoch #92: loss=0.3909210397137536
Epoch #93: loss=0.3836533592806922
Epoch #94: loss=0.3341585364606645
Epoch #95: loss=0.358395528462198
Epoch #96: loss=0.38892316321531933
Epoch #97: loss=0.3917004143198331
Epoch #98: loss=0.3479752971066369
Epoch #99: loss=0.3588248234656122
Epoch #100: loss=0.266170520749357
Epoch #101: loss=0.42310105595323777
Epoch #102: loss=0.3824897011121114
Epoch #103: loss=0.38287901547220016
Epoch #104: loss=0.35111988004710937
Epoch #105: loss=0.28518689423799515
Epoch #106: loss=0.3407139736745093
Epoch #107: loss=0.2790594804618094
Epoch #108: loss=0.5419424780540996
Epoch #109: loss=0.3235090474287669
Epoch #110: loss=0.28879435029294753
Epoch #111: loss=0.3345735627743933
Epoch #112: loss=0.26947279108895195
Epoch #113: loss=0.27567554430829155
Epoch #114: loss=0.37811512168910766
Epoch #115: loss=0.30659608128998017
Epoch #116: loss=0.267266152633561
Epoch #117: loss=0.23827717784378263
Epoch #118: loss=0.24072136564387214
Epoch #119: loss=0.2680974668926663
Epoch #120: loss=0.34398700793584186
Epoch #121: loss=0.3124845094150967
Epoch #122: loss=0.31686395903428394
Epoch #123: loss=0.2964785910314984
Epoch #124: loss=0.23335443519883686
Epoch #125: loss=0.2547076344490051
Epoch #126: loss=0.23917769599292013
Epoch #127: loss=0.22404897461334863
Epoch #128: loss=0.27326274414857227
Epoch #129: loss=0.24512713733646604
Epoch #130: loss=0.29461393256982166
Epoch #131: loss=0.27366725272602505
Epoch #132: loss=0.22863457103570303
Epoch #133: loss=0.2836253204279476
Epoch #134: loss=0.30397797789838576
Epoch #135: loss=0.3841981473896239
Epoch #136: loss=0.3132469918992784
Epoch #137: loss=0.2940455865528848
Epoch #138: loss=0.20999179614914787
Epoch #139: loss=0.21561218715376324
Epoch #140: loss=0.20547911276419958
Epoch #141: loss=0.22877595780624282
Epoch #142: loss=0.23665298314558136
Epoch #143: loss=0.181394186284807
Epoch #144: loss=0.1931681161125501
Epoch #145: loss=0.2818129178550508
Epoch #146: loss=0.2202712715499931
Epoch #147: loss=0.20696286029285854
Epoch #148: loss=0.2613965939316485
Epoch #149: loss=0.20435341654552353
Epoch #150: loss=0.1629277409778701
Epoch #151: loss=0.17333561761511695
Epoch #152: loss=0.18787579983472824
Epoch #153: loss=0.1912444829940796
Epoch #154: loss=0.17424847020043266
Epoch #155: loss=0.17673679689566293
Epoch #156: loss=0.12883692710763878
Epoch #157: loss=0.16021495891941917
Epoch #158: loss=0.20001111634903485
Epoch #159: loss=0.20163105179866156
Epoch #160: loss=0.19044924258357948
Epoch #161: loss=0.20045419244302642
Epoch #162: loss=0.14006208214494917
Epoch #163: loss=0.16992202090720335
Epoch #164: loss=0.21509735451804268
Epoch #165: loss=0.1962423332863384
Epoch #166: loss=0.18803448685341412
Epoch #167: loss=0.13564608266784084
Epoch #168: loss=0.1527262967493799
Epoch #169: loss=0.16065872564084
Epoch #170: loss=0.1732969977375534
Epoch #171: loss=0.17275317303008503
Epoch #172: loss=0.17868059025042587
Epoch #173: loss=0.18035567696723673
Epoch #174: loss=0.19764876241485277
Epoch #175: loss=0.15579228351513544
Epoch #176: loss=0.13374159816238615
Epoch #177: loss=0.23437238401836819
Epoch #178: loss=0.15978267995847595
Epoch #179: loss=0.11907311777273814
Epoch #180: loss=0.1837782979839378
Epoch #181: loss=0.16171136043137974
Epoch #182: loss=0.12928992675410378
Epoch #183: loss=0.1112316253905495
Epoch #184: loss=0.09837234620418814
Epoch #185: loss=0.18901284701294369
Epoch #186: loss=0.2574581599069966
Epoch #187: loss=0.2589193255537086
Epoch #188: loss=0.21200308286481434
Epoch #189: loss=0.15209868032899168
Epoch #190: loss=0.29644429228372043
Epoch #191: loss=0.2490432999200291
Epoch #192: loss=0.17631276241607136
Epoch #193: loss=0.14484022557735443
Epoch #194: loss=0.13870022570093474
Epoch #195: loss=0.11683184529344241
Epoch #196: loss=0.13277192186150286
Epoch #197: loss=0.13519712599615255
Epoch #198: loss=0.14344267774787214
Epoch #199: loss=0.18144828329483667
Epoch #200: loss=0.12482760349909465
Epoch #201: loss=0.14266504637069172
Epoch #202: loss=0.16951952874660492
Epoch #203: loss=0.13459309707913134
Epoch #204: loss=0.10733883414003584
Epoch #205: loss=0.16249654442071915
Epoch #206: loss=0.15516887522406048
Epoch #207: loss=0.14965778444376257
Epoch #208: loss=0.13502227370109823
Epoch #209: loss=0.1115625709709194
Epoch #210: loss=0.1453989435815149
Epoch #211: loss=0.30168236626519096
Epoch #212: loss=0.13620361954801613
Epoch #213: loss=0.18964099697768688
Epoch #214: loss=0.1529731500065989
Epoch #215: loss=0.10756710513184468
Epoch #216: loss=0.10991691156393951
Epoch #217: loss=0.10684750984526342
Epoch #218: loss=0.09690999860564868
Epoch #219: loss=0.08031410744620694
Epoch #220: loss=0.10576566867530346
Epoch #221: loss=0.09746029352148373
Epoch #222: loss=0.11200885464333826
Epoch #223: loss=0.10084275787489282
Epoch #224: loss=0.2091500208609634
Epoch #225: loss=0.12410665365556876
Epoch #226: loss=0.10714399938782056
Epoch #227: loss=0.11773239283098115
Epoch #228: loss=0.08962336296422614
Epoch #229: loss=0.10646332614123821
Epoch #230: loss=0.10616757782797019
Epoch #231: loss=0.08565662283864287
Epoch #232: loss=0.08854693629675442
Epoch #233: loss=0.14273293068011603
Epoch #234: loss=0.1061357840274771
Epoch #235: loss=0.1161888957851463
Epoch #236: loss=0.18401810661372212
Epoch #237: loss=0.11151947701970737
Epoch #238: loss=0.14233454854951966
Epoch #239: loss=0.13025654438469145
Epoch #240: loss=0.21798562310222122
Epoch #241: loss=0.14274706174102095
Epoch #242: loss=0.130250442876584
Epoch #243: loss=0.11865241546183825
Epoch #244: loss=0.13581080393244824
Epoch #245: loss=0.15842246802316773
Epoch #246: loss=0.12087486622234185
Epoch #247: loss=0.12069339191334115
Epoch #248: loss=0.07455657670895259
Epoch #249: loss=0.07331021093866891

Training time: 0:16:29.674049

Finished.
n2one setting etth2_ettm1 -> electricity
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth2_ettm1_epochs_250_seed_2021/model.pkl', muti_dataset='etth2_ettm1', random_seed=2021, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='electricity')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.56009e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.86955e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.56009e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.3576157109579589, 'MAE': 0.42327740505765626}
Finished.
------------------------- record done -------------------------
n2one setting etth2_ettm1 -> weather
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth2_ettm1_epochs_250_seed_2021/model.pkl', muti_dataset='etth2_ettm1', random_seed=2021, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='weather')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.5191e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.86854e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.5191e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.6785181585603213, 'MAE': 0.6673163848218223}
Finished.
------------------------- record done -------------------------
n2one setting etth2_ettm1 -> exchange
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth2_ettm1_epochs_250_seed_2021/model.pkl', muti_dataset='etth2_ettm1', random_seed=2021, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='exchange')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=3.42795e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.2130283766618603, 'MAE': 0.3166834831341911}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='etth2_ettm2', random_seed=2021, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/etth2_ettm2_epochs_250_seed_2021
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=5.3096648587120905
Epoch #1: loss=2.821292863951789
Epoch #2: loss=2.4014574421776667
Epoch #3: loss=2.2119291954570346
Epoch #4: loss=2.04726951652103
Epoch #5: loss=1.9969734615749783
Epoch #6: loss=1.8475431468751695
Epoch #7: loss=1.761988513999515
Epoch #8: loss=1.6769349707497492
Epoch #9: loss=1.5451130535867479
Epoch #10: loss=1.4688880509800382
Epoch #11: loss=1.4077787730428908
Epoch #12: loss=1.3510058124860127
Epoch #13: loss=1.2262858781549666
Epoch #14: loss=1.206334153811137
Epoch #15: loss=1.108988298310174
Epoch #16: loss=1.119999087519116
Epoch #17: loss=1.115106377336714
Epoch #18: loss=1.0877200033929613
Epoch #19: loss=1.0760121544202168
Epoch #20: loss=1.064607756005393
Epoch #21: loss=1.0746884180439844
Epoch #22: loss=0.9594762027263641
Epoch #23: loss=0.9943345950709449
Epoch #24: loss=0.89275923371315
Epoch #25: loss=0.8762901888953315
Epoch #26: loss=0.8743415474891663
Epoch #27: loss=0.7634642587767707
Epoch #28: loss=0.7886943320433298
Epoch #29: loss=0.7524832089742025
Epoch #30: loss=0.7967537575297885
Epoch #31: loss=0.8985318640867869
Epoch #32: loss=0.774151924583647
Epoch #33: loss=0.7128354940149519
Epoch #34: loss=0.7824629015392728
Epoch #35: loss=0.6634803745481703
Epoch #36: loss=0.6756942454311583
Epoch #37: loss=0.6791755921310849
Epoch #38: loss=0.6907005343172286
Epoch #39: loss=0.6815015309386783
Epoch #40: loss=0.6242169953054852
Epoch #41: loss=0.7111115124490526
Epoch #42: loss=0.7390412290891012
Epoch #43: loss=0.7042288564973407
Epoch #44: loss=0.5866597874297036
Epoch #45: loss=0.6285037481122546
Epoch #46: loss=0.5595291041665607
Epoch #47: loss=0.5539217922422621
Epoch #48: loss=0.5978159407774607
Epoch #49: loss=0.5475330733590655
Epoch #50: loss=0.66530784798993
Epoch #51: loss=0.558794293138716
Epoch #52: loss=0.6021559453672833
Epoch #53: loss=0.5368637475702498
Epoch #54: loss=0.5941279513968362
Epoch #55: loss=0.5382761673794852
Epoch #56: loss=0.5404251681433784
Epoch #57: loss=0.6220907751056883
Epoch #58: loss=0.5051826967133416
Epoch #59: loss=0.5166849576764636
Epoch #60: loss=0.4625607596503364
Epoch #61: loss=0.4378880825307634
Epoch #62: loss=0.5039798766374588
Epoch #63: loss=0.5704108526309332
Epoch #64: loss=0.5360754364066653
Epoch #65: loss=0.442633933491177
Epoch #66: loss=0.5236564957433276
Epoch #67: loss=0.527648796637853
Epoch #68: loss=0.5090035912063386
Epoch #69: loss=0.5363029258118736
Epoch #70: loss=0.4591058741013209
Epoch #71: loss=0.42395567728413475
Epoch #72: loss=0.42357796761724686
Epoch #73: loss=0.4577614019314448
Epoch #74: loss=0.42000385291046566
Epoch #75: loss=0.5074971798393462
Epoch #76: loss=0.3949675170911683
Epoch #77: loss=0.37331679794523454
Epoch #78: loss=0.35669417017036015
Epoch #79: loss=0.45001290407445693
Epoch #80: loss=0.46528390215502846
Epoch #81: loss=0.45733324852254653
Epoch #82: loss=0.488903456264072
Epoch #83: loss=0.4194899515973197
Epoch #84: loss=0.3472428884771135
Epoch #85: loss=0.3971592386563619
Epoch #86: loss=0.37371474670039284
Epoch #87: loss=0.45028062330351937
Epoch #88: loss=0.4309837420781453
Epoch #89: loss=0.3917798557215267
Epoch #90: loss=0.3617719378736284
Epoch #91: loss=0.33575501955217785
Epoch #92: loss=0.40578599439726937
Epoch #93: loss=0.4642153862449858
Epoch #94: loss=0.3746316846874025
Epoch #95: loss=0.36920567353566486
Epoch #96: loss=0.3658388935857349
Epoch #97: loss=0.4103814876741833
Epoch #98: loss=0.37943028079138863
Epoch #99: loss=0.3781694877478812
Epoch #100: loss=0.28953592975934345
Epoch #101: loss=0.4101027175784111
Epoch #102: loss=0.3449767496850755
Epoch #103: loss=0.37529218279653126
Epoch #104: loss=0.32406534420119393
Epoch #105: loss=0.30039772060182357
Epoch #106: loss=0.3767227563593123
Epoch #107: loss=0.3135874792933464
Epoch #108: loss=0.5623597800731659
Epoch #109: loss=0.34698358674844104
Epoch #110: loss=0.32602746619118583
Epoch #111: loss=0.3303091981344753
Epoch #112: loss=0.26930712742937934
Epoch #113: loss=0.26996564533975387
Epoch #114: loss=0.3791947878069348
Epoch #115: loss=0.31485649529430604
Epoch #116: loss=0.26529866539769703
Epoch #117: loss=0.2329738669925266
Epoch #118: loss=0.24314525640673107
Epoch #119: loss=0.24816618528631
Epoch #120: loss=0.32320767475499046
Epoch #121: loss=0.3180814865562651
Epoch #122: loss=0.32029224435488385
Epoch #123: loss=0.3160237926575873
Epoch #124: loss=0.23658075597551134
Epoch #125: loss=0.249496349443992
Epoch #126: loss=0.22784977323479122
Epoch #127: loss=0.24084400054481295
Epoch #128: loss=0.2578611825075414
Epoch #129: loss=0.2365847494867113
Epoch #130: loss=0.3268891034854783
Epoch #131: loss=0.31719856709241867
Epoch #132: loss=0.26177652759684455
Epoch #133: loss=0.35147582325670457
Epoch #134: loss=0.26548221541775596
Epoch #135: loss=0.2522327382531431
Epoch #136: loss=0.22273563924762937
Epoch #137: loss=0.2514139624933402
Epoch #138: loss=0.18816718790266249
Epoch #139: loss=0.22967113761438263
Epoch #140: loss=0.20940488204360008
Epoch #141: loss=0.2349293910794788
Epoch #142: loss=0.24882452893588278
Epoch #143: loss=0.1923318070669969
Epoch #144: loss=0.18045162326759762
Epoch #145: loss=0.2385262039800485
Epoch #146: loss=0.18248989971147644
Epoch #147: loss=0.16965223972996077
Epoch #148: loss=0.25719701250394184
Epoch #149: loss=0.19926276678840318
Epoch #150: loss=0.17247307838665116
Epoch #151: loss=0.1746234893798828
Epoch #152: loss=0.19895387813448906
Epoch #153: loss=0.22961686675747237
Epoch #154: loss=0.2116195952726735
Epoch #155: loss=0.1932490770187643
Epoch #156: loss=0.14196549190415275
Epoch #157: loss=0.18006064370274544
Epoch #158: loss=0.22436426911089155
Epoch #159: loss=0.2601432307726807
Epoch #160: loss=0.24053050991561678
Epoch #161: loss=0.20871964800688955
Epoch #162: loss=0.15512989668382537
Epoch #163: loss=0.15292677448855507
Epoch #164: loss=0.1583966567284531
Epoch #165: loss=0.15871087316837576
Epoch #166: loss=0.17079934746854836
Epoch #167: loss=0.13412801114221415
Epoch #168: loss=0.18089852316512
Epoch #169: loss=0.17314564312497774
Epoch #170: loss=0.22950107148951954
Epoch #171: loss=0.22775477253728443
Epoch #172: loss=0.20581819944911534
Epoch #173: loss=0.23767747233311334
Epoch #174: loss=0.2263431702223089
Epoch #175: loss=0.2052349199851354
Epoch #176: loss=0.18183491172062027
Epoch #177: loss=0.21007024910714892
Epoch #178: loss=0.15522060315642092
Epoch #179: loss=0.13882485901316008
Epoch #180: loss=0.19702896972497305
Epoch #181: loss=0.1794769037514925
Epoch #182: loss=0.13536900468170643
Epoch #183: loss=0.1187873776588175
Epoch #184: loss=0.09498936724331644
Epoch #185: loss=0.17812248340083492
Epoch #186: loss=0.18817153738604653
Epoch #187: loss=0.22368430884348023
Epoch #188: loss=0.19156373892393377
Epoch #189: loss=0.13124562510185772
Epoch #190: loss=0.33410970452759003
Epoch #191: loss=0.260539885610342
Epoch #192: loss=0.17640318349003792
Epoch #193: loss=0.15360625626312363
Epoch #194: loss=0.13582384255197313
Epoch #195: loss=0.11927956901490688
Epoch #196: loss=0.14454334725936255
Epoch #197: loss=0.14495142652756637
Epoch #198: loss=0.1397307866977321
Epoch #199: loss=0.18973335747917494
Epoch #200: loss=0.1340868559976419
Epoch #201: loss=0.1358531340956688
Epoch #202: loss=0.15639055458207926
Epoch #203: loss=0.1270973818997542
Epoch #204: loss=0.09753016299671596
Epoch #205: loss=0.16251696853174102
Epoch #206: loss=0.1819477706319756
Epoch #207: loss=0.16724061738285753
Epoch #208: loss=0.16161743841237491
Epoch #209: loss=0.1422518746306499
Epoch #210: loss=0.16993124617470634
Epoch #211: loss=0.34910742317636806
Epoch #212: loss=0.15152469111813438
Epoch #213: loss=0.21361442158619562
Epoch #214: loss=0.16782028869622284
Epoch #215: loss=0.12037348540292846
Epoch #216: loss=0.13456163596775797
Epoch #217: loss=0.12315661191112465
Epoch #218: loss=0.10217187501904038
Epoch #219: loss=0.08873569034039974
Epoch #220: loss=0.10597281571891573
Epoch #221: loss=0.09808736801561382
Epoch #222: loss=0.1192700383770797
Epoch #223: loss=0.10878164579884873
Epoch #224: loss=0.2184806890371773
Epoch #225: loss=0.12113820161256525
Epoch #226: loss=0.11758813075721264
Epoch #227: loss=0.1313694655481312
Epoch #228: loss=0.10864175607760747
Epoch #229: loss=0.10092854499816895
Epoch #230: loss=0.08429934963997868
Epoch #231: loss=0.08476704679843453
Epoch #232: loss=0.08353327276806037
Epoch #233: loss=0.12757465874569285
Epoch #234: loss=0.09273350176711877
Epoch #235: loss=0.09387000360422665
Epoch #236: loss=0.15513455205493504
Epoch #237: loss=0.1078826594683859
Epoch #238: loss=0.1175090940669179
Epoch #239: loss=0.10757585542483462
Epoch #240: loss=0.20158729474577639
Epoch #241: loss=0.12340757519834572
Epoch #242: loss=0.1304952193879419
Epoch #243: loss=0.1050321805394358
Epoch #244: loss=0.12328907661139965
Epoch #245: loss=0.13295705895870924
Epoch #246: loss=0.11424274411466387
Epoch #247: loss=0.1176837772751848
Epoch #248: loss=0.0735632386058569
Epoch #249: loss=0.08786185230645868

Training time: 0:16:46.302297

Finished.
n2one setting etth2_ettm2 -> electricity
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth2_ettm2_epochs_250_seed_2021/model.pkl', muti_dataset='etth2_ettm2', random_seed=2021, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='electricity')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.61404e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.95216e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.61404e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.3609303267385564, 'MAE': 0.42518711843509716}
Finished.
------------------------- record done -------------------------
n2one setting etth2_ettm2 -> weather
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth2_ettm2_epochs_250_seed_2021/model.pkl', muti_dataset='etth2_ettm2', random_seed=2021, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='weather')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.47927e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.74041e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.47927e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.8077734902012152, 'MAE': 0.7224442207030971}
Finished.
------------------------- record done -------------------------
n2one setting etth2_ettm2 -> exchange
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth2_ettm2_epochs_250_seed_2021/model.pkl', muti_dataset='etth2_ettm2', random_seed=2021, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='exchange')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=3.31306e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.20990970258517477, 'MAE': 0.3169702502013224}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='etth2_electricity', random_seed=2021, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/etth2_electricity_epochs_250_seed_2021
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=1.661043194554201
Epoch #1: loss=0.6861495276595034
Epoch #2: loss=0.4408177736147148
Epoch #3: loss=0.3581025819862034
Epoch #4: loss=0.31079226109857966
Epoch #5: loss=0.2605521268688324
Epoch #6: loss=0.2446620937936553
Epoch #7: loss=0.2000836889236802
Epoch #8: loss=0.18746056550795712
Epoch #9: loss=0.14808583672998882
Epoch #10: loss=0.14688580943180657
Epoch #11: loss=0.12221092636492557
Epoch #12: loss=0.1302721225883721
Epoch #13: loss=0.11495177567050588
Epoch #14: loss=0.09863396198488772
Epoch #15: loss=0.08609485276406859
Epoch #16: loss=0.09292106192399997
Epoch #17: loss=0.09805297987285728
Epoch #18: loss=0.07059142258154547
Epoch #19: loss=0.07013430573875312
Epoch #20: loss=0.06008584157098085
Epoch #21: loss=0.06414991787566644
Epoch #22: loss=0.05096584530660837
Epoch #23: loss=0.06074738026619321
Epoch #24: loss=0.04630129160671836
Epoch #25: loss=0.05827590996234837
Epoch #26: loss=0.04347074718538263
Epoch #27: loss=0.029544222328340544
Epoch #28: loss=0.05462065857278592
Epoch #29: loss=0.03190977625668094
Epoch #30: loss=0.04671684509887156
Epoch #31: loss=0.02951348291088191
Epoch #32: loss=0.039764549131732344
Epoch #33: loss=0.048221100303458005
Epoch #34: loss=0.03864996513868568
Epoch #35: loss=0.030254668571060614
Epoch #36: loss=0.03740261184980116
Epoch #37: loss=0.0340659977355688
Epoch #38: loss=0.04106355801355321
Epoch #39: loss=0.030349038249568833
Epoch #40: loss=0.04595717165914432
Epoch #41: loss=0.026524447687623295
Epoch #42: loss=0.02052690498119152
Epoch #43: loss=0.023205731935066556
Epoch #44: loss=0.05324980120455687
Epoch #45: loss=0.029553986138170132
Epoch #46: loss=0.02192915350599101
Epoch #47: loss=0.025930945248280584
Epoch #48: loss=0.03661560326340472
Epoch #49: loss=0.021251490897578527
Epoch #50: loss=0.03091291776076016
Epoch #51: loss=0.02445022487322936
Epoch #52: loss=0.030823774400831585
Epoch #53: loss=0.02260571548845846
Epoch #54: loss=0.020858304653878938
Epoch #55: loss=0.03228940771787609
Epoch #56: loss=0.023901177096044328
Epoch #57: loss=0.019663084533687387
Epoch #58: loss=0.019084059845559616
Epoch #59: loss=0.036189343788087504
Epoch #60: loss=0.016330045750492433
Epoch #61: loss=0.01578475272449063
Epoch #62: loss=0.020778314749960128
Epoch #63: loss=0.019598057986912887
Epoch #64: loss=0.027402613694969814
Epoch #65: loss=0.020720806494908886
Epoch #66: loss=0.01632564298868202
Epoch #67: loss=0.030912879824272085
Epoch #68: loss=0.04807203592840446
Epoch #69: loss=0.015020412992382218
Epoch #70: loss=0.013139234992062173
Epoch #71: loss=0.026370380901470328
Epoch #72: loss=0.015156348454538823
Epoch #73: loss=0.014299633057728387
Epoch #74: loss=0.022730805098700974
Epoch #75: loss=0.015553691166754039
Epoch #76: loss=0.01700212367101879
Epoch #77: loss=0.020384070618580687
Epoch #78: loss=0.016438911380047525
Epoch #79: loss=0.01481981817050837
Epoch #80: loss=0.021474293762854892
Epoch #81: loss=0.015579174371030204
Epoch #82: loss=0.018449332368085956
Epoch #83: loss=0.020491849737903443
Epoch #84: loss=0.017941544489387038
Epoch #85: loss=0.015708126016983355
Epoch #86: loss=0.01515664740127693
Epoch #87: loss=0.016370341510237932
Epoch #88: loss=0.017494859039377444
Epoch #89: loss=0.006937693510890978
Epoch #90: loss=0.016313251405358088
Epoch #91: loss=0.013031152406389393
Epoch #92: loss=0.018109983810906515
Epoch #93: loss=0.02293685732086199
Epoch #94: loss=0.0154183833038565
Epoch #95: loss=0.013194811802106148
Epoch #96: loss=0.011774179405886282
Epoch #97: loss=0.017512442584919016
Epoch #98: loss=0.012655166236871126
Epoch #99: loss=0.017018103006504557
Epoch #100: loss=0.011240905514569022
Epoch #101: loss=0.024282870623254696
Epoch #102: loss=0.010390079015295912
Epoch #103: loss=0.010498538933461532
Epoch #104: loss=0.015303822154059961
Epoch #105: loss=0.020840502169803807
Epoch #106: loss=0.02164811722575062
Epoch #107: loss=0.020542131677602146
Epoch #108: loss=0.011265259693950279
Epoch #109: loss=0.009290790710266022
Epoch #110: loss=0.031248512060456715
Epoch #111: loss=0.012077092647548231
Epoch #112: loss=0.010308014029649756
Epoch #113: loss=0.014901401914646225
Epoch #114: loss=0.010944317898886256
Epoch #115: loss=0.01321283275319729
Epoch #116: loss=0.018914747922525104
Epoch #117: loss=0.014059616444439331
Epoch #118: loss=0.008940763844204811
Epoch #119: loss=0.007707025637057264
Epoch #120: loss=0.017595518569400954
Epoch #121: loss=0.010818468926938694
Epoch #122: loss=0.020497689851298476
Epoch #123: loss=0.013532709678856475
Epoch #124: loss=0.011568493514837096
Epoch #125: loss=0.016021582401618814
Epoch #126: loss=0.017714531528948033
Epoch #127: loss=0.014749378510026783
Epoch #128: loss=0.017536375206496237
Epoch #129: loss=0.00900861283830969
Epoch #130: loss=0.010240153846275572
Epoch #131: loss=0.010772062590127106
Epoch #132: loss=0.013634954369591447
Epoch #133: loss=0.02304854441438404
Epoch #134: loss=0.02185022673483018
Epoch #135: loss=0.011796761552055036
Epoch #136: loss=0.014600341753266105
Epoch #137: loss=0.008171795026389258
Epoch #138: loss=0.01038317880472566
Epoch #139: loss=0.006570199043713595
Epoch #140: loss=0.006729236481411002
Epoch #141: loss=0.01361500946790684
Epoch #142: loss=0.011911557599577042
Epoch #143: loss=0.012439358537404787
Epoch #144: loss=0.011359764396130652
Epoch #145: loss=0.016482737522271303
Epoch #146: loss=0.01027040437421841
Epoch #147: loss=0.010805787544529998
Epoch #148: loss=0.010852182960839056
Epoch #149: loss=0.026586845241204506
Epoch #150: loss=0.011991181322295366
Epoch #151: loss=0.010551059076977076
Epoch #152: loss=0.014902632093364402
Epoch #153: loss=0.02366742340621763
Epoch #154: loss=0.012276975002151164
Epoch #155: loss=0.007876312946946866
Epoch #156: loss=0.013680130202130119
Epoch #157: loss=0.008460371672011139
Epoch #158: loss=0.008747559816117977
Epoch #159: loss=0.0071832832555384545
Epoch #160: loss=0.011590606955173936
Epoch #161: loss=0.017902626894327825
Epoch #162: loss=0.009036443046991225
Epoch #163: loss=0.009119361857833942
Epoch #164: loss=0.017526373298407148
Epoch #165: loss=0.012202254537508608
Epoch #166: loss=0.011251012297778777
Epoch #167: loss=0.007183984996184491
Epoch #168: loss=0.007857751150057572
Epoch #169: loss=0.00828263002617083
Epoch #170: loss=0.014256539020940049
Epoch #171: loss=0.012258485936000376
Epoch #172: loss=0.011268002598116276
Epoch #173: loss=0.007611511223353218
Epoch #174: loss=0.015034122064052342
Epoch #175: loss=0.027448954590576384
Epoch #176: loss=0.00871193069569261
Epoch #177: loss=0.007194819229104124
Epoch #178: loss=0.01029436809971585
Epoch #179: loss=0.014217373994342336
Epoch #180: loss=0.009295861043849472
Epoch #181: loss=0.016511047597038957
Epoch #182: loss=0.011604898940188322
Epoch #183: loss=0.00872740303428855
Epoch #184: loss=0.008186023424325579
Epoch #185: loss=0.006659243576971831
Epoch #186: loss=0.014541944777259388
Epoch #187: loss=0.015549353882173508
Epoch #188: loss=0.013635749808333746
Epoch #189: loss=0.006550848538342795
Epoch #190: loss=0.011690573742870162
Epoch #191: loss=0.007299811919127512
Epoch #192: loss=0.009222522347266842
Epoch #193: loss=0.009702596803324966
Epoch #194: loss=0.005383377028565298
Epoch #195: loss=0.012143182593185552
Epoch #196: loss=0.008431409171529727
Epoch #197: loss=0.029338315059626106
Epoch #198: loss=0.006957532288031028
Epoch #199: loss=0.010099108114046408
Epoch #200: loss=0.008801125328584642
Epoch #201: loss=0.006059793289307159
Epoch #202: loss=0.007216506483329306
Epoch #203: loss=0.013669285231139285
Epoch #204: loss=0.006655877942227185
Epoch #205: loss=0.0085908108850584
Epoch #206: loss=0.008837696889957709
Epoch #207: loss=0.01127925544864322
Epoch #208: loss=0.008909320200916959
Epoch #209: loss=0.014600014507400706
Epoch #210: loss=0.005256402767838331
Epoch #211: loss=0.0102965398109518
Epoch #212: loss=0.007604135930017858
Epoch #213: loss=0.006287651826757455
Epoch #214: loss=0.006346792758782479
Epoch #215: loss=0.011014736783935648
Epoch #216: loss=0.010854833838515689
Epoch #217: loss=0.00859573382686216
Epoch #218: loss=0.007784329102234063
Epoch #219: loss=0.008326288512052554
Epoch #220: loss=0.014498040075811399
Epoch #221: loss=0.009736607754326186
Epoch #222: loss=0.014941039746279385
Epoch #223: loss=0.008240522428223346
Epoch #224: loss=0.009190655714231003
Epoch #225: loss=0.006154116028838183
Epoch #226: loss=0.006280277303253903
Epoch #227: loss=0.009778188760076485
Epoch #228: loss=0.007808265326386346
Epoch #229: loss=0.008732792481168872
Epoch #230: loss=0.014478787816365295
Epoch #231: loss=0.007250038511102037
Epoch #232: loss=0.01063930653812926
Epoch #233: loss=0.00804420683633042
Epoch #234: loss=0.012036854169819868
Epoch #235: loss=0.012078573197557977
Epoch #236: loss=0.005209447603680552
Epoch #237: loss=0.010646674469526482
Epoch #238: loss=0.010325004978967184
Epoch #239: loss=0.007152919612985343
Epoch #240: loss=0.010664346149629947
Epoch #241: loss=0.006594257683545726
Epoch #242: loss=0.008246935929846808
Epoch #243: loss=0.010726632537226556
Epoch #244: loss=0.007883489103089853
Epoch #245: loss=0.008352040252225933
Epoch #246: loss=0.014056724146407427
Epoch #247: loss=0.0056013736862152615
Epoch #248: loss=0.008816982530289551
Epoch #249: loss=0.006416169981803694

Training time: 4:33:33.654261

Finished.
n2one setting etth2_electricity -> weather
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth2_electricity_epochs_250_seed_2021/model.pkl', muti_dataset='etth2_electricity', random_seed=2021, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='weather')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.18604e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.43599e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.18604e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.5904687955101463, 'MAE': 0.5723633947729534}
Finished.
------------------------- record done -------------------------
n2one setting etth2_electricity -> exchange
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth2_electricity_epochs_250_seed_2021/model.pkl', muti_dataset='etth2_electricity', random_seed=2021, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='exchange')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=4.66711e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.2128488430244104, 'MAE': 0.3212089184074395}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='etth2_traffic', random_seed=2021, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/etth2_traffic_epochs_250_seed_2021
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=1.1009080891504934
Epoch #1: loss=0.38477051935412665
Epoch #2: loss=0.2625257691970766
Epoch #3: loss=0.20624365539250358
Epoch #4: loss=0.14363323942564163
Epoch #5: loss=0.1299669387031205
Epoch #6: loss=0.10929343489323441
Epoch #7: loss=0.10265177083003439
Epoch #8: loss=0.08523621290742116
Epoch #9: loss=0.07331941296293971
Epoch #10: loss=0.06359335246857072
Epoch #11: loss=0.06018810497406094
Epoch #12: loss=0.0643582486186361
Epoch #13: loss=0.051513442740174414
Epoch #14: loss=0.04350102798245907
Epoch #15: loss=0.04805348111505184
Epoch #16: loss=0.04087094829784293
Epoch #17: loss=0.03843274483536515
Epoch #18: loss=0.04308733314927346
Epoch #19: loss=0.04056434038494927
Epoch #20: loss=0.03372227759084421
Epoch #21: loss=0.03238461398111251
Epoch #22: loss=0.033778769397085705
Epoch #23: loss=0.032455734398443216
Epoch #24: loss=0.023893207006860564
Epoch #25: loss=0.03009412384560381
Epoch #26: loss=0.02830970649215231
Epoch #27: loss=0.028826087569631422
Epoch #28: loss=0.02249043032910645
Epoch #29: loss=0.027951290799086214
Epoch #30: loss=0.02485982354588901
Epoch #31: loss=0.030287070379359914
Epoch #32: loss=0.02100503149942599
Epoch #33: loss=0.02080356822964297
Epoch #34: loss=0.04629012443777924
Epoch #35: loss=0.02717472613554435
Epoch #36: loss=0.020370996721818725
Epoch #37: loss=0.022871548558694662
Epoch #38: loss=0.022387634311728737
Epoch #39: loss=0.02371190564216075
Epoch #40: loss=0.01820594958752829
Epoch #41: loss=0.025458078779723878
Epoch #42: loss=0.021077670201900874
Epoch #43: loss=0.015922694045605413
Epoch #44: loss=0.018648743990825337
Epoch #45: loss=0.02600560090039729
Epoch #46: loss=0.021629268725033447
Epoch #47: loss=0.016478100324438624
Epoch #48: loss=0.017115917298605215
Epoch #49: loss=0.02075313954750479
Epoch #50: loss=0.0152302509715906
Epoch #51: loss=0.018901245810867953
Epoch #52: loss=0.028750612257933725
Epoch #53: loss=0.01685599103427714
Epoch #54: loss=0.03443784420046274
Epoch #55: loss=0.01488001641972254
Epoch #56: loss=0.01387697105216649
Epoch #57: loss=0.031985831125229444
Epoch #58: loss=0.019672298333441498
Epoch #59: loss=0.015226548337251011
Epoch #60: loss=0.01229280359336787
Epoch #61: loss=0.022412126028834724
Epoch #62: loss=0.01157449614652486
Epoch #63: loss=0.023576364967764663
Epoch #64: loss=0.014561517212271446
Epoch #65: loss=0.021034713553131308
Epoch #66: loss=0.014620344618121558
Epoch #67: loss=0.018600958511737035
Epoch #68: loss=0.015341819358707735
Epoch #69: loss=0.013152170258049581
Epoch #70: loss=0.015485598350401112
Epoch #71: loss=0.014255423446306991
Epoch #72: loss=0.016680693732513897
Epoch #73: loss=0.015344854488540697
Epoch #74: loss=0.01220995986577492
Epoch #75: loss=0.011345340703869661
Epoch #76: loss=0.01611237695667854
Epoch #77: loss=0.018044870842520953
Epoch #78: loss=0.019564549904752048
Epoch #79: loss=0.013070687017612796
Epoch #80: loss=0.018510813478265768
Epoch #81: loss=0.02035886784897575
Epoch #82: loss=0.01900562349486784
Epoch #83: loss=0.014659017345848492
Epoch #84: loss=0.012364399619601448
Epoch #85: loss=0.016155885911718928
Epoch #86: loss=0.015488808978843568
Epoch #87: loss=0.014133115665707416
Epoch #88: loss=0.016889214504499983
Epoch #89: loss=0.013542765043745561
Epoch #90: loss=0.018867655563958887
Epoch #91: loss=0.01471174823103357
Epoch #92: loss=0.01391766360253969
Epoch #93: loss=0.014007145072126137
Epoch #94: loss=0.016999719235365536
Epoch #95: loss=0.0121089280656645
Epoch #96: loss=0.015957002388052182
Epoch #97: loss=0.01677031475846721
Epoch #98: loss=0.01374603631357358
Epoch #99: loss=0.011964708236342197
Epoch #100: loss=0.014455004231472446
Epoch #101: loss=0.014038477797233752
Epoch #102: loss=0.013941633769291452
Epoch #103: loss=0.013965132784112318
Epoch #104: loss=0.025354314579117198
Epoch #105: loss=0.014495050532261598
Epoch #106: loss=0.015706244892055184
Epoch #107: loss=0.01631394630801458
Epoch #108: loss=0.009892131172886046
Epoch #109: loss=0.011527304419203016
Epoch #110: loss=0.015408818380330059
Epoch #111: loss=0.013462675563795884
Epoch #112: loss=0.016022565567108262
Epoch #113: loss=0.014557357416252453
Epoch #114: loss=0.011800504854027722
Epoch #115: loss=0.011243862552093144
Epoch #116: loss=0.015554011814729856
Epoch #117: loss=0.012738315036199651
Epoch #118: loss=0.01304923473341981
Epoch #119: loss=0.01675061700378397
Epoch #120: loss=0.008108001608341655
Epoch #121: loss=0.011789597042070507
Epoch #122: loss=0.020380977183455544
Epoch #123: loss=0.01198301158419216
Epoch #124: loss=0.012476474111512575
Epoch #125: loss=0.011244274048824335
Epoch #126: loss=0.01091596529255013
Epoch #127: loss=0.011831749275467927
Epoch #128: loss=0.015277232674753746
Epoch #129: loss=0.013269446085430082
Epoch #130: loss=0.011727845281259648
Epoch #131: loss=0.011271908077103147
Epoch #132: loss=0.01391228348639273
Epoch #133: loss=0.009785929635643098
Epoch #134: loss=0.011439280689717297
Epoch #135: loss=0.013551431084231241
Epoch #136: loss=0.015298424956076098
Epoch #137: loss=0.01043105987684321
Epoch #138: loss=0.010380663264614355
Epoch #139: loss=0.009546279590534019
Epoch #140: loss=0.009339569322610814
Epoch #141: loss=0.011288995800970682
Epoch #142: loss=0.008999110220627236
Epoch #143: loss=0.012882194507913182
Epoch #144: loss=0.013047391883111924
Epoch #145: loss=0.00978741290574782
Epoch #146: loss=0.011197902385274166
Epoch #147: loss=0.01202729712539809
Epoch #148: loss=0.00803146798548332
Epoch #149: loss=0.009195181791340665
Epoch #150: loss=0.013989961031254732
Epoch #151: loss=0.011239712010422412
Epoch #152: loss=0.015091117095149693
Epoch #153: loss=0.009109509706164157
Epoch #154: loss=0.009830070583945407
Epoch #155: loss=0.011461474631075191
Epoch #156: loss=0.012593875156164381
Epoch #157: loss=0.02938471201980611
Epoch #158: loss=0.009555889631276446
Epoch #159: loss=0.009631391277687326
Epoch #160: loss=0.009518790268195205
Epoch #161: loss=0.014690163802522716
Epoch #162: loss=0.008720170502164145
Epoch #163: loss=0.011010797475527606
Epoch #164: loss=0.01394612022181378
Epoch #165: loss=0.014178510385976598
Epoch #166: loss=0.011428075945477716
Epoch #167: loss=0.009219066331006175
Epoch #168: loss=0.009064987877372307
Epoch #169: loss=0.014455580379440416
Epoch #170: loss=0.013290937486678304
Epoch #171: loss=0.007079172355755981
Epoch #172: loss=0.00949178007505009
Epoch #173: loss=0.009361202536492825
Epoch #174: loss=0.014417087485119153
Epoch #175: loss=0.011032846262251157
Epoch #176: loss=0.007973663843368417
Epoch #177: loss=0.019061863048364363
Epoch #178: loss=0.013154935332552221
Epoch #179: loss=0.013070247923299966
Epoch #180: loss=0.01435799683026634
Epoch #181: loss=0.007983157220971074
Epoch #182: loss=0.0072240396516981146
Epoch #183: loss=0.017176729907207748
Epoch #184: loss=0.009290874782732396
Epoch #185: loss=0.009081359119025906
Epoch #186: loss=0.008321451810632015
Epoch #187: loss=0.010254464675326053
Epoch #188: loss=0.00827215941965066
Epoch #189: loss=0.014673730998665204
Epoch #190: loss=0.011621067271100613
Epoch #191: loss=0.01024093922436673
Epoch #192: loss=0.007020874941476136
Epoch #193: loss=0.008219727983978102
Epoch #194: loss=0.008007793949704089
Epoch #195: loss=0.01539542042052532
Epoch #196: loss=0.00714274442761585
Epoch #197: loss=0.00964725420628961
Epoch #198: loss=0.013370786570344802
Epoch #199: loss=0.012020616342707577
Epoch #200: loss=0.0075518171442000275
Epoch #201: loss=0.013049591806889691
Epoch #202: loss=0.01050787019531025
Epoch #203: loss=0.008538432462604167
Epoch #204: loss=0.0093138701763894
Epoch #205: loss=0.011886409812800315
Epoch #206: loss=0.010543288986587905
Epoch #207: loss=0.005751759731812395
Epoch #208: loss=0.00901068135119666
Epoch #209: loss=0.012977765769754259
Epoch #210: loss=0.009309799472361447
Epoch #211: loss=0.008430745743463053
Epoch #212: loss=0.00962380287618595
Epoch #213: loss=0.008332395824341783
Epoch #214: loss=0.011123803592204214
Epoch #215: loss=0.010198028191330095
Epoch #216: loss=0.01073053906331868
Epoch #217: loss=0.008462673838664156
Epoch #218: loss=0.0069501538056489995
Epoch #219: loss=0.01006914218571865
Epoch #220: loss=0.01510901853244728
Epoch #221: loss=0.009688564738339366
Epoch #222: loss=0.007545690399447779
Epoch #223: loss=0.012470030657581607
Epoch #224: loss=0.01131106852363555
Epoch #225: loss=0.011917105300408036
Epoch #226: loss=0.009872152129681438
Epoch #227: loss=0.007339903442909828
Epoch #228: loss=0.0077969635930153465
Epoch #229: loss=0.009196068079200845
Epoch #230: loss=0.009040449163809491
Epoch #231: loss=0.007328399644052166
Epoch #232: loss=0.009187027342725754
Epoch #233: loss=0.010469885070478545
Epoch #234: loss=0.010306673437206641
Epoch #235: loss=0.00888462253504685
Epoch #236: loss=0.012264136195211469
Epoch #237: loss=0.008461906560248549
Epoch #238: loss=0.008710502268162007
Epoch #239: loss=0.01109012692115244
Epoch #240: loss=0.012356735288115407
Epoch #241: loss=0.010490253799705727
Epoch #242: loss=0.007752703943442737
Epoch #243: loss=0.013586401318599683
Epoch #244: loss=0.007923693748143307
Epoch #245: loss=0.009791735924270097
Epoch #246: loss=0.00753521487301417
Epoch #247: loss=0.009270090445514684
Epoch #248: loss=0.009594919600584829
Epoch #249: loss=0.01124929107916319

Training time: 10:15:55.724988

Finished.
n2one setting etth2_traffic -> electricity
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth2_traffic_epochs_250_seed_2021/model.pkl', muti_dataset='etth2_traffic', random_seed=2021, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='electricity')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=9.14734e-09): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.06197e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=5.36175e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=9.14734e-09): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.4056911195546511, 'MAE': 0.45174819365271085}
Finished.
------------------------- record done -------------------------
n2one setting etth2_traffic -> weather
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth2_traffic_epochs_250_seed_2021/model.pkl', muti_dataset='etth2_traffic', random_seed=2021, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='weather')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.66622e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=3.40504e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.66622e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.6968148510438144, 'MAE': 0.6298913791752841}
Finished.
------------------------- record done -------------------------
n2one setting etth2_traffic -> exchange
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth2_traffic_epochs_250_seed_2021/model.pkl', muti_dataset='etth2_traffic', random_seed=2021, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='exchange')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
Evaluation result: {'MSE': 0.33931669738735093, 'MAE': 0.37877250986242705}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='etth2_weather', random_seed=2021, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/etth2_weather_epochs_250_seed_2021
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=7.32573744022485
Epoch #1: loss=3.0391833131963555
Epoch #2: loss=2.646637136285955
Epoch #3: loss=2.3729824116735747
Epoch #4: loss=2.4555720777222603
Epoch #5: loss=2.2320771181222163
Epoch #6: loss=2.0091992941769687
Epoch #7: loss=1.8699044892282197
Epoch #8: loss=1.8323379574400005
Epoch #9: loss=1.7337176221789736
Epoch #10: loss=1.6363998940496733
Epoch #11: loss=1.54129210024169
Epoch #12: loss=1.4691514679879853
Epoch #13: loss=1.399215907761545
Epoch #14: loss=1.306285402991555
Epoch #15: loss=1.1963544758883389
Epoch #16: loss=1.2625364715402776
Epoch #17: loss=1.1190873402537722
Epoch #18: loss=1.1353416840235393
Epoch #19: loss=1.2278464400407039
Epoch #20: loss=1.035714364413059
Epoch #21: loss=1.055709952657873
Epoch #22: loss=1.118243239142678
Epoch #23: loss=0.9625726703441504
Epoch #24: loss=0.9886316494508223
Epoch #25: loss=0.8594073544849049
Epoch #26: loss=0.9046918081514763
Epoch #27: loss=0.8131042375709071
Epoch #28: loss=0.7832336732835481
Epoch #29: loss=0.8451104200247562
Epoch #30: loss=0.8285443800868411
Epoch #31: loss=0.7558667208209182
Epoch #32: loss=0.7411183534246503
Epoch #33: loss=0.7511259949568546
Epoch #34: loss=0.7452032259016326
Epoch #35: loss=0.6851949953671658
Epoch #36: loss=0.7216936855605154
Epoch #37: loss=0.69946965304288
Epoch #38: loss=0.7112076165098132
Epoch #39: loss=0.7367416737657605
Epoch #40: loss=0.8170030171220953
Epoch #41: loss=0.6731165764909802
Epoch #42: loss=0.8210577269395193
Epoch #43: loss=0.8862237307158384
Epoch #44: loss=0.7672414021058516
Epoch #45: loss=0.6522238489353296
Epoch #46: loss=0.6977479466886232
Epoch #47: loss=0.6174429474454938
Epoch #48: loss=0.547084935686805
Epoch #49: loss=0.5916869739691416
Epoch #50: loss=0.5738897016554168
Epoch #51: loss=0.5118110062498035
Epoch #52: loss=0.5265628460681799
Epoch #53: loss=0.4515525689630797
Epoch #54: loss=0.4719785795067296
Epoch #55: loss=0.522710739663153
Epoch #56: loss=0.481334595969229
Epoch #57: loss=0.5181154401013346
Epoch #58: loss=0.5496816635131836
Epoch #59: loss=0.5020792881647745
Epoch #60: loss=0.5722769715569236
Epoch #61: loss=0.47578280622308905
Epoch #62: loss=0.4874176066933256
Epoch #63: loss=0.47131960410060303
Epoch #64: loss=0.4095354581421072
Epoch #65: loss=0.4073774530128999
Epoch #66: loss=0.4609475280299331
Epoch #67: loss=0.4208306679219911
Epoch #68: loss=0.34776508717825916
Epoch #69: loss=0.3772893818941983
Epoch #70: loss=0.43179465514240845
Epoch #71: loss=0.369213883172382
Epoch #72: loss=0.4106911357605096
Epoch #73: loss=0.32564838320919964
Epoch #74: loss=0.34301267157901416
Epoch #75: loss=0.3317007986885129
Epoch #76: loss=0.3994184479568944
Epoch #77: loss=0.3342574026548501
Epoch #78: loss=0.38256796246225183
Epoch #79: loss=0.32571100601644226
Epoch #80: loss=0.29118498572797485
Epoch #81: loss=0.3265881615154671
Epoch #82: loss=0.28280428368033783
Epoch #83: loss=0.3747399720278653
Epoch #84: loss=0.24524505978280847
Epoch #85: loss=0.32690613062092755
Epoch #86: loss=0.30251469666307623
Epoch #87: loss=0.3264906577991717
Epoch #88: loss=0.26899574426087464
Epoch #89: loss=0.29954003700704285
Epoch #90: loss=0.24276851975556576
Epoch #91: loss=0.268243489391876
Epoch #92: loss=0.30978170759750134
Epoch #93: loss=0.2705829549919475
Epoch #94: loss=0.2662658528848128
Epoch #95: loss=0.2813917366844235
Epoch #96: loss=0.21370431035757065
Epoch #97: loss=0.2362478734417395
Epoch #98: loss=0.2784942347895015
Epoch #99: loss=0.2178428622357773
Epoch #100: loss=0.23290519619529898
Epoch #101: loss=0.18003040010278876
Epoch #102: loss=0.21845535550153616
Epoch #103: loss=0.2062730730483026
Epoch #104: loss=0.1480263811395024
Epoch #105: loss=0.17698320306160234
Epoch #106: loss=0.22205290988539206
Epoch #107: loss=0.19670707032536017
Epoch #108: loss=0.28778835634390515
Epoch #109: loss=0.37241118920571875
Epoch #110: loss=0.20837339442787747
Epoch #111: loss=0.19078800375714447
Epoch #112: loss=0.20555711853684802
Epoch #113: loss=0.2447756820104339
Epoch #114: loss=0.2130532596598972
Epoch #115: loss=0.17425956735105225
Epoch #116: loss=0.20441793650388718
Epoch #117: loss=0.14000610194422983
Epoch #118: loss=0.228272882149075
Epoch #119: loss=0.2013565112243999
Epoch #120: loss=0.12782791900363835
Epoch #121: loss=0.1444611240065459
Epoch #122: loss=0.15149122803951753
Epoch #123: loss=0.16847571312929643
Epoch #124: loss=0.18051882494579663
Epoch #125: loss=0.23787192503611246
Epoch #126: loss=0.4468483599749478
Epoch #127: loss=0.3665046854452653
Epoch #128: loss=0.20132220130075107
Epoch #129: loss=0.20371646321181094
Epoch #130: loss=0.18428218387293094
Epoch #131: loss=0.1804740597127062
Epoch #132: loss=0.1650802172494657
Epoch #133: loss=0.19810685662157607
Epoch #134: loss=0.19988574731079015
Epoch #135: loss=0.1847771926585472
Epoch #136: loss=0.13059965667851042
Epoch #137: loss=0.12018825689500029
Epoch #138: loss=0.1250332531829675
Epoch #139: loss=0.13442022184079344
Epoch #140: loss=0.18026088996592796
Epoch #141: loss=0.2295743658235579
Epoch #142: loss=0.13367543290510323
Epoch #143: loss=0.1346893281196103
Epoch #144: loss=0.16185472995945901
Epoch #145: loss=0.17221894736091295
Epoch #146: loss=0.12804392951004434
Epoch #147: loss=0.1417193447211475
Epoch #148: loss=0.16731613569638945
Epoch #149: loss=0.10196461929290583
Epoch #150: loss=0.11770236571178291
Epoch #151: loss=0.13036048231702863
Epoch #152: loss=0.13731273845063918
Epoch #153: loss=0.13414002616297116
Epoch #154: loss=0.1486656784333966
Epoch #155: loss=0.1700080417548165
Epoch #156: loss=0.13924535341335065
Epoch #157: loss=0.14032058299265124
Epoch #158: loss=0.175537020193808
Epoch #159: loss=0.33692363401254016
Epoch #160: loss=0.38764132852807187
Epoch #161: loss=0.2510461737260674
Epoch #162: loss=0.12939508868889374
Epoch #163: loss=0.13390158478057745
Epoch #164: loss=0.1859818392179229
Epoch #165: loss=0.1034262679291494
Epoch #166: loss=0.1410734626944318
Epoch #167: loss=0.15016269040378658
Epoch #168: loss=0.14320678986383206
Epoch #169: loss=0.16827809009136577
Epoch #170: loss=0.11814392154866998
Epoch #171: loss=0.12896937470544467
Epoch #172: loss=0.10168326239694249
Epoch #173: loss=0.11088241168269605
Epoch #174: loss=0.11849601239417538
Epoch #175: loss=0.10354202889809103
Epoch #176: loss=0.1291772320753697
Epoch #177: loss=0.18300497780243555
Epoch #178: loss=0.12939945275359083
Epoch #179: loss=0.1464949477689736
Epoch #180: loss=0.09574425655106704
Epoch #181: loss=0.1031216039237651
Epoch #182: loss=0.08269915808782433
Epoch #183: loss=0.0907741209322756
Epoch #184: loss=0.0842186880269737
Epoch #185: loss=0.1131268713736173
Epoch #186: loss=0.10132018115484354
Epoch #187: loss=0.0765708266954982
Epoch #188: loss=0.09372135302559896
Epoch #189: loss=0.10490343804386529
Epoch #190: loss=0.07955735602952314
Epoch #191: loss=0.06125421951891798
Epoch #192: loss=0.07776518623259934
Epoch #193: loss=0.106286402114413
Epoch #194: loss=0.10266037798966422
Epoch #195: loss=0.130220383518573
Epoch #196: loss=0.08046774199288903
Epoch #197: loss=0.07281833458127397
Epoch #198: loss=0.13964486201152657
Epoch #199: loss=0.14615380634186845
Epoch #200: loss=0.11948301496379303
Epoch #201: loss=0.08391744733759851
Epoch #202: loss=0.0975559637176268
Epoch #203: loss=0.15278620526871897
Epoch #204: loss=0.10982009141959927
Epoch #205: loss=0.1371077507395636
Epoch #206: loss=0.08117816173894839
Epoch #207: loss=0.11821479880900095
Epoch #208: loss=0.110010849537723
Epoch #209: loss=0.08023208718408238
Epoch #210: loss=0.11604784818535502
Epoch #211: loss=0.09107365600313201
Epoch #212: loss=0.08301335110357314
Epoch #213: loss=0.11682196904086706
Epoch #214: loss=0.16307712978485858
Epoch #215: loss=0.08333419929399635
Epoch #216: loss=0.0820163328087691
Epoch #217: loss=0.09106060961317836
Epoch #218: loss=0.16472015823378708
Epoch #219: loss=0.17921780309442317
Epoch #220: loss=0.1039338070673473
Epoch #221: loss=0.13786924060321215
Epoch #222: loss=0.09021702278969866
Epoch #223: loss=0.1109119907698848
Epoch #224: loss=0.08738496397255045
Epoch #225: loss=0.17806040648032317
Epoch #226: loss=0.12507540986619212
Epoch #227: loss=0.07053101463525584
Epoch #228: loss=0.08945428399425565
Epoch #229: loss=0.08544519013076117
Epoch #230: loss=0.06769833612171086
Epoch #231: loss=0.06308659126586987
Epoch #232: loss=0.06482652061139092
Epoch #233: loss=0.12880852344361218
Epoch #234: loss=0.06774814926426519
Epoch #235: loss=0.0639992028926358
Epoch #236: loss=0.09609787070164175
Epoch #237: loss=0.11765200052071702
Epoch #238: loss=0.2636175582919157
Epoch #239: loss=0.21357928035837231
Epoch #240: loss=0.10784942097961903
Epoch #241: loss=0.4114918997229049
Epoch #242: loss=0.24567677904710625
Epoch #243: loss=0.14065154997462576
Epoch #244: loss=0.09587496417489919
Epoch #245: loss=0.09105875179397338
Epoch #246: loss=0.06928052654433431
Epoch #247: loss=0.07783979459693938
Epoch #248: loss=0.13597114694615206
Epoch #249: loss=0.09535206543902557

Training time: 0:31:46.161900

Finished.
n2one setting etth2_weather -> electricity
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth2_weather_epochs_250_seed_2021/model.pkl', muti_dataset='etth2_weather', random_seed=2021, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='electricity')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.5137e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=3.00271e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.5137e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.37785385367882596, 'MAE': 0.43218256311450987}
Finished.
------------------------- record done -------------------------
n2one setting etth2_weather -> exchange
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth2_weather_epochs_250_seed_2021/model.pkl', muti_dataset='etth2_weather', random_seed=2021, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='exchange')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=3.09666e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.19727564369464626, 'MAE': 0.309397293720989}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='etth2_exchange', random_seed=2021, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/etth2_exchange_epochs_250_seed_2021
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=4.586511452992757
Epoch #1: loss=2.1895824313163756
Epoch #2: loss=2.0051052808761596
Epoch #3: loss=1.8798025210698446
Epoch #4: loss=1.8408482551574707
Epoch #5: loss=1.8418120622634888
Epoch #6: loss=1.8092092752456665
Epoch #7: loss=1.674823260307312
Epoch #8: loss=1.6035961151123046
Epoch #9: loss=1.495607328414917
Epoch #10: loss=1.532883604367574
Epoch #11: loss=1.373661740620931
Epoch #12: loss=1.4304730415344238
Epoch #13: loss=1.3669740756352742
Epoch #14: loss=1.37639875014623
Epoch #15: loss=1.245117473602295
Epoch #16: loss=1.2448055108388265
Epoch #17: loss=1.183257532119751
Epoch #18: loss=1.2063857237497966
Epoch #19: loss=1.1735495726267497
Epoch #20: loss=1.1075509746869405
Epoch #21: loss=1.1037619868914286
Epoch #22: loss=1.0267831961313882
Epoch #23: loss=1.0628468910853068
Epoch #24: loss=1.053220021724701
Epoch #25: loss=1.0898614088694254
Epoch #26: loss=1.0087063908576965
Epoch #27: loss=1.0786096572875976
Epoch #28: loss=1.0794779698053996
Epoch #29: loss=0.9798320849736532
Epoch #30: loss=1.0953738252321878
Epoch #31: loss=0.9594676812489827
Epoch #32: loss=0.9731698433558146
Epoch #33: loss=0.920623783270518
Epoch #34: loss=0.835434631506602
Epoch #35: loss=0.8124614040056864
Epoch #36: loss=0.8526805162429809
Epoch #37: loss=0.8302009105682373
Epoch #38: loss=0.7547298868497213
Epoch #39: loss=0.7752368768056234
Epoch #40: loss=0.6545480767885844
Epoch #41: loss=0.8035698890686035
Epoch #42: loss=0.840166699886322
Epoch #43: loss=0.9126104156176249
Epoch #44: loss=0.6851842482884725
Epoch #45: loss=0.647237237294515
Epoch #46: loss=0.7386765162150065
Epoch #47: loss=0.5916948954264323
Epoch #48: loss=0.6594409883022309
Epoch #49: loss=0.7363827804724375
Epoch #50: loss=0.6136076211929321
Epoch #51: loss=0.6421732703844706
Epoch #52: loss=0.606801974773407
Epoch #53: loss=0.5967536707719167
Epoch #54: loss=0.5985271453857421
Epoch #55: loss=0.7129037419954936
Epoch #56: loss=0.6972365597883861
Epoch #57: loss=0.6326226770877839
Epoch #58: loss=0.6334084212779999
Epoch #59: loss=0.5137749115626017
Epoch #60: loss=0.6751675387223561
Epoch #61: loss=0.6155401567618052
Epoch #62: loss=0.5763500948746999
Epoch #63: loss=0.5182560324668884
Epoch #64: loss=0.5659796973069509
Epoch #65: loss=0.5646720409393311
Epoch #66: loss=0.48879966139793396
Epoch #67: loss=0.5659677187601725
Epoch #68: loss=0.4801042954126994
Epoch #69: loss=0.5165724029143651
Epoch #70: loss=0.5936985552310944
Epoch #71: loss=0.43330456217130026
Epoch #72: loss=0.4640768826007843
Epoch #73: loss=0.5176930159330368
Epoch #74: loss=0.5141438106695811
Epoch #75: loss=0.5554967492818832
Epoch #76: loss=0.4301919182141622
Epoch #77: loss=0.4741878738005956
Epoch #78: loss=0.44474191069602964
Epoch #79: loss=0.4254092961549759
Epoch #80: loss=0.5413487295309702
Epoch #81: loss=0.3944461911916733
Epoch #82: loss=0.39723959962526956
Epoch #83: loss=0.38139236668745674
Epoch #84: loss=0.3228772968053818
Epoch #85: loss=0.41609084804852803
Epoch #86: loss=0.49125290314356485
Epoch #87: loss=0.401343442996343
Epoch #88: loss=0.3424807826677958
Epoch #89: loss=0.4855276862780253
Epoch #90: loss=0.5003629932800929
Epoch #91: loss=0.484531307220459
Epoch #92: loss=0.49324773947397865
Epoch #93: loss=0.5426957319180171
Epoch #94: loss=0.48967038889726006
Epoch #95: loss=0.5075158605972926
Epoch #96: loss=0.4570304115613302
Epoch #97: loss=0.5024813731511434
Epoch #98: loss=0.5158442427714666
Epoch #99: loss=0.4042792449394862
Epoch #100: loss=0.3804920648535093
Epoch #101: loss=0.3407484362522761
Epoch #102: loss=0.30791861514250435
Epoch #103: loss=0.41934426724910734
Epoch #104: loss=0.3188319613536199
Epoch #105: loss=0.42319545646508533
Epoch #106: loss=0.3926139901081721
Epoch #107: loss=0.3563905348380407
Epoch #108: loss=0.35904410978158313
Epoch #109: loss=0.36953706641991935
Epoch #110: loss=0.40048188269138335
Epoch #111: loss=0.34535688658555347
Epoch #112: loss=0.3802256196737289
Epoch #113: loss=0.36776459018389385
Epoch #114: loss=0.4084586222966512
Epoch #115: loss=0.3411949669321378
Epoch #116: loss=0.31832366188367206
Epoch #117: loss=0.2735315268238386
Epoch #118: loss=0.38283438285191856
Epoch #119: loss=0.36328506966431934
Epoch #120: loss=0.31049970388412473
Epoch #121: loss=0.4371150424083074
Epoch #122: loss=0.4576226994395256
Epoch #123: loss=0.47227530578772225
Epoch #124: loss=0.30537096212307613
Epoch #125: loss=0.31510129670302073
Epoch #126: loss=0.3827001055081685
Epoch #127: loss=0.4017104183634122
Epoch #128: loss=0.3500443438688914
Epoch #129: loss=0.39711449046929675
Epoch #130: loss=0.3678608516852061
Epoch #131: loss=0.4510561486085256
Epoch #132: loss=0.38947279900312426
Epoch #133: loss=0.3214741145571073
Epoch #134: loss=0.3864074836174647
Epoch #135: loss=0.3364165335893631
Epoch #136: loss=0.2901699205239614
Epoch #137: loss=0.27015610635280607
Epoch #138: loss=0.2604188859462738
Epoch #139: loss=0.36712306588888166
Epoch #140: loss=0.2906461372971535
Epoch #141: loss=0.3389783243338267
Epoch #142: loss=0.28090310643116634
Epoch #143: loss=0.4075920045375824
Epoch #144: loss=0.29154414037863413
Epoch #145: loss=0.256530849635601
Epoch #146: loss=0.24729838371276855
Epoch #147: loss=0.2915242537856102
Epoch #148: loss=0.26358522375424703
Epoch #149: loss=0.24477445830901465
Epoch #150: loss=0.2416314959526062
Epoch #151: loss=0.2300167220334212
Epoch #152: loss=0.2544450417160988
Epoch #153: loss=0.2707451117535432
Epoch #154: loss=0.3302772214015325
Epoch #155: loss=0.4351810028155645
Epoch #156: loss=0.27697348097960156
Epoch #157: loss=0.3837446783979734
Epoch #158: loss=0.27802133510510124
Epoch #159: loss=0.22326556146144866
Epoch #160: loss=0.2439270108938217
Epoch #161: loss=0.24091635992129642
Epoch #162: loss=0.22318477630615235
Epoch #163: loss=0.20448726986845334
Epoch #164: loss=0.3369999185204506
Epoch #165: loss=0.2631474569439888
Epoch #166: loss=0.3141404042641322
Epoch #167: loss=0.27123191952705383
Epoch #168: loss=0.26799077689647677
Epoch #169: loss=0.21067814429601034
Epoch #170: loss=0.32073531995217003
Epoch #171: loss=0.2668613761663437
Epoch #172: loss=0.3104461818933487
Epoch #173: loss=0.38686833580334984
Epoch #174: loss=0.3232925663391749
Epoch #175: loss=0.31001407355070115
Epoch #176: loss=0.3120263978838921
Epoch #177: loss=0.3150638853510221
Epoch #178: loss=0.2479634925723076
Epoch #179: loss=0.22645594427982965
Epoch #180: loss=0.2625386081635952
Epoch #181: loss=0.303122441470623
Epoch #182: loss=0.3157155017058055
Epoch #183: loss=0.27559646318356196
Epoch #184: loss=0.348010623951753
Epoch #185: loss=0.2999415785074234
Epoch #186: loss=0.2263832872112592
Epoch #187: loss=0.27570991665124894
Epoch #188: loss=0.2448542465766271
Epoch #189: loss=0.26193390240271885
Epoch #190: loss=0.23633580083648364
Epoch #191: loss=0.22538679838180542
Epoch #192: loss=0.22298810631036758
Epoch #193: loss=0.2076329231262207
Epoch #194: loss=0.18850233107805253
Epoch #195: loss=0.24225669006506603
Epoch #196: loss=0.24935869326194127
Epoch #197: loss=0.3658406674861908
Epoch #198: loss=0.38505363861719766
Epoch #199: loss=0.2572083408633868
Epoch #200: loss=0.27567324340343474
Epoch #201: loss=0.2478797564903895
Epoch #202: loss=0.2303795004884402
Epoch #203: loss=0.22602778350313504
Epoch #204: loss=0.20803722540537517
Epoch #205: loss=0.21299461349844934
Epoch #206: loss=0.2255694421629111
Epoch #207: loss=0.250249969959259
Epoch #208: loss=0.1823025681078434
Epoch #209: loss=0.17363873620827994
Epoch #210: loss=0.2532473730544249
Epoch #211: loss=0.24476239134867986
Epoch #212: loss=0.214763226856788
Epoch #213: loss=0.20882606705029805
Epoch #214: loss=0.1944524313012759
Epoch #215: loss=0.22342105756203334
Epoch #216: loss=0.20779867122570675
Epoch #217: loss=0.18622391074895858
Epoch #218: loss=0.2716493546962738
Epoch #219: loss=0.47676687041918436
Epoch #220: loss=0.34618282069762546
Epoch #221: loss=0.26876095434029895
Epoch #222: loss=0.24615083485841752
Epoch #223: loss=0.24721922626097997
Epoch #224: loss=0.17525096262494724
Epoch #225: loss=0.21339212507009506
Epoch #226: loss=0.19250324442982675
Epoch #227: loss=0.23262205198407174
Epoch #228: loss=0.26511221155524256
Epoch #229: loss=0.23090120851993562
Epoch #230: loss=0.23082596560319266
Epoch #231: loss=0.18198687881231307
Epoch #232: loss=0.1961959679921468
Epoch #233: loss=0.24039794305960338
Epoch #234: loss=0.21484659612178802
Epoch #235: loss=0.231160007417202
Epoch #236: loss=0.18513443196813265
Epoch #237: loss=0.2281165326635043
Epoch #238: loss=0.23544228002429007
Epoch #239: loss=0.17397841960191726
Epoch #240: loss=0.17686005632082621
Epoch #241: loss=0.2200787305831909
Epoch #242: loss=0.1753340353568395
Epoch #243: loss=0.1776590973138809
Epoch #244: loss=0.22557139645020166
Epoch #245: loss=0.1598469523092111
Epoch #246: loss=0.1737581176062425
Epoch #247: loss=0.20056566273172696
Epoch #248: loss=0.15764899278680483
Epoch #249: loss=0.2001379534602165

Training time: 0:10:07.422992

Finished.
n2one setting etth2_exchange -> electricity
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth2_exchange_epochs_250_seed_2021/model.pkl', muti_dataset='etth2_exchange', random_seed=2021, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='electricity')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.2007e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.55744e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.2007e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.3553967449629746, 'MAE': 0.4249472379104556}
Finished.
------------------------- record done -------------------------
n2one setting etth2_exchange -> weather
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth2_exchange_epochs_250_seed_2021/model.pkl', muti_dataset='etth2_exchange', random_seed=2021, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='weather')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=9.53428e-09): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.80801e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=4.60027e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.4685156723094341, 'MAE': 0.49873780171053644}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='ettm1_ettm2', random_seed=2021, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/ettm1_ettm2_epochs_250_seed_2021
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=5.399217204614119
Epoch #1: loss=2.666775768453425
Epoch #2: loss=2.3240102854642
Epoch #3: loss=2.1442027471282263
Epoch #4: loss=1.92507911270315
Epoch #5: loss=1.7574984648010947
Epoch #6: loss=1.5621938705444336
Epoch #7: loss=1.4553947069428184
Epoch #8: loss=1.3030164295976812
Epoch #9: loss=1.3275959600101819
Epoch #10: loss=1.135668697682294
Epoch #11: loss=1.21322793581269
Epoch #12: loss=1.1229492913592944
Epoch #13: loss=1.0102988318963484
Epoch #14: loss=0.932379728013819
Epoch #15: loss=1.1037811555645682
Epoch #16: loss=0.8892132639884949
Epoch #17: loss=0.8129742443561554
Epoch #18: loss=0.7585934590209614
Epoch #19: loss=0.859804245558652
Epoch #20: loss=0.8834150975400751
Epoch #21: loss=0.7813876027410681
Epoch #22: loss=0.7576062137430365
Epoch #23: loss=0.7130916795947335
Epoch #24: loss=0.7250691110437567
Epoch #25: loss=0.5977998931299556
Epoch #26: loss=0.7242798019539226
Epoch #27: loss=0.6066760637543418
Epoch #28: loss=0.6399658566171472
Epoch #29: loss=0.5725594799626957
Epoch #30: loss=0.6079696836796674
Epoch #31: loss=0.6365590217438611
Epoch #32: loss=0.6586988839236173
Epoch #33: loss=0.5893726484342054
Epoch #34: loss=0.5116297724572095
Epoch #35: loss=0.49032594534483825
Epoch #36: loss=0.4842045280066403
Epoch #37: loss=0.5313876569271088
Epoch #38: loss=0.4582314139062708
Epoch #39: loss=0.5298391783779318
Epoch #40: loss=0.4322875304655595
Epoch #41: loss=0.5702804056080905
Epoch #42: loss=0.4739689745686271
Epoch #43: loss=0.49333045970309863
Epoch #44: loss=0.4090403657067906
Epoch #45: loss=0.4286733174865896
Epoch #46: loss=0.4301019568334926
Epoch #47: loss=0.3996408690105785
Epoch #48: loss=0.3779805783521045
Epoch #49: loss=0.4184803298928521
Epoch #50: loss=0.5069283165714957
Epoch #51: loss=0.37712169777263294
Epoch #52: loss=0.3774296126582406
Epoch #53: loss=0.37245497378436004
Epoch #54: loss=0.35518008267337625
Epoch #55: loss=0.3853145485574549
Epoch #56: loss=0.33026558838107367
Epoch #57: loss=0.32803499563173816
Epoch #58: loss=0.3073915188962763
Epoch #59: loss=0.2876727188175375
Epoch #60: loss=0.28822656179016287
Epoch #61: loss=0.2721034132621505
Epoch #62: loss=0.29566202448173
Epoch #63: loss=0.2797336314212192
Epoch #64: loss=0.3650153489275412
Epoch #65: loss=0.2802334515885873
Epoch #66: loss=0.24952199242331766
Epoch #67: loss=0.2616364143111489
Epoch #68: loss=0.24022447521036322
Epoch #69: loss=0.22036166963252155
Epoch #70: loss=0.26365649971095
Epoch #71: loss=0.25420272824439133
Epoch #72: loss=0.32822043008425017
Epoch #73: loss=0.25543549995530734
Epoch #74: loss=0.30465417219833896
Epoch #75: loss=0.20901271294463764
Epoch #76: loss=0.21826325086030093
Epoch #77: loss=0.2609833892096173
Epoch #78: loss=0.21557425470514732
Epoch #79: loss=0.1540961770171469
Epoch #80: loss=0.29470048167488794
Epoch #81: loss=0.21263220127333293
Epoch #82: loss=0.1920613975010135
Epoch #83: loss=0.16530494696714662
Epoch #84: loss=0.17308155142448164
Epoch #85: loss=0.18723341111432423
Epoch #86: loss=0.12022519958290187
Epoch #87: loss=0.13409118337387388
Epoch #88: loss=0.12544214623895558
Epoch #89: loss=0.1312816965986382
Epoch #90: loss=0.13494528897783972
Epoch #91: loss=0.16081337732347575
Epoch #92: loss=0.26663546094840224
Epoch #93: loss=0.19248734753240238
Epoch #94: loss=0.17504211955449797
Epoch #95: loss=0.11912241832099178
Epoch #96: loss=0.11462189053947275
Epoch #97: loss=0.12528526004065166
Epoch #98: loss=0.12910396080802788
Epoch #99: loss=0.13733679344030944
Epoch #100: loss=0.10603527995673093
Epoch #101: loss=0.11591871353712949
Epoch #102: loss=0.10454966373402964
Epoch #103: loss=0.19096418978138405
Epoch #104: loss=0.1276040883226828
Epoch #105: loss=0.11538555269891565
Epoch #106: loss=0.11945699036798695
Epoch #107: loss=0.14932383139702407
Epoch #108: loss=0.11279043335128915
Epoch #109: loss=0.11757542413066734
Epoch #110: loss=0.111801705238494
Epoch #111: loss=0.11402447741817344
Epoch #112: loss=0.1096733058379455
Epoch #113: loss=0.1976818817413666
Epoch #114: loss=0.1733807926489548
Epoch #115: loss=0.2556999044662172
Epoch #116: loss=0.13893785463138061
Epoch #117: loss=0.0974676835943352
Epoch #118: loss=0.10258215208622543
Epoch #119: loss=0.11029714371331713
Epoch #120: loss=0.10023463127965276
Epoch #121: loss=0.16775914493270896
Epoch #122: loss=0.11692136983302506
Epoch #123: loss=0.08554815848103979
Epoch #124: loss=0.08439486829394643
Epoch #125: loss=0.08322323189879005
Epoch #126: loss=0.07143699771470645
Epoch #127: loss=0.07333244061605497
Epoch #128: loss=0.05959022646261887
Epoch #129: loss=0.051776632835919205
Epoch #130: loss=0.0745144070718776
Epoch #131: loss=0.05429576091806997
Epoch #132: loss=0.08626433936032382
Epoch #133: loss=0.08573338126933033
Epoch #134: loss=0.05977574380284006
Epoch #135: loss=0.06772448495030403
Epoch #136: loss=0.08193623198365624
Epoch #137: loss=0.05786438646133651
Epoch #138: loss=0.05747096452184699
Epoch #139: loss=0.12847843139686368
Epoch #140: loss=0.11666987057436597
Epoch #141: loss=0.07731415957889774
Epoch #142: loss=0.06988894075832584
Epoch #143: loss=0.06323528687723658
Epoch #144: loss=0.061255822381512684
Epoch #145: loss=0.09380735567008908
Epoch #146: loss=0.0954798258502375
Epoch #147: loss=0.09674040355126966
Epoch #148: loss=0.07241350344636223
Epoch #149: loss=0.0711963117461313
Epoch #150: loss=0.04587956378236413
Epoch #151: loss=0.06255185019902208
Epoch #152: loss=0.06288446646861055
Epoch #153: loss=0.06302864815701138
Epoch #154: loss=0.11668961444361643
Epoch #155: loss=0.11921430472284555
Epoch #156: loss=0.13351988208226181
Epoch #157: loss=0.07814690419896082
Epoch #158: loss=0.07842826106670228
Epoch #159: loss=0.04723135635934093
Epoch #160: loss=0.08742166217416525
Epoch #161: loss=0.17760718342932788
Epoch #162: loss=0.11766105001284317
Epoch #163: loss=0.09126154494217852
Epoch #164: loss=0.07309635855596174
Epoch #165: loss=0.08211481986059384
Epoch #166: loss=0.18494867723943156
Epoch #167: loss=0.08053570765663277
Epoch #168: loss=0.10772066165439108
Epoch #169: loss=0.06487313971262086
Epoch #170: loss=0.04606321530247277
Epoch #171: loss=0.04788738137788393
Epoch #172: loss=0.038951738314195114
Epoch #173: loss=0.053439080037853935
Epoch #174: loss=0.0698200324550271
Epoch #175: loss=0.07641973033208739
Epoch #176: loss=0.06573633680289442
Epoch #177: loss=0.055740175218406046
Epoch #178: loss=0.06188610263846137
Epoch #179: loss=0.0676390201869336
Epoch #180: loss=0.06979425137185237
Epoch #181: loss=0.05745592535558072
Epoch #182: loss=0.05767801780761643
Epoch #183: loss=0.05248392673886635
Epoch #184: loss=0.038896995300257746
Epoch #185: loss=0.03883400732990016
Epoch #186: loss=0.04048287711868232
Epoch #187: loss=0.04255341039970517
Epoch #188: loss=0.04912708009677855
Epoch #189: loss=0.05603328046642921
Epoch #190: loss=0.11466798207468608
Epoch #191: loss=0.092736952514811
Epoch #192: loss=0.06336711563000624
Epoch #193: loss=0.03615660559047352
Epoch #194: loss=0.07692860287021507
Epoch #195: loss=0.04364625326442448
Epoch #196: loss=0.03783358218656345
Epoch #197: loss=0.04776564881798218
Epoch #198: loss=0.04255560167472471
Epoch #199: loss=0.04657214850356633
Epoch #200: loss=0.16236445350064474
Epoch #201: loss=0.04474221144548871
Epoch #202: loss=0.10200326771221378
Epoch #203: loss=0.07780627796257084
Epoch #204: loss=0.08715060471811077
Epoch #205: loss=0.04032401613552462
Epoch #206: loss=0.021951078564267267
Epoch #207: loss=0.04209589238532565
Epoch #208: loss=0.034462345340712505
Epoch #209: loss=0.05945921765471047
Epoch #210: loss=0.0689107351089743
Epoch #211: loss=0.06490286795253103
Epoch #212: loss=0.0370708837766539
Epoch #213: loss=0.06798062854531137
Epoch #214: loss=0.06764555476944555
Epoch #215: loss=0.03865993899208578
Epoch #216: loss=0.04346851716664704
Epoch #217: loss=0.07200488918037577
Epoch #218: loss=0.11276779074052518
Epoch #219: loss=0.12652246450836008
Epoch #220: loss=0.08877298362891782
Epoch #221: loss=0.048499125987291336
Epoch #222: loss=0.05314198767089031
Epoch #223: loss=0.050848450723358175
Epoch #224: loss=0.03748384685340253
Epoch #225: loss=0.027043316449800677
Epoch #226: loss=0.03682315421544693
Epoch #227: loss=0.041655455588955774
Epoch #228: loss=0.04049631167965179
Epoch #229: loss=0.02962456658397886
Epoch #230: loss=0.03338116729124026
Epoch #231: loss=0.039973315042020244
Epoch #232: loss=0.09942178572104736
Epoch #233: loss=0.05408892027017745
Epoch #234: loss=0.06255836609158326
Epoch #235: loss=0.06241487254473296
Epoch #236: loss=0.05105834854343398
Epoch #237: loss=0.05257570749910718
Epoch #238: loss=0.06557201244868338
Epoch #239: loss=0.0885259088721465
Epoch #240: loss=0.06259887487712232
Epoch #241: loss=0.0413927920374342
Epoch #242: loss=0.037659983383491635
Epoch #243: loss=0.05635340562598272
Epoch #244: loss=0.12220077496021986
Epoch #245: loss=0.0816506419161504
Epoch #246: loss=0.04588771717284213
Epoch #247: loss=0.03334279129789634
Epoch #248: loss=0.042669747744432905
Epoch #249: loss=0.24936040097170256

Training time: 0:22:33.203897

Finished.
n2one setting ettm1_ettm2 -> electricity
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/ettm1_ettm2_epochs_250_seed_2021/model.pkl', muti_dataset='ettm1_ettm2', random_seed=2021, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='electricity')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.35929e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.65968e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.35929e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.36871320167607075, 'MAE': 0.42662767590998724}
Finished.
------------------------- record done -------------------------
n2one setting ettm1_ettm2 -> weather
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/ettm1_ettm2_epochs_250_seed_2021/model.pkl', muti_dataset='ettm1_ettm2', random_seed=2021, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='weather')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.46374e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.76994e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.4040766872707967, 'MAE': 0.4894719530496015}
Finished.
------------------------- record done -------------------------
n2one setting ettm1_ettm2 -> exchange
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/ettm1_ettm2_epochs_250_seed_2021/model.pkl', muti_dataset='ettm1_ettm2', random_seed=2021, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='exchange')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=3.51297e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.2618055628614516, 'MAE': 0.3422622226521872}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='ettm1_electricity', random_seed=2021, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/ettm1_electricity_epochs_250_seed_2021
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=1.8054934188903096
Epoch #1: loss=0.7520519655691572
Epoch #2: loss=0.5274580702724228
Epoch #3: loss=0.3896555486303496
Epoch #4: loss=0.33096316784440755
Epoch #5: loss=0.31310576312692767
Epoch #6: loss=0.24822610841099038
Epoch #7: loss=0.2163957187075572
Epoch #8: loss=0.21269287234060016
Epoch #9: loss=0.186091332797365
Epoch #10: loss=0.15391254425048828
Epoch #11: loss=0.15330545616571803
Epoch #12: loss=0.13165578534490013
Epoch #13: loss=0.13832570641604533
Epoch #14: loss=0.12611434722027506
Epoch #15: loss=0.10958424948874008
Epoch #16: loss=0.08999097925689385
Epoch #17: loss=0.09169831364808312
Epoch #18: loss=0.07613460269913706
Epoch #19: loss=0.09349857569189675
Epoch #20: loss=0.0746989446041365
Epoch #21: loss=0.08334018066469355
Epoch #22: loss=0.062159367062779795
Epoch #23: loss=0.06403472739081066
Epoch #24: loss=0.058281011427808775
Epoch #25: loss=0.06605771999716399
Epoch #26: loss=0.05941978982559978
Epoch #27: loss=0.04801834181809506
Epoch #28: loss=0.0464935832017337
Epoch #29: loss=0.04532929006346827
Epoch #30: loss=0.04611277831606118
Epoch #31: loss=0.044413738673642636
Epoch #32: loss=0.05122751865188131
Epoch #33: loss=0.03941313174962773
Epoch #34: loss=0.03730276112615815
Epoch #35: loss=0.03861209435854107
Epoch #36: loss=0.03121519767109821
Epoch #37: loss=0.03138964079427028
Epoch #38: loss=0.03930253229042268
Epoch #39: loss=0.031573236508979405
Epoch #40: loss=0.02982449189392311
Epoch #41: loss=0.037237654248650845
Epoch #42: loss=0.02994128627455737
Epoch #43: loss=0.029357486977438296
Epoch #44: loss=0.03501392232782925
Epoch #45: loss=0.022869017838461167
Epoch #46: loss=0.027094311036691295
Epoch #47: loss=0.0340022800570208
Epoch #48: loss=0.021663304181637647
Epoch #49: loss=0.02811011142400374
Epoch #50: loss=0.02273690735560797
Epoch #51: loss=0.030483698445731348
Epoch #52: loss=0.060865854864788564
Epoch #53: loss=0.02756174135580098
Epoch #54: loss=0.0253849713081292
Epoch #55: loss=0.02255090050390888
Epoch #56: loss=0.051867982941744176
Epoch #57: loss=0.042154113135121046
Epoch #58: loss=0.023784104909983474
Epoch #59: loss=0.027887910509801817
Epoch #60: loss=0.025146453432719144
Epoch #61: loss=0.019155689442238534
Epoch #62: loss=0.02076621244609614
Epoch #63: loss=0.018163784232456237
Epoch #64: loss=0.019242480944586828
Epoch #65: loss=0.02473133610679308
Epoch #66: loss=0.023655145885969838
Epoch #67: loss=0.021695897016808928
Epoch #68: loss=0.03044867130131068
Epoch #69: loss=0.02264158087761523
Epoch #70: loss=0.01193948473650906
Epoch #71: loss=0.032049361997878695
Epoch #72: loss=0.02065475058516751
Epoch #73: loss=0.019152282901123693
Epoch #74: loss=0.01915498405340105
Epoch #75: loss=0.022204262906466668
Epoch #76: loss=0.01642352270722636
Epoch #77: loss=0.023668644107535976
Epoch #78: loss=0.02128105466352237
Epoch #79: loss=0.019013207507143023
Epoch #80: loss=0.016020130538606053
Epoch #81: loss=0.016072094637174048
Epoch #82: loss=0.01868803710911664
Epoch #83: loss=0.021624987854142616
Epoch #84: loss=0.021799912763351338
Epoch #85: loss=0.027612990504397215
Epoch #86: loss=0.015769699951941944
Epoch #87: loss=0.012381372884959432
Epoch #88: loss=0.02416874357977467
Epoch #89: loss=0.031761282480410186
Epoch #90: loss=0.020981046568927724
Epoch #91: loss=0.01848726089870563
Epoch #92: loss=0.019710848320427025
Epoch #93: loss=0.014522621112858809
Epoch #94: loss=0.011091167094075144
Epoch #95: loss=0.013653861681007814
Epoch #96: loss=0.018521787680430248
Epoch #97: loss=0.016547255657216344
Epoch #98: loss=0.013855897803076948
Epoch #99: loss=0.013813836410633254
Epoch #100: loss=0.01630337974487899
Epoch #101: loss=0.0211072666503921
Epoch #102: loss=0.01763234824045275
Epoch #103: loss=0.015814736377306182
Epoch #104: loss=0.02283952816221891
Epoch #105: loss=0.019034316626002912
Epoch #106: loss=0.020489371811244526
Epoch #107: loss=0.015193602379484394
Epoch #108: loss=0.017034111768028316
Epoch #109: loss=0.02551013231417725
Epoch #110: loss=0.016393557667426475
Epoch #111: loss=0.014629385825940857
Epoch #112: loss=0.01028289629727832
Epoch #113: loss=0.009832743318268961
Epoch #114: loss=0.01358712220168953
Epoch #115: loss=0.02788359222439937
Epoch #116: loss=0.015869992607376954
Epoch #117: loss=0.01819541186310985
Epoch #118: loss=0.01985724943098107
Epoch #119: loss=0.010844147843392903
Epoch #120: loss=0.016451160503405494
Epoch #121: loss=0.007755616326509212
Epoch #122: loss=0.015037797140876247
Epoch #123: loss=0.019267177477504227
Epoch #124: loss=0.010866686018009906
Epoch #125: loss=0.014682288713969222
Epoch #126: loss=0.01104308297196319
Epoch #127: loss=0.013406308241965563
Epoch #128: loss=0.013908288931629778
Epoch #129: loss=0.011632749224530465
Epoch #130: loss=0.020169524002556567
Epoch #131: loss=0.013767962307172704
Epoch #132: loss=0.015763125630447652
Epoch #133: loss=0.010913113263963038
Epoch #134: loss=0.012239975139782035
Epoch #135: loss=0.023566485068610733
Epoch #136: loss=0.014881983061365962
Epoch #137: loss=0.010906962392759139
Epoch #138: loss=0.017755031014311532
Epoch #139: loss=0.023936182032416417
Epoch #140: loss=0.018173107703539672
Epoch #141: loss=0.013481739170867557
Epoch #142: loss=0.01665642822293862
Epoch #143: loss=0.011111312624205925
Epoch #144: loss=0.009552247781083493
Epoch #145: loss=0.016434274425646656
Epoch #146: loss=0.01984696217576962
Epoch #147: loss=0.01124104454751191
Epoch #148: loss=0.01894449966493994
Epoch #149: loss=0.009269332742537316
Epoch #150: loss=0.008736864841264022
Epoch #151: loss=0.00809689643664383
Epoch #152: loss=0.015574802279350325
Epoch #153: loss=0.019446688851114384
Epoch #154: loss=0.020771629609383575
Epoch #155: loss=0.013672411376582346
Epoch #156: loss=0.01512405523646017
Epoch #157: loss=0.010332869480648751
Epoch #158: loss=0.0075854741409280705
Epoch #159: loss=0.01180944240551038
Epoch #160: loss=0.008008226445726685
Epoch #161: loss=0.010990999058360257
Epoch #162: loss=0.012087657079798399
Epoch #163: loss=0.01714655842017719
Epoch #164: loss=0.013976810708310994
Epoch #165: loss=0.009466594651219412
Epoch #166: loss=0.010240492339157041
Epoch #167: loss=0.010069117795098123
Epoch #168: loss=0.012951666796060997
Epoch #169: loss=0.01326554992757962
Epoch #170: loss=0.00821678704978037
Epoch #171: loss=0.012825620282828494
Epoch #172: loss=0.009496026352982048
Epoch #173: loss=0.013049027838685313
Epoch #174: loss=0.009602947328562275
Epoch #175: loss=0.0088346491248525
Epoch #176: loss=0.021071186055308656
Epoch #177: loss=0.01582626543966464
Epoch #178: loss=0.01258555079946373
Epoch #179: loss=0.01009898498264795
Epoch #180: loss=0.013727644340244559
Epoch #181: loss=0.009477958110503358
Epoch #182: loss=0.013699329718621308
Epoch #183: loss=0.009271366625683416
Epoch #184: loss=0.008567403217638051
Epoch #185: loss=0.011519469499955493
Epoch #186: loss=0.012689024963344626
Epoch #187: loss=0.012948321966988774
Epoch #188: loss=0.017957717317005014
Epoch #189: loss=0.01090524634123704
Epoch #190: loss=0.008348615114936179
Epoch #191: loss=0.011949466370380657
Epoch #192: loss=0.011382037088035112
Epoch #193: loss=0.007337158463668452
Epoch #194: loss=0.008948544661671552
Epoch #195: loss=0.010091981745545237
Epoch #196: loss=0.006592992198815168
Epoch #197: loss=0.015386845798516388
Epoch #198: loss=0.023603115829355814
Epoch #199: loss=0.01185699044611075
Epoch #200: loss=0.012570428285230209
Epoch #201: loss=0.008330139763080318
Epoch #202: loss=0.009926785035981062
Epoch #203: loss=0.00817236218707361
Epoch #204: loss=0.012045543728338791
Epoch #205: loss=0.010261225817424908
Epoch #206: loss=0.01151533414141965
Epoch #207: loss=0.010442707486782386
Epoch #208: loss=0.009577387317780866
Epoch #209: loss=0.012672252867932584
Epoch #210: loss=0.007221159396965915
Epoch #211: loss=0.011771066091514293
Epoch #212: loss=0.01252123687063212
Epoch #213: loss=0.009913468539433593
Epoch #214: loss=0.025977107979120905
Epoch #215: loss=0.04112145466153532
Epoch #216: loss=0.024514450992672167
Epoch #217: loss=0.008105262029112712
Epoch #218: loss=0.007562662960327839
Epoch #219: loss=0.007767986176657864
Epoch #220: loss=0.0076356670802216475
Epoch #221: loss=0.009544721619942765
Epoch #222: loss=0.008464898350957152
Epoch #223: loss=0.012565329487604531
Epoch #224: loss=0.011151395569112241
Epoch #225: loss=0.01282795771734834
Epoch #226: loss=0.012077294166725134
Epoch #227: loss=0.00802625964368702
Epoch #228: loss=0.009122638987708298
Epoch #229: loss=0.00838344745760144
Epoch #230: loss=0.013416255235178283
Epoch #231: loss=0.009384899428840137
Epoch #232: loss=0.006369616587436617
Epoch #233: loss=0.008837012032149304
Epoch #234: loss=0.009338084651007883
Epoch #235: loss=0.011346246389106213
Epoch #236: loss=0.010145977671056178
Epoch #237: loss=0.009047831285777165
Epoch #238: loss=0.030166913880945567
Epoch #239: loss=0.012649146599782047
Epoch #240: loss=0.01640858624799525
Epoch #241: loss=0.008927491535513286
Epoch #242: loss=0.004695113954624843
Epoch #243: loss=0.008720537663647113
Epoch #244: loss=0.005786271290436348
Epoch #245: loss=0.008665771285669473
Epoch #246: loss=0.010824037594117606
Epoch #247: loss=0.008219022321527408
Epoch #248: loss=0.010031012575743799
Epoch #249: loss=0.011877389359777949

Training time: 4:42:32.188987

Finished.
n2one setting ettm1_electricity -> weather
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/ettm1_electricity_epochs_250_seed_2021/model.pkl', muti_dataset='ettm1_electricity', random_seed=2021, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='weather')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.31253e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.52617e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.31253e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.6806717410894771, 'MAE': 0.6358943878517267}
Finished.
------------------------- record done -------------------------
n2one setting ettm1_electricity -> exchange
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/ettm1_electricity_epochs_250_seed_2021/model.pkl', muti_dataset='ettm1_electricity', random_seed=2021, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='exchange')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=5.53213e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.31621969807816147, 'MAE': 0.3732577859801445}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='ettm1_traffic', random_seed=2021, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/ettm1_traffic_epochs_250_seed_2021
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=1.1263322347491331
Epoch #1: loss=0.3912148522927862
Epoch #2: loss=0.2820911616650234
Epoch #3: loss=0.2053308549922766
Epoch #4: loss=0.1699216541709359
Epoch #5: loss=0.14336143005191257
Epoch #6: loss=0.12454134057929904
Epoch #7: loss=0.09611085114652826
Epoch #8: loss=0.0880176933495091
Epoch #9: loss=0.07786354478790007
Epoch #10: loss=0.06725248644599971
Epoch #11: loss=0.0666695793723647
Epoch #12: loss=0.06031209809725884
Epoch #13: loss=0.057957182998927936
Epoch #14: loss=0.056340719882040396
Epoch #15: loss=0.05955626694048154
Epoch #16: loss=0.056163089318661805
Epoch #17: loss=0.04623210911141903
Epoch #18: loss=0.04049004268067604
Epoch #19: loss=0.04552674774154658
Epoch #20: loss=0.042288864493628224
Epoch #21: loss=0.0496198694525182
Epoch #22: loss=0.03780782375887958
Epoch #23: loss=0.03088932288549243
Epoch #24: loss=0.03347742368887474
Epoch #25: loss=0.033469099268335356
Epoch #26: loss=0.039684438329608204
Epoch #27: loss=0.027199442388238074
Epoch #28: loss=0.03094659120850969
Epoch #29: loss=0.03153342356129772
Epoch #30: loss=0.02799256021684443
Epoch #31: loss=0.03326528512111055
Epoch #32: loss=0.025090445484459305
Epoch #33: loss=0.02836195411456494
Epoch #34: loss=0.024398398967667826
Epoch #35: loss=0.02581708515352737
Epoch #36: loss=0.04406510973426557
Epoch #37: loss=0.03145943784151218
Epoch #38: loss=0.020624607509999022
Epoch #39: loss=0.0227641513565009
Epoch #40: loss=0.02176801500485128
Epoch #41: loss=0.022001682602592935
Epoch #42: loss=0.021542192684787347
Epoch #43: loss=0.019813730497844517
Epoch #44: loss=0.025212925621831122
Epoch #45: loss=0.022439173226394635
Epoch #46: loss=0.017216310407251613
Epoch #47: loss=0.023143136080136822
Epoch #48: loss=0.022544297508968473
Epoch #49: loss=0.01915304126409453
Epoch #50: loss=0.016244723969501874
Epoch #51: loss=0.020908511410149152
Epoch #52: loss=0.024894917805512187
Epoch #53: loss=0.020013586212019464
Epoch #54: loss=0.01897409737506457
Epoch #55: loss=0.020752155826878303
Epoch #56: loss=0.01740242131202421
Epoch #57: loss=0.016962286138630774
Epoch #58: loss=0.02309948284764916
Epoch #59: loss=0.01932819484726911
Epoch #60: loss=0.013155888210943946
Epoch #61: loss=0.017282042643184004
Epoch #62: loss=0.032012398404782406
Epoch #63: loss=0.017909184567372123
Epoch #64: loss=0.015784756089923403
Epoch #65: loss=0.013990785590636214
Epoch #66: loss=0.019489272280532453
Epoch #67: loss=0.014273897707677006
Epoch #68: loss=0.015079851753002659
Epoch #69: loss=0.01734379346200819
Epoch #70: loss=0.014145276162379719
Epoch #71: loss=0.015184714295728001
Epoch #72: loss=0.017628163109890335
Epoch #73: loss=0.020186496630830015
Epoch #74: loss=0.01995258926886549
Epoch #75: loss=0.01239769441791184
Epoch #76: loss=0.014503421324914917
Epoch #77: loss=0.01833831958389651
Epoch #78: loss=0.015622853001959426
Epoch #79: loss=0.0120933126119597
Epoch #80: loss=0.017770666805299196
Epoch #81: loss=0.019180088958135975
Epoch #82: loss=0.012826939905242342
Epoch #83: loss=0.013933263644538734
Epoch #84: loss=0.02053961944428965
Epoch #85: loss=0.01306727483566159
Epoch #86: loss=0.01536971795710576
Epoch #87: loss=0.014800835667154831
Epoch #88: loss=0.019950464844595926
Epoch #89: loss=0.013935465836910294
Epoch #90: loss=0.014071602343044479
Epoch #91: loss=0.031815385877775916
Epoch #92: loss=0.018308562278200286
Epoch #93: loss=0.012897302854414067
Epoch #94: loss=0.012972039783477222
Epoch #95: loss=0.015412841770181353
Epoch #96: loss=0.014624558498134531
Epoch #97: loss=0.015370288598409856
Epoch #98: loss=0.014329284028497386
Epoch #99: loss=0.01227335016135304
Epoch #100: loss=0.014068989233957403
Epoch #101: loss=0.01083492837788405
Epoch #102: loss=0.015669308838474255
Epoch #103: loss=0.011968513652766315
Epoch #104: loss=0.013652758856957676
Epoch #105: loss=0.01330830193641313
Epoch #106: loss=0.013624901371070036
Epoch #107: loss=0.010748974174284562
Epoch #108: loss=0.015205973215922042
Epoch #109: loss=0.014218143910458439
Epoch #110: loss=0.01465880831290556
Epoch #111: loss=0.01255665980943533
Epoch #112: loss=0.015746961388209926
Epoch #113: loss=0.015836472588512333
Epoch #114: loss=0.010575800058701292
Epoch #115: loss=0.017437497997589257
Epoch #116: loss=0.012126585435445494
Epoch #117: loss=0.012523632959256275
Epoch #118: loss=0.017807482234306338
Epoch #119: loss=0.014693546978482616
Epoch #120: loss=0.011210647696131901
Epoch #121: loss=0.01884358257628958
Epoch #122: loss=0.01271957271540498
Epoch #123: loss=0.011936239422508862
Epoch #124: loss=0.016552288029748275
Epoch #125: loss=0.009759571522063968
Epoch #126: loss=0.01313679686569868
Epoch #127: loss=0.0130198404029503
Epoch #128: loss=0.017429948841285105
Epoch #129: loss=0.010835691720550242
Epoch #130: loss=0.010251767804751755
Epoch #131: loss=0.012578113559504943
Epoch #132: loss=0.016122039740668868
Epoch #133: loss=0.011023891987844465
Epoch #134: loss=0.0120321176050712
Epoch #135: loss=0.01337618149794587
Epoch #136: loss=0.010210713044768546
Epoch #137: loss=0.009756055257957422
Epoch #138: loss=0.009909968421208264
Epoch #139: loss=0.010858729235864728
Epoch #140: loss=0.0126291972358726
Epoch #141: loss=0.009467837579191124
Epoch #142: loss=0.015806397456685904
Epoch #143: loss=0.011760710007003012
Epoch #144: loss=0.011117383567030932
Epoch #145: loss=0.01574222975418389
Epoch #146: loss=0.01449428567654034
Epoch #147: loss=0.027118406225585794
Epoch #148: loss=0.012527372643949234
Epoch #149: loss=0.012262972217452693
Epoch #150: loss=0.012762012413275115
Epoch #151: loss=0.009448762883603965
Epoch #152: loss=0.0094655818627752
Epoch #153: loss=0.010426963612667012
Epoch #154: loss=0.009171987980025963
Epoch #155: loss=0.01452452235709543
Epoch #156: loss=0.010388439682414723
Epoch #157: loss=0.009460480237765854
Epoch #158: loss=0.025071073299751778
Epoch #159: loss=0.011291280516528586
Epoch #160: loss=0.009979512407451752
Epoch #161: loss=0.011811591109242514
Epoch #162: loss=0.011258828028030255
Epoch #163: loss=0.010580766790723567
Epoch #164: loss=0.00879001738535965
Epoch #165: loss=0.014571669084086601
Epoch #166: loss=0.011300688375133348
Epoch #167: loss=0.008474084657716527
Epoch #168: loss=0.010852506275622538
Epoch #169: loss=0.009101121158129456
Epoch #170: loss=0.019854001019488463
Epoch #171: loss=0.00954192680516627
Epoch #172: loss=0.011456371441075178
Epoch #173: loss=0.01645741526294555
Epoch #174: loss=0.01238047751079396
Epoch #175: loss=0.007413033809193345
Epoch #176: loss=0.01476020012926787
Epoch #177: loss=0.01957206398410427
Epoch #178: loss=0.011192800266552718
Epoch #179: loss=0.007387648721064939
Epoch #180: loss=0.011985953999251203
Epoch #181: loss=0.009700832677731263
Epoch #182: loss=0.014635053575845016
Epoch #183: loss=0.011573440454703439
Epoch #184: loss=0.009460557838012426
Epoch #185: loss=0.008008856389716637
Epoch #186: loss=0.011125743458680459
Epoch #187: loss=0.013211681593939201
Epoch #188: loss=0.023301462475443124
Epoch #189: loss=0.012335445471744044
Epoch #190: loss=0.011771600999637909
Epoch #191: loss=0.010167410518028075
Epoch #192: loss=0.008397000214147124
Epoch #193: loss=0.010093111629291936
Epoch #194: loss=0.011201305591601575
Epoch #195: loss=0.009995644543725193
Epoch #196: loss=0.012074885284024737
Epoch #197: loss=0.011563409600249331
Epoch #198: loss=0.012534684060663647
Epoch #199: loss=0.007854923298895143
Epoch #200: loss=0.012476166497029467
Epoch #201: loss=0.010228378655024102
Epoch #202: loss=0.010326614719315487
Epoch #203: loss=0.011324327129295644
Epoch #204: loss=0.009654151706393986
Epoch #205: loss=0.0073677334635817065
Epoch #206: loss=0.011592784372440653
Epoch #207: loss=0.010168179372603043
Epoch #208: loss=0.008625927707510424
Epoch #209: loss=0.009274508543181867
Epoch #210: loss=0.009521017176577051
Epoch #211: loss=0.009829042678255372
Epoch #212: loss=0.01160719340581384
Epoch #213: loss=0.013149908663988229
Epoch #214: loss=0.008109065558378509
Epoch #215: loss=0.010330889953043923
Epoch #216: loss=0.011285833567023524
Epoch #217: loss=0.011343525445617012
Epoch #218: loss=0.008349267198804996
Epoch #219: loss=0.006699677809353071
Epoch #220: loss=0.008427383900458489
Epoch #221: loss=0.010379258978726926
Epoch #222: loss=0.009569185322926938
Epoch #223: loss=0.012809066275731562
Epoch #224: loss=0.011506560602443842
Epoch #225: loss=0.00902648339336766
Epoch #226: loss=0.00534564866178519
Epoch #227: loss=0.010527390888518066
Epoch #228: loss=0.010999582404441991
Epoch #229: loss=0.009935228819034077
Epoch #230: loss=0.005980147732620982
Epoch #231: loss=0.010825359976586683
Epoch #232: loss=0.009388756087816743
Epoch #233: loss=0.010732052344510136
Epoch #234: loss=0.019901907994676053
Epoch #235: loss=0.010321904025741326
Epoch #236: loss=0.012860184270782686
Epoch #237: loss=0.005413484585605869
Epoch #238: loss=0.009213966049906293
Epoch #239: loss=0.007792718518540164
Epoch #240: loss=0.010102484555762178
Epoch #241: loss=0.007351071448196108
Epoch #242: loss=0.011022325560798199
Epoch #243: loss=0.011340898126862486
Epoch #244: loss=0.009755008830302177
Epoch #245: loss=0.010264317910455607
Epoch #246: loss=0.0047803789393031275
Epoch #247: loss=0.010785798690471034
Epoch #248: loss=0.009379139569809484
Epoch #249: loss=0.008389550476827854

Training time: 10:15:11.674088

Finished.
n2one setting ettm1_traffic -> electricity
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/ettm1_traffic_epochs_250_seed_2021/model.pkl', muti_dataset='ettm1_traffic', random_seed=2021, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='electricity')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=9.55295e-09): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.90723e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=4.87489e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=9.55295e-09): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.4022189848741343, 'MAE': 0.452346069337385}
Finished.
------------------------- record done -------------------------
n2one setting ettm1_traffic -> weather
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/ettm1_traffic_epochs_250_seed_2021/model.pkl', muti_dataset='ettm1_traffic', random_seed=2021, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='weather')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.54819e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=3.11316e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.54819e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.8645751549607648, 'MAE': 0.7583783491508627}
Finished.
------------------------- record done -------------------------
n2one setting ettm1_traffic -> exchange
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/ettm1_traffic_epochs_250_seed_2021/model.pkl', muti_dataset='ettm1_traffic', random_seed=2021, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='exchange')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=5.85706e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.3210510399459757, 'MAE': 0.370515143001912}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='ettm1_weather', random_seed=2021, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/ettm1_weather_epochs_250_seed_2021
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=5.6760842155765845
Epoch #1: loss=2.6703229788187386
Epoch #2: loss=2.672926516146273
Epoch #3: loss=2.171053180823455
Epoch #4: loss=2.002017001847963
Epoch #5: loss=1.8031989400451247
Epoch #6: loss=1.787927862760183
Epoch #7: loss=1.5754667069460895
Epoch #8: loss=1.4602945720827258
Epoch #9: loss=1.6269992847700376
Epoch #10: loss=1.337890573450037
Epoch #11: loss=1.2325301202567849
Epoch #12: loss=1.1779757947535128
Epoch #13: loss=1.2139823468955788
Epoch #14: loss=1.0625962196169674
Epoch #15: loss=0.996276193373912
Epoch #16: loss=0.8836837024302096
Epoch #17: loss=0.936647304006525
Epoch #18: loss=0.9547721953005404
Epoch #19: loss=0.9263569181029861
Epoch #20: loss=0.9217340768994512
Epoch #21: loss=0.8554256993371088
Epoch #22: loss=0.8113136549253721
Epoch #23: loss=0.8112658745533711
Epoch #24: loss=0.731809591924822
Epoch #25: loss=0.7326796650886536
Epoch #26: loss=0.687252262154141
Epoch #27: loss=0.6844722386952993
Epoch #28: loss=0.678131290384241
Epoch #29: loss=0.6598663885851164
Epoch #30: loss=0.8433751979389706
Epoch #31: loss=0.6338299601464659
Epoch #32: loss=0.5822490945055678
Epoch #33: loss=0.618099864270236
Epoch #34: loss=0.6223274098860251
Epoch #35: loss=0.5490581369077837
Epoch #36: loss=0.543074989641035
Epoch #37: loss=0.5102360546588898
Epoch #38: loss=0.5106795631550454
Epoch #39: loss=0.6413657254463917
Epoch #40: loss=0.5707333506764593
Epoch #41: loss=0.48632983098158966
Epoch #42: loss=0.48495258028442795
Epoch #43: loss=0.4914515662837673
Epoch #44: loss=0.5910449358257087
Epoch #45: loss=0.5641988765549015
Epoch #46: loss=0.45399533252458313
Epoch #47: loss=0.42398194203505646
Epoch #48: loss=0.4380639984801009
Epoch #49: loss=0.39917925805658905
Epoch #50: loss=0.4507213508760607
Epoch #51: loss=0.3676760643720627
Epoch #52: loss=0.40301308841318695
Epoch #53: loss=0.38497550866088354
Epoch #54: loss=0.3643456085308178
Epoch #55: loss=0.4202080703264958
Epoch #56: loss=0.3895960969699396
Epoch #57: loss=0.4754490127434602
Epoch #58: loss=0.4539703984518309
Epoch #59: loss=0.4491292882610012
Epoch #60: loss=0.41479139835447876
Epoch #61: loss=0.4189009585896054
Epoch #62: loss=0.32685069699545166
Epoch #63: loss=0.3219519120615882
Epoch #64: loss=0.40257920123435353
Epoch #65: loss=0.3301948316999384
Epoch #66: loss=0.3072723367729703
Epoch #67: loss=0.22909696843173052
Epoch #68: loss=0.2793980889223717
Epoch #69: loss=0.27649639546871185
Epoch #70: loss=0.2666701525449753
Epoch #71: loss=0.27657381243802404
Epoch #72: loss=0.30264094673298503
Epoch #73: loss=0.31821577452324534
Epoch #74: loss=0.2599201188296885
Epoch #75: loss=0.30279703398008606
Epoch #76: loss=0.2468408734814541
Epoch #77: loss=0.2290391915956059
Epoch #78: loss=0.3004880185062821
Epoch #79: loss=0.21813948150422122
Epoch #80: loss=0.2253790856213183
Epoch #81: loss=0.22733579636425585
Epoch #82: loss=0.26068851069824117
Epoch #83: loss=0.18672780813397588
Epoch #84: loss=0.22062482886217735
Epoch #85: loss=0.465607258717756
Epoch #86: loss=0.2909306356230298
Epoch #87: loss=0.24680880980717168
Epoch #88: loss=0.20186221599578857
Epoch #89: loss=0.18857576557107875
Epoch #90: loss=0.23063988117752848
Epoch #91: loss=0.22312017046921961
Epoch #92: loss=0.17825765847354322
Epoch #93: loss=0.20546456486792178
Epoch #94: loss=0.21709940099232905
Epoch #95: loss=0.1599838301740788
Epoch #96: loss=0.19518237520714063
Epoch #97: loss=0.18119204849810214
Epoch #98: loss=0.2244251548639826
Epoch #99: loss=0.20708590141824773
Epoch #100: loss=0.4858697141344483
Epoch #101: loss=0.658040231144106
Epoch #102: loss=0.25639288006602107
Epoch #103: loss=0.22991546837461962
Epoch #104: loss=0.25933708029018865
Epoch #105: loss=0.33828435012617625
Epoch #106: loss=0.2858027652711482
Epoch #107: loss=0.250923363240184
Epoch #108: loss=0.23164756072534098
Epoch #109: loss=0.18450302469569282
Epoch #110: loss=0.25363508531370677
Epoch #111: loss=0.31068656452604243
Epoch #112: loss=0.19615605896389163
Epoch #113: loss=0.2347006384988089
Epoch #114: loss=0.16133376875439207
Epoch #115: loss=0.11699663837616509
Epoch #116: loss=0.18623338458505836
Epoch #117: loss=0.1394699416249185
Epoch #118: loss=0.12665638005411303
Epoch #119: loss=0.10486625920276384
Epoch #120: loss=0.10127744783420821
Epoch #121: loss=0.13676299804167166
Epoch #122: loss=0.1547473819070571
Epoch #123: loss=0.11795951999925278
Epoch #124: loss=0.13152794288219632
Epoch #125: loss=0.20523403031197754
Epoch #126: loss=0.13143401351329442
Epoch #127: loss=0.11628410028847488
Epoch #128: loss=0.14693330459900805
Epoch #129: loss=0.1408988828393253
Epoch #130: loss=0.10409188200090383
Epoch #131: loss=0.09730344658365121
Epoch #132: loss=0.13480141446799845
Epoch #133: loss=0.10174996345429807
Epoch #134: loss=0.113961369554336
Epoch #135: loss=0.10440586034107853
Epoch #136: loss=0.09786954970174544
Epoch #137: loss=0.0954295450167076
Epoch #138: loss=0.10419956972268787
Epoch #139: loss=0.12182212661246995
Epoch #140: loss=0.1429498417151941
Epoch #141: loss=0.11184610230093067
Epoch #142: loss=0.15045912755099503
Epoch #143: loss=0.10474184618608372
Epoch #144: loss=0.22476202199185216
Epoch #145: loss=0.1313591162497933
Epoch #146: loss=0.08867312695931744
Epoch #147: loss=0.12023413750166828
Epoch #148: loss=0.12123978067491506
Epoch #149: loss=0.1063624592246236
Epoch #150: loss=0.08410246871613167
Epoch #151: loss=0.10413650735407262
Epoch #152: loss=0.09578304360243114
Epoch #153: loss=0.14437525961044673
Epoch #154: loss=0.18646927384307255
Epoch #155: loss=0.10980806570198084
Epoch #156: loss=0.0741975792255756
Epoch #157: loss=0.08171831997665199
Epoch #158: loss=0.1029301346657244
Epoch #159: loss=0.05575027699406083
Epoch #160: loss=0.13814609960929766
Epoch #161: loss=0.08338802876706058
Epoch #162: loss=0.07036016826089975
Epoch #163: loss=0.09821746976593056
Epoch #164: loss=0.07544695646376223
Epoch #165: loss=0.08853809485161626
Epoch #166: loss=0.08320262398872827
Epoch #167: loss=0.11897377075778472
Epoch #168: loss=0.10358824460087596
Epoch #169: loss=0.08836262834233206
Epoch #170: loss=0.04802075614901008
Epoch #171: loss=0.11753106474675037
Epoch #172: loss=0.10816636369437785
Epoch #173: loss=0.09275614067509368
Epoch #174: loss=0.07882672528157363
Epoch #175: loss=0.07717094258279414
Epoch #176: loss=0.08623858969155196
Epoch #177: loss=0.08002718399606042
Epoch #178: loss=0.05706063222542808
Epoch #179: loss=0.06316358153079007
Epoch #180: loss=0.04159115169298004
Epoch #181: loss=0.06440880962622327
Epoch #182: loss=0.056144635243391666
Epoch #183: loss=0.06538872546642213
Epoch #184: loss=0.08782654426790573
Epoch #185: loss=0.16878971598438314
Epoch #186: loss=0.09305995974589039
Epoch #187: loss=0.05064733303781297
Epoch #188: loss=0.05107313865241972
Epoch #189: loss=0.07526004437759921
Epoch #190: loss=0.06111484267623038
Epoch #191: loss=0.04573868640471954
Epoch #192: loss=0.06278845020947424
Epoch #193: loss=0.1216294097940664
Epoch #194: loss=0.1270081353046604
Epoch #195: loss=0.11132424135969297
Epoch #196: loss=0.061818454968365466
Epoch #197: loss=0.057327244153900725
Epoch #198: loss=0.06576490820058294
Epoch #199: loss=0.07722478354903492
Epoch #200: loss=0.07124082016683109
Epoch #201: loss=0.038415482141882985
Epoch #202: loss=0.07346354694281881
Epoch #203: loss=0.061529286205768585
Epoch #204: loss=0.05042132103463282
Epoch #205: loss=0.06417922235119182
Epoch #206: loss=0.0515611391464198
Epoch #207: loss=0.07804379247229647
Epoch #208: loss=0.10564325181012219
Epoch #209: loss=0.08861055723517328
Epoch #210: loss=0.06727386771927814
Epoch #211: loss=0.10507818152876319
Epoch #212: loss=0.05985581401634861
Epoch #213: loss=0.04284288648616623
Epoch #214: loss=0.09151395594046728
Epoch #215: loss=0.054370404019750455
Epoch #216: loss=0.03403193402934719
Epoch #217: loss=0.041901777032762766
Epoch #218: loss=0.06220208649599069
Epoch #219: loss=0.058028888636948286
Epoch #220: loss=0.06138116881452702
Epoch #221: loss=0.047174276179961255
Epoch #222: loss=0.07902556375877277
Epoch #223: loss=0.04409323323115304
Epoch #224: loss=0.057727986493626156
Epoch #225: loss=0.042123579218782285
Epoch #226: loss=0.03716744351628665
Epoch #227: loss=0.09337800439145114
Epoch #228: loss=0.10331753759670097
Epoch #229: loss=0.11729123564185323
Epoch #230: loss=0.0886171194850593
Epoch #231: loss=0.09395360267041503
Epoch #232: loss=0.060198518372065316
Epoch #233: loss=0.03783310526931608
Epoch #234: loss=0.03628715220838785
Epoch #235: loss=0.0437492358976522
Epoch #236: loss=0.09483645130200563
Epoch #237: loss=0.14085265933661847
Epoch #238: loss=0.0542874439946703
Epoch #239: loss=0.03671738945853871
Epoch #240: loss=0.1532225500339189
Epoch #241: loss=0.05858846414028793
Epoch #242: loss=0.05863172241260071
Epoch #243: loss=0.04437492397689336
Epoch #244: loss=0.04083915392087924
Epoch #245: loss=0.054466094771349755
Epoch #246: loss=0.10422942841526221
Epoch #247: loss=0.18478228438746286
Epoch #248: loss=0.06993935680067218
Epoch #249: loss=0.05357391087690721

Training time: 0:38:37.168746

Finished.
n2one setting ettm1_weather -> electricity
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/ettm1_weather_epochs_250_seed_2021/model.pkl', muti_dataset='ettm1_weather', random_seed=2021, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='electricity')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.58647e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=3.0894e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.58647e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.36527572917694523, 'MAE': 0.4239675122537887}
Finished.
------------------------- record done -------------------------
n2one setting ettm1_weather -> exchange
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/ettm1_weather_epochs_250_seed_2021/model.pkl', muti_dataset='ettm1_weather', random_seed=2021, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='exchange')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.88711e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=5.81981e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.2413112450431525, 'MAE': 0.3283571224044736}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='ettm1_exchange', random_seed=2021, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/ettm1_exchange_epochs_250_seed_2021
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=5.174403190612793
Epoch #1: loss=2.601129544408698
Epoch #2: loss=2.240921039330332
Epoch #3: loss=2.054166831468281
Epoch #4: loss=2.0326706171035767
Epoch #5: loss=1.911883467122128
Epoch #6: loss=1.8129505107277317
Epoch #7: loss=1.8158842325210571
Epoch #8: loss=1.6887891480797215
Epoch #9: loss=1.6064631750709133
Epoch #10: loss=1.5258655548095703
Epoch #11: loss=1.4765561756334806
Epoch #12: loss=1.3797208133496737
Epoch #13: loss=1.3369688172089427
Epoch #14: loss=1.3065900990837498
Epoch #15: loss=1.3610157558792515
Epoch #16: loss=1.3548370976197093
Epoch #17: loss=1.3676247596740723
Epoch #18: loss=1.2388233009137606
Epoch #19: loss=1.2470486791510331
Epoch #20: loss=1.1335499568989402
Epoch #21: loss=1.0937828045142324
Epoch #22: loss=1.1077442514268976
Epoch #23: loss=1.0205567385021008
Epoch #24: loss=1.111660947925166
Epoch #25: loss=1.0672458974938643
Epoch #26: loss=1.0689384560835988
Epoch #27: loss=1.1810918638580723
Epoch #28: loss=1.0512869954109192
Epoch #29: loss=0.9131198086236653
Epoch #30: loss=0.9296187381995352
Epoch #31: loss=0.8910558098240903
Epoch #32: loss=0.8084237983352259
Epoch #33: loss=0.8863760828971863
Epoch #34: loss=0.8060564304652967
Epoch #35: loss=0.91703032192431
Epoch #36: loss=1.0450422512857538
Epoch #37: loss=0.9174456910083169
Epoch #38: loss=0.792506616366537
Epoch #39: loss=0.7648315178720575
Epoch #40: loss=0.7579786997092398
Epoch #41: loss=0.7499714431009794
Epoch #42: loss=0.7269574733156907
Epoch #43: loss=0.8210789122079548
Epoch #44: loss=0.7610751641424078
Epoch #45: loss=0.7645440760411715
Epoch #46: loss=0.7250354446862873
Epoch #47: loss=0.6107773357316068
Epoch #48: loss=0.6753789782524109
Epoch #49: loss=0.5916921135626341
Epoch #50: loss=0.8640382211459311
Epoch #51: loss=0.6786431011400724
Epoch #52: loss=0.646067024845826
Epoch #53: loss=0.6047601150838953
Epoch #54: loss=0.5757623424655512
Epoch #55: loss=0.5250787295793232
Epoch #56: loss=0.6632011579839807
Epoch #57: loss=0.5950821716534463
Epoch #58: loss=0.6337047187905562
Epoch #59: loss=0.5935293216454355
Epoch #60: loss=0.6661152416153958
Epoch #61: loss=0.7051086268926922
Epoch #62: loss=0.6403303663981589
Epoch #63: loss=0.5635508361615633
Epoch #64: loss=0.5744127780199051
Epoch #65: loss=0.632391095161438
Epoch #66: loss=0.6062732244792738
Epoch #67: loss=0.5544380200536627
Epoch #68: loss=0.5503817467313064
Epoch #69: loss=0.6301506569511012
Epoch #70: loss=0.5779910213068912
Epoch #71: loss=0.526645009454928
Epoch #72: loss=0.5345222150024614
Epoch #73: loss=0.54156904000985
Epoch #74: loss=0.49207411471166107
Epoch #75: loss=0.5851880311965942
Epoch #76: loss=0.519668067756452
Epoch #77: loss=0.4675734160762084
Epoch #78: loss=0.4785012296940151
Epoch #79: loss=0.4399360120296478
Epoch #80: loss=0.49741745151971517
Epoch #81: loss=0.5107323590077852
Epoch #82: loss=0.4413576518234454
Epoch #83: loss=0.5117844751006678
Epoch #84: loss=0.46037507998315913
Epoch #85: loss=0.4982092372680965
Epoch #86: loss=0.49285196473723963
Epoch #87: loss=0.509192282432004
Epoch #88: loss=0.43600233056043325
Epoch #89: loss=0.4713872906408812
Epoch #90: loss=0.4317258596420288
Epoch #91: loss=0.36737096152807536
Epoch #92: loss=0.39799224310799647
Epoch #93: loss=0.3743932843208313
Epoch #94: loss=0.40140313459070104
Epoch #95: loss=0.3536960168888694
Epoch #96: loss=0.35568668183527496
Epoch #97: loss=0.34263650367134496
Epoch #98: loss=0.5090869571033277
Epoch #99: loss=0.418281479101432
Epoch #100: loss=0.4784359500596398
Epoch #101: loss=0.3997761497372075
Epoch #102: loss=0.4226391958562951
Epoch #103: loss=0.34495351738051366
Epoch #104: loss=0.4210317103486312
Epoch #105: loss=0.3650043089138834
Epoch #106: loss=0.314649655630714
Epoch #107: loss=0.31033809012488317
Epoch #108: loss=0.2959755757137349
Epoch #109: loss=0.3305134185050663
Epoch #110: loss=0.3533727189427928
Epoch #111: loss=0.41397700576405777
Epoch #112: loss=0.4439946201286818
Epoch #113: loss=0.3780871841468309
Epoch #114: loss=0.35579302671708557
Epoch #115: loss=0.3518365178453295
Epoch #116: loss=0.36566180226049927
Epoch #117: loss=0.32879988535454396
Epoch #118: loss=0.30674315989017487
Epoch #119: loss=0.3167965388611743
Epoch #120: loss=0.3671236728367053
Epoch #121: loss=0.2739654116724667
Epoch #122: loss=0.34593345852274643
Epoch #123: loss=0.2891147277857128
Epoch #124: loss=0.31770479208544683
Epoch #125: loss=0.3030585638786617
Epoch #126: loss=0.3446480929851532
Epoch #127: loss=0.24875870621518084
Epoch #128: loss=0.2671801816476019
Epoch #129: loss=0.29501316186628845
Epoch #130: loss=0.26098451175187765
Epoch #131: loss=0.2653725347236583
Epoch #132: loss=0.23122666776180267
Epoch #133: loss=0.2506915797528468
Epoch #134: loss=0.3220586823789697
Epoch #135: loss=0.24567981848591253
Epoch #136: loss=0.3360651562872686
Epoch #137: loss=0.2775436663313916
Epoch #138: loss=0.2299044634166517
Epoch #139: loss=0.2867607180225222
Epoch #140: loss=0.29640614319788783
Epoch #141: loss=0.2510799196989913
Epoch #142: loss=0.2697963667543311
Epoch #143: loss=0.2818638635309119
Epoch #144: loss=0.26328601727360174
Epoch #145: loss=0.24099035717939077
Epoch #146: loss=0.33356124633236933
Epoch #147: loss=0.2828783181152846
Epoch #148: loss=0.2780431728614004
Epoch #149: loss=0.27193384499926315
Epoch #150: loss=0.2847595097203004
Epoch #151: loss=0.25981668735805313
Epoch #152: loss=0.23168616937963585
Epoch #153: loss=0.232185933542879
Epoch #154: loss=0.19597191206718745
Epoch #155: loss=0.1998126342108375
Epoch #156: loss=0.33310152315779734
Epoch #157: loss=0.31147837168292
Epoch #158: loss=0.22220659020699954
Epoch #159: loss=0.21572086332659973
Epoch #160: loss=0.18120531384882174
Epoch #161: loss=0.19062735022682892
Epoch #162: loss=0.2572458841298756
Epoch #163: loss=0.2505088793603997
Epoch #164: loss=0.22290370848618055
Epoch #165: loss=0.26228815395581095
Epoch #166: loss=0.37013788834998484
Epoch #167: loss=0.2925475102506186
Epoch #168: loss=0.3163720400709855
Epoch #169: loss=0.26329657043281357
Epoch #170: loss=0.20121177952540548
Epoch #171: loss=0.2603206089452693
Epoch #172: loss=0.2257756010482186
Epoch #173: loss=0.18232770852352442
Epoch #174: loss=0.1684787994936893
Epoch #175: loss=0.2389082006717983
Epoch #176: loss=0.18571358017231288
Epoch #177: loss=0.13558934315254814
Epoch #178: loss=0.1684533330170732
Epoch #179: loss=0.35488301868501465
Epoch #180: loss=0.18275539047623934
Epoch #181: loss=0.1741334394012627
Epoch #182: loss=0.20968083547134148
Epoch #183: loss=0.18285240938788966
Epoch #184: loss=0.1421225272903317
Epoch #185: loss=0.1790880466762342
Epoch #186: loss=0.14896086074019732
Epoch #187: loss=0.2589466701212682
Epoch #188: loss=0.18877994229919032
Epoch #189: loss=0.19506540541586123
Epoch #190: loss=0.17488116947443863
Epoch #191: loss=0.21855623784818148
Epoch #192: loss=0.17035673362644096
Epoch #193: loss=0.12354655701078866
Epoch #194: loss=0.13885980570002607
Epoch #195: loss=0.1432303390220592
Epoch #196: loss=0.20702979184295
Epoch #197: loss=0.11588455226860549
Epoch #198: loss=0.140720603105269
Epoch #199: loss=0.17222617135236137
Epoch #200: loss=0.15090437155020864
Epoch #201: loss=0.27391807009515007
Epoch #202: loss=0.15751249107875323
Epoch #203: loss=0.10760032052272245
Epoch #204: loss=0.18151348025390976
Epoch #205: loss=0.1293473677023461
Epoch #206: loss=0.13371698205408297
Epoch #207: loss=0.15356960598575442
Epoch #208: loss=0.10418671055843956
Epoch #209: loss=0.1590696624235103
Epoch #210: loss=0.16508140846302635
Epoch #211: loss=0.14846302157169894
Epoch #212: loss=0.17373496235201233
Epoch #213: loss=0.16293166989558622
Epoch #214: loss=0.15394288870064834
Epoch #215: loss=0.2016720287501812
Epoch #216: loss=0.19857878296783096
Epoch #217: loss=0.18982332199811935
Epoch #218: loss=0.14322359428593986
Epoch #219: loss=0.12224753122580678
Epoch #220: loss=0.15396303546271825
Epoch #221: loss=0.1869844598205466
Epoch #222: loss=0.19786310215529643
Epoch #223: loss=0.1807208178859008
Epoch #224: loss=0.13903452828526497
Epoch #225: loss=0.10330730990359657
Epoch #226: loss=0.14358327655415787
Epoch #227: loss=0.10976784264570788
Epoch #228: loss=0.1257903540604993
Epoch #229: loss=0.11227037914489445
Epoch #230: loss=0.23581064316002945
Epoch #231: loss=0.2052857844453109
Epoch #232: loss=0.20989158396658145
Epoch #233: loss=0.15300936369519486
Epoch #234: loss=0.12175742301501725
Epoch #235: loss=0.11318996176123619
Epoch #236: loss=0.11817571285523866
Epoch #237: loss=0.1627752000564023
Epoch #238: loss=0.20751675414411644
Epoch #239: loss=0.23197326220964132
Epoch #240: loss=0.24534917348309568
Epoch #241: loss=0.2778325037736642
Epoch #242: loss=0.20022279337832802
Epoch #243: loss=0.2934572133970888
Epoch #244: loss=0.18408684275652232
Epoch #245: loss=0.29397086014873103
Epoch #246: loss=0.16422770799774872
Epoch #247: loss=0.2147268323521865
Epoch #248: loss=0.12154295334690496
Epoch #249: loss=0.10635040700435638

Training time: 0:16:45.718850

Finished.
n2one setting ettm1_exchange -> electricity
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/ettm1_exchange_epochs_250_seed_2021/model.pkl', muti_dataset='ettm1_exchange', random_seed=2021, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='electricity')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.50205e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=3.0378e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.50205e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.3646484568831201, 'MAE': 0.4263952710454719}
Finished.
------------------------- record done -------------------------
n2one setting ettm1_exchange -> weather
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/ettm1_exchange_epochs_250_seed_2021/model.pkl', muti_dataset='ettm1_exchange', random_seed=2021, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='weather')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.37435e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.52925e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.4201370478717884, 'MAE': 0.4705119545432599}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='ettm2_electricity', random_seed=2021, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/ettm2_electricity_epochs_250_seed_2021
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=1.834659780903035
Epoch #1: loss=0.7582408838781965
Epoch #2: loss=0.5299361016915505
Epoch #3: loss=0.3916048319583916
Epoch #4: loss=0.3421046147684017
Epoch #5: loss=0.3139762511723731
Epoch #6: loss=0.2507354280496218
Epoch #7: loss=0.2212423697921885
Epoch #8: loss=0.217738744028541
Epoch #9: loss=0.18888589275260287
Epoch #10: loss=0.1548477747892759
Epoch #11: loss=0.15841046915984297
Epoch #12: loss=0.13299177986491156
Epoch #13: loss=0.1357259832165507
Epoch #14: loss=0.13322071763630733
Epoch #15: loss=0.10974950708605978
Epoch #16: loss=0.08832750306184213
Epoch #17: loss=0.08556124329263995
Epoch #18: loss=0.08030310912753444
Epoch #19: loss=0.09126496727084629
Epoch #20: loss=0.07213763088683586
Epoch #21: loss=0.08973846803082669
Epoch #22: loss=0.059980682391920184
Epoch #23: loss=0.06430082048811243
Epoch #24: loss=0.061443181162845924
Epoch #25: loss=0.061476841042974
Epoch #26: loss=0.05215564391615026
Epoch #27: loss=0.04952193034464965
Epoch #28: loss=0.04746207895587726
Epoch #29: loss=0.05047532944261175
Epoch #30: loss=0.04405899832855506
Epoch #31: loss=0.04661768864200686
Epoch #32: loss=0.05306640052634953
Epoch #33: loss=0.037621894120575344
Epoch #34: loss=0.041613660925993674
Epoch #35: loss=0.03288117014586993
Epoch #36: loss=0.0346119894473589
Epoch #37: loss=0.03187858812365098
Epoch #38: loss=0.04235235641988174
Epoch #39: loss=0.031370947725162
Epoch #40: loss=0.028792280856665808
Epoch #41: loss=0.03319938453994355
Epoch #42: loss=0.028419185668252498
Epoch #43: loss=0.03109157659781042
Epoch #44: loss=0.031127571526645536
Epoch #45: loss=0.02088122947473943
Epoch #46: loss=0.029512352878171725
Epoch #47: loss=0.032752004645607735
Epoch #48: loss=0.025108912353423503
Epoch #49: loss=0.03197622316656634
Epoch #50: loss=0.023735952915985653
Epoch #51: loss=0.02763946366030723
Epoch #52: loss=0.08191617186944258
Epoch #53: loss=0.029433426346927208
Epoch #54: loss=0.02393823731003374
Epoch #55: loss=0.02721927055096007
Epoch #56: loss=0.042410525162311564
Epoch #57: loss=0.035698115020563984
Epoch #58: loss=0.024816643078502894
Epoch #59: loss=0.02917362010584729
Epoch #60: loss=0.026571088907777343
Epoch #61: loss=0.016426402939755627
Epoch #62: loss=0.022369823313363076
Epoch #63: loss=0.015963173952130375
Epoch #64: loss=0.02255474946983909
Epoch #65: loss=0.02783948299545144
Epoch #66: loss=0.024598682261210956
Epoch #67: loss=0.020439437778487085
Epoch #68: loss=0.026011240936490338
Epoch #69: loss=0.02245285618294266
Epoch #70: loss=0.012889750081104653
Epoch #71: loss=0.031141108060423958
Epoch #72: loss=0.026260282333760737
Epoch #73: loss=0.019943807916316003
Epoch #74: loss=0.019295700690751132
Epoch #75: loss=0.02305800671105733
Epoch #76: loss=0.024289738423687528
Epoch #77: loss=0.01682754221767838
Epoch #78: loss=0.019722075551167603
Epoch #79: loss=0.023060760640326035
Epoch #80: loss=0.025089228324134866
Epoch #81: loss=0.023066474583437555
Epoch #82: loss=0.015288425864741564
Epoch #83: loss=0.018597743392596584
Epoch #84: loss=0.02097114279279934
Epoch #85: loss=0.019160568478506012
Epoch #86: loss=0.016326914009673
Epoch #87: loss=0.012160931415827832
Epoch #88: loss=0.017922404175729444
Epoch #89: loss=0.03920966003536168
Epoch #90: loss=0.02400904749022202
Epoch #91: loss=0.01858981034237364
Epoch #92: loss=0.018242792189556052
Epoch #93: loss=0.01693676321182389
Epoch #94: loss=0.012480778601939274
Epoch #95: loss=0.017673306199560694
Epoch #96: loss=0.017673779613687657
Epoch #97: loss=0.01944732366693248
Epoch #98: loss=0.016769383248700535
Epoch #99: loss=0.01291586611241436
Epoch #100: loss=0.02219098708865861
Epoch #101: loss=0.015606291342108702
Epoch #102: loss=0.016985816217633527
Epoch #103: loss=0.021013294026488438
Epoch #104: loss=0.027170517936830004
Epoch #105: loss=0.017862774131900957
Epoch #106: loss=0.01568929066969901
Epoch #107: loss=0.014907826652507724
Epoch #108: loss=0.015775135343092078
Epoch #109: loss=0.022752469083987155
Epoch #110: loss=0.01132597165173116
Epoch #111: loss=0.019027724371716685
Epoch #112: loss=0.010847785549570738
Epoch #113: loss=0.01400272740897616
Epoch #114: loss=0.0163976988333161
Epoch #115: loss=0.022160042984080107
Epoch #116: loss=0.015746982122458664
Epoch #117: loss=0.012751006456765246
Epoch #118: loss=0.016558078322798035
Epoch #119: loss=0.009380149832296925
Epoch #120: loss=0.010667917619673927
Epoch #121: loss=0.014747958787413964
Epoch #122: loss=0.018647891935224068
Epoch #123: loss=0.02265769704693861
Epoch #124: loss=0.012027088121766314
Epoch #125: loss=0.013778333479334482
Epoch #126: loss=0.009755308893951027
Epoch #127: loss=0.012900878537827766
Epoch #128: loss=0.01621023838686005
Epoch #129: loss=0.012353618964409114
Epoch #130: loss=0.01855711972183851
Epoch #131: loss=0.013522732118214762
Epoch #132: loss=0.012303584868211793
Epoch #133: loss=0.017163917675024565
Epoch #134: loss=0.0120519173584201
Epoch #135: loss=0.012276769579067008
Epoch #136: loss=0.016864174607295595
Epoch #137: loss=0.013311038761545557
Epoch #138: loss=0.02083426692369829
Epoch #139: loss=0.017780832128237417
Epoch #140: loss=0.018910121976528374
Epoch #141: loss=0.015843583593504077
Epoch #142: loss=0.011979136659121365
Epoch #143: loss=0.01674328249698948
Epoch #144: loss=0.00976186698779158
Epoch #145: loss=0.014740471236186147
Epoch #146: loss=0.010993609533095396
Epoch #147: loss=0.012992433555626187
Epoch #148: loss=0.024861148572933624
Epoch #149: loss=0.012163143984746211
Epoch #150: loss=0.008376794674974469
Epoch #151: loss=0.00932927206628507
Epoch #152: loss=0.011750759993885302
Epoch #153: loss=0.015038821749643041
Epoch #154: loss=0.01489168878110667
Epoch #155: loss=0.011098339032495384
Epoch #156: loss=0.018697429586346372
Epoch #157: loss=0.010517454269662618
Epoch #158: loss=0.01006481108942605
Epoch #159: loss=0.008461040587999706
Epoch #160: loss=0.010309370232241773
Epoch #161: loss=0.01578056485516984
Epoch #162: loss=0.007972234534923722
Epoch #163: loss=0.0173253986261454
Epoch #164: loss=0.011687582839807743
Epoch #165: loss=0.006688123195276439
Epoch #166: loss=0.011776557675049449
Epoch #167: loss=0.009791774459951838
Epoch #168: loss=0.013179640106664668
Epoch #169: loss=0.014191875270713705
Epoch #170: loss=0.013075582354616358
Epoch #171: loss=0.01436133604042277
Epoch #172: loss=0.013353798128012287
Epoch #173: loss=0.010499749206432915
Epoch #174: loss=0.010666021118091324
Epoch #175: loss=0.011325513824633973
Epoch #176: loss=0.026694374255582142
Epoch #177: loss=0.014867076705138378
Epoch #178: loss=0.010027556143036949
Epoch #179: loss=0.009410839614280252
Epoch #180: loss=0.010969782021971061
Epoch #181: loss=0.014245598844507799
Epoch #182: loss=0.013027951442524341
Epoch #183: loss=0.008822453009099307
Epoch #184: loss=0.007256088808576187
Epoch #185: loss=0.008576428184320124
Epoch #186: loss=0.011230362083146444
Epoch #187: loss=0.013538481249840067
Epoch #188: loss=0.020502722327189284
Epoch #189: loss=0.011377381481523402
Epoch #190: loss=0.00926604756649937
Epoch #191: loss=0.010654130429331022
Epoch #192: loss=0.008355116796348783
Epoch #193: loss=0.0069704374450284155
Epoch #194: loss=0.009802726324712886
Epoch #195: loss=0.012037444834847071
Epoch #196: loss=0.008025290091127207
Epoch #197: loss=0.020710910534539124
Epoch #198: loss=0.018373576366673618
Epoch #199: loss=0.008962688069136292
Epoch #200: loss=0.01004250993824519
Epoch #201: loss=0.011263717684918644
Epoch #202: loss=0.011123420837624924
Epoch #203: loss=0.010348930985001281
Epoch #204: loss=0.013104869193027177
Epoch #205: loss=0.008752537000621388
Epoch #206: loss=0.012094448594424134
Epoch #207: loss=0.009092882192915963
Epoch #208: loss=0.006444440118783344
Epoch #209: loss=0.010903085181254652
Epoch #210: loss=0.008864589762748596
Epoch #211: loss=0.013022658093593991
Epoch #212: loss=0.017994616311625578
Epoch #213: loss=0.010616377449428931
Epoch #214: loss=0.01949676108253413
Epoch #215: loss=0.03810324350456814
Epoch #216: loss=0.02128213264696098
Epoch #217: loss=0.008635630690862975
Epoch #218: loss=0.008588333868303094
Epoch #219: loss=0.006013341616304081
Epoch #220: loss=0.008589694051423352
Epoch #221: loss=0.007107756209296051
Epoch #222: loss=0.00835042222226263
Epoch #223: loss=0.010599959907574293
Epoch #224: loss=0.01413545670045304
Epoch #225: loss=0.019276460234871055
Epoch #226: loss=0.014547111576279984
Epoch #227: loss=0.009096743674413722
Epoch #228: loss=0.00795815747586452
Epoch #229: loss=0.00672084702687627
Epoch #230: loss=0.011600989364480308
Epoch #231: loss=0.008646997429587576
Epoch #232: loss=0.008445260274688242
Epoch #233: loss=0.006334679487374532
Epoch #234: loss=0.009580277562466119
Epoch #235: loss=0.009756604842988898
Epoch #236: loss=0.0145927834990245
Epoch #237: loss=0.005114476432016047
Epoch #238: loss=0.0319447748209434
Epoch #239: loss=0.012075708423834434
Epoch #240: loss=0.011732333129186307
Epoch #241: loss=0.008922127678608859
Epoch #242: loss=0.00522117412055447
Epoch #243: loss=0.007152573486663126
Epoch #244: loss=0.012099481544282753
Epoch #245: loss=0.0065214519633726405
Epoch #246: loss=0.011944665375683899
Epoch #247: loss=0.01027386173299452
Epoch #248: loss=0.011352444889166965
Epoch #249: loss=0.011681921990761528

Training time: 4:32:55.118329

Finished.
n2one setting ettm2_electricity -> weather
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/ettm2_electricity_epochs_250_seed_2021/model.pkl', muti_dataset='ettm2_electricity', random_seed=2021, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='weather')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.16255e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.29486e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=5.49914e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.16255e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.6628293051932301, 'MAE': 0.6133847191027366}
Finished.
------------------------- record done -------------------------
n2one setting ettm2_electricity -> exchange
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/ettm2_electricity_epochs_250_seed_2021/model.pkl', muti_dataset='ettm2_electricity', random_seed=2021, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='exchange')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=4.3384e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.273680947061222, 'MAE': 0.354186563831249}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='ettm2_traffic', random_seed=2021, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/ettm2_traffic_epochs_250_seed_2021
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=1.1231954937196953
Epoch #1: loss=0.39524442880901955
Epoch #2: loss=0.28559412666407646
Epoch #3: loss=0.21011091679129815
Epoch #4: loss=0.1748166830201056
Epoch #5: loss=0.14787103838254223
Epoch #6: loss=0.11650954550910925
Epoch #7: loss=0.09905532465870028
Epoch #8: loss=0.08737915650197867
Epoch #9: loss=0.07911844790314686
Epoch #10: loss=0.06724368365223328
Epoch #11: loss=0.07164298790695843
Epoch #12: loss=0.06033588514662212
Epoch #13: loss=0.059520905595251444
Epoch #14: loss=0.05721087334517556
Epoch #15: loss=0.06070754984111012
Epoch #16: loss=0.05603672543973721
Epoch #17: loss=0.042621692277988225
Epoch #18: loss=0.04348917024192493
Epoch #19: loss=0.04810990119792461
Epoch #20: loss=0.042643132096557596
Epoch #21: loss=0.04892990022939326
Epoch #22: loss=0.03730122666640094
Epoch #23: loss=0.030115463067233384
Epoch #24: loss=0.034730477386979944
Epoch #25: loss=0.03235010266835286
Epoch #26: loss=0.04423018991117732
Epoch #27: loss=0.028137334900879368
Epoch #28: loss=0.032537085590094654
Epoch #29: loss=0.032944061500281695
Epoch #30: loss=0.032386522545300014
Epoch #31: loss=0.03207336377511431
Epoch #32: loss=0.024741574718893204
Epoch #33: loss=0.024515305746977196
Epoch #34: loss=0.0271473945833781
Epoch #35: loss=0.025389696988727863
Epoch #36: loss=0.03993813069667966
Epoch #37: loss=0.027033852987638957
Epoch #38: loss=0.026428738738840097
Epoch #39: loss=0.020293143300737233
Epoch #40: loss=0.02476001635646151
Epoch #41: loss=0.019798758073738175
Epoch #42: loss=0.02207330493912536
Epoch #43: loss=0.024843056791169698
Epoch #44: loss=0.02317014017463998
Epoch #45: loss=0.02268454757323574
Epoch #46: loss=0.02030440527479829
Epoch #47: loss=0.018747664889561297
Epoch #48: loss=0.021904502821822713
Epoch #49: loss=0.022097527813115705
Epoch #50: loss=0.018638905487225403
Epoch #51: loss=0.01847958888714271
Epoch #52: loss=0.030207015968630367
Epoch #53: loss=0.01927071347841214
Epoch #54: loss=0.020138654101641052
Epoch #55: loss=0.030061966931086807
Epoch #56: loss=0.01699804378281916
Epoch #57: loss=0.017966550568111757
Epoch #58: loss=0.021264800662321507
Epoch #59: loss=0.01715712428089223
Epoch #60: loss=0.015422209460762481
Epoch #61: loss=0.017989143958640097
Epoch #62: loss=0.03697317288663102
Epoch #63: loss=0.019061819679903363
Epoch #64: loss=0.018873891596965606
Epoch #65: loss=0.015580536541875712
Epoch #66: loss=0.020968444711280152
Epoch #67: loss=0.01358234980354301
Epoch #68: loss=0.015195797534906803
Epoch #69: loss=0.015732639515745793
Epoch #70: loss=0.017254168338732813
Epoch #71: loss=0.02006682311654766
Epoch #72: loss=0.016060773864586358
Epoch #73: loss=0.023046221642189297
Epoch #74: loss=0.024844822873343272
Epoch #75: loss=0.013249137994440172
Epoch #76: loss=0.015904584352806093
Epoch #77: loss=0.01595132074914638
Epoch #78: loss=0.015865398159241016
Epoch #79: loss=0.013820306723817377
Epoch #80: loss=0.0165416294290993
Epoch #81: loss=0.02449778035706611
Epoch #82: loss=0.016248346020646003
Epoch #83: loss=0.012242104594725347
Epoch #84: loss=0.01911753131410604
Epoch #85: loss=0.013676519394865022
Epoch #86: loss=0.013995656007282087
Epoch #87: loss=0.02120824561524765
Epoch #88: loss=0.019753085239310318
Epoch #89: loss=0.015932937979859268
Epoch #90: loss=0.014945180386467277
Epoch #91: loss=0.03012151274378577
Epoch #92: loss=0.01645349733375988
Epoch #93: loss=0.011926074486682836
Epoch #94: loss=0.013690926017570933
Epoch #95: loss=0.017004341173040675
Epoch #96: loss=0.0172917360924227
Epoch #97: loss=0.01660575083704166
Epoch #98: loss=0.015607523366439243
Epoch #99: loss=0.012592102561746367
Epoch #100: loss=0.015462187578688821
Epoch #101: loss=0.013578715687977708
Epoch #102: loss=0.013931942245691485
Epoch #103: loss=0.013045465096839057
Epoch #104: loss=0.012415183369493814
Epoch #105: loss=0.013632782698925776
Epoch #106: loss=0.01924749593150307
Epoch #107: loss=0.011454604521291363
Epoch #108: loss=0.013003791416437155
Epoch #109: loss=0.013953727745757207
Epoch #110: loss=0.014397263247179687
Epoch #111: loss=0.0130611556365591
Epoch #112: loss=0.016838087245480842
Epoch #113: loss=0.017049242820703282
Epoch #114: loss=0.008104750805448811
Epoch #115: loss=0.01709055340019954
Epoch #116: loss=0.016919710538974127
Epoch #117: loss=0.011657130348222503
Epoch #118: loss=0.017532669685100647
Epoch #119: loss=0.013659993892252597
Epoch #120: loss=0.014240719475274007
Epoch #121: loss=0.011897616253178705
Epoch #122: loss=0.016502323661029942
Epoch #123: loss=0.014471781687711804
Epoch #124: loss=0.02537085145786842
Epoch #125: loss=0.011489549737786639
Epoch #126: loss=0.011497936460059104
Epoch #127: loss=0.012506487081894162
Epoch #128: loss=0.013594988788203907
Epoch #129: loss=0.012206366048958903
Epoch #130: loss=0.009461770733845883
Epoch #131: loss=0.0127355522083385
Epoch #132: loss=0.013084575570638388
Epoch #133: loss=0.01177856486991052
Epoch #134: loss=0.013418547848262477
Epoch #135: loss=0.01019752229334395
Epoch #136: loss=0.013423858081960874
Epoch #137: loss=0.010130215923817677
Epoch #138: loss=0.009067408150542829
Epoch #139: loss=0.01145645564355736
Epoch #140: loss=0.012571992078335436
Epoch #141: loss=0.01083123288090679
Epoch #142: loss=0.015576494789059847
Epoch #143: loss=0.010032756047449763
Epoch #144: loss=0.011238816032681682
Epoch #145: loss=0.010864452315632569
Epoch #146: loss=0.013399959950110974
Epoch #147: loss=0.02453939514503364
Epoch #148: loss=0.010697881859279945
Epoch #149: loss=0.010850505016596517
Epoch #150: loss=0.012062371740356901
Epoch #151: loss=0.009830694050583667
Epoch #152: loss=0.012611472956151601
Epoch #153: loss=0.009940375815014854
Epoch #154: loss=0.01118936283922424
Epoch #155: loss=0.009830861850999715
Epoch #156: loss=0.008933498979505992
Epoch #157: loss=0.013395372779710797
Epoch #158: loss=0.019877267795910137
Epoch #159: loss=0.010622792923385012
Epoch #160: loss=0.01183377472723926
Epoch #161: loss=0.009598682191740841
Epoch #162: loss=0.011581175851052625
Epoch #163: loss=0.010831279331553655
Epoch #164: loss=0.011058705964783512
Epoch #165: loss=0.01304633964999205
Epoch #166: loss=0.010301797126660473
Epoch #167: loss=0.011203839321641299
Epoch #168: loss=0.009927723929789403
Epoch #169: loss=0.009553458830603065
Epoch #170: loss=0.024042251330633934
Epoch #171: loss=0.00792897461143858
Epoch #172: loss=0.01139382890109981
Epoch #173: loss=0.01270248638568014
Epoch #174: loss=0.011551453202384399
Epoch #175: loss=0.006377011230144754
Epoch #176: loss=0.01942934190263619
Epoch #177: loss=0.026524844836265774
Epoch #178: loss=0.010588657576090711
Epoch #179: loss=0.009643454622446755
Epoch #180: loss=0.010469945306054404
Epoch #181: loss=0.005579062860924084
Epoch #182: loss=0.016371431411377327
Epoch #183: loss=0.009274144309943857
Epoch #184: loss=0.010967942566345772
Epoch #185: loss=0.012526240475440584
Epoch #186: loss=0.011826545170432048
Epoch #187: loss=0.007764546160077711
Epoch #188: loss=0.02305897215436205
Epoch #189: loss=0.01581327371232725
Epoch #190: loss=0.00876576108098556
Epoch #191: loss=0.011325064420766482
Epoch #192: loss=0.010663580099205547
Epoch #193: loss=0.008378690007743602
Epoch #194: loss=0.007745156153891303
Epoch #195: loss=0.011922306791703348
Epoch #196: loss=0.008470387534489815
Epoch #197: loss=0.017865133018794024
Epoch #198: loss=0.010362370057217003
Epoch #199: loss=0.009537404975815855
Epoch #200: loss=0.011553009280043545
Epoch #201: loss=0.011716337475945953
Epoch #202: loss=0.01243082154181327
Epoch #203: loss=0.01469442461134094
Epoch #204: loss=0.010309282185286837
Epoch #205: loss=0.006228039104990772
Epoch #206: loss=0.014531425449935579
Epoch #207: loss=0.0095738310250587
Epoch #208: loss=0.007885049681216831
Epoch #209: loss=0.009524555328512246
Epoch #210: loss=0.013192013751352441
Epoch #211: loss=0.011506278858381272
Epoch #212: loss=0.0077063099900953995
Epoch #213: loss=0.010891668162871907
Epoch #214: loss=0.009835860338001374
Epoch #215: loss=0.011593791077786658
Epoch #216: loss=0.010954544182937754
Epoch #217: loss=0.008776830991968526
Epoch #218: loss=0.01168107539731401
Epoch #219: loss=0.007623169159861304
Epoch #220: loss=0.011059101579268383
Epoch #221: loss=0.010024465336480347
Epoch #222: loss=0.00909694381266777
Epoch #223: loss=0.009848791831543718
Epoch #224: loss=0.011002744557423984
Epoch #225: loss=0.0116596411951505
Epoch #226: loss=0.0059285847445694415
Epoch #227: loss=0.009591142001500078
Epoch #228: loss=0.011726995303459841
Epoch #229: loss=0.009159195847642092
Epoch #230: loss=0.0068031885212425725
Epoch #231: loss=0.011008665903036287
Epoch #232: loss=0.00803595889180489
Epoch #233: loss=0.01619531808182051
Epoch #234: loss=0.023722201310791797
Epoch #235: loss=0.0072482285884070356
Epoch #236: loss=0.010526237178061703
Epoch #237: loss=0.009558797918199023
Epoch #238: loss=0.00905198621051996
Epoch #239: loss=0.009345538811656813
Epoch #240: loss=0.008924251540934982
Epoch #241: loss=0.008948924940820615
Epoch #242: loss=0.012895547481491642
Epoch #243: loss=0.006086688997238775
Epoch #244: loss=0.008383173622939517
Epoch #245: loss=0.012713150267657543
Epoch #246: loss=0.006391212589935296
Epoch #247: loss=0.0088411864322627
Epoch #248: loss=0.009162419048458804
Epoch #249: loss=0.0099270718134871

Training time: 10:19:58.517362

Finished.
n2one setting ettm2_traffic -> electricity
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/ettm2_traffic_epochs_250_seed_2021/model.pkl', muti_dataset='ettm2_traffic', random_seed=2021, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='electricity')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=9.88588e-09): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.97665e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=5.2453e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=9.88588e-09): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.41495231541445954, 'MAE': 0.4583235776994698}
Finished.
------------------------- record done -------------------------
n2one setting ettm2_traffic -> weather
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/ettm2_traffic_epochs_250_seed_2021/model.pkl', muti_dataset='ettm2_traffic', random_seed=2021, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='weather')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.29246e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.7162e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.6435723086014145, 'MAE': 0.6318683311915058}
Finished.
------------------------- record done -------------------------
n2one setting ettm2_traffic -> exchange
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/ettm2_traffic_epochs_250_seed_2021/model.pkl', muti_dataset='ettm2_traffic', random_seed=2021, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='exchange')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=5.87973e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.4471799193617242, 'MAE': 0.434692886555809}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='ettm2_weather', random_seed=2021, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/ettm2_weather_epochs_250_seed_2021
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=5.625145970164119
Epoch #1: loss=2.6459328355015934
Epoch #2: loss=2.6512187300501644
Epoch #3: loss=2.16276034471151
Epoch #4: loss=1.9927823607986037
Epoch #5: loss=1.800229955363918
Epoch #6: loss=1.761968950967531
Epoch #7: loss=1.5470474604013804
Epoch #8: loss=1.447206806492161
Epoch #9: loss=1.6624913570043203
Epoch #10: loss=1.327971236125843
Epoch #11: loss=1.215032226330525
Epoch #12: loss=1.1650259961953033
Epoch #13: loss=1.1933309693594236
Epoch #14: loss=1.063534448275695
Epoch #15: loss=0.9921444187293181
Epoch #16: loss=0.8893918250058148
Epoch #17: loss=0.9489820889524512
Epoch #18: loss=0.8809638216688827
Epoch #19: loss=0.8344240011395635
Epoch #20: loss=0.8599234320021965
Epoch #21: loss=0.8443660897177618
Epoch #22: loss=0.8091105625436112
Epoch #23: loss=0.7813975601582914
Epoch #24: loss=0.7253225574622283
Epoch #25: loss=0.7213829304720905
Epoch #26: loss=0.6760328653696421
Epoch #27: loss=0.6476428089915095
Epoch #28: loss=0.6544487452184832
Epoch #29: loss=0.649148054219581
Epoch #30: loss=0.8211961670501812
Epoch #31: loss=0.6172344877913192
Epoch #32: loss=0.5707438241791081
Epoch #33: loss=0.5829093867057079
Epoch #34: loss=0.5867143974110887
Epoch #35: loss=0.5310324406301653
Epoch #36: loss=0.5292842484809257
Epoch #37: loss=0.4916566400914579
Epoch #38: loss=0.491504873778369
Epoch #39: loss=0.6374971898826393
Epoch #40: loss=0.5614507826598915
Epoch #41: loss=0.48269669026941864
Epoch #42: loss=0.5153461002014779
Epoch #43: loss=0.509382384854394
Epoch #44: loss=0.6941764145284086
Epoch #45: loss=0.5769840470842413
Epoch #46: loss=0.44618880265467875
Epoch #47: loss=0.42027832124684306
Epoch #48: loss=0.4280057678351531
Epoch #49: loss=0.44405351458369074
Epoch #50: loss=0.4448441496571979
Epoch #51: loss=0.3947918527835124
Epoch #52: loss=0.3888110143912805
Epoch #53: loss=0.38770957532766703
Epoch #54: loss=0.37283821645620707
Epoch #55: loss=0.42813417637670365
Epoch #56: loss=0.37248582171427236
Epoch #57: loss=0.355518191247373
Epoch #58: loss=0.38806985681121414
Epoch #59: loss=0.3311539976983457
Epoch #60: loss=0.349732457785993
Epoch #61: loss=0.39980025911653366
Epoch #62: loss=0.3066925096350747
Epoch #63: loss=0.2980684432628992
Epoch #64: loss=0.38356628530734294
Epoch #65: loss=0.32041389636091283
Epoch #66: loss=0.297760354908737
Epoch #67: loss=0.21915346827055956
Epoch #68: loss=0.2485936747209446
Epoch #69: loss=0.2394717136347616
Epoch #70: loss=0.21641893725137454
Epoch #71: loss=0.26922352088464274
Epoch #72: loss=0.2839074837597641
Epoch #73: loss=0.301499357698737
Epoch #74: loss=0.23589922568282565
Epoch #75: loss=0.2634703420303963
Epoch #76: loss=0.23249053934941422
Epoch #77: loss=0.21481601813355009
Epoch #78: loss=0.2932904805686023
Epoch #79: loss=0.20897316288303686
Epoch #80: loss=0.21138463853984266
Epoch #81: loss=0.21082947181688771
Epoch #82: loss=0.24425145721918828
Epoch #83: loss=0.19112262754021464
Epoch #84: loss=0.2084246851705216
Epoch #85: loss=0.4449538828150646
Epoch #86: loss=0.27052607991405436
Epoch #87: loss=0.23806946764926654
Epoch #88: loss=0.19021606062715118
Epoch #89: loss=0.17062835979300575
Epoch #90: loss=0.23465380153140505
Epoch #91: loss=0.2317177582834218
Epoch #92: loss=0.17670097141652494
Epoch #93: loss=0.3758439095439138
Epoch #94: loss=0.7076236613698907
Epoch #95: loss=0.26316273031202525
Epoch #96: loss=0.2613447316595026
Epoch #97: loss=0.45171035725522685
Epoch #98: loss=0.33031883996886174
Epoch #99: loss=0.25481177785912074
Epoch #100: loss=0.2774884443025331
Epoch #101: loss=0.2338056585474594
Epoch #102: loss=0.17792005424161214
Epoch #103: loss=0.17163809086825396
Epoch #104: loss=0.17497774394782814
Epoch #105: loss=0.2410655168665422
Epoch #106: loss=0.23388967139495387
Epoch #107: loss=0.2332389310405061
Epoch #108: loss=0.21899349705593005
Epoch #109: loss=0.17626720848115715
Epoch #110: loss=0.24717092222055873
Epoch #111: loss=0.2761054556514766
Epoch #112: loss=0.2005454683424653
Epoch #113: loss=0.23771131330647985
Epoch #114: loss=0.1560396443347673
Epoch #115: loss=0.1166973506679406
Epoch #116: loss=0.1867245026939624
Epoch #117: loss=0.13688432284303614
Epoch #118: loss=0.12785342959938822
Epoch #119: loss=0.09933576239524661
Epoch #120: loss=0.09815614338259439
Epoch #121: loss=0.12869993577132355
Epoch #122: loss=0.1500282588137968
Epoch #123: loss=0.11633829681857212
Epoch #124: loss=0.1371671604747708
Epoch #125: loss=0.19149022468843976
Epoch #126: loss=0.1351645781180343
Epoch #127: loss=0.1048053811128075
Epoch #128: loss=0.1514582675252412
Epoch #129: loss=0.14495638496166952
Epoch #130: loss=0.12025385511082572
Epoch #131: loss=0.10492241659478561
Epoch #132: loss=0.14422000954682762
Epoch #133: loss=0.10155542179740765
Epoch #134: loss=0.11011508263244822
Epoch #135: loss=0.09805744750475562
Epoch #136: loss=0.08450013031629292
Epoch #137: loss=0.08434383556045391
Epoch #138: loss=0.08352643593743041
Epoch #139: loss=0.10601136833429337
Epoch #140: loss=0.14459123960822015
Epoch #141: loss=0.11956529847874835
Epoch #142: loss=0.10087300555126087
Epoch #143: loss=0.09107517944397153
Epoch #144: loss=0.16188004124607588
Epoch #145: loss=0.1037398396009529
Epoch #146: loss=0.07688475299525906
Epoch #147: loss=0.11478301590761623
Epoch #148: loss=0.10623736684588161
Epoch #149: loss=0.08630579357614389
Epoch #150: loss=0.08090346402211769
Epoch #151: loss=0.09408014252580502
Epoch #152: loss=0.09838868592035126
Epoch #153: loss=0.15798742037165808
Epoch #154: loss=0.167418098077178
Epoch #155: loss=0.10125382350304642
Epoch #156: loss=0.07476459579491937
Epoch #157: loss=0.07802812850757225
Epoch #158: loss=0.09463961689254723
Epoch #159: loss=0.055633953797656135
Epoch #160: loss=0.13456244424388214
Epoch #161: loss=0.07710097891253394
Epoch #162: loss=0.07224926266919922
Epoch #163: loss=0.10360128593605918
Epoch #164: loss=0.07545672568517763
Epoch #165: loss=0.0734117356053478
Epoch #166: loss=0.07830316799919347
Epoch #167: loss=0.09873103725447042
Epoch #168: loss=0.09583320478732521
Epoch #169: loss=0.0840030706714134
Epoch #170: loss=0.04974070533707335
Epoch #171: loss=0.1417059227824211
Epoch #172: loss=0.13032066263258457
Epoch #173: loss=0.11294232792145498
Epoch #174: loss=0.08151215979376354
Epoch #175: loss=0.0762718715579123
Epoch #176: loss=0.09479989388303177
Epoch #177: loss=0.0773440348236142
Epoch #178: loss=0.05455288890044432
Epoch #179: loss=0.06008409160013135
Epoch #180: loss=0.04966823155110752
Epoch #181: loss=0.06941919514556995
Epoch #182: loss=0.05522081678783571
Epoch #183: loss=0.0652487556877974
Epoch #184: loss=0.06507295747665134
Epoch #185: loss=0.1732416207826621
Epoch #186: loss=0.10326205883678552
Epoch #187: loss=0.05296630455130661
Epoch #188: loss=0.05220786139771745
Epoch #189: loss=0.07831450440996401
Epoch #190: loss=0.07044010684900992
Epoch #191: loss=0.047813685233327186
Epoch #192: loss=0.06534222918688445
Epoch #193: loss=0.13392836854767962
Epoch #194: loss=0.12438387021019652
Epoch #195: loss=0.1019105972671831
Epoch #196: loss=0.05378601143791063
Epoch #197: loss=0.060646030774994475
Epoch #198: loss=0.0730885346042546
Epoch #199: loss=0.09193934336606716
Epoch #200: loss=0.08894391030677266
Epoch #201: loss=0.08804928513898237
Epoch #202: loss=0.11193409285231216
Epoch #203: loss=0.09851713544009505
Epoch #204: loss=0.07318477951795668
Epoch #205: loss=0.07840776790839595
Epoch #206: loss=0.19503384556721998
Epoch #207: loss=0.08708254675808791
Epoch #208: loss=0.18734464290979747
Epoch #209: loss=0.11452047270093416
Epoch #210: loss=0.06642995971078808
Epoch #211: loss=0.10906497225467418
Epoch #212: loss=0.05935602453914848
Epoch #213: loss=0.057959199077575595
Epoch #214: loss=0.09971576465948208
Epoch #215: loss=0.071132727546265
Epoch #216: loss=0.050871471245143865
Epoch #217: loss=0.05403875227312784
Epoch #218: loss=0.07427177646172207
Epoch #219: loss=0.06466722455681176
Epoch #220: loss=0.06305917143519665
Epoch #221: loss=0.05078183869655068
Epoch #222: loss=0.08613748574075666
Epoch #223: loss=0.05642475580444207
Epoch #224: loss=0.059963688254356384
Epoch #225: loss=0.044592396620459655
Epoch #226: loss=0.042951981436360524
Epoch #227: loss=0.06051399201356076
Epoch #228: loss=0.057191799017223155
Epoch #229: loss=0.05206474062760134
Epoch #230: loss=0.07314570781749648
Epoch #231: loss=0.08618456731877618
Epoch #232: loss=0.0551770214947897
Epoch #233: loss=0.04645925938982416
Epoch #234: loss=0.03365275811252964
Epoch #235: loss=0.041549038821579634
Epoch #236: loss=0.09219302080974386
Epoch #237: loss=0.13445261901093497
Epoch #238: loss=0.06089177093393094
Epoch #239: loss=0.03703042379001508
Epoch #240: loss=0.14768098269564076
Epoch #241: loss=0.04826858028064709
Epoch #242: loss=0.055435884429292905
Epoch #243: loss=0.047299816524861635
Epoch #244: loss=0.043430361015772495
Epoch #245: loss=0.055060464435735264
Epoch #246: loss=0.09922982370984312
Epoch #247: loss=0.1483835404285708
Epoch #248: loss=0.06278309734487855
Epoch #249: loss=0.04799940880085971

Training time: 0:39:32.861879

Finished.
n2one setting ettm2_weather -> electricity
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/ettm2_weather_epochs_250_seed_2021/model.pkl', muti_dataset='ettm2_weather', random_seed=2021, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='electricity')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.71006e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=3.31628e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.71006e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.379110634351948, 'MAE': 0.43207876315624116}
Finished.
------------------------- record done -------------------------
n2one setting ettm2_weather -> exchange
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/ettm2_weather_epochs_250_seed_2021/model.pkl', muti_dataset='ettm2_weather', random_seed=2021, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='exchange')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=3.19762e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=3.19762e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.21901667061087593, 'MAE': 0.31751705226549054}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='ettm2_exchange', random_seed=2021, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/ettm2_exchange_epochs_250_seed_2021
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=5.537246164522673
Epoch #1: loss=2.648046562546178
Epoch #2: loss=2.265340334490726
Epoch #3: loss=2.082962839226974
Epoch #4: loss=2.0738966276771142
Epoch #5: loss=1.9517917382089716
Epoch #6: loss=1.8504129397241693
Epoch #7: loss=1.8497310876846313
Epoch #8: loss=1.770571558099044
Epoch #9: loss=1.6478432981591475
Epoch #10: loss=1.5528669984717118
Epoch #11: loss=1.5281997291665328
Epoch #12: loss=1.4245973449004323
Epoch #13: loss=1.3755312530617965
Epoch #14: loss=1.3544331412566335
Epoch #15: loss=1.3907168476205123
Epoch #16: loss=1.3737872588007074
Epoch #17: loss=1.3828631263030202
Epoch #18: loss=1.2594675955019499
Epoch #19: loss=1.2586284750386287
Epoch #20: loss=1.1732889915767468
Epoch #21: loss=1.136836682495318
Epoch #22: loss=1.1474611947410984
Epoch #23: loss=1.04433600526107
Epoch #24: loss=1.1662395188682957
Epoch #25: loss=1.1441579963031567
Epoch #26: loss=1.0556082066736723
Epoch #27: loss=1.122875618307214
Epoch #28: loss=1.0396402415476347
Epoch #29: loss=0.9219380868108649
Epoch #30: loss=0.9262859915432177
Epoch #31: loss=0.9115292210327951
Epoch #32: loss=0.8247635897837187
Epoch #33: loss=0.9193079753925926
Epoch #34: loss=0.8613506900636774
Epoch #35: loss=0.936222349342547
Epoch #36: loss=1.07099722874792
Epoch #37: loss=0.882391443378047
Epoch #38: loss=0.804471050438128
Epoch #39: loss=0.778865331097653
Epoch #40: loss=0.7832010858937314
Epoch #41: loss=0.7519388732157255
Epoch #42: loss=0.7433629977075678
Epoch #43: loss=0.8398281273088957
Epoch #44: loss=0.7724829912185669
Epoch #45: loss=0.7291395711271387
Epoch #46: loss=0.7236368122853731
Epoch #47: loss=0.6153176411202079
Epoch #48: loss=0.6893376457063776
Epoch #49: loss=0.5877827123591775
Epoch #50: loss=0.8815986702316686
Epoch #51: loss=0.7173263838416651
Epoch #52: loss=0.6908251094190698
Epoch #53: loss=0.6185008836419958
Epoch #54: loss=0.626181224459096
Epoch #55: loss=0.7025656888359472
Epoch #56: loss=0.7684163059058943
Epoch #57: loss=0.6532000099357805
Epoch #58: loss=0.630206573950617
Epoch #59: loss=0.6159401049739436
Epoch #60: loss=0.6436218556604887
Epoch #61: loss=0.6918601895633497
Epoch #62: loss=0.6386749038570806
Epoch #63: loss=0.5960311826906706
Epoch #64: loss=0.5909108080362019
Epoch #65: loss=0.6434981619056902
Epoch #66: loss=0.6022056623509056
Epoch #67: loss=0.5251469941515672
Epoch #68: loss=0.5066942180457868
Epoch #69: loss=0.5421680188492725
Epoch #70: loss=0.5347420541863692
Epoch #71: loss=0.5116063277972372
Epoch #72: loss=0.5015362924651096
Epoch #73: loss=0.5135427227145747
Epoch #74: loss=0.4574104422017148
Epoch #75: loss=0.526821347443681
Epoch #76: loss=0.511127056259858
Epoch #77: loss=0.5162003110898169
Epoch #78: loss=0.5651868346490359
Epoch #79: loss=0.5428889974167472
Epoch #80: loss=0.5778863461394059
Epoch #81: loss=0.4776133204761304
Epoch #82: loss=0.4032679153116126
Epoch #83: loss=0.4708911870655261
Epoch #84: loss=0.3900485564219324
Epoch #85: loss=0.4232458150700519
Epoch #86: loss=0.457838688241808
Epoch #87: loss=0.4919802044567309
Epoch #88: loss=0.43508144742564153
Epoch #89: loss=0.4557747260520333
Epoch #90: loss=0.4352288873572099
Epoch #91: loss=0.3841643623615566
Epoch #92: loss=0.40258730634262685
Epoch #93: loss=0.40325439760559484
Epoch #94: loss=0.45821830551875264
Epoch #95: loss=0.548439862696748
Epoch #96: loss=0.4530300498008728
Epoch #97: loss=0.3508323399644149
Epoch #98: loss=0.468811759823247
Epoch #99: loss=0.37312576174736023
Epoch #100: loss=0.45239462350544174
Epoch #101: loss=0.3954642007225438
Epoch #102: loss=0.41574802367310776
Epoch #103: loss=0.35514421290472936
Epoch #104: loss=0.43116407096385956
Epoch #105: loss=0.36825948090929733
Epoch #106: loss=0.3143362693096462
Epoch #107: loss=0.3039842475401728
Epoch #108: loss=0.29475608468055725
Epoch #109: loss=0.31905121944452586
Epoch #110: loss=0.32639328037437637
Epoch #111: loss=0.3790494658445057
Epoch #112: loss=0.4200365178679165
Epoch #113: loss=0.3432662761525104
Epoch #114: loss=0.35551996372247996
Epoch #115: loss=0.3579156563470238
Epoch #116: loss=0.38551219866464015
Epoch #117: loss=0.3781737724417134
Epoch #118: loss=0.38150894092886073
Epoch #119: loss=0.3402561051280875
Epoch #120: loss=0.3119622853241469
Epoch #121: loss=0.24550838180278478
Epoch #122: loss=0.3288626906118895
Epoch #123: loss=0.2843570881768277
Epoch #124: loss=0.309484778266204
Epoch #125: loss=0.2654924878948613
Epoch #126: loss=0.3031717464327812
Epoch #127: loss=0.2186355975113417
Epoch #128: loss=0.2598419416891901
Epoch #129: loss=0.2852942433796431
Epoch #130: loss=0.25787392846847834
Epoch #131: loss=0.2597125878459529
Epoch #132: loss=0.23459402275712868
Epoch #133: loss=0.26967579753775345
Epoch #134: loss=0.32990034551996933
Epoch #135: loss=0.25813640654087067
Epoch #136: loss=0.33082738832423564
Epoch #137: loss=0.2806442925020268
Epoch #138: loss=0.2379628953180815
Epoch #139: loss=0.2324683254486636
Epoch #140: loss=0.2553987397175086
Epoch #141: loss=0.20997534809928192
Epoch #142: loss=0.2409438396755018
Epoch #143: loss=0.23924394226387927
Epoch #144: loss=0.2574054496852975
Epoch #145: loss=0.23444772707788566
Epoch #146: loss=0.31362244878944595
Epoch #147: loss=0.25317980545131785
Epoch #148: loss=0.2742022503363459
Epoch #149: loss=0.2645564718466056
Epoch #150: loss=0.303905009439117
Epoch #151: loss=0.2787944164715315
Epoch #152: loss=0.20996294288258804
Epoch #153: loss=0.2249651904168882
Epoch #154: loss=0.18949259896027415
Epoch #155: loss=0.20397309409944633
Epoch #156: loss=0.3696599700733235
Epoch #157: loss=0.3570224901563243
Epoch #158: loss=0.2701163323302018
Epoch #159: loss=0.2712418358576925
Epoch #160: loss=0.2282925452056684
Epoch #161: loss=0.2600354611089355
Epoch #162: loss=0.3081162320940118
Epoch #163: loss=0.2549947915892852
Epoch #164: loss=0.2154625893423432
Epoch #165: loss=0.25920483352322327
Epoch #166: loss=0.288009371020292
Epoch #167: loss=0.19145055664213081
Epoch #168: loss=0.2960427011314191
Epoch #169: loss=0.2744340100570729
Epoch #170: loss=0.21653141316614652
Epoch #171: loss=0.2937194505020192
Epoch #172: loss=0.27193728559895564
Epoch #173: loss=0.19951459255657697
Epoch #174: loss=0.1761659670032953
Epoch #175: loss=0.26653479941581426
Epoch #176: loss=0.19627035095503456
Epoch #177: loss=0.13469033100103078
Epoch #178: loss=0.1654047930711194
Epoch #179: loss=0.3531550788565686
Epoch #180: loss=0.1883330502008137
Epoch #181: loss=0.17667443364074356
Epoch #182: loss=0.21632107719779015
Epoch #183: loss=0.1970347401342894
Epoch #184: loss=0.1604210126556848
Epoch #185: loss=0.19247513617339887
Epoch #186: loss=0.16224458813667297
Epoch #187: loss=0.28600185325271205
Epoch #188: loss=0.20405702802695727
Epoch #189: loss=0.21550627954696355
Epoch #190: loss=0.2175148454935927
Epoch #191: loss=0.2650353873246594
Epoch #192: loss=0.2186705117162905
Epoch #193: loss=0.13437958120515472
Epoch #194: loss=0.14515351778582522
Epoch #195: loss=0.15236788105807805
Epoch #196: loss=0.2036655453082762
Epoch #197: loss=0.10578648079382746
Epoch #198: loss=0.11957732293950885
Epoch #199: loss=0.16235302114172986
Epoch #200: loss=0.14324437317095304
Epoch #201: loss=0.2601680467395406
Epoch #202: loss=0.14927663810943304
Epoch #203: loss=0.10195588045998623
Epoch #204: loss=0.18176284118702538
Epoch #205: loss=0.11996796021336004
Epoch #206: loss=0.12676935407676196
Epoch #207: loss=0.14504324605590418
Epoch #208: loss=0.10869505138773668
Epoch #209: loss=0.17344432677093305
Epoch #210: loss=0.16949232785325302
Epoch #211: loss=0.13523562213307933
Epoch #212: loss=0.14311889559030533
Epoch #213: loss=0.17759758234024048
Epoch #214: loss=0.13595829139414586
Epoch #215: loss=0.1694917321989411
Epoch #216: loss=0.1705418052641969
Epoch #217: loss=0.1817242111030378
Epoch #218: loss=0.1083328316086217
Epoch #219: loss=0.11236692866996716
Epoch #220: loss=0.14209031235230596
Epoch #221: loss=0.1286122314631939
Epoch #222: loss=0.13076805401789515
Epoch #223: loss=0.1478822523433911
Epoch #224: loss=0.137432630124845
Epoch #225: loss=0.1159967044858556
Epoch #226: loss=0.16049283821331828
Epoch #227: loss=0.12037589185331997
Epoch #228: loss=0.14405425049756704
Epoch #229: loss=0.1456454647214789
Epoch #230: loss=0.2354191294626186
Epoch #231: loss=0.1859097053345881
Epoch #232: loss=0.17754942786536718
Epoch #233: loss=0.14453360732448728
Epoch #234: loss=0.12551477198538027
Epoch #235: loss=0.0996318512449139
Epoch #236: loss=0.1155535920866226
Epoch #237: loss=0.12326308261407048
Epoch #238: loss=0.16765697927851425
Epoch #239: loss=0.21500597511859318
Epoch #240: loss=0.22545000420589195
Epoch #241: loss=0.21193491275373258
Epoch #242: loss=0.1668607057317307
Epoch #243: loss=0.27104186168626737
Epoch #244: loss=0.16839247451801048
Epoch #245: loss=0.2593095567273466
Epoch #246: loss=0.13763005717804558
Epoch #247: loss=0.19279191623392858
Epoch #248: loss=0.13104172894044927
Epoch #249: loss=0.12165532104278866

Training time: 0:16:47.350093

Finished.
n2one setting ettm2_exchange -> electricity
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/ettm2_exchange_epochs_250_seed_2021/model.pkl', muti_dataset='ettm2_exchange', random_seed=2021, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='electricity')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.61099e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=3.15105e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.61099e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.3688632984672152, 'MAE': 0.4319766048779915}
Finished.
------------------------- record done -------------------------
n2one setting ettm2_exchange -> weather
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/ettm2_exchange_epochs_250_seed_2021/model.pkl', muti_dataset='ettm2_exchange', random_seed=2021, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='weather')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.44954e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.75507e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.5347841859670254, 'MAE': 0.5388125156272318}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='electricity_traffic', random_seed=2021, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/electricity_traffic_epochs_250_seed_2021
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=0.8394211052790196
Epoch #1: loss=0.29531168419354464
Epoch #2: loss=0.21206358409252896
Epoch #3: loss=0.1500160304714539
Epoch #4: loss=0.10506838734717512
Epoch #5: loss=0.08938076915825613
Epoch #6: loss=0.0738690069082271
Epoch #7: loss=0.06289187896940054
Epoch #8: loss=0.0532082188457238
Epoch #9: loss=0.04505487620855728
Epoch #10: loss=0.04136396908072955
Epoch #11: loss=0.06350575673526951
Epoch #12: loss=0.03714236264339068
Epoch #13: loss=0.03964337295323566
Epoch #14: loss=0.03389815086717714
Epoch #15: loss=0.036694713786719894
Epoch #16: loss=0.030555483061247148
Epoch #17: loss=0.031010633318673972
Epoch #18: loss=0.03874714430523934
Epoch #19: loss=0.023659535251488548
Epoch #20: loss=0.029153342903253172
Epoch #21: loss=0.03661862770062009
Epoch #22: loss=0.02430560628411975
Epoch #23: loss=0.02904648784078053
Epoch #24: loss=0.02049331506517411
Epoch #25: loss=0.026378763929644753
Epoch #26: loss=0.030624964194756476
Epoch #27: loss=0.021092002691692396
Epoch #28: loss=0.02271851461513766
Epoch #29: loss=0.030512802945138983
Epoch #30: loss=0.023605369776959702
Epoch #31: loss=0.02233726410861446
Epoch #32: loss=0.03308123278532757
Epoch #33: loss=0.018950034536748796
Epoch #34: loss=0.02114981175894319
Epoch #35: loss=0.017250989637519087
Epoch #36: loss=0.019908853149000597
Epoch #37: loss=0.022377948513121082
Epoch #38: loss=0.021579626404287865
Epoch #39: loss=0.0177244045392546
Epoch #40: loss=0.018005790921990523
Epoch #41: loss=0.015392065643185682
Epoch #42: loss=0.017927677242531317
Epoch #43: loss=0.020710654328644138
Epoch #44: loss=0.01693118645077247
Epoch #45: loss=0.015358507375604657
Epoch #46: loss=0.018814232886181587
Epoch #47: loss=0.016014573389512943
Epoch #48: loss=0.018152717600512812
Epoch #49: loss=0.01991889981976305
Epoch #50: loss=0.013587083623479636
Epoch #51: loss=0.020734494141019983
Epoch #52: loss=0.014676349090565971
Epoch #53: loss=0.017898221027323722
Epoch #54: loss=0.01377391174045137
Epoch #55: loss=0.01568434512078821
Epoch #56: loss=0.014859017700069865
Epoch #57: loss=0.0126917565596508
Epoch #58: loss=0.013429833464908027
Epoch #59: loss=0.01359437229774561
Epoch #60: loss=0.014561132366820384
Epoch #61: loss=0.01830299704517486
Epoch #62: loss=0.022153115987954646
Epoch #63: loss=0.01787891352115418
Epoch #64: loss=0.013158145118149514
Epoch #65: loss=0.015665982637287922
Epoch #66: loss=0.019694350256182297
Epoch #67: loss=0.011022702417416508
Epoch #68: loss=0.014256620001801055
Epoch #69: loss=0.01650790893443209
Epoch #70: loss=0.01697325676644972
Epoch #71: loss=0.015314800219589865
Epoch #72: loss=0.012757428818087497
Epoch #73: loss=0.01237151325252836
Epoch #74: loss=0.014784128583220974
Epoch #75: loss=0.013859876155099869
Epoch #76: loss=0.013303202315714267
Epoch #77: loss=0.013875095295704156
Epoch #78: loss=0.012658827010949952
Epoch #79: loss=0.012148744985864704
Epoch #80: loss=0.015031239548540125
Epoch #81: loss=0.017436575561799054
Epoch #82: loss=0.011853892050997614
Epoch #83: loss=0.0173409846468631
Epoch #84: loss=0.013634967517364763
Epoch #85: loss=0.018651025213542477
Epoch #86: loss=0.01157195250167069
Epoch #87: loss=0.01034423935579934
Epoch #88: loss=0.014584482363845392
Epoch #89: loss=0.016973266908458087
Epoch #90: loss=0.009753549618849515
Epoch #91: loss=0.0103119470592439
Epoch #92: loss=0.015889712833964353
Epoch #93: loss=0.013890566957993662
Epoch #94: loss=0.014065396414501104
Epoch #95: loss=0.01077258042408199
Epoch #96: loss=0.009441879371550943
Epoch #97: loss=0.01923328237501796
Epoch #98: loss=0.011383622073779388
Epoch #99: loss=0.014048157119838126
Epoch #100: loss=0.016018756550973682
Epoch #101: loss=0.011102151685198616
Epoch #102: loss=0.02041131345223949
Epoch #103: loss=0.00795540523967534
Epoch #104: loss=0.010171715183925071
Epoch #105: loss=0.012025344214462759
Epoch #106: loss=0.012826666629034592
Epoch #107: loss=0.009884685594747445
Epoch #108: loss=0.010065561429895512
Epoch #109: loss=0.014354159174573741
Epoch #110: loss=0.013637199346842094
Epoch #111: loss=0.009141245347847028
Epoch #112: loss=0.012869162924796054
Epoch #113: loss=0.013832910398676927
Epoch #114: loss=0.010319363136831588
Epoch #115: loss=0.01265289608989864
Epoch #116: loss=0.013298149874657456
Epoch #117: loss=0.00864806014272498
Epoch #118: loss=0.009752009461362947
Epoch #119: loss=0.012057962371957498
Epoch #120: loss=0.01047994572951597
Epoch #121: loss=0.011334266898590604
Epoch #122: loss=0.009938710115624423
Epoch #123: loss=0.008961057160980605
Epoch #124: loss=0.013306939264741512
Epoch #125: loss=0.010002456489474228
Epoch #126: loss=0.011466030414638976
Epoch #127: loss=0.006414638599732194
Epoch #128: loss=0.010316033796138631
Epoch #129: loss=0.00970388051417869
Epoch #130: loss=0.013099428875228534
Epoch #131: loss=0.010117902674166377
Epoch #132: loss=0.00980621725363623
Epoch #133: loss=0.00992830024049767
Epoch #134: loss=0.00969057832725852
Epoch #135: loss=0.010589234017303417
Epoch #136: loss=0.00937980778849258
Epoch #137: loss=0.016579041699470484
Epoch #138: loss=0.01047119641133487
Epoch #139: loss=0.011128824599469943
Epoch #140: loss=0.010739497571458548
Epoch #141: loss=0.012465011905565848
Epoch #142: loss=0.007824225543981158
Epoch #143: loss=0.011267068665113992
Epoch #144: loss=0.01087752418599436
Epoch #145: loss=0.01106341232888425
Epoch #146: loss=0.008302219442118158
Epoch #147: loss=0.011201900788421475
Epoch #148: loss=0.009501996918493648
Epoch #149: loss=0.012201214675813556
Epoch #150: loss=0.00914774589018127
Epoch #151: loss=0.009891002385785849
Epoch #152: loss=0.006871382115192381
Epoch #153: loss=0.011871406414179211
Epoch #154: loss=0.007549820118509933
Epoch #155: loss=0.008999299104986399
Epoch #156: loss=0.008817276687273522
Epoch #157: loss=0.007492420226612049
Epoch #158: loss=0.010161594682507703
Epoch #159: loss=0.009588000930816265
Epoch #160: loss=0.009653400131906202
Epoch #161: loss=0.01047247244735315
Epoch #162: loss=0.007641297143575301
Epoch #163: loss=0.011271325615994867
Epoch #164: loss=0.013141723038183504
Epoch #165: loss=0.008633142089316133
Epoch #166: loss=0.014269121346220477
Epoch #167: loss=0.006979082728889519
Epoch #168: loss=0.01199668140560422
Epoch #169: loss=0.007426942593202255
Epoch #170: loss=0.008634493787080519
Epoch #171: loss=0.007820918760681889
Epoch #172: loss=0.011915656650228904
Epoch #173: loss=0.0075958841013404905
Epoch #174: loss=0.010178214250848253
Epoch #175: loss=0.007452268185429714
Epoch #176: loss=0.009552661714438583
Epoch #177: loss=0.010901120323640557
Epoch #178: loss=0.010194777398859498
Epoch #179: loss=0.011244553045993857
Epoch #180: loss=0.00964676281197534
Epoch #181: loss=0.009982986149587432
Epoch #182: loss=0.008908942097755479
Epoch #183: loss=0.006050722254838737
Epoch #184: loss=0.009049171639407771
Epoch #185: loss=0.007332823835750861
Epoch #186: loss=0.009957687699966205
Epoch #187: loss=0.006578917491807227
Epoch #188: loss=0.012823582286858893
Epoch #189: loss=0.008557334228036752
Epoch #190: loss=0.009379315378921503
Epoch #191: loss=0.010400964372645167
Epoch #192: loss=0.010884983913878455
Epoch #193: loss=0.008025520385714564
Epoch #194: loss=0.011241223257360154
Epoch #195: loss=0.00843402113122291
Epoch #196: loss=0.009954706827879845
Epoch #197: loss=0.00977727162345331
Epoch #198: loss=0.013802567263843732
Epoch #199: loss=0.0071515512720054675
Epoch #200: loss=0.008991309412448436
Epoch #201: loss=0.014458438074363365
Epoch #202: loss=0.011123064719735891
Epoch #203: loss=0.005270228642205309
Epoch #204: loss=0.012871288735612291
Epoch #205: loss=0.00953275633708738
Epoch #206: loss=0.010646354948335967
Epoch #207: loss=0.006532254965161023
Epoch #208: loss=0.009509856017663762
Epoch #209: loss=0.0097263325454013
Epoch #210: loss=0.008786972324683808
Epoch #211: loss=0.013178506987556123
Epoch #212: loss=0.009708241063914288
Epoch #213: loss=0.009357066679267065
Epoch #214: loss=0.010130994330188957
Epoch #215: loss=0.0059315082317916434
Epoch #216: loss=0.010098294473936823
Epoch #217: loss=0.01355876850668067
Epoch #218: loss=0.008035102428301884
Epoch #219: loss=0.008528744599800431
Epoch #220: loss=0.0070711851930633975
Epoch #221: loss=0.008870881698090126
Epoch #222: loss=0.00945146028488258
Epoch #223: loss=0.009026328951103843
Epoch #224: loss=0.00843706516165967
Epoch #225: loss=0.007899411429944532
Epoch #226: loss=0.01050556551465843
Epoch #227: loss=0.0063401800413400095
Epoch #228: loss=0.00593793833606918
Epoch #229: loss=0.006795663616907672
Epoch #230: loss=0.011526744138012602
Epoch #231: loss=0.0062473978495485305
Epoch #232: loss=0.0073901819598603015
Epoch #233: loss=0.008153618343383153
Epoch #234: loss=0.008044745397777829
Epoch #235: loss=0.008838920185941679
Epoch #236: loss=0.009053631743229906
Epoch #237: loss=0.008411388287373332
Epoch #238: loss=0.008584829264813746
Epoch #239: loss=0.008526716168604977
Epoch #240: loss=0.010253304697509761
Epoch #241: loss=0.007100623956545306
Epoch #242: loss=0.009350568527949035
Epoch #243: loss=0.00925665988116409
Epoch #244: loss=0.006161797281436786
Epoch #245: loss=0.010787474544707446
Epoch #246: loss=0.011673834235859099
Epoch #247: loss=0.007127933830410732
Epoch #248: loss=0.007516542021259211
Epoch #249: loss=0.009162528567122748

Training time: 14:49:22.322946

Finished.
n2one setting electricity_traffic -> weather
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/electricity_traffic_epochs_250_seed_2021/model.pkl', muti_dataset='electricity_traffic', random_seed=2021, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='weather')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.59747e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=3.23574e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.5930474475755568, 'MAE': 0.6036354422768974}
Finished.
------------------------- record done -------------------------
n2one setting electricity_traffic -> exchange
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/electricity_traffic_epochs_250_seed_2021/model.pkl', muti_dataset='electricity_traffic', random_seed=2021, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='exchange')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=5.12293e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.21284504415202618, 'MAE': 0.3168378430173435}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='electricity_weather', random_seed=2021, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/electricity_weather_epochs_250_seed_2021
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=1.6905833929347718
Epoch #1: loss=0.7579398478142466
Epoch #2: loss=0.5858637328965519
Epoch #3: loss=0.45036093972258334
Epoch #4: loss=0.38734334290714706
Epoch #5: loss=0.34219215123213675
Epoch #6: loss=0.30603896171627537
Epoch #7: loss=0.26888268322869063
Epoch #8: loss=0.25309965564125897
Epoch #9: loss=0.21518706983379396
Epoch #10: loss=0.20250993782402116
Epoch #11: loss=0.20488683833211918
Epoch #12: loss=0.16359246009021397
Epoch #13: loss=0.15962276403773415
Epoch #14: loss=0.1901233865951942
Epoch #15: loss=0.14663358686507952
Epoch #16: loss=0.13859142736674043
Epoch #17: loss=0.13052930546017305
Epoch #18: loss=0.1120923893950257
Epoch #19: loss=0.10387209618211823
Epoch #20: loss=0.08383355189855572
Epoch #21: loss=0.09663359205686736
Epoch #22: loss=0.10809966578532107
Epoch #23: loss=0.07479279941103557
Epoch #24: loss=0.08000789950427302
Epoch #25: loss=0.08097520026697722
Epoch #26: loss=0.07359714493804989
Epoch #27: loss=0.05539852676387093
Epoch #28: loss=0.06738329118369979
Epoch #29: loss=0.05678688813006285
Epoch #30: loss=0.06327028312218463
Epoch #31: loss=0.08485578192580459
Epoch #32: loss=0.06239761982478137
Epoch #33: loss=0.05685109653045834
Epoch #34: loss=0.05440076499417832
Epoch #35: loss=0.05353960172898574
Epoch #36: loss=0.05321410984090752
Epoch #37: loss=0.03615535753752274
Epoch #38: loss=0.04546627886754377
Epoch #39: loss=0.04488067006724497
Epoch #40: loss=0.04372755644147845
Epoch #41: loss=0.046903989886360443
Epoch #42: loss=0.05121897918276854
Epoch #43: loss=0.047836682588796024
Epoch #44: loss=0.04177283297255313
Epoch #45: loss=0.046696014225252123
Epoch #46: loss=0.04232968910736829
Epoch #47: loss=0.04259623978313821
Epoch #48: loss=0.048588965123691076
Epoch #49: loss=0.03280021104913323
Epoch #50: loss=0.03780811717626384
Epoch #51: loss=0.03777956267448427
Epoch #52: loss=0.03727136957841611
Epoch #53: loss=0.03459165143775043
Epoch #54: loss=0.029190344972501076
Epoch #55: loss=0.03320320559160982
Epoch #56: loss=0.03314596867243832
Epoch #57: loss=0.023632364876981643
Epoch #58: loss=0.03630370440757337
Epoch #59: loss=0.04530973232992702
Epoch #60: loss=0.02036888896345405
Epoch #61: loss=0.033039810218491325
Epoch #62: loss=0.028653655791996042
Epoch #63: loss=0.0263707184760163
Epoch #64: loss=0.024016780003781837
Epoch #65: loss=0.0228382815574685
Epoch #66: loss=0.025296216268999562
Epoch #67: loss=0.02355135361743173
Epoch #68: loss=0.038237416361571325
Epoch #69: loss=0.040502201352450966
Epoch #70: loss=0.021886291145078667
Epoch #71: loss=0.027805434091698368
Epoch #72: loss=0.020827367938035073
Epoch #73: loss=0.021393053318838177
Epoch #74: loss=0.03139388017854264
Epoch #75: loss=0.03157873861646006
Epoch #76: loss=0.027938830344508312
Epoch #77: loss=0.029371725363785708
Epoch #78: loss=0.039318153626280127
Epoch #79: loss=0.016188485405987576
Epoch #80: loss=0.021758381714131844
Epoch #81: loss=0.02032424581353888
Epoch #82: loss=0.01930164283940633
Epoch #83: loss=0.023633627732276317
Epoch #84: loss=0.01820761643357505
Epoch #85: loss=0.02397478722111035
Epoch #86: loss=0.019020670596161263
Epoch #87: loss=0.027679286862291416
Epoch #88: loss=0.021509131250610464
Epoch #89: loss=0.021276102660147348
Epoch #90: loss=0.024086923031498723
Epoch #91: loss=0.01859881963244685
Epoch #92: loss=0.0214810062529283
Epoch #93: loss=0.029443978885039742
Epoch #94: loss=0.0332313957290324
Epoch #95: loss=0.026553173506194967
Epoch #96: loss=0.0210231980521927
Epoch #97: loss=0.025027935414961525
Epoch #98: loss=0.016824527799456465
Epoch #99: loss=0.017696764678439336
Epoch #100: loss=0.02048963116349717
Epoch #101: loss=0.024537984675980073
Epoch #102: loss=0.03527338793487838
Epoch #103: loss=0.0225289480249741
Epoch #104: loss=0.03370965020939096
Epoch #105: loss=0.027185738155979296
Epoch #106: loss=0.015943707732694434
Epoch #107: loss=0.023860971191233718
Epoch #108: loss=0.018944865118971185
Epoch #109: loss=0.028166114278500014
Epoch #110: loss=0.01838574553044749
Epoch #111: loss=0.01687556579254463
Epoch #112: loss=0.01972657876343853
Epoch #113: loss=0.018691544656699487
Epoch #114: loss=0.018231260743574576
Epoch #115: loss=0.024857970882745283
Epoch #116: loss=0.022902476008490307
Epoch #117: loss=0.016116578707113187
Epoch #118: loss=0.014877657626449568
Epoch #119: loss=0.044805599207220256
Epoch #120: loss=0.016019567179457374
Epoch #121: loss=0.0134937797969285
Epoch #122: loss=0.015401906349631804
Epoch #123: loss=0.018523091212638336
Epoch #124: loss=0.030126799864195267
Epoch #125: loss=0.02183238824799936
Epoch #126: loss=0.02543772813550033
Epoch #127: loss=0.03943239818800171
Epoch #128: loss=0.03256810870879096
Epoch #129: loss=0.015314465387145954
Epoch #130: loss=0.01442558102119145
Epoch #131: loss=0.017519627150244774
Epoch #132: loss=0.0169006574485298
Epoch #133: loss=0.01682530053618152
Epoch #134: loss=0.027680099524774987
Epoch #135: loss=0.03727885194526051
Epoch #136: loss=0.02158346203436286
Epoch #137: loss=0.010196646407578055
Epoch #138: loss=0.015764243016841414
Epoch #139: loss=0.013045917021383073
Epoch #140: loss=0.013082143313839216
Epoch #141: loss=0.024383212167465043
Epoch #142: loss=0.021502644784804917
Epoch #143: loss=0.03391187496433673
Epoch #144: loss=0.023474511002849315
Epoch #145: loss=0.012267936243016746
Epoch #146: loss=0.01926815577363998
Epoch #147: loss=0.015203172335789005
Epoch #148: loss=0.009306877853415278
Epoch #149: loss=0.01187544450467796
Epoch #150: loss=0.016190077663299975
Epoch #151: loss=0.020277281420275847
Epoch #152: loss=0.016163002612218206
Epoch #153: loss=0.01639232721571577
Epoch #154: loss=0.014464355678423723
Epoch #155: loss=0.010007756260480002
Epoch #156: loss=0.01609437179973789
Epoch #157: loss=0.022315330850363035
Epoch #158: loss=0.02570632767446892
Epoch #159: loss=0.015622729160628344
Epoch #160: loss=0.01132844857576636
Epoch #161: loss=0.013149844979906256
Epoch #162: loss=0.012881740872004749
Epoch #163: loss=0.013790145004604291
Epoch #164: loss=0.021904135796393102
Epoch #165: loss=0.015597897383145537
Epoch #166: loss=0.02372701197757952
Epoch #167: loss=0.010731859513429618
Epoch #168: loss=0.018547523715929194
Epoch #169: loss=0.031921436177648504
Epoch #170: loss=0.020271131774190522
Epoch #171: loss=0.019212514853929587
Epoch #172: loss=0.01175759064249348
Epoch #173: loss=0.011132346148054473
Epoch #174: loss=0.013427168981847152
Epoch #175: loss=0.01342643198412266
Epoch #176: loss=0.015605819640045057
Epoch #177: loss=0.012046665693660098
Epoch #178: loss=0.015762221394576434
Epoch #179: loss=0.012569281648843139
Epoch #180: loss=0.013062995897659023
Epoch #181: loss=0.011762763785871997
Epoch #182: loss=0.01799837857205018
Epoch #183: loss=0.01612219640999222
Epoch #184: loss=0.013914191929666428
Epoch #185: loss=0.014777422924888366
Epoch #186: loss=0.013974404623997008
Epoch #187: loss=0.019323631606359297
Epoch #188: loss=0.012607604174127249
Epoch #189: loss=0.009833491987398468
Epoch #190: loss=0.014438786289435932
Epoch #191: loss=0.011121400533413772
Epoch #192: loss=0.0194041756562478
Epoch #193: loss=0.01807040740552651
Epoch #194: loss=0.014248351526255578
Epoch #195: loss=0.016539028506210713
Epoch #196: loss=0.009037616137386318
Epoch #197: loss=0.013502188073999586
Epoch #198: loss=0.012664744465562223
Epoch #199: loss=0.011611547881518611
Epoch #200: loss=0.01336269297684685
Epoch #201: loss=0.02037816256907385
Epoch #202: loss=0.013874234470286505
Epoch #203: loss=0.029907990990626635
Epoch #204: loss=0.031198599790954313
Epoch #205: loss=0.02687290963053875
Epoch #206: loss=0.016264249674334413
Epoch #207: loss=0.03020233650024516
Epoch #208: loss=0.013359012831537917
Epoch #209: loss=0.010138273899633335
Epoch #210: loss=0.009083650179823956
Epoch #211: loss=0.010376413764434756
Epoch #212: loss=0.012527356439394881
Epoch #213: loss=0.012180975762461594
Epoch #214: loss=0.02343308998748891
Epoch #215: loss=0.01164300171829497
Epoch #216: loss=0.014317157590994611
Epoch #217: loss=0.013759304892965552
Epoch #218: loss=0.011804032485538917
Epoch #219: loss=0.010269033773741833
Epoch #220: loss=0.011736702053507075
Epoch #221: loss=0.010031318235827036
Epoch #222: loss=0.01150662158247686
Epoch #223: loss=0.014470890768426564
Epoch #224: loss=0.020941094823801618
Epoch #225: loss=0.014101805575547594
Epoch #226: loss=0.011899049311134191
Epoch #227: loss=0.015024442490564675
Epoch #228: loss=0.009995489917158401
Epoch #229: loss=0.014770364040751762
Epoch #230: loss=0.011739342087381237
Epoch #231: loss=0.015739903787804516
Epoch #232: loss=0.012564101800764038
Epoch #233: loss=0.012388575086215785
Epoch #234: loss=0.016266326109847854
Epoch #235: loss=0.04415557918746711
Epoch #236: loss=0.015085864133102235
Epoch #237: loss=0.017187540499078814
Epoch #238: loss=0.009649531944971585
Epoch #239: loss=0.010426670701463737
Epoch #240: loss=0.018648594573391093
Epoch #241: loss=0.008542170113785278
Epoch #242: loss=0.013106579434350387
Epoch #243: loss=0.011000028756206925
Epoch #244: loss=0.013393211346981853
Epoch #245: loss=0.01000668103196081
Epoch #246: loss=0.010736936583470216
Epoch #247: loss=0.01745950101498348
Epoch #248: loss=0.00999500239591694
Epoch #249: loss=0.010442482866145542

Training time: 6:14:45.940801

Finished.
n2one setting electricity_weather -> exchange
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/electricity_weather_epochs_250_seed_2021/model.pkl', muti_dataset='electricity_weather', random_seed=2021, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='exchange')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=5.56489e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.3022539644855672, 'MAE': 0.3695723757188633}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='electricity_exchange', random_seed=2021, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/electricity_exchange_epochs_250_seed_2021
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=1.6412204357266065
Epoch #1: loss=0.7108313061727216
Epoch #2: loss=0.4622229759363418
Epoch #3: loss=0.39911880228657126
Epoch #4: loss=0.3317981849687802
Epoch #5: loss=0.3007664123838796
Epoch #6: loss=0.2602816470488227
Epoch #7: loss=0.2265489226416614
Epoch #8: loss=0.2038331812185357
Epoch #9: loss=0.19867526640192715
Epoch #10: loss=0.17548742679748855
Epoch #11: loss=0.16199842536974823
Epoch #12: loss=0.15365879695103887
Epoch #13: loss=0.12669831803096826
Epoch #14: loss=0.1382041767601909
Epoch #15: loss=0.10239271572748579
Epoch #16: loss=0.10872515856562957
Epoch #17: loss=0.11909088135716762
Epoch #18: loss=0.09965601007490897
Epoch #19: loss=0.08224212834866304
Epoch #20: loss=0.09548980804969837
Epoch #21: loss=0.08809884306677479
Epoch #22: loss=0.06524806538183457
Epoch #23: loss=0.0683882434464606
Epoch #24: loss=0.07165734165724605
Epoch #25: loss=0.06018241955318395
Epoch #26: loss=0.06188363353735951
Epoch #27: loss=0.07210870544740358
Epoch #28: loss=0.06706798076346562
Epoch #29: loss=0.06359999184142855
Epoch #30: loss=0.06305450990960274
Epoch #31: loss=0.05775058077857759
Epoch #32: loss=0.04442993041812966
Epoch #33: loss=0.050555185721989006
Epoch #34: loss=0.0496386426471253
Epoch #35: loss=0.0436302003440784
Epoch #36: loss=0.049281454023743745
Epoch #37: loss=0.041703568099174186
Epoch #38: loss=0.04691280151284257
Epoch #39: loss=0.03914396828012173
Epoch #40: loss=0.03690827374790985
Epoch #41: loss=0.04966586546428887
Epoch #42: loss=0.043369726206410934
Epoch #43: loss=0.03729705766585053
Epoch #44: loss=0.039463311735268874
Epoch #45: loss=0.053854863378743
Epoch #46: loss=0.045266881639613434
Epoch #47: loss=0.04146128179474329
Epoch #48: loss=0.03496964589839852
Epoch #49: loss=0.03491608711953153
Epoch #50: loss=0.044585681461664
Epoch #51: loss=0.03389529548698646
Epoch #52: loss=0.025251209909254883
Epoch #53: loss=0.028998494618161834
Epoch #54: loss=0.039508377712454905
Epoch #55: loss=0.03131476287251638
Epoch #56: loss=0.030195554641236785
Epoch #57: loss=0.0343153102084213
Epoch #58: loss=0.02660296204015839
Epoch #59: loss=0.02945958367176637
Epoch #60: loss=0.03920824476623891
Epoch #61: loss=0.02987462403939182
Epoch #62: loss=0.03974672292761142
Epoch #63: loss=0.03090198797677675
Epoch #64: loss=0.026098238652087973
Epoch #65: loss=0.02339088815154004
Epoch #66: loss=0.023359031540675704
Epoch #67: loss=0.07294470614910771
Epoch #68: loss=0.03755763202189471
Epoch #69: loss=0.029730120098422137
Epoch #70: loss=0.021497711295468624
Epoch #71: loss=0.02078645528891438
Epoch #72: loss=0.04357245309231661
Epoch #73: loss=0.0331914621298442
Epoch #74: loss=0.03892699654474515
Epoch #75: loss=0.026847357508205556
Epoch #76: loss=0.03227109127779885
Epoch #77: loss=0.024935573330262285
Epoch #78: loss=0.028057896480810657
Epoch #79: loss=0.027261822395618314
Epoch #80: loss=0.01809879714986847
Epoch #81: loss=0.04841032596503166
Epoch #82: loss=0.03547957989968937
Epoch #83: loss=0.031533503090385294
Epoch #84: loss=0.02346079982286404
Epoch #85: loss=0.02318824179212939
Epoch #86: loss=0.022588994436809583
Epoch #87: loss=0.029119272876635623
Epoch #88: loss=0.02788309639498116
Epoch #89: loss=0.02970388321960373
Epoch #90: loss=0.027560838894482624
Epoch #91: loss=0.01700412497627846
Epoch #92: loss=0.019124276668900528
Epoch #93: loss=0.01969987055142396
Epoch #94: loss=0.03353076399301142
Epoch #95: loss=0.040689466088036454
Epoch #96: loss=0.020809850412580867
Epoch #97: loss=0.020292811972367944
Epoch #98: loss=0.024794351502737785
Epoch #99: loss=0.023838769160791393
Epoch #100: loss=0.03193932236136874
Epoch #101: loss=0.038350113689486154
Epoch #102: loss=0.02853514923958963
Epoch #103: loss=0.021937510291486696
Epoch #104: loss=0.022530010922622263
Epoch #105: loss=0.01961449033115059
Epoch #106: loss=0.019638929878769154
Epoch #107: loss=0.023256586740036864
Epoch #108: loss=0.01806700753106801
Epoch #109: loss=0.015852994693508288
Epoch #110: loss=0.01813898573411589
Epoch #111: loss=0.021799366410232876
Epoch #112: loss=0.0207519729778816
Epoch #113: loss=0.01868520862328053
Epoch #114: loss=0.027754689812890924
Epoch #115: loss=0.020738016189101482
Epoch #116: loss=0.017946138004662673
Epoch #117: loss=0.015806488134214616
Epoch #118: loss=0.0326339053729617
Epoch #119: loss=0.041833015209018595
Epoch #120: loss=0.015547031002390937
Epoch #121: loss=0.015839013081018483
Epoch #122: loss=0.018307864251631483
Epoch #123: loss=0.023082024284063817
Epoch #124: loss=0.022090287023686768
Epoch #125: loss=0.02306158589567185
Epoch #126: loss=0.017072217840960358
Epoch #127: loss=0.01741742938772866
Epoch #128: loss=0.019615314302344644
Epoch #129: loss=0.024697583837867756
Epoch #130: loss=0.018230167735936997
Epoch #131: loss=0.03524085439813796
Epoch #132: loss=0.023129230411334026
Epoch #133: loss=0.024474652319167116
Epoch #134: loss=0.023422375712830342
Epoch #135: loss=0.020054925451094798
Epoch #136: loss=0.016950270997701456
Epoch #137: loss=0.014527373735072504
Epoch #138: loss=0.020184527424809532
Epoch #139: loss=0.010415037857909265
Epoch #140: loss=0.013534528111662478
Epoch #141: loss=0.014250923262514162
Epoch #142: loss=0.024563301081831444
Epoch #143: loss=0.0273951597707799
Epoch #144: loss=0.0298501855660496
Epoch #145: loss=0.02250809669880321
Epoch #146: loss=0.01633553269781359
Epoch #147: loss=0.016967133388537517
Epoch #148: loss=0.022072860614062603
Epoch #149: loss=0.01635443923943945
Epoch #150: loss=0.025292904189459436
Epoch #151: loss=0.016346063405080023
Epoch #152: loss=0.01406294966813978
Epoch #153: loss=0.018126335942630845
Epoch #154: loss=0.016296047445756065
Epoch #155: loss=0.021880122169138903
Epoch #156: loss=0.028947902184494948
Epoch #157: loss=0.0185704928722677
Epoch #158: loss=0.012407803193044688
Epoch #159: loss=0.015711224022675958
Epoch #160: loss=0.015881142472511434
Epoch #161: loss=0.018019806381702808
Epoch #162: loss=0.015540641064452727
Epoch #163: loss=0.012726582326865429
Epoch #164: loss=0.02210677116751914
Epoch #165: loss=0.017307502320559984
Epoch #166: loss=0.016887711410235196
Epoch #167: loss=0.016761548169619632
Epoch #168: loss=0.011613649848316396
Epoch #169: loss=0.015025640996901946
Epoch #170: loss=0.016940274129170176
Epoch #171: loss=0.02607051767394694
Epoch #172: loss=0.03337049699002045
Epoch #173: loss=0.02328484344910434
Epoch #174: loss=0.041878744820769774
Epoch #175: loss=0.01623494967221676
Epoch #176: loss=0.010884229500889835
Epoch #177: loss=0.04312688853353896
Epoch #178: loss=0.024508199374804324
Epoch #179: loss=0.016693922401158887
Epoch #180: loss=0.011218031434965742
Epoch #181: loss=0.011079330988600568
Epoch #182: loss=0.009813807815285739
Epoch #183: loss=0.018819059564811277
Epoch #184: loss=0.018943094316867144
Epoch #185: loss=0.022015851570079584
Epoch #186: loss=0.01637503021063374
Epoch #187: loss=0.014806426449404254
Epoch #188: loss=0.013648217719290293
Epoch #189: loss=0.017635412329502154
Epoch #190: loss=0.018085368287499775
Epoch #191: loss=0.01382955230364157
Epoch #192: loss=0.016985995064618342
Epoch #193: loss=0.010828536461616264
Epoch #194: loss=0.02095577965446643
Epoch #195: loss=0.011385214918591706
Epoch #196: loss=0.03548866500872779
Epoch #197: loss=0.014020344679630262
Epoch #198: loss=0.00937509350770572
Epoch #199: loss=0.020534080737355336
Epoch #200: loss=0.023477097904050614
Epoch #201: loss=0.015903789372081475
Epoch #202: loss=0.02503112444971153
Epoch #203: loss=0.021780196564572106
Epoch #204: loss=0.015787809349965917
Epoch #205: loss=0.017856356507736204
Epoch #206: loss=0.01012626456646645
Epoch #207: loss=0.019422113370485775
Epoch #208: loss=0.020924275122104513
Epoch #209: loss=0.014763786760901914
Epoch #210: loss=0.016021818787480158
Epoch #211: loss=0.0170042687506163
Epoch #212: loss=0.018984525781521138
Epoch #213: loss=0.009400966215778232
Epoch #214: loss=0.013714585578356414
Epoch #215: loss=0.00861222523441156
Epoch #216: loss=0.014309760758980238
Epoch #217: loss=0.013420688192746503
Epoch #218: loss=0.016562416027144367
Epoch #219: loss=0.015428271361359177
Epoch #220: loss=0.010006892151538266
Epoch #221: loss=0.024564026176255752
Epoch #222: loss=0.02267015444172835
Epoch #223: loss=0.017778381682107207
Epoch #224: loss=0.018996928559331716
Epoch #225: loss=0.020821904413802633
Epoch #226: loss=0.017842882937726234
Epoch #227: loss=0.012906863816977001
Epoch #228: loss=0.01420240463157493
Epoch #229: loss=0.017352669356506895
Epoch #230: loss=0.011937869536318167
Epoch #231: loss=0.013947026465105114
Epoch #232: loss=0.015922008319701263
Epoch #233: loss=0.011633789573730987
Epoch #234: loss=0.020664168825538655
Epoch #235: loss=0.022072456846449602
Epoch #236: loss=0.018043817651506207
Epoch #237: loss=0.015945716191183686
Epoch #238: loss=0.01871667700939271
Epoch #239: loss=0.022705785564279983
Epoch #240: loss=0.010219219900946498
Epoch #241: loss=0.013296204315509794
Epoch #242: loss=0.02788145557232767
Epoch #243: loss=0.012586640449800589
Epoch #244: loss=0.010546661239052985
Epoch #245: loss=0.00911271139394809
Epoch #246: loss=0.007347903545241022
Epoch #247: loss=0.013906414654193655
Epoch #248: loss=0.013928280893315513
Epoch #249: loss=0.01265486477135389

Training time: 4:43:52.978045

Finished.
n2one setting electricity_exchange -> weather
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/electricity_exchange_epochs_250_seed_2021/model.pkl', muti_dataset='electricity_exchange', random_seed=2021, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='weather')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.41069e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=3.00987e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.42216600438484486, 'MAE': 0.4755482923406766}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='traffic_weather', random_seed=2021, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/traffic_weather_epochs_250_seed_2021
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=1.0968072623148695
Epoch #1: loss=0.4317072455578291
Epoch #2: loss=0.3115229817728202
Epoch #3: loss=0.22984164594127252
Epoch #4: loss=0.19848642463015542
Epoch #5: loss=0.16493496872266536
Epoch #6: loss=0.1435183200750265
Epoch #7: loss=0.12201107572344644
Epoch #8: loss=0.10574606377925086
Epoch #9: loss=0.09463930433628627
Epoch #10: loss=0.08087608729791199
Epoch #11: loss=0.07519984783957133
Epoch #12: loss=0.07677469756045202
Epoch #13: loss=0.07534079093051874
Epoch #14: loss=0.06117543454967892
Epoch #15: loss=0.05098963079151807
Epoch #16: loss=0.05946180812447114
Epoch #17: loss=0.046830535061114155
Epoch #18: loss=0.05997358743171627
Epoch #19: loss=0.04483419696607434
Epoch #20: loss=0.048207541934774584
Epoch #21: loss=0.03484074599927644
Epoch #22: loss=0.04768145085744526
Epoch #23: loss=0.03601862631238963
Epoch #24: loss=0.033340209040230745
Epoch #25: loss=0.03605483011130206
Epoch #26: loss=0.04057831821810068
Epoch #27: loss=0.032904690074124
Epoch #28: loss=0.030676432387449208
Epoch #29: loss=0.035867143036018394
Epoch #30: loss=0.03477494909151705
Epoch #31: loss=0.0407742829855714
Epoch #32: loss=0.029456576360373857
Epoch #33: loss=0.034585806382778056
Epoch #34: loss=0.023015149769652216
Epoch #35: loss=0.032817821001617224
Epoch #36: loss=0.024801605163317232
Epoch #37: loss=0.02680722000461485
Epoch #38: loss=0.03333161360723688
Epoch #39: loss=0.02116376122403135
Epoch #40: loss=0.0211941087200922
Epoch #41: loss=0.024951344061887253
Epoch #42: loss=0.026627025408562807
Epoch #43: loss=0.02481773785912738
Epoch #44: loss=0.027407230127619597
Epoch #45: loss=0.02432953115675372
Epoch #46: loss=0.024536039004037325
Epoch #47: loss=0.01863053114173634
Epoch #48: loss=0.021304252624447743
Epoch #49: loss=0.02886344199797751
Epoch #50: loss=0.02372509216306876
Epoch #51: loss=0.028126140468573483
Epoch #52: loss=0.023984078777445846
Epoch #53: loss=0.02451019518965958
Epoch #54: loss=0.02259525468102069
Epoch #55: loss=0.019431994701442017
Epoch #56: loss=0.024600183121259073
Epoch #57: loss=0.019193578391783737
Epoch #58: loss=0.026667276552043487
Epoch #59: loss=0.01945034890760891
Epoch #60: loss=0.02188883487890036
Epoch #61: loss=0.022874694335343095
Epoch #62: loss=0.01709644824180171
Epoch #63: loss=0.022369984377226437
Epoch #64: loss=0.022298584633087582
Epoch #65: loss=0.016713211437662894
Epoch #66: loss=0.018836844841693214
Epoch #67: loss=0.017393401747245366
Epoch #68: loss=0.029628477799485915
Epoch #69: loss=0.02709170338389126
Epoch #70: loss=0.016858932584065834
Epoch #71: loss=0.02030629166767687
Epoch #72: loss=0.021702327466357987
Epoch #73: loss=0.013653485339894253
Epoch #74: loss=0.020069682311677262
Epoch #75: loss=0.01765480162423767
Epoch #76: loss=0.018359943299000005
Epoch #77: loss=0.01786903392942124
Epoch #78: loss=0.018327011297842873
Epoch #79: loss=0.022330531002906454
Epoch #80: loss=0.019195932757455767
Epoch #81: loss=0.034994404516780646
Epoch #82: loss=0.018197373900080607
Epoch #83: loss=0.017681023771401343
Epoch #84: loss=0.017022268537697824
Epoch #85: loss=0.013917974082154993
Epoch #86: loss=0.01644871403268635
Epoch #87: loss=0.027367228502814075
Epoch #88: loss=0.013949710255261167
Epoch #89: loss=0.016463658642818256
Epoch #90: loss=0.02019533167211947
Epoch #91: loss=0.016451282501998118
Epoch #92: loss=0.017191065956246126
Epoch #93: loss=0.016225838167818373
Epoch #94: loss=0.01078088271157236
Epoch #95: loss=0.015027051425653274
Epoch #96: loss=0.02791056848893588
Epoch #97: loss=0.020776791133906593
Epoch #98: loss=0.020294484308029857
Epoch #99: loss=0.015176555649747103
Epoch #100: loss=0.016217837991011497
Epoch #101: loss=0.016791262394297124
Epoch #102: loss=0.01574405352165503
Epoch #103: loss=0.02122977964815643
Epoch #104: loss=0.015057387158883209
Epoch #105: loss=0.010144485403461577
Epoch #106: loss=0.02120176684190671
Epoch #107: loss=0.013141319832652083
Epoch #108: loss=0.03195419204894279
Epoch #109: loss=0.027538967048377008
Epoch #110: loss=0.018208889762382233
Epoch #111: loss=0.013972044216545229
Epoch #112: loss=0.012224233670913073
Epoch #113: loss=0.014291803867997038
Epoch #114: loss=0.01317127700459169
Epoch #115: loss=0.01458869196115605
Epoch #116: loss=0.027827778212426196
Epoch #117: loss=0.023592862471527008
Epoch #118: loss=0.01091811536327015
Epoch #119: loss=0.010388738812564354
Epoch #120: loss=0.01837024587367269
Epoch #121: loss=0.010222498000602785
Epoch #122: loss=0.011836532818428546
Epoch #123: loss=0.016149772565489634
Epoch #124: loss=0.015591382617728683
Epoch #125: loss=0.011224798953851138
Epoch #126: loss=0.017737225416157843
Epoch #127: loss=0.014923099763207673
Epoch #128: loss=0.01229489775512054
Epoch #129: loss=0.015694746133384476
Epoch #130: loss=0.012224842435390602
Epoch #131: loss=0.016882961064651567
Epoch #132: loss=0.01621366487302726
Epoch #133: loss=0.01910988994394109
Epoch #134: loss=0.014738782683510893
Epoch #135: loss=0.013769706874456035
Epoch #136: loss=0.014798710866884206
Epoch #137: loss=0.012664829321013033
Epoch #138: loss=0.018140246514490543
Epoch #139: loss=0.024377676953238052
Epoch #140: loss=0.010708596680381783
Epoch #141: loss=0.010382944451021513
Epoch #142: loss=0.011666047306691227
Epoch #143: loss=0.03686235024583807
Epoch #144: loss=0.022268147032829456
Epoch #145: loss=0.014158250363244725
Epoch #146: loss=0.011645027643516933
Epoch #147: loss=0.012238350712888516
Epoch #148: loss=0.014755676725267345
Epoch #149: loss=0.021894659404170926
Epoch #150: loss=0.013034325313374456
Epoch #151: loss=0.021510468056861486
Epoch #152: loss=0.015560607776032164
Epoch #153: loss=0.013802183630547448
Epoch #154: loss=0.012359901813528399
Epoch #155: loss=0.014504540887050884
Epoch #156: loss=0.010829076281732193
Epoch #157: loss=0.012772588574883123
Epoch #158: loss=0.01523125572255283
Epoch #159: loss=0.017498061987967335
Epoch #160: loss=0.01000239869376077
Epoch #161: loss=0.009989326442869774
Epoch #162: loss=0.015385037760515351
Epoch #163: loss=0.014967911578751556
Epoch #164: loss=0.01238828928902106
Epoch #165: loss=0.01683344440538644
Epoch #166: loss=0.011410798608070566
Epoch #167: loss=0.016555038372619468
Epoch #168: loss=0.015912733798465632
Epoch #169: loss=0.011464748203211415
Epoch #170: loss=0.010354754865319764
Epoch #171: loss=0.013288086242101673
Epoch #172: loss=0.011069956131823265
Epoch #173: loss=0.01730751018427278
Epoch #174: loss=0.011141696592882933
Epoch #175: loss=0.00742328840043466
Epoch #176: loss=0.013392365361551539
Epoch #177: loss=0.012426826664146197
Epoch #178: loss=0.011498847739301153
Epoch #179: loss=0.010877167644450621
Epoch #180: loss=0.01228431037976931
Epoch #181: loss=0.012849381765853003
Epoch #182: loss=0.02031014013479213
Epoch #183: loss=0.016779750646205398
Epoch #184: loss=0.014076042936734224
Epoch #185: loss=0.009744416599771596
Epoch #186: loss=0.013473744565786334
Epoch #187: loss=0.01242502352936565
Epoch #188: loss=0.010771529736810916
Epoch #189: loss=0.01125115660268109
Epoch #190: loss=0.01040385461301248
Epoch #191: loss=0.011918105470219932
Epoch #192: loss=0.012022539727941211
Epoch #193: loss=0.016411233025218616
Epoch #194: loss=0.01832676288245853
Epoch #195: loss=0.008443681311070355
Epoch #196: loss=0.012202306523610238
Epoch #197: loss=0.009152551063314538
Epoch #198: loss=0.01026349576234163
Epoch #199: loss=0.009118205144044917
Epoch #200: loss=0.0137846128479789
Epoch #201: loss=0.01481921630175412
Epoch #202: loss=0.014250380381437592
Epoch #203: loss=0.007606408535139053
Epoch #204: loss=0.010791853241162162
Epoch #205: loss=0.011797050975711737
Epoch #206: loss=0.011452875290119622
Epoch #207: loss=0.015072315541573032
Epoch #208: loss=0.006674335239327152
Epoch #209: loss=0.01755842039386374
Epoch #210: loss=0.015404204403485227
Epoch #211: loss=0.012135789131059789
Epoch #212: loss=0.012013902390085481
Epoch #213: loss=0.018009874672676594
Epoch #214: loss=0.011270118267356674
Epoch #215: loss=0.010502251775374912
Epoch #216: loss=0.011031706893612176
Epoch #217: loss=0.011556695058381002
Epoch #218: loss=0.012795901174720237
Epoch #219: loss=0.010488780391791774
Epoch #220: loss=0.014260892861186533
Epoch #221: loss=0.009574502437776123
Epoch #222: loss=0.011402564802247562
Epoch #223: loss=0.009094615480969404
Epoch #224: loss=0.008433440435723454
Epoch #225: loss=0.016027423847550085
Epoch #226: loss=0.011222779125222718
Epoch #227: loss=0.011701194489645999
Epoch #228: loss=0.009920555569470371
Epoch #229: loss=0.009177059105441658
Epoch #230: loss=0.012075323921520862
Epoch #231: loss=0.013580237229738536
Epoch #232: loss=0.015078755935775234
Epoch #233: loss=0.009555968710497706
Epoch #234: loss=0.013013873356694685
Epoch #235: loss=0.017954066541147635
Epoch #236: loss=0.01208332467998647
Epoch #237: loss=0.007540002883287925
Epoch #238: loss=0.013089337280853378
Epoch #239: loss=0.014234118526236433
Epoch #240: loss=0.012955720141149327
Epoch #241: loss=0.010248281473429193
Epoch #242: loss=0.008200163808158994
Epoch #243: loss=0.008206306733349643
Epoch #244: loss=0.011505699864092339
Epoch #245: loss=0.010686842069730005
Epoch #246: loss=0.00848579694426204
Epoch #247: loss=0.011644051179967203
Epoch #248: loss=0.0110967804131088
Epoch #249: loss=0.01944867148326798

Training time: 7:08:38.519894

Finished.
n2one setting traffic_weather -> electricity
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/traffic_weather_epochs_250_seed_2021/model.pkl', muti_dataset='traffic_weather', random_seed=2021, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='electricity')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=8.75122e-09): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.04366e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=5.104e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=8.75122e-09): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.3883260165258943, 'MAE': 0.44030288697753367}
Finished.
------------------------- record done -------------------------
n2one setting traffic_weather -> exchange
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/traffic_weather_epochs_250_seed_2021/model.pkl', muti_dataset='traffic_weather', random_seed=2021, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='exchange')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
Evaluation result: {'MSE': 0.4810303378637149, 'MAE': 0.4475978017732797}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='traffic_exchange', random_seed=2021, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/traffic_exchange_epochs_250_seed_2021
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=1.067282566017118
Epoch #1: loss=0.36774687373090065
Epoch #2: loss=0.2746586117422444
Epoch #3: loss=0.19681075639252005
Epoch #4: loss=0.1680721708198731
Epoch #5: loss=0.13833875746476923
Epoch #6: loss=0.11793541895204235
Epoch #7: loss=0.09813849639318797
Epoch #8: loss=0.08668870024340249
Epoch #9: loss=0.0736662564542273
Epoch #10: loss=0.06746252897495254
Epoch #11: loss=0.06592504946011836
Epoch #12: loss=0.06277386382564731
Epoch #13: loss=0.06281023412332709
Epoch #14: loss=0.0559451440370631
Epoch #15: loss=0.04645473872386347
Epoch #16: loss=0.05796825452038654
Epoch #17: loss=0.04623189332343681
Epoch #18: loss=0.05098200428015927
Epoch #19: loss=0.05938976076565383
Epoch #20: loss=0.04619125849433543
Epoch #21: loss=0.04302308912945633
Epoch #22: loss=0.03965471727645089
Epoch #23: loss=0.04513734581361086
Epoch #24: loss=0.037924223685042995
Epoch #25: loss=0.03414475516571353
Epoch #26: loss=0.03771724171465498
Epoch #27: loss=0.03437083635583584
Epoch #28: loss=0.03423547408457888
Epoch #29: loss=0.02798194210888047
Epoch #30: loss=0.026836909142713566
Epoch #31: loss=0.034003460417694316
Epoch #32: loss=0.027841954879166787
Epoch #33: loss=0.030466816329796018
Epoch #34: loss=0.02832689148202624
Epoch #35: loss=0.030312755238666233
Epoch #36: loss=0.03351853967178613
Epoch #37: loss=0.025842853757584917
Epoch #38: loss=0.028800575272581574
Epoch #39: loss=0.029088784894911335
Epoch #40: loss=0.02740614963320737
Epoch #41: loss=0.022628246907053794
Epoch #42: loss=0.02337797062321641
Epoch #43: loss=0.02972987887179265
Epoch #44: loss=0.021580931363124185
Epoch #45: loss=0.02499448051100768
Epoch #46: loss=0.023638342500225396
Epoch #47: loss=0.02959611382651218
Epoch #48: loss=0.023118106962446573
Epoch #49: loss=0.022866956808606977
Epoch #50: loss=0.02474894394807721
Epoch #51: loss=0.030016559805592587
Epoch #52: loss=0.02653425889606213
Epoch #53: loss=0.030710998459333748
Epoch #54: loss=0.01921137596839161
Epoch #55: loss=0.02346429729957305
Epoch #56: loss=0.016964052736708337
Epoch #57: loss=0.02403036531572091
Epoch #58: loss=0.01681292945473594
Epoch #59: loss=0.03504875800137451
Epoch #60: loss=0.02365762995630931
Epoch #61: loss=0.01994130584578921
Epoch #62: loss=0.022025957266899127
Epoch #63: loss=0.019812668901975212
Epoch #64: loss=0.015052826396435843
Epoch #65: loss=0.022215124701106943
Epoch #66: loss=0.023544144271819414
Epoch #67: loss=0.020994288523229182
Epoch #68: loss=0.020667985654457797
Epoch #69: loss=0.020964402182030373
Epoch #70: loss=0.019630744554946768
Epoch #71: loss=0.02062536829349253
Epoch #72: loss=0.04085632688345805
Epoch #73: loss=0.01848267218741674
Epoch #74: loss=0.021145509098905363
Epoch #75: loss=0.020378967353688746
Epoch #76: loss=0.016671744829991127
Epoch #77: loss=0.025068901991672363
Epoch #78: loss=0.02244317067409705
Epoch #79: loss=0.015138996687507653
Epoch #80: loss=0.016709614202250384
Epoch #81: loss=0.021254224129604583
Epoch #82: loss=0.020866176395759994
Epoch #83: loss=0.022598371219997904
Epoch #84: loss=0.01777299776427105
Epoch #85: loss=0.019747290031211024
Epoch #86: loss=0.024408884613972757
Epoch #87: loss=0.01617120748008441
Epoch #88: loss=0.01689965399805103
Epoch #89: loss=0.016189052854884072
Epoch #90: loss=0.015976121719931947
Epoch #91: loss=0.015563892431981358
Epoch #92: loss=0.015462200414325729
Epoch #93: loss=0.016284142868444804
Epoch #94: loss=0.01758262787379549
Epoch #95: loss=0.015019413070377208
Epoch #96: loss=0.01899786162425646
Epoch #97: loss=0.0216422478097689
Epoch #98: loss=0.015221049815641138
Epoch #99: loss=0.026560277209433877
Epoch #100: loss=0.01636905658808831
Epoch #101: loss=0.014313353259543268
Epoch #102: loss=0.01612849207227011
Epoch #103: loss=0.015331782926149645
Epoch #104: loss=0.013465756993177454
Epoch #105: loss=0.01742233002652687
Epoch #106: loss=0.02003069790883054
Epoch #107: loss=0.014757014369664328
Epoch #108: loss=0.014835354210665445
Epoch #109: loss=0.025036218810569624
Epoch #110: loss=0.013176676347964154
Epoch #111: loss=0.016112719690855377
Epoch #112: loss=0.016799429603270254
Epoch #113: loss=0.014447991812304625
Epoch #114: loss=0.019948594561937243
Epoch #115: loss=0.014450774396182274
Epoch #116: loss=0.02032960390842027
Epoch #117: loss=0.014516021391469182
Epoch #118: loss=0.013512048824528371
Epoch #119: loss=0.02110382729676959
Epoch #120: loss=0.013837383851113237
Epoch #121: loss=0.022737217558335094
Epoch #122: loss=0.014195181540582844
Epoch #123: loss=0.016385961411120746
Epoch #124: loss=0.017901179767294047
Epoch #125: loss=0.013375600483054402
Epoch #126: loss=0.016452386149007226
Epoch #127: loss=0.01638790825281517
Epoch #128: loss=0.014803487708333455
Epoch #129: loss=0.015301193480899065
Epoch #130: loss=0.009868458571463111
Epoch #131: loss=0.015614235770805665
Epoch #132: loss=0.015526738670020164
Epoch #133: loss=0.015501378493028259
Epoch #134: loss=0.013862995837544809
Epoch #135: loss=0.012497044900424201
Epoch #136: loss=0.01624201903900469
Epoch #137: loss=0.01944740551467664
Epoch #138: loss=0.016260290437012687
Epoch #139: loss=0.017173387254079825
Epoch #140: loss=0.03172506697880018
Epoch #141: loss=0.01017177399801861
Epoch #142: loss=0.013701183627554932
Epoch #143: loss=0.013653637032892994
Epoch #144: loss=0.0179920309165027
Epoch #145: loss=0.01750711943387541
Epoch #146: loss=0.01688071186058812
Epoch #147: loss=0.021834613919832165
Epoch #148: loss=0.013057129142870163
Epoch #149: loss=0.009672433965512947
Epoch #150: loss=0.021622194810810597
Epoch #151: loss=0.024569020429813442
Epoch #152: loss=0.012438831485348925
Epoch #153: loss=0.008707208523323396
Epoch #154: loss=0.015643673106404448
Epoch #155: loss=0.012655297866266424
Epoch #156: loss=0.010913042204595711
Epoch #157: loss=0.018923275737124014
Epoch #158: loss=0.011845379673786216
Epoch #159: loss=0.013157140082029369
Epoch #160: loss=0.01976854455688749
Epoch #161: loss=0.014359269219406496
Epoch #162: loss=0.017065557086406724
Epoch #163: loss=0.012935381719739519
Epoch #164: loss=0.014327472272856247
Epoch #165: loss=0.01480882694652363
Epoch #166: loss=0.01291068388358556
Epoch #167: loss=0.012880927027235258
Epoch #168: loss=0.014387683455458301
Epoch #169: loss=0.010497909887719247
Epoch #170: loss=0.010748964168021626
Epoch #171: loss=0.01320887228862741
Epoch #172: loss=0.015465858101579842
Epoch #173: loss=0.020750818739580953
Epoch #174: loss=0.01206443432351205
Epoch #175: loss=0.008794584373913233
Epoch #176: loss=0.009539401772413296
Epoch #177: loss=0.01069835603461954
Epoch #178: loss=0.012716348832152013
Epoch #179: loss=0.01970470794777643
Epoch #180: loss=0.01092612721054713
Epoch #181: loss=0.028103839855704017
Epoch #182: loss=0.013189978799053426
Epoch #183: loss=0.01667148503033316
Epoch #184: loss=0.013842107045153778
Epoch #185: loss=0.01178457610996242
Epoch #186: loss=0.012952373238416233
Epoch #187: loss=0.011869641526698553
Epoch #188: loss=0.01402443969992241
Epoch #189: loss=0.01051281533640911
Epoch #190: loss=0.016234444812807957
Epoch #191: loss=0.013810238194412381
Epoch #192: loss=0.014641343415969042
Epoch #193: loss=0.014880362938689443
Epoch #194: loss=0.010200789460933087
Epoch #195: loss=0.010154627454694596
Epoch #196: loss=0.009439142855940033
Epoch #197: loss=0.016499648447551273
Epoch #198: loss=0.016344961951066602
Epoch #199: loss=0.010671388499449468
Epoch #200: loss=0.01617338815754167
Epoch #201: loss=0.012636301419517057
Epoch #202: loss=0.01398050342395436
Epoch #203: loss=0.012113496693842304
Epoch #204: loss=0.01253923897595672
Epoch #205: loss=0.012993617521796891
Epoch #206: loss=0.016486537748177994
Epoch #207: loss=0.013190872660281416
Epoch #208: loss=0.018470099559635855
Epoch #209: loss=0.013129551150908308
Epoch #210: loss=0.016085019116169483
Epoch #211: loss=0.0117966461924357
Epoch #212: loss=0.01769612301910435
Epoch #213: loss=0.01243061887725235
Epoch #214: loss=0.008791097259147618
Epoch #215: loss=0.010268433200658835
Epoch #216: loss=0.012764134840192086
Epoch #217: loss=0.014088703432232784
Epoch #218: loss=0.01214694557761661
Epoch #219: loss=0.01854022235473104
Epoch #220: loss=0.014656623106486808
Epoch #221: loss=0.01111664252158993
Epoch #222: loss=0.00846996305426101
Epoch #223: loss=0.009500517609508123
Epoch #224: loss=0.01759038082487552
Epoch #225: loss=0.00930315247595032
Epoch #226: loss=0.01169115344156784
Epoch #227: loss=0.009972433503145143
Epoch #228: loss=0.01835970333553235
Epoch #229: loss=0.011942610439015473
Epoch #230: loss=0.010375735100412918
Epoch #231: loss=0.02751921515981399
Epoch #232: loss=0.009024926474861179
Epoch #233: loss=0.010071580158776555
Epoch #234: loss=0.01113501944983627
Epoch #235: loss=0.011016744833296411
Epoch #236: loss=0.00992322490378631
Epoch #237: loss=0.009557084827707283
Epoch #238: loss=0.01297201821207532
Epoch #239: loss=0.010790988696714323
Epoch #240: loss=0.011845673020676866
Epoch #241: loss=0.010196700306534339
Epoch #242: loss=0.016563755877305294
Epoch #243: loss=0.009358744069372628
Epoch #244: loss=0.012761583184822055
Epoch #245: loss=0.010271949432830423
Epoch #246: loss=0.012868860132668445
Epoch #247: loss=0.010278084986253063
Epoch #248: loss=0.011528096831187568
Epoch #249: loss=0.010113320557293368

Training time: 10:26:15.305669

Finished.
n2one setting traffic_exchange -> electricity
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/traffic_exchange_epochs_250_seed_2021/model.pkl', muti_dataset='traffic_exchange', random_seed=2021, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='electricity')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=8.72519e-09): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.0165e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=4.99522e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=8.72519e-09): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.37848051380893455, 'MAE': 0.4341253257003843}
Finished.
------------------------- record done -------------------------
n2one setting traffic_exchange -> weather
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/traffic_exchange_epochs_250_seed_2021/model.pkl', muti_dataset='traffic_exchange', random_seed=2021, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='weather')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.68749e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=3.39876e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.68749e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.6702041952040813, 'MAE': 0.6266258392442265}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='weather_exchange', random_seed=2021, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/weather_exchange_epochs_250_seed_2021
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=6.276438530753641
Epoch #1: loss=4.789230595616734
Epoch #2: loss=2.6369902035769295
Epoch #3: loss=2.3119180237545685
Epoch #4: loss=2.3379999364123627
Epoch #5: loss=2.252953389111687
Epoch #6: loss=1.970504434669719
Epoch #7: loss=1.9333386736757614
Epoch #8: loss=1.9347994257422054
Epoch #9: loss=1.8220691856215983
Epoch #10: loss=1.7270513632718254
Epoch #11: loss=1.6403540267663843
Epoch #12: loss=1.5511686521417953
Epoch #13: loss=1.547635250231799
Epoch #14: loss=1.4028316806344425
Epoch #15: loss=1.406239860198077
Epoch #16: loss=1.2680809690671808
Epoch #17: loss=1.246267203022452
Epoch #18: loss=1.1844580331269432
Epoch #19: loss=1.1498430546592264
Epoch #20: loss=1.124589509823743
Epoch #21: loss=1.1140533983707428
Epoch #22: loss=1.063559286734637
Epoch #23: loss=1.0312602923196905
Epoch #24: loss=0.9935056076330298
Epoch #25: loss=1.0202748091781841
Epoch #26: loss=0.9615685694357928
Epoch #27: loss=1.02123081333497
Epoch #28: loss=0.9401902065557592
Epoch #29: loss=0.8538168949239394
Epoch #30: loss=0.9126957058906555
Epoch #31: loss=0.912982619860593
Epoch #32: loss=0.9088514306965996
Epoch #33: loss=0.9302583214114694
Epoch #34: loss=0.8072199278018054
Epoch #35: loss=0.754005037686404
Epoch #36: loss=0.8806145664523629
Epoch #37: loss=0.8826013821012834
Epoch #38: loss=0.8448896004873163
Epoch #39: loss=0.8319802336833056
Epoch #40: loss=0.7151275838122648
Epoch #41: loss=0.6999229350510765
Epoch #42: loss=0.7023433289107155
Epoch #43: loss=0.6860994687851738
Epoch #44: loss=0.6900867574355182
Epoch #45: loss=0.7009699598831289
Epoch #46: loss=0.6130669046850765
Epoch #47: loss=0.723488582407727
Epoch #48: loss=0.6885408128009123
Epoch #49: loss=0.6047442248638939
Epoch #50: loss=0.5866450667381287
Epoch #51: loss=0.6175696762169108
Epoch #52: loss=0.61395104404758
Epoch #53: loss=0.6115670879097546
Epoch #54: loss=0.5937305268119363
Epoch #55: loss=0.5458860090550255
Epoch #56: loss=0.5494284138959997
Epoch #57: loss=0.5748695801286137
Epoch #58: loss=0.6248503765639137
Epoch #59: loss=0.5711216514601427
Epoch #60: loss=0.8092288953416488
Epoch #61: loss=0.5967567843549392
Epoch #62: loss=0.5900369716041228
Epoch #63: loss=0.6851665254901437
Epoch #64: loss=0.645997991456705
Epoch #65: loss=0.47538862245924335
Epoch #66: loss=0.46049295716425953
Epoch #67: loss=0.528890476507299
Epoch #68: loss=0.48046983252553377
Epoch #69: loss=0.5180273590718999
Epoch #70: loss=0.5302858335130355
Epoch #71: loss=0.43762535952469883
Epoch #72: loss=0.43143462214399786
Epoch #73: loss=0.3592260282705812
Epoch #74: loss=0.4767436652499087
Epoch #75: loss=0.4040519866873236
Epoch #76: loss=0.35169583778170976
Epoch #77: loss=0.4137175784391515
Epoch #78: loss=0.39884236498790626
Epoch #79: loss=0.40770002586000104
Epoch #80: loss=0.4327917638070443
Epoch #81: loss=0.4557928179993349
Epoch #82: loss=0.4223228039986947
Epoch #83: loss=0.4703681232298122
Epoch #84: loss=0.39179016956511664
Epoch #85: loss=0.35275369093698616
Epoch #86: loss=0.2639305135783027
Epoch #87: loss=0.3031361418611863
Epoch #88: loss=0.31635406613349915
Epoch #89: loss=0.38317182703929786
Epoch #90: loss=0.38426074166508284
Epoch #91: loss=0.4599324214984389
Epoch #92: loss=0.3177245938602616
Epoch #93: loss=0.3326871807084364
Epoch #94: loss=0.30301961697199764
Epoch #95: loss=0.2598586533876026
Epoch #96: loss=0.4268478957169196
Epoch #97: loss=0.4272378560374765
Epoch #98: loss=0.34083550335729823
Epoch #99: loss=0.33647696542389255
Epoch #100: loss=0.399055994608823
Epoch #101: loss=0.40440801574903373
Epoch #102: loss=0.3010009557885282
Epoch #103: loss=0.2632100117557189
Epoch #104: loss=0.33457598309306535
Epoch #105: loss=0.3140898228568189
Epoch #106: loss=0.33858037455117
Epoch #107: loss=0.28532534706241947
Epoch #108: loss=0.22803137100794735
Epoch #109: loss=0.24997682365424492
Epoch #110: loss=0.3002230695941869
Epoch #111: loss=0.29263872762813287
Epoch #112: loss=0.27046210976207957
Epoch #113: loss=0.3113815276061787
Epoch #114: loss=0.25042766376453285
Epoch #115: loss=0.2496411375263158
Epoch #116: loss=0.226897803518702
Epoch #117: loss=0.2402442253249533
Epoch #118: loss=0.2944217681446496
Epoch #119: loss=0.25570959685479894
Epoch #120: loss=0.1854319568066036
Epoch #121: loss=0.23719730403493433
Epoch #122: loss=0.20127363656373584
Epoch #123: loss=0.21042972573024385
Epoch #124: loss=0.2205981329521712
Epoch #125: loss=0.21542516810929074
Epoch #126: loss=0.16470429969622807
Epoch #127: loss=0.3080691252561176
Epoch #128: loss=0.3293089264017694
Epoch #129: loss=0.22489002675694578
Epoch #130: loss=0.18698328207520878
Epoch #131: loss=0.1561070992885267
Epoch #132: loss=0.24794749959426768
Epoch #133: loss=0.17810744792222977
Epoch #134: loss=0.18134015710914836
Epoch #135: loss=0.2207401103394873
Epoch #136: loss=0.26411064166356535
Epoch #137: loss=0.17604547804769347
Epoch #138: loss=0.24068515243775704
Epoch #139: loss=0.24227612246485317
Epoch #140: loss=0.1690110540127053
Epoch #141: loss=0.19604901213418036
Epoch #142: loss=0.20292562859899857
Epoch #143: loss=0.21990997486693017
Epoch #144: loss=0.15447175995830228
Epoch #145: loss=0.19515549753080397
Epoch #146: loss=0.17191876986009233
Epoch #147: loss=0.25108713875798616
Epoch #148: loss=0.3048469300436623
Epoch #149: loss=0.3210883070440853
Epoch #150: loss=0.26409752666950226
Epoch #151: loss=0.1860007618499153
Epoch #152: loss=0.15911645573728225
Epoch #153: loss=0.18182126640835228
Epoch #154: loss=0.22018171156592228
Epoch #155: loss=0.17103709740673795
Epoch #156: loss=0.12979981456609332
Epoch #157: loss=0.13196580615990303
Epoch #158: loss=0.1333776731044054
Epoch #159: loss=0.15244096342255087
Epoch #160: loss=0.13074736812097185
Epoch #161: loss=0.1170039563165868
Epoch #162: loss=0.14878787757719264
Epoch #163: loss=0.14805089846691666
Epoch #164: loss=0.12759066647028222
Epoch #165: loss=0.11659960328217815
Epoch #166: loss=0.1017396322944585
Epoch #167: loss=0.08948324007146499
Epoch #168: loss=0.08275465579593883
Epoch #169: loss=0.1088086834506077
Epoch #170: loss=0.08967965693377397
Epoch #171: loss=0.10245255945140824
Epoch #172: loss=0.11234640395816635
Epoch #173: loss=0.11507591101176598
Epoch #174: loss=0.17521131772767096
Epoch #175: loss=0.20573612466892777
Epoch #176: loss=0.25308624633094845
Epoch #177: loss=0.1697380433187765
Epoch #178: loss=0.17462307740660274
Epoch #179: loss=0.16673104245873058
Epoch #180: loss=0.16067877839155056
Epoch #181: loss=0.1488654931240222
Epoch #182: loss=0.14357472375473557
Epoch #183: loss=0.21748710982501507
Epoch #184: loss=0.17297622802502968
Epoch #185: loss=0.14898512928801425
Epoch #186: loss=0.11638934209066279
Epoch #187: loss=0.118200577028534
Epoch #188: loss=0.11031264083131272
Epoch #189: loss=0.2133995645734317
Epoch #190: loss=0.16264968924224377
Epoch #191: loss=0.2812348121010205
Epoch #192: loss=0.2630772695821874
Epoch #193: loss=0.17887578784104655
Epoch #194: loss=0.23191017536994288
Epoch #195: loss=0.2062064759871539
Epoch #196: loss=0.1727548780686715
Epoch #197: loss=0.11253029590143877
Epoch #198: loss=0.14672077901880531
Epoch #199: loss=0.1662755015799228
Epoch #200: loss=0.15485084933393142
Epoch #201: loss=0.09602613944341154
Epoch #202: loss=0.0986788980114986
Epoch #203: loss=0.10268173416090362
Epoch #204: loss=0.08897512449937708
Epoch #205: loss=0.12553375471821604
Epoch #206: loss=0.1850720954620663
Epoch #207: loss=0.1707078215173062
Epoch #208: loss=0.11915475283475484
Epoch #209: loss=0.1177917412646553
Epoch #210: loss=0.14099374758627484
Epoch #211: loss=0.19672194923109868
Epoch #212: loss=0.11704249745782684
Epoch #213: loss=0.14730156339047587
Epoch #214: loss=0.1375624461866477
Epoch #215: loss=0.07310650125145912
Epoch #216: loss=0.09471528768977698
Epoch #217: loss=0.12396463445004295
Epoch #218: loss=0.08995429886614575
Epoch #219: loss=0.169326251537046
Epoch #220: loss=0.17191525291213217
Epoch #221: loss=0.09764097094097558
Epoch #222: loss=0.12952967201743057
Epoch #223: loss=0.150526135447709
Epoch #224: loss=0.16198349075720592
Epoch #225: loss=0.10459306441685733
Epoch #226: loss=0.10776670950957958
Epoch #227: loss=0.10600748773225967
Epoch #228: loss=0.06691150044036262
Epoch #229: loss=0.1045770878520082
Epoch #230: loss=0.10395737233407357
Epoch #231: loss=0.09117436737698667
Epoch #232: loss=0.08438011606716934
Epoch #233: loss=0.09497215193422402
Epoch #234: loss=0.09260320126572076
Epoch #235: loss=0.13103647398598053
Epoch #236: loss=0.12225467054282918
Epoch #237: loss=0.07176952288650415
Epoch #238: loss=0.09018062389291384
Epoch #239: loss=0.08191731374929934
Epoch #240: loss=0.12175292779198464
Epoch #241: loss=0.11105683287057806
Epoch #242: loss=0.07719563451759956
Epoch #243: loss=0.08006617311826524
Epoch #244: loss=0.0788934543619261
Epoch #245: loss=0.09156371291507692
Epoch #246: loss=0.17513127838644912
Epoch #247: loss=0.07839646020575482
Epoch #248: loss=0.09034794138963609
Epoch #249: loss=0.07145692709395114

Training time: 0:32:13.794614

Finished.
n2one setting weather_exchange -> electricity
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/weather_exchange_epochs_250_seed_2021/model.pkl', muti_dataset='weather_exchange', random_seed=2021, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='electricity')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.74019e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=3.46556e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.74019e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.3591031641193649, 'MAE': 0.4244624824592654}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='etth1_etth2', random_seed=2022, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/etth1_etth2_epochs_250_seed_2022
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=4.658781613622393
Epoch #1: loss=2.6508946078164235
Epoch #2: loss=2.2049190998077393
Epoch #3: loss=2.046473034790584
Epoch #4: loss=1.7949281930923462
Epoch #5: loss=1.7243659922054835
Epoch #6: loss=1.5920985000474113
Epoch #7: loss=1.494282943861825
Epoch #8: loss=1.385665169784001
Epoch #9: loss=1.2916795100484575
Epoch #10: loss=1.279732848916735
Epoch #11: loss=1.1659490466117859
Epoch #12: loss=1.2268415944916862
Epoch #13: loss=1.1406683496066503
Epoch #14: loss=1.0747670148100172
Epoch #15: loss=1.0569945105484553
Epoch #16: loss=1.0453521396432603
Epoch #17: loss=0.9792417628424508
Epoch #18: loss=0.876268914767674
Epoch #19: loss=0.8500403123242515
Epoch #20: loss=0.8390855491161346
Epoch #21: loss=0.8674837010247367
Epoch #22: loss=0.8471081384590694
Epoch #23: loss=0.8173205426761082
Epoch #24: loss=0.7787843814917973
Epoch #25: loss=0.722088737147195
Epoch #26: loss=0.8259813487529755
Epoch #27: loss=0.8073795480387551
Epoch #28: loss=0.6465352092470441
Epoch #29: loss=0.7941336674349648
Epoch #30: loss=0.6247574218681881
Epoch #31: loss=0.6819221249648503
Epoch #32: loss=0.7669121325016022
Epoch #33: loss=0.6330271661281586
Epoch #34: loss=0.5954253141369138
Epoch #35: loss=0.5940980783530644
Epoch #36: loss=0.546620460493224
Epoch #37: loss=0.5987311069454465
Epoch #38: loss=0.5590435692242214
Epoch #39: loss=0.5527128257921764
Epoch #40: loss=0.5685122502701623
Epoch #41: loss=0.4594515434333256
Epoch #42: loss=0.41850324826581137
Epoch #43: loss=0.5132540443113872
Epoch #44: loss=0.5455998352595738
Epoch #45: loss=0.7455832724060331
Epoch #46: loss=0.5979370049067906
Epoch #47: loss=0.5329592632395881
Epoch #48: loss=0.5144538794245038
Epoch #49: loss=0.39862519076892305
Epoch #50: loss=0.4769422539642879
Epoch #51: loss=0.5141054902757917
Epoch #52: loss=0.496801169855254
Epoch #53: loss=0.5160090348550251
Epoch #54: loss=0.4758571812084743
Epoch #55: loss=0.4306992271116802
Epoch #56: loss=0.5112761195216861
Epoch #57: loss=0.3772974269730704
Epoch #58: loss=0.3884928120034082
Epoch #59: loss=0.35462826064654757
Epoch #60: loss=0.37950104794331957
Epoch #61: loss=0.4490261503628322
Epoch #62: loss=0.42060153612068724
Epoch #63: loss=0.3852567821741104
Epoch #64: loss=0.34245395234652926
Epoch #65: loss=0.3468291791422026
Epoch #66: loss=0.42209274215357645
Epoch #67: loss=0.40828261630875723
Epoch #68: loss=0.3156614846416882
Epoch #69: loss=0.4032835087605885
Epoch #70: loss=0.29482422343322207
Epoch #71: loss=0.3285856917500496
Epoch #72: loss=0.2955902282680784
Epoch #73: loss=0.3202221617102623
Epoch #74: loss=0.3657712425504412
Epoch #75: loss=0.36179888035569874
Epoch #76: loss=0.3394738318664687
Epoch #77: loss=0.3657461681536266
Epoch #78: loss=0.3400178945490292
Epoch #79: loss=0.28116798933063236
Epoch #80: loss=0.3870364044393812
Epoch #81: loss=0.3071293830871582
Epoch #82: loss=0.35490525726761135
Epoch #83: loss=0.28697652369737625
Epoch #84: loss=0.3539909507547106
Epoch #85: loss=0.30251334820474896
Epoch #86: loss=0.33297873181956156
Epoch #87: loss=0.30146097924028126
Epoch #88: loss=0.30682201896395
Epoch #89: loss=0.2461159559232848
Epoch #90: loss=0.30917253345251083
Epoch #91: loss=0.37594386296612875
Epoch #92: loss=0.26846009492874146
Epoch #93: loss=0.3321268079536302
Epoch #94: loss=0.27357384881802965
Epoch #95: loss=0.2583645316106932
Epoch #96: loss=0.2790248766541481
Epoch #97: loss=0.354284684572901
Epoch #98: loss=0.31006294063159395
Epoch #99: loss=0.30936495108263834
Epoch #100: loss=0.3506105233516012
Epoch #101: loss=0.3477740277137075
Epoch #102: loss=0.2790322782737868
Epoch #103: loss=0.28742468782833647
Epoch #104: loss=0.3134759504880224
Epoch #105: loss=0.2572354855281966
Epoch #106: loss=0.21988163675580705
Epoch #107: loss=0.2762302926608494
Epoch #108: loss=0.2337467766233853
Epoch #109: loss=0.2727176153234073
Epoch #110: loss=0.2762463539838791
Epoch #111: loss=0.2682209334203175
Epoch #112: loss=0.22226874743189132
Epoch #113: loss=0.2912624989237104
Epoch #114: loss=0.2311570325068065
Epoch #115: loss=0.21231778072459356
Epoch #116: loss=0.2235237338713237
Epoch #117: loss=0.2215448660509927
Epoch #118: loss=0.2705858913915498
Epoch #119: loss=0.2550060195582254
Epoch #120: loss=0.3106544826711927
Epoch #121: loss=0.38387781913791386
Epoch #122: loss=0.27156292859997067
Epoch #123: loss=0.27211729756423403
Epoch #124: loss=0.25372266662972315
Epoch #125: loss=0.24895471121583665
Epoch #126: loss=0.2657577597669193
Epoch #127: loss=0.22737912301506316
Epoch #128: loss=0.21601311330284392
Epoch #129: loss=0.24106077424117497
Epoch #130: loss=0.21393108367919922
Epoch #131: loss=0.25391284057072233
Epoch #132: loss=0.21929762406008585
Epoch #133: loss=0.20590685520853316
Epoch #134: loss=0.2771346707429205
Epoch #135: loss=0.25546029210090637
Epoch #136: loss=0.2806416494505746
Epoch #137: loss=0.18954736207212722
Epoch #138: loss=0.28026377516133444
Epoch #139: loss=0.23218360436814173
Epoch #140: loss=0.2330519642148699
Epoch #141: loss=0.2592453094465392
Epoch #142: loss=0.2735967050705637
Epoch #143: loss=0.2125593689935548
Epoch #144: loss=0.2582997019801821
Epoch #145: loss=0.2542627773114613
Epoch #146: loss=0.23339439822094782
Epoch #147: loss=0.20032049928392684
Epoch #148: loss=0.24486861644046648
Epoch #149: loss=0.2296525053679943
Epoch #150: loss=0.18003403821161815
Epoch #151: loss=0.2891042413456099
Epoch #152: loss=0.23326678999832698
Epoch #153: loss=0.16772501649601118
Epoch #154: loss=0.18168409328375543
Epoch #155: loss=0.14929665039692605
Epoch #156: loss=0.187361655490739
Epoch #157: loss=0.1789336252425398
Epoch #158: loss=0.20251705231411116
Epoch #159: loss=0.17500620122466767
Epoch #160: loss=0.16397035654102052
Epoch #161: loss=0.15336560670818603
Epoch #162: loss=0.2867008868072714
Epoch #163: loss=0.1784003744167941
Epoch #164: loss=0.17123764434031077
Epoch #165: loss=0.1783336140215397
Epoch #166: loss=0.18609457250152314
Epoch #167: loss=0.2505399542195456
Epoch #168: loss=0.1987006472689765
Epoch #169: loss=0.25338918822152273
Epoch #170: loss=0.19721051678061485
Epoch #171: loss=0.16276813351682254
Epoch #172: loss=0.22493244494710649
Epoch #173: loss=0.14999704914433615
Epoch #174: loss=0.1673404084784644
Epoch #175: loss=0.21417379964675223
Epoch #176: loss=0.20674327814153262
Epoch #177: loss=0.21985710838011333
Epoch #178: loss=0.13819721075040953
Epoch #179: loss=0.14658931589552335
Epoch #180: loss=0.1390951866550105
Epoch #181: loss=0.1278354019990989
Epoch #182: loss=0.20944315567612648
Epoch #183: loss=0.20509688236883708
Epoch #184: loss=0.15181708495531762
Epoch #185: loss=0.14986273273825645
Epoch #186: loss=0.1996129655412265
Epoch #187: loss=0.2754695974290371
Epoch #188: loss=0.16965847781726293
Epoch #189: loss=0.17209680857402937
Epoch #190: loss=0.21777640017015593
Epoch #191: loss=0.1570980442421777
Epoch #192: loss=0.14900528639554977
Epoch #193: loss=0.14042942172714643
Epoch #194: loss=0.14978605189493724
Epoch #195: loss=0.18893275835684367
Epoch #196: loss=0.20031070283481053
Epoch #197: loss=0.24750692237700736
Epoch #198: loss=0.16626804428441183
Epoch #199: loss=0.1958928294479847
Epoch #200: loss=0.20170268576060021
Epoch #201: loss=0.2193569913506508
Epoch #202: loss=0.18423404172062874
Epoch #203: loss=0.21987138635345868
Epoch #204: loss=0.16867120883294515
Epoch #205: loss=0.15265655677233422
Epoch #206: loss=0.14913629474384443
Epoch #207: loss=0.15523219374673708
Epoch #208: loss=0.11868684313126973
Epoch #209: loss=0.1727095539016383
Epoch #210: loss=0.12452959428940501
Epoch #211: loss=0.11770460116011756
Epoch #212: loss=0.14189553473676955
Epoch #213: loss=0.1582502621625151
Epoch #214: loss=0.1753846678350653
Epoch #215: loss=0.12971671670675278
Epoch #216: loss=0.11423215642571449
Epoch #217: loss=0.11937829426356725
Epoch #218: loss=0.11951200717261859
Epoch #219: loss=0.13577193021774292
Epoch #220: loss=0.1410522154931511
Epoch #221: loss=0.13153556042483874
Epoch #222: loss=0.14121063532573835
Epoch #223: loss=0.15211192199162074
Epoch #224: loss=0.17094055616429873
Epoch #225: loss=0.09425077135009426
Epoch #226: loss=0.10659354233316012
Epoch #227: loss=0.09925507700869016
Epoch #228: loss=0.11827512191874641
Epoch #229: loss=0.22079712737883841
Epoch #230: loss=0.1979068267558302
Epoch #231: loss=0.1295830671276365
Epoch #232: loss=0.15862330315368517
Epoch #233: loss=0.12699564120599202
Epoch #234: loss=0.1660963073372841
Epoch #235: loss=0.11870745675904411
Epoch #236: loss=0.1402496700840337
Epoch #237: loss=0.11161250567861966
Epoch #238: loss=0.13366973559771264
Epoch #239: loss=0.1044926866889
Epoch #240: loss=0.10898363270929881
Epoch #241: loss=0.1910478871847902
Epoch #242: loss=0.10598356037267617
Epoch #243: loss=0.15637274219521455
Epoch #244: loss=0.17027074417897634
Epoch #245: loss=0.1477857596640076
Epoch #246: loss=0.1284255129950387
Epoch #247: loss=0.14425089529582433
Epoch #248: loss=0.18389889491455896
Epoch #249: loss=0.15039627200790814

Training time: 0:10:21.901285

Finished.
n2one setting etth1_etth2 -> electricity
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth1_etth2_epochs_250_seed_2022/model.pkl', muti_dataset='etth1_etth2', random_seed=2022, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='electricity')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.4155e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.65138e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.4155e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.3622822264152878, 'MAE': 0.4261068156640698}
Finished.
------------------------- record done -------------------------
n2one setting etth1_etth2 -> weather
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth1_etth2_epochs_250_seed_2022/model.pkl', muti_dataset='etth1_etth2', random_seed=2022, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='weather')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.47e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.73426e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.73426e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.44755188935139, 'MAE': 0.49335282424084653}
Finished.
------------------------- record done -------------------------
n2one setting etth1_etth2 -> exchange
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth1_etth2_epochs_250_seed_2022/model.pkl', muti_dataset='etth1_etth2', random_seed=2022, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='exchange')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=4.31243e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.19492759657073194, 'MAE': 0.30587667588880413}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='etth1_ettm1', random_seed=2022, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/etth1_ettm1_epochs_250_seed_2022
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=5.068359772364299
Epoch #1: loss=2.711091068055895
Epoch #2: loss=2.3788356251186795
Epoch #3: loss=2.1471133828163147
Epoch #4: loss=2.012014561229282
Epoch #5: loss=1.8891513215170965
Epoch #6: loss=1.7742955618434482
Epoch #7: loss=1.60672042104933
Epoch #8: loss=1.5669580962922838
Epoch #9: loss=1.4579954081111484
Epoch #10: loss=1.3874814245435927
Epoch #11: loss=1.3435468077659607
Epoch #12: loss=1.2818405363294814
Epoch #13: loss=1.3312746087710063
Epoch #14: loss=1.181239366531372
Epoch #15: loss=1.1448602709505293
Epoch #16: loss=1.135293851296107
Epoch #17: loss=1.0860236320230696
Epoch #18: loss=1.0153147909376357
Epoch #19: loss=1.0111041135258145
Epoch #20: loss=0.9272388617197672
Epoch #21: loss=1.008106552892261
Epoch #22: loss=0.8097991579108768
Epoch #23: loss=0.9557934006055196
Epoch #24: loss=0.89047728644477
Epoch #25: loss=0.8465286824438307
Epoch #26: loss=0.7795150644249387
Epoch #27: loss=0.8737934297985501
Epoch #28: loss=0.8690835105048286
Epoch #29: loss=0.7486984199947782
Epoch #30: loss=0.806594643327925
Epoch #31: loss=0.6858599219057295
Epoch #32: loss=0.7975664701726701
Epoch #33: loss=0.6966591212484572
Epoch #34: loss=0.755745937426885
Epoch #35: loss=0.6934915582338969
Epoch #36: loss=0.6167123599184884
Epoch #37: loss=0.6136034975449244
Epoch #38: loss=0.6410310582982169
Epoch #39: loss=0.638573514090644
Epoch #40: loss=0.712265110678143
Epoch #41: loss=0.6118384980493121
Epoch #42: loss=0.5504892203542922
Epoch #43: loss=0.5666359663009644
Epoch #44: loss=0.6287363866964976
Epoch #45: loss=0.5554961893293593
Epoch #46: loss=0.5740772469176186
Epoch #47: loss=0.6848456246985329
Epoch #48: loss=0.6035488777690463
Epoch #49: loss=0.6379385623666975
Epoch #50: loss=0.6604788733853234
Epoch #51: loss=0.5813975003030565
Epoch #52: loss=0.5311822328302596
Epoch #53: loss=0.5122080875767602
Epoch #54: loss=0.523361227578587
Epoch #55: loss=0.5692024330298106
Epoch #56: loss=0.4858853850099776
Epoch #57: loss=0.4737345212035709
Epoch #58: loss=0.4554844664202796
Epoch #59: loss=0.4761803448200226
Epoch #60: loss=0.41171645124753314
Epoch #61: loss=0.40845153397983974
Epoch #62: loss=0.4683907727400462
Epoch #63: loss=0.4598613944318559
Epoch #64: loss=0.43658017615477246
Epoch #65: loss=0.3938633832666609
Epoch #66: loss=0.4075096646944682
Epoch #67: loss=0.474937046567599
Epoch #68: loss=0.5301549765798781
Epoch #69: loss=0.4306494759188758
Epoch #70: loss=0.406984332535002
Epoch #71: loss=0.4016522955563333
Epoch #72: loss=0.4014555596643024
Epoch #73: loss=0.44352121816741097
Epoch #74: loss=0.4333018710215886
Epoch #75: loss=0.5308538046148088
Epoch #76: loss=0.4825187242693371
Epoch #77: loss=0.43802645636929405
Epoch #78: loss=0.3581407806939549
Epoch #79: loss=0.41139302485518986
Epoch #80: loss=0.4515423857503467
Epoch #81: loss=0.5319515350792143
Epoch #82: loss=0.39197809828652275
Epoch #83: loss=0.38569245901372695
Epoch #84: loss=0.4155416753556993
Epoch #85: loss=0.3546772052844365
Epoch #86: loss=0.442504297528002
Epoch #87: loss=0.6264607823557324
Epoch #88: loss=0.45450517700778115
Epoch #89: loss=0.3737624188264211
Epoch #90: loss=0.3565506769551171
Epoch #91: loss=0.3719399505191379
Epoch #92: loss=0.4111649427149031
Epoch #93: loss=0.4459524419572618
Epoch #94: loss=0.5124283755819002
Epoch #95: loss=0.4193914482990901
Epoch #96: loss=0.40251266956329346
Epoch #97: loss=0.3496716966231664
Epoch #98: loss=0.29437608520189923
Epoch #99: loss=0.326978985634115
Epoch #100: loss=0.300283196899626
Epoch #101: loss=0.3139563409818543
Epoch #102: loss=0.33257924599779976
Epoch #103: loss=0.3376118549042278
Epoch #104: loss=0.3188477067483796
Epoch #105: loss=0.29677342540687984
Epoch #106: loss=0.3205207950539059
Epoch #107: loss=0.3639257285330031
Epoch #108: loss=0.3119901766379674
Epoch #109: loss=0.32597454885641736
Epoch #110: loss=0.3033779478735394
Epoch #111: loss=0.2371580352385839
Epoch #112: loss=0.32589076376623577
Epoch #113: loss=0.3339642998245027
Epoch #114: loss=0.2762940277655919
Epoch #115: loss=0.3476506902111901
Epoch #116: loss=0.3334740988082356
Epoch #117: loss=0.37933528506093556
Epoch #118: loss=0.3142223176028993
Epoch #119: loss=0.2560340737303098
Epoch #120: loss=0.23103914741012785
Epoch #121: loss=0.21590343117713928
Epoch #122: loss=0.22409569720427194
Epoch #123: loss=0.2839805881182353
Epoch #124: loss=0.25823526332775754
Epoch #125: loss=0.248983948594994
Epoch #126: loss=0.27771302726533675
Epoch #127: loss=0.24371388223436144
Epoch #128: loss=0.22959416401055124
Epoch #129: loss=0.2664521651135551
Epoch #130: loss=0.258960557066732
Epoch #131: loss=0.23195561435487536
Epoch #132: loss=0.2701214949289958
Epoch #133: loss=0.19454077465666664
Epoch #134: loss=0.15062001471718153
Epoch #135: loss=0.21520865625805324
Epoch #136: loss=0.31236297223303056
Epoch #137: loss=0.28738854494359756
Epoch #138: loss=0.1823275060289436
Epoch #139: loss=0.1796782852874862
Epoch #140: loss=0.18129237865408263
Epoch #141: loss=0.19066477359996903
Epoch #142: loss=0.16766439750790596
Epoch #143: loss=0.18180863848990864
Epoch #144: loss=0.23963228323393398
Epoch #145: loss=0.21388792370756468
Epoch #146: loss=0.2014642602039708
Epoch #147: loss=0.263673421409395
Epoch #148: loss=0.272276162273354
Epoch #149: loss=0.1855030076371299
Epoch #150: loss=0.2039686143398285
Epoch #151: loss=0.19964914272228876
Epoch #152: loss=0.2305513694882393
Epoch #153: loss=0.24141595595412785
Epoch #154: loss=0.19488384657435948
Epoch #155: loss=0.1756996586918831
Epoch #156: loss=0.21880487642354435
Epoch #157: loss=0.41358017259173924
Epoch #158: loss=0.20959364250302315
Epoch #159: loss=0.1966343406173918
Epoch #160: loss=0.16164749074313375
Epoch #161: loss=0.13926294073462486
Epoch #162: loss=0.24666224378678533
Epoch #163: loss=0.22143954411149025
Epoch #164: loss=0.16313280454940265
Epoch #165: loss=0.22663509390420383
Epoch #166: loss=0.156956580777963
Epoch #167: loss=0.17841986773742569
Epoch #168: loss=0.15845751100116306
Epoch #169: loss=0.12893089093267918
Epoch #170: loss=0.15769237341980138
Epoch #171: loss=0.16878901980817318
Epoch #172: loss=0.20660028151339954
Epoch #173: loss=0.15714215052624544
Epoch #174: loss=0.18312578048143122
Epoch #175: loss=0.12163217924535275
Epoch #176: loss=0.16597630663050544
Epoch #177: loss=0.14326906721625063
Epoch #178: loss=0.1388303196678559
Epoch #179: loss=0.1919628812207116
Epoch #180: loss=0.15837175999250677
Epoch #181: loss=0.1365299562199248
Epoch #182: loss=0.23210947463909784
Epoch #183: loss=0.2731081545352936
Epoch #184: loss=0.3011337448325422
Epoch #185: loss=0.1704572623388635
Epoch #186: loss=0.12683205927411714
Epoch #187: loss=0.14052402952478993
Epoch #188: loss=0.1283188828577598
Epoch #189: loss=0.12466972859369384
Epoch #190: loss=0.18579627035392654
Epoch #191: loss=0.1293856476744016
Epoch #192: loss=0.17646076364649665
Epoch #193: loss=0.129443703012334
Epoch #194: loss=0.11818918627169397
Epoch #195: loss=0.1324604352315267
Epoch #196: loss=0.11130181017021339
Epoch #197: loss=0.12714678359528384
Epoch #198: loss=0.3933846635950936
Epoch #199: loss=0.21986323884791797
Epoch #200: loss=0.18869169780777562
Epoch #201: loss=0.14309240298138726
Epoch #202: loss=0.1085657003439135
Epoch #203: loss=0.13520662962562507
Epoch #204: loss=0.13655837252736092
Epoch #205: loss=0.1517159781522221
Epoch #206: loss=0.14848116329974598
Epoch #207: loss=0.15603412128984928
Epoch #208: loss=0.16532880336874062
Epoch #209: loss=0.12575799164672694
Epoch #210: loss=0.12795401881966326
Epoch #211: loss=0.16799213364720345
Epoch #212: loss=0.1476861625495884
Epoch #213: loss=0.15544378819564977
Epoch #214: loss=0.13490201843281588
Epoch #215: loss=0.11293953522625896
Epoch #216: loss=0.1030992851075199
Epoch #217: loss=0.07718699446154965
Epoch #218: loss=0.114505332791143
Epoch #219: loss=0.13681546619368923
Epoch #220: loss=0.10468383888817495
Epoch #221: loss=0.10855532354778713
Epoch #222: loss=0.21654734843307072
Epoch #223: loss=0.1179233609388272
Epoch #224: loss=0.1252338600655397
Epoch #225: loss=0.12337658368051052
Epoch #226: loss=0.13233169747723472
Epoch #227: loss=0.12630547251966265
Epoch #228: loss=0.13885871631403765
Epoch #229: loss=0.19149713611437214
Epoch #230: loss=0.1798451046148936
Epoch #231: loss=0.10645676052404775
Epoch #232: loss=0.11675997171550989
Epoch #233: loss=0.09021724325915177
Epoch #234: loss=0.09504170591632526
Epoch #235: loss=0.09779590833932161
Epoch #236: loss=0.1067392858159211
Epoch #237: loss=0.11921316250744793
Epoch #238: loss=0.07104750764038828
Epoch #239: loss=0.1735296461524235
Epoch #240: loss=0.18400141700274414
Epoch #241: loss=0.14297215557760662
Epoch #242: loss=0.10450428858813313
Epoch #243: loss=0.099595973164671
Epoch #244: loss=0.15632111879272592
Epoch #245: loss=0.1193059179931879
Epoch #246: loss=0.08432278130203485
Epoch #247: loss=0.08271231264289883
Epoch #248: loss=0.16914536172731054
Epoch #249: loss=0.17826182001994717

Training time: 0:16:34.005715

Finished.
n2one setting etth1_ettm1 -> electricity
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth1_ettm1_epochs_250_seed_2022/model.pkl', muti_dataset='etth1_ettm1', random_seed=2022, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='electricity')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.46082e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.85233e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.46082e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.35549474419373406, 'MAE': 0.41851938916421944}
Finished.
------------------------- record done -------------------------
n2one setting etth1_ettm1 -> weather
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth1_ettm1_epochs_250_seed_2022/model.pkl', muti_dataset='etth1_ettm1', random_seed=2022, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='weather')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.47247e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.70433e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.47247e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.694263468291409, 'MAE': 0.6710182951725234}
Finished.
------------------------- record done -------------------------
n2one setting etth1_ettm1 -> exchange
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth1_ettm1_epochs_250_seed_2022/model.pkl', muti_dataset='etth1_ettm1', random_seed=2022, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='exchange')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=4.53909e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.21964773902016488, 'MAE': 0.318170007599432}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='etth1_ettm2', random_seed=2022, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/etth1_ettm2_epochs_250_seed_2022
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=5.368159360355801
Epoch #1: loss=2.7376510169770985
Epoch #2: loss=2.43675168355306
Epoch #3: loss=2.190810614162021
Epoch #4: loss=2.0722793340682983
Epoch #5: loss=1.961471438407898
Epoch #6: loss=1.8471525841289096
Epoch #7: loss=1.660530063841078
Epoch #8: loss=1.6140668723318312
Epoch #9: loss=1.5014966328938801
Epoch #10: loss=1.4355646438068814
Epoch #11: loss=1.3776837322447035
Epoch #12: loss=1.324657724963294
Epoch #13: loss=1.3431970212194655
Epoch #14: loss=1.2151816818449233
Epoch #15: loss=1.1401216785113018
Epoch #16: loss=1.135367343823115
Epoch #17: loss=1.081964049074385
Epoch #18: loss=1.0390183462036982
Epoch #19: loss=1.0363896290461223
Epoch #20: loss=0.954996128877004
Epoch #21: loss=1.0276598102516599
Epoch #22: loss=0.8595709171560075
Epoch #23: loss=0.9859499997562833
Epoch #24: loss=0.9532383051183488
Epoch #25: loss=0.8762227363056607
Epoch #26: loss=0.8395595749219259
Epoch #27: loss=0.8982662955919901
Epoch #28: loss=0.8724984129269918
Epoch #29: loss=0.7582466801007589
Epoch #30: loss=0.7796477013164096
Epoch #31: loss=0.7148643996980455
Epoch #32: loss=0.8594669269190894
Epoch #33: loss=0.7469024128384061
Epoch #34: loss=0.7842073738574982
Epoch #35: loss=0.7069581399361292
Epoch #36: loss=0.6214737660355039
Epoch #37: loss=0.5998914341131846
Epoch #38: loss=0.6365544829103682
Epoch #39: loss=0.6223241455025144
Epoch #40: loss=0.7069795959525638
Epoch #41: loss=0.6380012747314241
Epoch #42: loss=0.5793893618716134
Epoch #43: loss=0.5988043828143014
Epoch #44: loss=0.6543520771794848
Epoch #45: loss=0.5631378210253186
Epoch #46: loss=0.5961388299862543
Epoch #47: loss=0.675280566016833
Epoch #48: loss=0.583720799949434
Epoch #49: loss=0.614146524005466
Epoch #50: loss=0.6553853468762504
Epoch #51: loss=0.5692191239860322
Epoch #52: loss=0.5260474185148875
Epoch #53: loss=0.5375534743070602
Epoch #54: loss=0.6061801264683405
Epoch #55: loss=0.6418519069751104
Epoch #56: loss=0.5118449860148959
Epoch #57: loss=0.4637895425160726
Epoch #58: loss=0.4799617264005873
Epoch #59: loss=0.5355205800798204
Epoch #60: loss=0.43996721009413403
Epoch #61: loss=0.4207599808772405
Epoch #62: loss=0.4794428182972802
Epoch #63: loss=0.47863079441918266
Epoch #64: loss=0.4421287559800678
Epoch #65: loss=0.39150921834839714
Epoch #66: loss=0.41387367579672074
Epoch #67: loss=0.4945998655425178
Epoch #68: loss=0.5378781151440408
Epoch #69: loss=0.43675507108370465
Epoch #70: loss=0.4197833008236355
Epoch #71: loss=0.43338101108868915
Epoch #72: loss=0.425002568297916
Epoch #73: loss=0.45494194163216484
Epoch #74: loss=0.4585856778754128
Epoch #75: loss=0.6087943100266986
Epoch #76: loss=0.5484367228216596
Epoch #77: loss=0.4977974577082528
Epoch #78: loss=0.4127875102890862
Epoch #79: loss=0.44285084472762215
Epoch #80: loss=0.48302844497892594
Epoch #81: loss=0.576766871743732
Epoch #82: loss=0.42286461260583663
Epoch #83: loss=0.42723265952534145
Epoch #84: loss=0.4591978026760949
Epoch #85: loss=0.3710469702879588
Epoch #86: loss=0.4131539795133803
Epoch #87: loss=0.5103590049677424
Epoch #88: loss=0.38850758141941494
Epoch #89: loss=0.3306825773583518
Epoch #90: loss=0.3365558841162258
Epoch #91: loss=0.340793165895674
Epoch #92: loss=0.3742145266797807
Epoch #93: loss=0.4442702117893431
Epoch #94: loss=0.5185799847046534
Epoch #95: loss=0.41425322824054295
Epoch #96: loss=0.4279000692897373
Epoch #97: loss=0.3468666697541873
Epoch #98: loss=0.3013099796242184
Epoch #99: loss=0.33969271928071976
Epoch #100: loss=0.32448428455326295
Epoch #101: loss=0.32942744923962486
Epoch #102: loss=0.35431217485004
Epoch #103: loss=0.32903149558438194
Epoch #104: loss=0.32316850539710784
Epoch #105: loss=0.2959604561328888
Epoch #106: loss=0.32066968083381653
Epoch #107: loss=0.36051396280527115
Epoch #108: loss=0.3033275339338515
Epoch #109: loss=0.3214823289049996
Epoch #110: loss=0.3149005182915264
Epoch #111: loss=0.25758495430151623
Epoch #112: loss=0.35148870531055665
Epoch #113: loss=0.3232709483967887
Epoch #114: loss=0.273699299328857
Epoch #115: loss=0.3453173562884331
Epoch #116: loss=0.3204108766383595
Epoch #117: loss=0.3916064624985059
Epoch #118: loss=0.31956343683931565
Epoch #119: loss=0.277652319934633
Epoch #120: loss=0.24769059982564715
Epoch #121: loss=0.22525621371136773
Epoch #122: loss=0.2423453852534294
Epoch #123: loss=0.31521059738265145
Epoch #124: loss=0.31162888142797684
Epoch #125: loss=0.3021448378761609
Epoch #126: loss=0.31847043252653545
Epoch #127: loss=0.2698742002248764
Epoch #128: loss=0.24104326797856224
Epoch #129: loss=0.2651808485388756
Epoch #130: loss=0.2688822067446179
Epoch #131: loss=0.24796699318620893
Epoch #132: loss=0.3042508785923322
Epoch #133: loss=0.2076280497842365
Epoch #134: loss=0.17182361914051902
Epoch #135: loss=0.21970879327919748
Epoch #136: loss=0.3028314494424396
Epoch #137: loss=0.2838044481145011
Epoch #138: loss=0.19913362173570526
Epoch #139: loss=0.1893333407739798
Epoch #140: loss=0.21859335982137257
Epoch #141: loss=0.24366186103887028
Epoch #142: loss=0.22294377701150048
Epoch #143: loss=0.22383789262837833
Epoch #144: loss=0.3115839639471637
Epoch #145: loss=0.22298263675636715
Epoch #146: loss=0.19047588606675467
Epoch #147: loss=0.21844805321759647
Epoch #148: loss=0.24173641784323585
Epoch #149: loss=0.17469678901963764
Epoch #150: loss=0.18892743604050743
Epoch #151: loss=0.18593044951558113
Epoch #152: loss=0.2147469897237089
Epoch #153: loss=0.22113654928074944
Epoch #154: loss=0.19514903840091494
Epoch #155: loss=0.17244567018416193
Epoch #156: loss=0.20771029384599793
Epoch #157: loss=0.4179082475602627
Epoch #158: loss=0.19117191889219814
Epoch #159: loss=0.1766329366299841
Epoch #160: loss=0.16015486543377241
Epoch #161: loss=0.1600352997581164
Epoch #162: loss=0.27176498663094306
Epoch #163: loss=0.2226438033911917
Epoch #164: loss=0.18510773322648472
Epoch #165: loss=0.23382026784949833
Epoch #166: loss=0.16564203964339363
Epoch #167: loss=0.1685055684712198
Epoch #168: loss=0.16669523219267526
Epoch #169: loss=0.14025047090318468
Epoch #170: loss=0.18500924069020483
Epoch #171: loss=0.18482446629140112
Epoch #172: loss=0.21408183748523393
Epoch #173: loss=0.1622663616306252
Epoch #174: loss=0.21278061345219612
Epoch #175: loss=0.1812471714284685
Epoch #176: loss=0.1841131452884939
Epoch #177: loss=0.1681164794911941
Epoch #178: loss=0.15051743533048365
Epoch #179: loss=0.1778923747026258
Epoch #180: loss=0.15968799591064453
Epoch #181: loss=0.1382133642004596
Epoch #182: loss=0.23225774450434578
Epoch #183: loss=0.21082420647144318
Epoch #184: loss=0.24691514919201532
Epoch #185: loss=0.15296442268623245
Epoch #186: loss=0.11672135380407174
Epoch #187: loss=0.13869557819432682
Epoch #188: loss=0.1247770647621817
Epoch #189: loss=0.12813872057530615
Epoch #190: loss=0.17832252714369032
Epoch #191: loss=0.12614492802984184
Epoch #192: loss=0.14515063787500063
Epoch #193: loss=0.12135247762004535
Epoch #194: loss=0.12733566326399645
Epoch #195: loss=0.16475635208189487
Epoch #196: loss=0.14259281692405543
Epoch #197: loss=0.19324297830462456
Epoch #198: loss=0.4813693579700258
Epoch #199: loss=0.3344285483989451
Epoch #200: loss=0.25399526001678574
Epoch #201: loss=0.20407433932026228
Epoch #202: loss=0.15328510416050753
Epoch #203: loss=0.16309997625648975
Epoch #204: loss=0.15965074321462047
Epoch #205: loss=0.1689607221633196
Epoch #206: loss=0.16122816834184858
Epoch #207: loss=0.16128220243586433
Epoch #208: loss=0.1957540156112777
Epoch #209: loss=0.14209380000829697
Epoch #210: loss=0.11906985669500297
Epoch #211: loss=0.13160885560015836
Epoch #212: loss=0.12765523398088086
Epoch #213: loss=0.14590339155660736
Epoch #214: loss=0.1559673235234287
Epoch #215: loss=0.1221902502907647
Epoch #216: loss=0.11402502635286914
Epoch #217: loss=0.09503159133924378
Epoch #218: loss=0.13106653549604946
Epoch #219: loss=0.12935194000601768
Epoch #220: loss=0.10447593736979696
Epoch #221: loss=0.12895850568181938
Epoch #222: loss=0.1724786590784788
Epoch #223: loss=0.12340253425969018
Epoch #224: loss=0.13588818866345617
Epoch #225: loss=0.14067049283120367
Epoch #226: loss=0.12536213174462318
Epoch #227: loss=0.1316752110918363
Epoch #228: loss=0.1398292359792524
Epoch #229: loss=0.23017202607459492
Epoch #230: loss=0.17298265960481432
Epoch #231: loss=0.12047126040690476
Epoch #232: loss=0.12024570867005321
Epoch #233: loss=0.08591762743890285
Epoch #234: loss=0.09395184761120214
Epoch #235: loss=0.0920350088013543
Epoch #236: loss=0.10529877369602521
Epoch #237: loss=0.12212498486042023
Epoch #238: loss=0.07108818646520376
Epoch #239: loss=0.17325194490452608
Epoch #240: loss=0.19443896888858742
Epoch #241: loss=0.1664150340689553
Epoch #242: loss=0.11980482066671054
Epoch #243: loss=0.10031155600315994
Epoch #244: loss=0.17209639855557018
Epoch #245: loss=0.16027311608195305
Epoch #246: loss=0.11764388759103087
Epoch #247: loss=0.1004876084625721
Epoch #248: loss=0.1696422201477819
Epoch #249: loss=0.17006680203808677

Training time: 0:16:43.240317

Finished.
n2one setting etth1_ettm2 -> electricity
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth1_ettm2_epochs_250_seed_2022/model.pkl', muti_dataset='etth1_ettm2', random_seed=2022, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='electricity')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.50964e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.90403e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.50964e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.36967750083259365, 'MAE': 0.4274908284726487}
Finished.
------------------------- record done -------------------------
n2one setting etth1_ettm2 -> weather
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth1_ettm2_epochs_250_seed_2022/model.pkl', muti_dataset='etth1_ettm2', random_seed=2022, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='weather')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.55731e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=3.0526e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.55731e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.48307093893473735, 'MAE': 0.515006308198747}
Finished.
------------------------- record done -------------------------
n2one setting etth1_ettm2 -> exchange
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth1_ettm2_epochs_250_seed_2022/model.pkl', muti_dataset='etth1_ettm2', random_seed=2022, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='exchange')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=4.26172e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.22671769527101998, 'MAE': 0.3197735210120785}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='etth1_electricity', random_seed=2022, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/etth1_electricity_epochs_250_seed_2022
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=1.6062939108871832
Epoch #1: loss=0.6325688521127876
Epoch #2: loss=0.4353627016904151
Epoch #3: loss=0.3496597036719322
Epoch #4: loss=0.2978098542861095
Epoch #5: loss=0.25186942045281574
Epoch #6: loss=0.2254154593797355
Epoch #7: loss=0.19026258708227697
Epoch #8: loss=0.14703843951588724
Epoch #9: loss=0.14654216348625174
Epoch #10: loss=0.14096597816068224
Epoch #11: loss=0.12344648532296826
Epoch #12: loss=0.09630737727984967
Epoch #13: loss=0.10006841575363423
Epoch #14: loss=0.12579274450133487
Epoch #15: loss=0.07976664314273654
Epoch #16: loss=0.10938934344662035
Epoch #17: loss=0.06810817213604091
Epoch #18: loss=0.06720444156004615
Epoch #19: loss=0.06686882546353268
Epoch #20: loss=0.049898780195224215
Epoch #21: loss=0.049920132803935105
Epoch #22: loss=0.05726029843705275
Epoch #23: loss=0.05596730599418373
Epoch #24: loss=0.04356513887348517
Epoch #25: loss=0.047305713149310064
Epoch #26: loss=0.04923771397727428
Epoch #27: loss=0.048553065990279545
Epoch #28: loss=0.042293808841793915
Epoch #29: loss=0.03565131869485102
Epoch #30: loss=0.04348976849493139
Epoch #31: loss=0.03914396111603554
Epoch #32: loss=0.033382071286479645
Epoch #33: loss=0.04538461526503741
Epoch #34: loss=0.02805206409468111
Epoch #35: loss=0.04586075952066472
Epoch #36: loss=0.027904327114811167
Epoch #37: loss=0.036781352905630386
Epoch #38: loss=0.0291460623060319
Epoch #39: loss=0.025896913258404267
Epoch #40: loss=0.03178904348524378
Epoch #41: loss=0.026500327501264288
Epoch #42: loss=0.037556253697314274
Epoch #43: loss=0.027590872735500607
Epoch #44: loss=0.03295594045009305
Epoch #45: loss=0.02646374983447547
Epoch #46: loss=0.023475177223345536
Epoch #47: loss=0.025091577252435566
Epoch #48: loss=0.020378298785611305
Epoch #49: loss=0.019654064819361536
Epoch #50: loss=0.02050393301867255
Epoch #51: loss=0.021886557977711895
Epoch #52: loss=0.021850768331205472
Epoch #53: loss=0.02520105237653763
Epoch #54: loss=0.02178922152358232
Epoch #55: loss=0.02105873785259929
Epoch #56: loss=0.02851566554987008
Epoch #57: loss=0.016987617861936664
Epoch #58: loss=0.015069921715177948
Epoch #59: loss=0.09942525405530454
Epoch #60: loss=0.0177009242367448
Epoch #61: loss=0.022553179077321392
Epoch #62: loss=0.017753886720861263
Epoch #63: loss=0.01793288529876097
Epoch #64: loss=0.01882493172888644
Epoch #65: loss=0.04055561219865922
Epoch #66: loss=0.02924245944397138
Epoch #67: loss=0.0162110694505916
Epoch #68: loss=0.017968412589497593
Epoch #69: loss=0.016415173991046642
Epoch #70: loss=0.021473045936478984
Epoch #71: loss=0.016776170978562554
Epoch #72: loss=0.01527939059132023
Epoch #73: loss=0.029686557975166093
Epoch #74: loss=0.01599341706428166
Epoch #75: loss=0.02334124480428106
Epoch #76: loss=0.020646839613086408
Epoch #77: loss=0.020168697530341267
Epoch #78: loss=0.01909198514476884
Epoch #79: loss=0.017410067848509132
Epoch #80: loss=0.018213903732319566
Epoch #81: loss=0.010649932068190537
Epoch #82: loss=0.04354270518595557
Epoch #83: loss=0.02149501200690837
Epoch #84: loss=0.0202020266332533
Epoch #85: loss=0.019202950708326748
Epoch #86: loss=0.013850344810989776
Epoch #87: loss=0.01710874703002912
Epoch #88: loss=0.013703867233544303
Epoch #89: loss=0.01443396602003415
Epoch #90: loss=0.01938826114006011
Epoch #91: loss=0.014014548545940381
Epoch #92: loss=0.025434597059601646
Epoch #93: loss=0.01672240599731506
Epoch #94: loss=0.016897898831613443
Epoch #95: loss=0.011841414481573227
Epoch #96: loss=0.030405721346075398
Epoch #97: loss=0.016649449102171674
Epoch #98: loss=0.019035478334954836
Epoch #99: loss=0.012233920682153506
Epoch #100: loss=0.021079912837793504
Epoch #101: loss=0.012734951563399547
Epoch #102: loss=0.014430321904400465
Epoch #103: loss=0.014490550457749789
Epoch #104: loss=0.016703007854563866
Epoch #105: loss=0.011141838587496839
Epoch #106: loss=0.014334933347638027
Epoch #107: loss=0.016310294972244288
Epoch #108: loss=0.01408894008157559
Epoch #109: loss=0.014399999261424854
Epoch #110: loss=0.015792540656991882
Epoch #111: loss=0.015185297342945776
Epoch #112: loss=0.01235469939079272
Epoch #113: loss=0.013445197348163474
Epoch #114: loss=0.01621351586321153
Epoch #115: loss=0.012381464831932823
Epoch #116: loss=0.017132740873465793
Epoch #117: loss=0.011739285975104062
Epoch #118: loss=0.011822999396472147
Epoch #119: loss=0.032987681256178424
Epoch #120: loss=0.014508582090301632
Epoch #121: loss=0.014141683831417694
Epoch #122: loss=0.016718807792441535
Epoch #123: loss=0.007647298351224801
Epoch #124: loss=0.019442313523230528
Epoch #125: loss=0.01473919321311192
Epoch #126: loss=0.015224023863549697
Epoch #127: loss=0.017659959715518463
Epoch #128: loss=0.01544002007491455
Epoch #129: loss=0.012673035700750117
Epoch #130: loss=0.011361270713678095
Epoch #131: loss=0.010229628182281797
Epoch #132: loss=0.011426965415189554
Epoch #133: loss=0.01876907416253078
Epoch #134: loss=0.012713925175442611
Epoch #135: loss=0.0130123188956953
Epoch #136: loss=0.009679217895964251
Epoch #137: loss=0.013317269945507245
Epoch #138: loss=0.0065812442743463625
Epoch #139: loss=0.008898892320380915
Epoch #140: loss=0.016164247863343917
Epoch #141: loss=0.009766321187404484
Epoch #142: loss=0.013788134459989634
Epoch #143: loss=0.014729651383964754
Epoch #144: loss=0.01650046835431301
Epoch #145: loss=0.035385657274008095
Epoch #146: loss=0.011072509358111746
Epoch #147: loss=0.02990097976353576
Epoch #148: loss=0.022362069264778343
Epoch #149: loss=0.013611478027575308
Epoch #150: loss=0.011161595589173219
Epoch #151: loss=0.00841423846355699
Epoch #152: loss=0.00945740144619244
Epoch #153: loss=0.009968794188685801
Epoch #154: loss=0.0074822232603676125
Epoch #155: loss=0.01299451393477772
Epoch #156: loss=0.010655222805703156
Epoch #157: loss=0.015895167107817364
Epoch #158: loss=0.012830807520048238
Epoch #159: loss=0.009284704871012165
Epoch #160: loss=0.02018283477667776
Epoch #161: loss=0.015299027225587562
Epoch #162: loss=0.01354737169174591
Epoch #163: loss=0.011581065472167144
Epoch #164: loss=0.008467879703036998
Epoch #165: loss=0.009149948101655422
Epoch #166: loss=0.013732274382363824
Epoch #167: loss=0.021341726270138014
Epoch #168: loss=0.011801012583973494
Epoch #169: loss=0.010287023028578871
Epoch #170: loss=0.010138506016984338
Epoch #171: loss=0.01169491289839221
Epoch #172: loss=0.018035983085943237
Epoch #173: loss=0.033744965137973454
Epoch #174: loss=0.010001562604263266
Epoch #175: loss=0.011066472872210775
Epoch #176: loss=0.00790583455800959
Epoch #177: loss=0.010059841792631564
Epoch #178: loss=0.010964239594702632
Epoch #179: loss=0.016664560892223096
Epoch #180: loss=0.011459276066289464
Epoch #181: loss=0.010404177878915174
Epoch #182: loss=0.010515293161538798
Epoch #183: loss=0.009138080172195303
Epoch #184: loss=0.009919871573246626
Epoch #185: loss=0.009765612673285564
Epoch #186: loss=0.01617614284215251
Epoch #187: loss=0.005909309948037844
Epoch #188: loss=0.0057981860341764025
Epoch #189: loss=0.01515612995682182
Epoch #190: loss=0.021778345789842692
Epoch #191: loss=0.011753995670360604
Epoch #192: loss=0.005689020941290215
Epoch #193: loss=0.011724632120160273
Epoch #194: loss=0.005932494865946199
Epoch #195: loss=0.01973008059150477
Epoch #196: loss=0.008640895453359785
Epoch #197: loss=0.008038500972161716
Epoch #198: loss=0.012661314923983499
Epoch #199: loss=0.013579972581327438
Epoch #200: loss=0.01075610076272491
Epoch #201: loss=0.01092813920943782
Epoch #202: loss=0.009317472103270476
Epoch #203: loss=0.009005233133016523
Epoch #204: loss=0.011129704956213729
Epoch #205: loss=0.009500263915691814
Epoch #206: loss=0.010495578725190503
Epoch #207: loss=0.007227561526833153
Epoch #208: loss=0.017018307865882668
Epoch #209: loss=0.012980232605137067
Epoch #210: loss=0.007608808576528312
Epoch #211: loss=0.01019414649494049
Epoch #212: loss=0.0077414397267758185
Epoch #213: loss=0.01053764341992153
Epoch #214: loss=0.028582805027637246
Epoch #215: loss=0.00871828504457925
Epoch #216: loss=0.010677399039417562
Epoch #217: loss=0.005830898663922065
Epoch #218: loss=0.009803272629275253
Epoch #219: loss=0.013750765107344217
Epoch #220: loss=0.008648760721393402
Epoch #221: loss=0.014538472834740277
Epoch #222: loss=0.011190799288714316
Epoch #223: loss=0.01014969326729997
Epoch #224: loss=0.011202385197975673
Epoch #225: loss=0.01419663799127348
Epoch #226: loss=0.0067115180066072595
Epoch #227: loss=0.006997282731257345
Epoch #228: loss=0.011900292200418956
Epoch #229: loss=0.011851846494054614
Epoch #230: loss=0.010150199921155014
Epoch #231: loss=0.008453872142038053
Epoch #232: loss=0.017933614913334433
Epoch #233: loss=0.008541937099369356
Epoch #234: loss=0.007415324060009259
Epoch #235: loss=0.009620315255721457
Epoch #236: loss=0.015217136609419136
Epoch #237: loss=0.009617906166457076
Epoch #238: loss=0.009805393034894666
Epoch #239: loss=0.007778041933417581
Epoch #240: loss=0.007281776383113139
Epoch #241: loss=0.00941563348435632
Epoch #242: loss=0.009278130640024489
Epoch #243: loss=0.02048858375893338
Epoch #244: loss=0.010525887500279635
Epoch #245: loss=0.006089018200837905
Epoch #246: loss=0.0060089776353579065
Epoch #247: loss=0.010593613743478229
Epoch #248: loss=0.013645155606887206
Epoch #249: loss=0.00992574693177963

Training time: 4:27:15.299991

Finished.
n2one setting etth1_electricity -> weather
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth1_electricity_epochs_250_seed_2022/model.pkl', muti_dataset='etth1_electricity', random_seed=2022, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='weather')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.17612e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.50519e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.17612e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.8996416327416966, 'MAE': 0.760006304970698}
Finished.
------------------------- record done -------------------------
n2one setting etth1_electricity -> exchange
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth1_electricity_epochs_250_seed_2022/model.pkl', muti_dataset='etth1_electricity', random_seed=2022, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='exchange')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=4.79317e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=4.79317e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.38318979589188845, 'MAE': 0.4049132419242714}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='etth1_traffic', random_seed=2022, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/etth1_traffic_epochs_250_seed_2022
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=0.9724614788254057
Epoch #1: loss=0.3324599101950851
Epoch #2: loss=0.24468417378234644
Epoch #3: loss=0.17521757314408196
Epoch #4: loss=0.1369653288682256
Epoch #5: loss=0.11348970511325895
Epoch #6: loss=0.09607641160342376
Epoch #7: loss=0.08105692204186571
Epoch #8: loss=0.08717382401849547
Epoch #9: loss=0.06592324581554522
Epoch #10: loss=0.06228731528098156
Epoch #11: loss=0.08834119688535176
Epoch #12: loss=0.055176374318680464
Epoch #13: loss=0.045792180484844534
Epoch #14: loss=0.04339993287304414
Epoch #15: loss=0.037631632782405264
Epoch #16: loss=0.04197658582106104
Epoch #17: loss=0.034338068666356705
Epoch #18: loss=0.035054758437871675
Epoch #19: loss=0.04023374602176095
Epoch #20: loss=0.03571107075522869
Epoch #21: loss=0.028244834118501268
Epoch #22: loss=0.035194944203440076
Epoch #23: loss=0.026393471955256355
Epoch #24: loss=0.030354419086418907
Epoch #25: loss=0.02821871969458766
Epoch #26: loss=0.02644645168030178
Epoch #27: loss=0.030868706843329183
Epoch #28: loss=0.02347685048436763
Epoch #29: loss=0.03802575395639206
Epoch #30: loss=0.023942384815944188
Epoch #31: loss=0.02538847741753295
Epoch #32: loss=0.022063827817167515
Epoch #33: loss=0.02688183396493628
Epoch #34: loss=0.02513681058340542
Epoch #35: loss=0.022989343113190376
Epoch #36: loss=0.026308880116368814
Epoch #37: loss=0.02434442201425655
Epoch #38: loss=0.01544465243192636
Epoch #39: loss=0.018707711132303826
Epoch #40: loss=0.023803579100966026
Epoch #41: loss=0.02507596443988912
Epoch #42: loss=0.015612853303555961
Epoch #43: loss=0.018885708508530247
Epoch #44: loss=0.021301718299655978
Epoch #45: loss=0.014736889563988513
Epoch #46: loss=0.021268555600776458
Epoch #47: loss=0.022328338720696422
Epoch #48: loss=0.018561177272995305
Epoch #49: loss=0.02259859156126898
Epoch #50: loss=0.019421948208883554
Epoch #51: loss=0.017999238284225432
Epoch #52: loss=0.01193361625353183
Epoch #53: loss=0.020188834796170944
Epoch #54: loss=0.014521391004152753
Epoch #55: loss=0.022546459477214235
Epoch #56: loss=0.016805481045012297
Epoch #57: loss=0.018574594548700852
Epoch #58: loss=0.01805287593046216
Epoch #59: loss=0.026379794856092396
Epoch #60: loss=0.01503965729423954
Epoch #61: loss=0.018348267957171902
Epoch #62: loss=0.017994097932065023
Epoch #63: loss=0.018839813370050775
Epoch #64: loss=0.012884889104978531
Epoch #65: loss=0.04062514285751489
Epoch #66: loss=0.013474682830335729
Epoch #67: loss=0.01204318853246688
Epoch #68: loss=0.017302401180050074
Epoch #69: loss=0.012106186975313724
Epoch #70: loss=0.012881146781638177
Epoch #71: loss=0.01579847983793292
Epoch #72: loss=0.018691451629097022
Epoch #73: loss=0.013226144302138572
Epoch #74: loss=0.0225339272849624
Epoch #75: loss=0.014293673349700073
Epoch #76: loss=0.01355890900032462
Epoch #77: loss=0.016762309509500438
Epoch #78: loss=0.013188770709085623
Epoch #79: loss=0.015546765622869885
Epoch #80: loss=0.018873115603410682
Epoch #81: loss=0.009758247774937919
Epoch #82: loss=0.014689459817972188
Epoch #83: loss=0.01739833000401663
Epoch #84: loss=0.012772403940101882
Epoch #85: loss=0.015597236804973326
Epoch #86: loss=0.015754361387728837
Epoch #87: loss=0.014649852001521108
Epoch #88: loss=0.016617888065485843
Epoch #89: loss=0.02383768004363926
Epoch #90: loss=0.012160372752761248
Epoch #91: loss=0.01381866413563671
Epoch #92: loss=0.01634877526120393
Epoch #93: loss=0.013174213791849902
Epoch #94: loss=0.011964282133791733
Epoch #95: loss=0.01978945365361548
Epoch #96: loss=0.01620193465422063
Epoch #97: loss=0.01126827842218885
Epoch #98: loss=0.013195759818810965
Epoch #99: loss=0.014360008937450312
Epoch #100: loss=0.020003925092537487
Epoch #101: loss=0.01262873500620119
Epoch #102: loss=0.015003789157837885
Epoch #103: loss=0.009574058659473044
Epoch #104: loss=0.011266948717742219
Epoch #105: loss=0.012771979575274487
Epoch #106: loss=0.011844497735638084
Epoch #107: loss=0.012965583878138474
Epoch #108: loss=0.01824708524338528
Epoch #109: loss=0.009264096465313827
Epoch #110: loss=0.014161817679403808
Epoch #111: loss=0.016541573328952817
Epoch #112: loss=0.012114287658352877
Epoch #113: loss=0.013993385969185602
Epoch #114: loss=0.012866503317983473
Epoch #115: loss=0.016443866443606223
Epoch #116: loss=0.01622639327536493
Epoch #117: loss=0.009211870800521304
Epoch #118: loss=0.01693429037440548
Epoch #119: loss=0.01191949831851398
Epoch #120: loss=0.011664437699108625
Epoch #121: loss=0.01314421744474078
Epoch #122: loss=0.012751783761951399
Epoch #123: loss=0.01099633095665223
Epoch #124: loss=0.00924181427635275
Epoch #125: loss=0.014378293346032842
Epoch #126: loss=0.014681181169866013
Epoch #127: loss=0.015749282697174148
Epoch #128: loss=0.01392562268941526
Epoch #129: loss=0.008188511692059855
Epoch #130: loss=0.018172000818167812
Epoch #131: loss=0.011324840555475517
Epoch #132: loss=0.01137435173426789
Epoch #133: loss=0.011168268534059608
Epoch #134: loss=0.01063145110944272
Epoch #135: loss=0.013102247181167785
Epoch #136: loss=0.009470232311839568
Epoch #137: loss=0.014107769072102101
Epoch #138: loss=0.0118890238329094
Epoch #139: loss=0.016143399327748238
Epoch #140: loss=0.011331246669367837
Epoch #141: loss=0.010183455783019638
Epoch #142: loss=0.012778266763907533
Epoch #143: loss=0.009008479278823478
Epoch #144: loss=0.012408487505726387
Epoch #145: loss=0.00810047957556356
Epoch #146: loss=0.011508593110050695
Epoch #147: loss=0.01101220408049964
Epoch #148: loss=0.012639092528525155
Epoch #149: loss=0.010414369235754868
Epoch #150: loss=0.013105277094398696
Epoch #151: loss=0.017839806336327858
Epoch #152: loss=0.014504086575001988
Epoch #153: loss=0.00971513199072703
Epoch #154: loss=0.009245657979922533
Epoch #155: loss=0.024068303272508296
Epoch #156: loss=0.010245510895533693
Epoch #157: loss=0.014175877636824182
Epoch #158: loss=0.00823856665954713
Epoch #159: loss=0.011696107137616846
Epoch #160: loss=0.013087208393134092
Epoch #161: loss=0.00948738899842597
Epoch #162: loss=0.007220359076345589
Epoch #163: loss=0.015122398516902447
Epoch #164: loss=0.007931580764799948
Epoch #165: loss=0.010428439022085529
Epoch #166: loss=0.007978121270513365
Epoch #167: loss=0.010194487825397218
Epoch #168: loss=0.010711265336405037
Epoch #169: loss=0.011015160371718994
Epoch #170: loss=0.012171676308976302
Epoch #171: loss=0.011403609611098285
Epoch #172: loss=0.010029780646014858
Epoch #173: loss=0.007288706776836196
Epoch #174: loss=0.013289299021301935
Epoch #175: loss=0.008416603437068383
Epoch #176: loss=0.02225173040726131
Epoch #177: loss=0.00856408940833003
Epoch #178: loss=0.00968656565346171
Epoch #179: loss=0.008034170775829548
Epoch #180: loss=0.008655524270734814
Epoch #181: loss=0.010342643605990428
Epoch #182: loss=0.010475842656205452
Epoch #183: loss=0.01566236486054742
Epoch #184: loss=0.008727444235863881
Epoch #185: loss=0.009667580967073213
Epoch #186: loss=0.01113650188541901
Epoch #187: loss=0.01121853164879319
Epoch #188: loss=0.012745535840601653
Epoch #189: loss=0.01546700719664727
Epoch #190: loss=0.009396687413510085
Epoch #191: loss=0.007502831060206116
Epoch #192: loss=0.00907705274576796
Epoch #193: loss=0.011692115689372758
Epoch #194: loss=0.008967237822873297
Epoch #195: loss=0.008515218212001987
Epoch #196: loss=0.012836682739188268
Epoch #197: loss=0.011090648314550276
Epoch #198: loss=0.010234642291719335
Epoch #199: loss=0.0099657950812912
Epoch #200: loss=0.013355607960539246
Epoch #201: loss=0.014459557244756449
Epoch #202: loss=0.010443011932462182
Epoch #203: loss=0.007666980640851781
Epoch #204: loss=0.007903230135254449
Epoch #205: loss=0.015385399101347173
Epoch #206: loss=0.007581134823195677
Epoch #207: loss=0.011346569532488478
Epoch #208: loss=0.007030081586943823
Epoch #209: loss=0.010045503387554947
Epoch #210: loss=0.01021584894838045
Epoch #211: loss=0.010433635869226807
Epoch #212: loss=0.010363697011825672
Epoch #213: loss=0.00859495539530861
Epoch #214: loss=0.010416701982635496
Epoch #215: loss=0.007343055233270251
Epoch #216: loss=0.01106292360535393
Epoch #217: loss=0.010837121462639394
Epoch #218: loss=0.01285668487345234
Epoch #219: loss=0.01058910389210315
Epoch #220: loss=0.008619588240806948
Epoch #221: loss=0.007076464447020569
Epoch #222: loss=0.008198191072879293
Epoch #223: loss=0.009148555991669336
Epoch #224: loss=0.017321616027718416
Epoch #225: loss=0.008594478200514902
Epoch #226: loss=0.011350634255043528
Epoch #227: loss=0.00730207500692218
Epoch #228: loss=0.00930469870452514
Epoch #229: loss=0.010107395337546317
Epoch #230: loss=0.01167571343385354
Epoch #231: loss=0.014842433055871625
Epoch #232: loss=0.00960121405864471
Epoch #233: loss=0.009695030403391057
Epoch #234: loss=0.00681432326382701
Epoch #235: loss=0.011700385164279787
Epoch #236: loss=0.007918072704191483
Epoch #237: loss=0.01045095561338502
Epoch #238: loss=0.010336342969868049
Epoch #239: loss=0.008810058739394007
Epoch #240: loss=0.0077967626741691845
Epoch #241: loss=0.010908232139547674
Epoch #242: loss=0.00857798199111599
Epoch #243: loss=0.011850383151208399
Epoch #244: loss=0.006956821265048012
Epoch #245: loss=0.010040752515268472
Epoch #246: loss=0.021980182440138035
Epoch #247: loss=0.005684543517083294
Epoch #248: loss=0.007609736380792987
Epoch #249: loss=0.012758251941210661

Training time: 10:13:01.477948

Finished.
n2one setting etth1_traffic -> electricity
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth1_traffic_epochs_250_seed_2022/model.pkl', muti_dataset='etth1_traffic', random_seed=2022, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='electricity')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=6.48081e-09): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.52098e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=3.80187e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=6.48081e-09): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.39930070956850905, 'MAE': 0.4480149005573359}
Finished.
------------------------- record done -------------------------
n2one setting etth1_traffic -> weather
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth1_traffic_epochs_250_seed_2022/model.pkl', muti_dataset='etth1_traffic', random_seed=2022, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='weather')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.18714e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.41598e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.614780770294988, 'MAE': 0.628829352243034}
Finished.
------------------------- record done -------------------------
n2one setting etth1_traffic -> exchange
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth1_traffic_epochs_250_seed_2022/model.pkl', muti_dataset='etth1_traffic', random_seed=2022, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='exchange')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=4.69609e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.32671512172557177, 'MAE': 0.3763145449796535}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='etth1_weather', random_seed=2022, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/etth1_weather_epochs_250_seed_2022
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=5.651195728417599
Epoch #1: loss=2.7016184763474898
Epoch #2: loss=2.936304175492489
Epoch #3: loss=2.2043971654140586
Epoch #4: loss=2.12375545501709
Epoch #5: loss=1.92605650063717
Epoch #6: loss=1.7745090939781882
Epoch #7: loss=1.6250404914220173
Epoch #8: loss=1.6960594509587144
Epoch #9: loss=1.5179244315985478
Epoch #10: loss=1.4045539949879502
Epoch #11: loss=1.3165300289789836
Epoch #12: loss=1.2413280172781511
Epoch #13: loss=1.2218797080444568
Epoch #14: loss=1.201169201821992
Epoch #15: loss=1.1346066250945583
Epoch #16: loss=1.185070691686688
Epoch #17: loss=1.217740580891118
Epoch #18: loss=1.068753188306635
Epoch #19: loss=1.0559493429733045
Epoch #20: loss=0.8756650632077997
Epoch #21: loss=0.9121911814718535
Epoch #22: loss=0.9038433378392999
Epoch #23: loss=0.8024714282064727
Epoch #24: loss=0.8869205803582163
Epoch #25: loss=0.8244072704604177
Epoch #26: loss=0.8004701372348901
Epoch #27: loss=0.7179285829717462
Epoch #28: loss=0.7754216374772968
Epoch #29: loss=0.7426891146284161
Epoch #30: loss=0.6964981519814694
Epoch #31: loss=0.7117842995759213
Epoch #32: loss=0.6135813476461353
Epoch #33: loss=0.7087265903299506
Epoch #34: loss=0.6370537877082825
Epoch #35: loss=0.6524837017059326
Epoch #36: loss=0.7346756051887166
Epoch #37: loss=0.6398495065443444
Epoch #38: loss=0.6215306457244989
Epoch #39: loss=0.5987355013688406
Epoch #40: loss=0.6504866273114176
Epoch #41: loss=0.5611452920870348
Epoch #42: loss=0.6093570165561907
Epoch #43: loss=0.5252946895180326
Epoch #44: loss=0.5345589852694309
Epoch #45: loss=0.5228373318007498
Epoch #46: loss=0.5511536029252139
Epoch #47: loss=0.6960758006933964
Epoch #48: loss=0.5652124619845188
Epoch #49: loss=0.5991274922183065
Epoch #50: loss=0.5123256490086064
Epoch #51: loss=0.5481653665051316
Epoch #52: loss=0.5642145465720784
Epoch #53: loss=0.522769750067682
Epoch #54: loss=0.507685431025245
Epoch #55: loss=0.5308500456087517
Epoch #56: loss=0.42700108163284534
Epoch #57: loss=0.4342569505626505
Epoch #58: loss=0.4589563761696671
Epoch #59: loss=0.3848399325753703
Epoch #60: loss=0.416419448726105
Epoch #61: loss=0.4319803877310319
Epoch #62: loss=0.40821553811882483
Epoch #63: loss=0.4452655626968904
Epoch #64: loss=0.40609825696005963
Epoch #65: loss=0.4106401444384546
Epoch #66: loss=0.3639473549344323
Epoch #67: loss=0.3653357209581317
Epoch #68: loss=0.3585884787819602
Epoch #69: loss=0.39755061584891693
Epoch #70: loss=0.3396919107798374
Epoch #71: loss=0.36384593085809186
Epoch #72: loss=0.3948504134561076
Epoch #73: loss=0.31779149581085553
Epoch #74: loss=0.3191703007076726
Epoch #75: loss=0.30543849684975366
Epoch #76: loss=0.35643719272180036
Epoch #77: loss=0.2928382722717343
Epoch #78: loss=0.37892299709898053
Epoch #79: loss=0.3411444646842552
Epoch #80: loss=0.3734345259991559
Epoch #81: loss=0.34429337310068536
Epoch #82: loss=0.32876050065864215
Epoch #83: loss=0.2835633664420157
Epoch #84: loss=0.25860225973707257
Epoch #85: loss=0.33485358321305475
Epoch #86: loss=0.23098882694136014
Epoch #87: loss=0.2679273549354438
Epoch #88: loss=0.2611477765621561
Epoch #89: loss=0.22834567725658417
Epoch #90: loss=0.2711301337588917
Epoch #91: loss=0.2738991260077014
Epoch #92: loss=0.2959092887062015
Epoch #93: loss=0.2501043702165286
Epoch #94: loss=0.2873735822962992
Epoch #95: loss=0.3156015024040685
Epoch #96: loss=0.3948508713281516
Epoch #97: loss=0.317175170463143
Epoch #98: loss=0.3321791049657446
Epoch #99: loss=0.3674310278711897
Epoch #100: loss=0.24969237139730743
Epoch #101: loss=0.2322909224665526
Epoch #102: loss=0.2192996609391588
Epoch #103: loss=0.20836019199905972
Epoch #104: loss=0.25547936626456
Epoch #105: loss=0.2414376229950876
Epoch #106: loss=0.21221518606850595
Epoch #107: loss=0.19214888323437085
Epoch #108: loss=0.2404431513313091
Epoch #109: loss=0.22483944012360138
Epoch #110: loss=0.2443065017913327
Epoch #111: loss=0.2917191907763481
Epoch #112: loss=0.19846916943788528
Epoch #113: loss=0.3092129562388767
Epoch #114: loss=0.27385193290132465
Epoch #115: loss=0.19170356225786786
Epoch #116: loss=0.19089456015464032
Epoch #117: loss=0.24842116654370772
Epoch #118: loss=0.27195213757681125
Epoch #119: loss=0.4665297403028517
Epoch #120: loss=0.399772508577867
Epoch #121: loss=0.21286141195080496
Epoch #122: loss=0.19582577681902683
Epoch #123: loss=0.1937144554474137
Epoch #124: loss=0.16905507038940082
Epoch #125: loss=0.1667348931007313
Epoch #126: loss=0.15617886433998743
Epoch #127: loss=0.2317947449557709
Epoch #128: loss=0.19114713269201192
Epoch #129: loss=0.19644779181390098
Epoch #130: loss=0.155524848424124
Epoch #131: loss=0.15410033703753442
Epoch #132: loss=0.21919941382877756
Epoch #133: loss=0.17913020684412032
Epoch #134: loss=0.15952350402420218
Epoch #135: loss=0.1951339321606087
Epoch #136: loss=0.15267967771400104
Epoch #137: loss=0.1411981163828662
Epoch #138: loss=0.18085793191284844
Epoch #139: loss=0.16720984210119103
Epoch #140: loss=0.22460221645958495
Epoch #141: loss=0.175835270434618
Epoch #142: loss=0.1684680123898116
Epoch #143: loss=0.1342499487553582
Epoch #144: loss=0.18601025765140852
Epoch #145: loss=0.13485796546394174
Epoch #146: loss=0.15732855891639536
Epoch #147: loss=0.18581065321058937
Epoch #148: loss=0.14623637249072394
Epoch #149: loss=0.1921904975491943
Epoch #150: loss=0.1251187990560676
Epoch #151: loss=0.1184805960140445
Epoch #152: loss=0.12382017871872945
Epoch #153: loss=0.10902729421628243
Epoch #154: loss=0.15067854128552205
Epoch #155: loss=0.1326776472004977
Epoch #156: loss=0.1786741731744824
Epoch #157: loss=0.17411054535345596
Epoch #158: loss=0.14904762262647803
Epoch #159: loss=0.13858844723665353
Epoch #160: loss=0.13810454026767702
Epoch #161: loss=0.10647932225556085
Epoch #162: loss=0.1210098211286646
Epoch #163: loss=0.14182850007306447
Epoch #164: loss=0.1248946918569731
Epoch #165: loss=0.1239048010584983
Epoch #166: loss=0.12548896506654494
Epoch #167: loss=0.1129762866167408
Epoch #168: loss=0.1438638511480707
Epoch #169: loss=0.15972129594195972
Epoch #170: loss=0.1133510105198983
Epoch #171: loss=0.11346168849955905
Epoch #172: loss=0.09819700123008454
Epoch #173: loss=0.10843919110343311
Epoch #174: loss=0.1190285977314819
Epoch #175: loss=0.11346598522681178
Epoch #176: loss=0.1407203565944325
Epoch #177: loss=0.11544529230079868
Epoch #178: loss=0.11920726344440923
Epoch #179: loss=0.1481067935061274
Epoch #180: loss=0.20302891110380492
Epoch #181: loss=0.19906103983521461
Epoch #182: loss=0.12952504645694385
Epoch #183: loss=0.11706974854071935
Epoch #184: loss=0.11731022500404806
Epoch #185: loss=0.11410607341112512
Epoch #186: loss=0.10920479255869533
Epoch #187: loss=0.1124119164817261
Epoch #188: loss=0.0948132158567508
Epoch #189: loss=0.07684532541668776
Epoch #190: loss=0.10140643505887552
Epoch #191: loss=0.08885782889344475
Epoch #192: loss=0.16854245774447918
Epoch #193: loss=0.1288402687306657
Epoch #194: loss=0.09527811430620425
Epoch #195: loss=0.08374266490114457
Epoch #196: loss=0.08448858683307965
Epoch #197: loss=0.12929171193955522
Epoch #198: loss=0.09293144472846479
Epoch #199: loss=0.08814852374295394
Epoch #200: loss=0.06910381522594077
Epoch #201: loss=0.11045602754209981
Epoch #202: loss=0.11170665812537525
Epoch #203: loss=0.0896813886973894
Epoch #204: loss=0.11217992747146072
Epoch #205: loss=0.08169050537275546
Epoch #206: loss=0.09456822874419617
Epoch #207: loss=0.09269637874130046
Epoch #208: loss=0.10569332032041116
Epoch #209: loss=0.08778770651781198
Epoch #210: loss=0.14586465862212758
Epoch #211: loss=0.08061478241826549
Epoch #212: loss=0.09830892193272259
Epoch #213: loss=0.07910285796970129
Epoch #214: loss=0.07900049421710498
Epoch #215: loss=0.1144095691993381
Epoch #216: loss=0.08383727361532775
Epoch #217: loss=0.1506287937588764
Epoch #218: loss=0.12233659433144511
Epoch #219: loss=0.08217874095972741
Epoch #220: loss=0.08272035894068805
Epoch #221: loss=0.06888771116394889
Epoch #222: loss=0.07377665992261785
Epoch #223: loss=0.11351821935650977
Epoch #224: loss=0.09096036546609619
Epoch #225: loss=0.09004232394650127
Epoch #226: loss=0.07309821010990576
Epoch #227: loss=0.0773879669493798
Epoch #228: loss=0.07901658196792458
Epoch #229: loss=0.06000163916949973
Epoch #230: loss=0.08395176990465684
Epoch #231: loss=0.07842894666122668
Epoch #232: loss=0.0912728841614091
Epoch #233: loss=0.06619127983735366
Epoch #234: loss=0.11709434335882013
Epoch #235: loss=0.0985506149855527
Epoch #236: loss=0.12626155544862602
Epoch #237: loss=0.07312609650420421
Epoch #238: loss=0.0701632531303348
Epoch #239: loss=0.15719257289487304
Epoch #240: loss=0.09980205350527258
Epoch #241: loss=0.11195400557621862
Epoch #242: loss=0.10716966353356838
Epoch #243: loss=0.13004975252305018
Epoch #244: loss=0.10789134962992235
Epoch #245: loss=0.06684923730790615
Epoch #246: loss=0.07402044828190948
Epoch #247: loss=0.07451027905512037
Epoch #248: loss=0.05621354832229289
Epoch #249: loss=0.07319944557931388

Training time: 0:31:37.674174

Finished.
n2one setting etth1_weather -> electricity
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth1_weather_epochs_250_seed_2022/model.pkl', muti_dataset='etth1_weather', random_seed=2022, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='electricity')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.29561e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.48689e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=5.82482e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.29561e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.3584370646362153, 'MAE': 0.4236941777752614}
Finished.
------------------------- record done -------------------------
n2one setting etth1_weather -> exchange
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth1_weather_epochs_250_seed_2022/model.pkl', muti_dataset='etth1_weather', random_seed=2022, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='exchange')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=3.32747e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.20353572300356923, 'MAE': 0.31367042753987}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='etth1_exchange', random_seed=2022, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/etth1_exchange_epochs_250_seed_2022
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=4.583468770980835
Epoch #1: loss=2.4224153757095337
Epoch #2: loss=2.055339312553406
Epoch #3: loss=1.9454762061436972
Epoch #4: loss=1.857820494969686
Epoch #5: loss=1.7357737064361571
Epoch #6: loss=1.6497268835703531
Epoch #7: loss=1.6051946004231772
Epoch #8: loss=1.5615601062774658
Epoch #9: loss=1.4650904496510824
Epoch #10: loss=1.514602009455363
Epoch #11: loss=1.4394657293955484
Epoch #12: loss=1.4344391266504923
Epoch #13: loss=1.3498905420303344
Epoch #14: loss=1.3164031028747558
Epoch #15: loss=1.2214950482050577
Epoch #16: loss=1.2469613313674928
Epoch #17: loss=1.1296738862991333
Epoch #18: loss=1.230765199661255
Epoch #19: loss=1.1342819809913636
Epoch #20: loss=1.2034143964449564
Epoch #21: loss=1.1686328490575155
Epoch #22: loss=1.0691465536753337
Epoch #23: loss=1.0265014012654623
Epoch #24: loss=1.0538660963376363
Epoch #25: loss=1.0096805930137633
Epoch #26: loss=1.0852129022280375
Epoch #27: loss=0.9962384382883708
Epoch #28: loss=0.85412544409434
Epoch #29: loss=0.9484952966372172
Epoch #30: loss=0.9453827977180481
Epoch #31: loss=1.0097561001777648
Epoch #32: loss=0.9809677084287007
Epoch #33: loss=0.944478166103363
Epoch #34: loss=0.840529187520345
Epoch #35: loss=0.9010740121205648
Epoch #36: loss=0.8681262811024983
Epoch #37: loss=0.7341953595479329
Epoch #38: loss=0.9039348483085632
Epoch #39: loss=0.774625035127004
Epoch #40: loss=0.6895493110020955
Epoch #41: loss=0.7127637028694153
Epoch #42: loss=0.7012083490689596
Epoch #43: loss=0.6880258540312449
Epoch #44: loss=0.7640794078509013
Epoch #45: loss=0.6685876051584879
Epoch #46: loss=0.6844187259674073
Epoch #47: loss=0.6656197985013326
Epoch #48: loss=0.6032351652781168
Epoch #49: loss=0.5577982465426127
Epoch #50: loss=0.6901265859603882
Epoch #51: loss=0.7595668892065685
Epoch #52: loss=0.6581408619880676
Epoch #53: loss=0.5419025262196858
Epoch #54: loss=0.6189301649729411
Epoch #55: loss=0.5275295893351237
Epoch #56: loss=0.5312391062577565
Epoch #57: loss=0.5740587015946707
Epoch #58: loss=0.49081714351971945
Epoch #59: loss=0.6175322651863098
Epoch #60: loss=0.7962714513142903
Epoch #61: loss=0.7494931399822236
Epoch #62: loss=0.5598158240318298
Epoch #63: loss=0.5830174048741659
Epoch #64: loss=0.6925000369548797
Epoch #65: loss=0.6055537343025208
Epoch #66: loss=0.5701228708028794
Epoch #67: loss=0.5121419628461202
Epoch #68: loss=0.4713040510813395
Epoch #69: loss=0.4400131106376648
Epoch #70: loss=0.5301715393861135
Epoch #71: loss=0.5029715657234192
Epoch #72: loss=0.45962523221969603
Epoch #73: loss=0.43572209576765697
Epoch #74: loss=0.46925926208496094
Epoch #75: loss=0.48306949536005656
Epoch #76: loss=0.5377709567546844
Epoch #77: loss=0.5138888915379842
Epoch #78: loss=0.4388125717639923
Epoch #79: loss=0.504924621184667
Epoch #80: loss=0.5335939784844717
Epoch #81: loss=0.5030944367249807
Epoch #82: loss=0.45682910084724426
Epoch #83: loss=0.35006077190240226
Epoch #84: loss=0.36459631522496544
Epoch #85: loss=0.4329583356777827
Epoch #86: loss=0.4137906243403753
Epoch #87: loss=0.5103381514549256
Epoch #88: loss=0.4533508598804474
Epoch #89: loss=0.42297048767407736
Epoch #90: loss=0.5367905914783477
Epoch #91: loss=0.4686086306969325
Epoch #92: loss=0.4718338718016942
Epoch #93: loss=0.38766537805398305
Epoch #94: loss=0.5080543289581935
Epoch #95: loss=0.39704585472742715
Epoch #96: loss=0.34004981617132823
Epoch #97: loss=0.3984975069761276
Epoch #98: loss=0.39273895223935446
Epoch #99: loss=0.40384815682967506
Epoch #100: loss=0.49013929466406503
Epoch #101: loss=0.38217118084430696
Epoch #102: loss=0.45230757097403207
Epoch #103: loss=0.3042223870754242
Epoch #104: loss=0.356472901503245
Epoch #105: loss=0.42544597884019214
Epoch #106: loss=0.3819772864381472
Epoch #107: loss=0.31248605251312256
Epoch #108: loss=0.5327212959527969
Epoch #109: loss=0.37828161120414733
Epoch #110: loss=0.3803226053714752
Epoch #111: loss=0.3792216906944911
Epoch #112: loss=0.40943351686000823
Epoch #113: loss=0.45905496875445045
Epoch #114: loss=0.33516231377919514
Epoch #115: loss=0.38459118604660036
Epoch #116: loss=0.33974391023317974
Epoch #117: loss=0.37098442067702614
Epoch #118: loss=0.32172270119190216
Epoch #119: loss=0.3389594405889511
Epoch #120: loss=0.521471381187439
Epoch #121: loss=0.33760377317667006
Epoch #122: loss=0.267669844130675
Epoch #123: loss=0.22807498027880987
Epoch #124: loss=0.2925741901000341
Epoch #125: loss=0.4661012167731921
Epoch #126: loss=0.40793510029713315
Epoch #127: loss=0.340404586493969
Epoch #128: loss=0.27567676504453026
Epoch #129: loss=0.31064648628234864
Epoch #130: loss=0.3745748281478882
Epoch #131: loss=0.34091667930285136
Epoch #132: loss=0.3799122388164202
Epoch #133: loss=0.2813155318299929
Epoch #134: loss=0.2911584968368212
Epoch #135: loss=0.3196510444084803
Epoch #136: loss=0.37933798929055534
Epoch #137: loss=0.42897973557313285
Epoch #138: loss=0.2672301545739174
Epoch #139: loss=0.3694597393274307
Epoch #140: loss=0.3053118685881297
Epoch #141: loss=0.31887831290562946
Epoch #142: loss=0.3060008575518926
Epoch #143: loss=0.24057909001906713
Epoch #144: loss=0.31886487603187563
Epoch #145: loss=0.22934575726588566
Epoch #146: loss=0.27261468519767124
Epoch #147: loss=0.2656487832466761
Epoch #148: loss=0.32288110852241514
Epoch #149: loss=0.37404135515292486
Epoch #150: loss=0.4395886038740476
Epoch #151: loss=0.35436393817265827
Epoch #152: loss=0.24757335285345713
Epoch #153: loss=0.23709820210933685
Epoch #154: loss=0.2593271444241206
Epoch #155: loss=0.2484346921245257
Epoch #156: loss=0.25273140196998917
Epoch #157: loss=0.23698675086100895
Epoch #158: loss=0.2676104341944059
Epoch #159: loss=0.315553417801857
Epoch #160: loss=0.3061792547504107
Epoch #161: loss=0.24911140253146488
Epoch #162: loss=0.29714523752530414
Epoch #163: loss=0.2771412325402101
Epoch #164: loss=0.2858299771944682
Epoch #165: loss=0.3012753536303838
Epoch #166: loss=0.2858015353480975
Epoch #167: loss=0.3308048407236735
Epoch #168: loss=0.3461102093259493
Epoch #169: loss=0.24072667409976323
Epoch #170: loss=0.2304420530796051
Epoch #171: loss=0.21519762724637986
Epoch #172: loss=0.22833255678415298
Epoch #173: loss=0.16526898170510929
Epoch #174: loss=0.24580336784323056
Epoch #175: loss=0.20184270590543746
Epoch #176: loss=0.2633399476607641
Epoch #177: loss=0.22857243741552036
Epoch #178: loss=0.24768493821223578
Epoch #179: loss=0.325188976029555
Epoch #180: loss=0.304473448296388
Epoch #181: loss=0.31123788356781007
Epoch #182: loss=0.2635855535666148
Epoch #183: loss=0.3107679754495621
Epoch #184: loss=0.26625882188479105
Epoch #185: loss=0.28763828227917354
Epoch #186: loss=0.2063833639025688
Epoch #187: loss=0.2128894088168939
Epoch #188: loss=0.23342760652303696
Epoch #189: loss=0.2313631904621919
Epoch #190: loss=0.31573901772499086
Epoch #191: loss=0.22904881909489633
Epoch #192: loss=0.1973380148410797
Epoch #193: loss=0.20825220197439193
Epoch #194: loss=0.1966230129202207
Epoch #195: loss=0.27907536054650944
Epoch #196: loss=0.18987853129704793
Epoch #197: loss=0.20700993090867997
Epoch #198: loss=0.1803784062465032
Epoch #199: loss=0.2597508231798808
Epoch #200: loss=0.18919974838693937
Epoch #201: loss=0.22720189069708188
Epoch #202: loss=0.31099904427925745
Epoch #203: loss=0.23035916611552237
Epoch #204: loss=0.29441377719243367
Epoch #205: loss=0.20952169224619865
Epoch #206: loss=0.1574230395257473
Epoch #207: loss=0.22033300350109736
Epoch #208: loss=0.1773005060851574
Epoch #209: loss=0.24431574617822965
Epoch #210: loss=0.18276085183024407
Epoch #211: loss=0.23470110967755317
Epoch #212: loss=0.28178149089217186
Epoch #213: loss=0.25360857546329496
Epoch #214: loss=0.2881039813160896
Epoch #215: loss=0.251377671957016
Epoch #216: loss=0.25025015970071157
Epoch #217: loss=0.2735174593826135
Epoch #218: loss=0.22807357758283614
Epoch #219: loss=0.3026222993930181
Epoch #220: loss=0.46237938006718954
Epoch #221: loss=0.5850276738405228
Epoch #222: loss=0.5129928757747014
Epoch #223: loss=0.32062845428784686
Epoch #224: loss=0.27448356250921885
Epoch #225: loss=0.25483986288309096
Epoch #226: loss=0.24455219581723214
Epoch #227: loss=0.23288092066844304
Epoch #228: loss=0.19174098918835322
Epoch #229: loss=0.209461639324824
Epoch #230: loss=0.24716956814130148
Epoch #231: loss=0.2321618730823199
Epoch #232: loss=0.21801548426349956
Epoch #233: loss=0.21416975259780885
Epoch #234: loss=0.21359544843435288
Epoch #235: loss=0.20688410674532254
Epoch #236: loss=0.32234371453523636
Epoch #237: loss=0.28795139913757645
Epoch #238: loss=0.2110145315527916
Epoch #239: loss=0.1637496384481589
Epoch #240: loss=0.24010314072171848
Epoch #241: loss=0.18474411318699518
Epoch #242: loss=0.23113185465335845
Epoch #243: loss=0.19850648045539857
Epoch #244: loss=0.21451534802714983
Epoch #245: loss=0.20998499169945717
Epoch #246: loss=0.2144377795358499
Epoch #247: loss=0.13683699841300648
Epoch #248: loss=0.18181416094303132
Epoch #249: loss=0.19765455946326255

Training time: 0:08:46.622310

Finished.
n2one setting etth1_exchange -> electricity
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth1_exchange_epochs_250_seed_2022/model.pkl', muti_dataset='etth1_exchange', random_seed=2022, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='electricity')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.01096e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.01475e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=5.07032e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.01096e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.35701858705424766, 'MAE': 0.4231022829410786}
Finished.
------------------------- record done -------------------------
n2one setting etth1_exchange -> weather
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth1_exchange_epochs_250_seed_2022/model.pkl', muti_dataset='etth1_exchange', random_seed=2022, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='weather')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=8.82709e-09): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.73513e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=4.07511e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.3541790652606034, 'MAE': 0.42829251772088966}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='etth2_ettm1', random_seed=2022, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/etth2_ettm1_epochs_250_seed_2022
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=5.074671851264106
Epoch #1: loss=2.6866960657967462
Epoch #2: loss=2.365681423081292
Epoch #3: loss=2.135685463746389
Epoch #4: loss=2.0155366725391812
Epoch #5: loss=1.8851853277948167
Epoch #6: loss=1.762036919593811
Epoch #7: loss=1.6002801656723022
Epoch #8: loss=1.5682133767339919
Epoch #9: loss=1.4470715787675645
Epoch #10: loss=1.3685455587175157
Epoch #11: loss=1.328595068719652
Epoch #12: loss=1.273111833466424
Epoch #13: loss=1.3010346624586318
Epoch #14: loss=1.1708857284651861
Epoch #15: loss=1.136457943254047
Epoch #16: loss=1.1183840499983893
Epoch #17: loss=1.0562717848353915
Epoch #18: loss=1.0145191583368514
Epoch #19: loss=1.0071078340212505
Epoch #20: loss=0.9291092256704966
Epoch #21: loss=1.032038281361262
Epoch #22: loss=0.8013897736867269
Epoch #23: loss=0.9639878074328104
Epoch #24: loss=0.8907023039129045
Epoch #25: loss=0.8627304699685838
Epoch #26: loss=0.7970792253812155
Epoch #27: loss=0.8836716016133627
Epoch #28: loss=0.8634423977798886
Epoch #29: loss=0.7429238557815552
Epoch #30: loss=0.7561533053716024
Epoch #31: loss=0.6746658616595798
Epoch #32: loss=0.7838625742329491
Epoch #33: loss=0.728682408730189
Epoch #34: loss=0.7265703098641502
Epoch #35: loss=0.6652406868007448
Epoch #36: loss=0.6004346940252516
Epoch #37: loss=0.5876654303736157
Epoch #38: loss=0.6024519304434458
Epoch #39: loss=0.5880109502209557
Epoch #40: loss=0.6637377821736865
Epoch #41: loss=0.6043744815720452
Epoch #42: loss=0.5364875925911797
Epoch #43: loss=0.5623307178417841
Epoch #44: loss=0.5993395381503634
Epoch #45: loss=0.5705748266643949
Epoch #46: loss=0.6008402390612496
Epoch #47: loss=0.6977179861731
Epoch #48: loss=0.5801864895555708
Epoch #49: loss=0.65131004816956
Epoch #50: loss=0.6636661191781362
Epoch #51: loss=0.5617097450627221
Epoch #52: loss=0.5246771623690923
Epoch #53: loss=0.5399748368395699
Epoch #54: loss=0.5794755783345964
Epoch #55: loss=0.6660869204335742
Epoch #56: loss=0.5151999394098917
Epoch #57: loss=0.44553626742627883
Epoch #58: loss=0.4798196454842885
Epoch #59: loss=0.509237117237515
Epoch #60: loss=0.42301933964093524
Epoch #61: loss=0.44187110993597245
Epoch #62: loss=0.471452749437756
Epoch #63: loss=0.45499127440982395
Epoch #64: loss=0.44234785437583923
Epoch #65: loss=0.38551724784904057
Epoch #66: loss=0.3992267979515923
Epoch #67: loss=0.4809812671608395
Epoch #68: loss=0.5300996626416842
Epoch #69: loss=0.403853898247083
Epoch #70: loss=0.405929383304384
Epoch #71: loss=0.44237828254699707
Epoch #72: loss=0.45634201169013977
Epoch #73: loss=0.6664181699355444
Epoch #74: loss=0.8561936070521673
Epoch #75: loss=0.6391697641876009
Epoch #76: loss=0.5221369216839472
Epoch #77: loss=0.45729100041919285
Epoch #78: loss=0.38672230309910244
Epoch #79: loss=0.42934171193175846
Epoch #80: loss=0.4679882940318849
Epoch #81: loss=0.5616554005278481
Epoch #82: loss=0.43292618294556934
Epoch #83: loss=0.40426723327901626
Epoch #84: loss=0.43080054471890133
Epoch #85: loss=0.32734932170973885
Epoch #86: loss=0.37752853085597354
Epoch #87: loss=0.49632707569334245
Epoch #88: loss=0.3498626972238223
Epoch #89: loss=0.30780989428361255
Epoch #90: loss=0.3340399960676829
Epoch #91: loss=0.33648528903722763
Epoch #92: loss=0.3821946034828822
Epoch #93: loss=0.43646494547526044
Epoch #94: loss=0.48508041186465156
Epoch #95: loss=0.37840240117576385
Epoch #96: loss=0.37497950676414704
Epoch #97: loss=0.3432805927263366
Epoch #98: loss=0.2933308175868458
Epoch #99: loss=0.3485874641272757
Epoch #100: loss=0.3837646063831117
Epoch #101: loss=0.40943901903099483
Epoch #102: loss=0.33217667125993305
Epoch #103: loss=0.3186560562915272
Epoch #104: loss=0.29211395647790694
Epoch #105: loss=0.2819368897212876
Epoch #106: loss=0.32037388781706494
Epoch #107: loss=0.35931311464971966
Epoch #108: loss=0.3288214322593477
Epoch #109: loss=0.3305762948261367
Epoch #110: loss=0.3138800321353806
Epoch #111: loss=0.246468648314476
Epoch #112: loss=0.3398093606034915
Epoch #113: loss=0.30873511069350773
Epoch #114: loss=0.24774623579449123
Epoch #115: loss=0.3297738234202067
Epoch #116: loss=0.3007950418525272
Epoch #117: loss=0.3596087023615837
Epoch #118: loss=0.2961782440543175
Epoch #119: loss=0.25727546794546974
Epoch #120: loss=0.24721312688456643
Epoch #121: loss=0.20391986270745596
Epoch #122: loss=0.21513239708211687
Epoch #123: loss=0.2612168946199947
Epoch #124: loss=0.22216124004787868
Epoch #125: loss=0.22893468538920084
Epoch #126: loss=0.27575694107347065
Epoch #127: loss=0.22840462293889788
Epoch #128: loss=0.22935377723640865
Epoch #129: loss=0.26637386944558883
Epoch #130: loss=0.28174998197290635
Epoch #131: loss=0.3122399093376266
Epoch #132: loss=0.3696465351515346
Epoch #133: loss=0.24584085825416777
Epoch #134: loss=0.23766961279842588
Epoch #135: loss=0.28333206723133725
Epoch #136: loss=0.34572666303979027
Epoch #137: loss=0.30571457495292026
Epoch #138: loss=0.19538932169477144
Epoch #139: loss=0.18719954623116386
Epoch #140: loss=0.20427820003694958
Epoch #141: loss=0.18972542219691807
Epoch #142: loss=0.18101808801293373
Epoch #143: loss=0.17338070025046667
Epoch #144: loss=0.2430233247578144
Epoch #145: loss=0.20167609345581797
Epoch #146: loss=0.19696652847859594
Epoch #147: loss=0.21314783725473616
Epoch #148: loss=0.23190190725856358
Epoch #149: loss=0.18545212927791807
Epoch #150: loss=0.2360408620701896
Epoch #151: loss=0.21413395098514026
Epoch #152: loss=0.21393455730544197
Epoch #153: loss=0.23758144138587844
Epoch #154: loss=0.2013651488555802
Epoch #155: loss=0.20657918519443935
Epoch #156: loss=0.24353288776344723
Epoch #157: loss=0.4511165428492758
Epoch #158: loss=0.20529110729694366
Epoch #159: loss=0.22074523899290296
Epoch #160: loss=0.1779595087799761
Epoch #161: loss=0.15646909632616574
Epoch #162: loss=0.2692539845075872
Epoch #163: loss=0.2159285590880447
Epoch #164: loss=0.1628081136279636
Epoch #165: loss=0.21279294499092632
Epoch #166: loss=0.15138787569271195
Epoch #167: loss=0.16352362806598345
Epoch #168: loss=0.14617932505077785
Epoch #169: loss=0.14208703219062752
Epoch #170: loss=0.15053336053258842
Epoch #171: loss=0.18106527088416946
Epoch #172: loss=0.2018308730589019
Epoch #173: loss=0.1709139415373405
Epoch #174: loss=0.1924667124532991
Epoch #175: loss=0.1314127283791701
Epoch #176: loss=0.18277072347700596
Epoch #177: loss=0.12755910182992616
Epoch #178: loss=0.12210514106684262
Epoch #179: loss=0.17065927531156275
Epoch #180: loss=0.1455706720136934
Epoch #181: loss=0.11325235292315483
Epoch #182: loss=0.17946217105620438
Epoch #183: loss=0.16042342368099424
Epoch #184: loss=0.1978945941146877
Epoch #185: loss=0.11940176184806559
Epoch #186: loss=0.10799601156678465
Epoch #187: loss=0.11731604321135415
Epoch #188: loss=0.10311278473171923
Epoch #189: loss=0.10951830156975323
Epoch #190: loss=0.1528257541358471
Epoch #191: loss=0.11344553757872847
Epoch #192: loss=0.1470081808252467
Epoch #193: loss=0.14556759595870972
Epoch #194: loss=0.14041814414991272
Epoch #195: loss=0.16045857614113224
Epoch #196: loss=0.15520958436859977
Epoch #197: loss=0.16756158135831356
Epoch #198: loss=0.42973830012811554
Epoch #199: loss=0.22771991085675028
Epoch #200: loss=0.18489027126795715
Epoch #201: loss=0.14316252060234547
Epoch #202: loss=0.12237743412454923
Epoch #203: loss=0.14470148562557167
Epoch #204: loss=0.13297869347863728
Epoch #205: loss=0.17128587369289663
Epoch #206: loss=0.14783179739283192
Epoch #207: loss=0.16281259887748295
Epoch #208: loss=0.18308320620821583
Epoch #209: loss=0.17233947416146597
Epoch #210: loss=0.1435258196045955
Epoch #211: loss=0.1517683102024926
Epoch #212: loss=0.1578339727388488
Epoch #213: loss=0.17107531014415953
Epoch #214: loss=0.14484660223954254
Epoch #215: loss=0.130057820222444
Epoch #216: loss=0.09760370467685991
Epoch #217: loss=0.08788967484401332
Epoch #218: loss=0.12484871616794004
Epoch #219: loss=0.13276320333696073
Epoch #220: loss=0.08884870861139563
Epoch #221: loss=0.08922274731513527
Epoch #222: loss=0.1619556202656693
Epoch #223: loss=0.10191587224188778
Epoch #224: loss=0.10786141951878865
Epoch #225: loss=0.11159120810528596
Epoch #226: loss=0.10885683322946231
Epoch #227: loss=0.10867835539910528
Epoch #228: loss=0.09285966369012992
Epoch #229: loss=0.19402271157337558
Epoch #230: loss=0.17595977832873663
Epoch #231: loss=0.12828225787315103
Epoch #232: loss=0.12447762158181933
Epoch #233: loss=0.09121057111769915
Epoch #234: loss=0.10707811669756968
Epoch #235: loss=0.11748183394471805
Epoch #236: loss=0.15719510428607464
Epoch #237: loss=0.1487425629877382
Epoch #238: loss=0.08856553108327919
Epoch #239: loss=0.183551293073429
Epoch #240: loss=0.18249226320121023
Epoch #241: loss=0.13285311249395212
Epoch #242: loss=0.10192179369429748
Epoch #243: loss=0.08596592934595214
Epoch #244: loss=0.12426547592298852
Epoch #245: loss=0.08508565618346135
Epoch #246: loss=0.07785702041453785
Epoch #247: loss=0.08185705149339305
Epoch #248: loss=0.1358494967636135
Epoch #249: loss=0.17389096826728848

Training time: 0:16:58.066618

Finished.
n2one setting etth2_ettm1 -> electricity
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth2_ettm1_epochs_250_seed_2022/model.pkl', muti_dataset='etth2_ettm1', random_seed=2022, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='electricity')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.44178e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.76575e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.44178e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.36829967001517083, 'MAE': 0.42477246853689504}
Finished.
------------------------- record done -------------------------
n2one setting etth2_ettm1 -> weather
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth2_ettm1_epochs_250_seed_2022/model.pkl', muti_dataset='etth2_ettm1', random_seed=2022, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='weather')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.28112e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.48362e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.28112e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.600437361895089, 'MAE': 0.6087240253471212}
Finished.
------------------------- record done -------------------------
n2one setting etth2_ettm1 -> exchange
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth2_ettm1_epochs_250_seed_2022/model.pkl', muti_dataset='etth2_ettm1', random_seed=2022, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='exchange')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=3.94922e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.23999240955313667, 'MAE': 0.32695981508024685}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='etth2_ettm2', random_seed=2022, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/etth2_ettm2_epochs_250_seed_2022
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=5.368314107259114
Epoch #1: loss=2.7154305113686457
Epoch #2: loss=2.418630838394165
Epoch #3: loss=2.176856908533308
Epoch #4: loss=2.0693409310446844
Epoch #5: loss=1.9519428544574313
Epoch #6: loss=1.8358906375037298
Epoch #7: loss=1.650252878665924
Epoch #8: loss=1.6133066945605807
Epoch #9: loss=1.4900135861502752
Epoch #10: loss=1.4068299730618794
Epoch #11: loss=1.3577662110328674
Epoch #12: loss=1.3039125402768452
Epoch #13: loss=1.3243637416097853
Epoch #14: loss=1.2017496294445462
Epoch #15: loss=1.144344922569063
Epoch #16: loss=1.1330312854713864
Epoch #17: loss=1.0801705055766635
Epoch #18: loss=1.031420671277576
Epoch #19: loss=1.0288395484288533
Epoch #20: loss=0.9438671105437808
Epoch #21: loss=1.037709458006753
Epoch #22: loss=0.831355568435457
Epoch #23: loss=0.9784501592318217
Epoch #24: loss=0.9076724251111349
Epoch #25: loss=0.8692490193578932
Epoch #26: loss=0.8247352772288852
Epoch #27: loss=0.9030586746003892
Epoch #28: loss=0.9055182370874617
Epoch #29: loss=0.7983135945267148
Epoch #30: loss=0.815489282210668
Epoch #31: loss=0.6796764135360718
Epoch #32: loss=0.8306010895305209
Epoch #33: loss=0.7130447096294827
Epoch #34: loss=0.7355268150568008
Epoch #35: loss=0.6754315263695188
Epoch #36: loss=0.6069528410832087
Epoch #37: loss=0.5969987329509523
Epoch #38: loss=0.6257829086648093
Epoch #39: loss=0.6256874004999796
Epoch #40: loss=0.6970003909534879
Epoch #41: loss=0.6606132984161377
Epoch #42: loss=0.6017518838246664
Epoch #43: loss=0.6439441740512848
Epoch #44: loss=0.6458513455258476
Epoch #45: loss=0.5993405116928948
Epoch #46: loss=0.592973541882303
Epoch #47: loss=0.6601031637854047
Epoch #48: loss=0.6729118327299753
Epoch #49: loss=0.6309515353706148
Epoch #50: loss=0.6575523383087583
Epoch #51: loss=0.5670768171548843
Epoch #52: loss=0.5265448076857461
Epoch #53: loss=0.5351900657018026
Epoch #54: loss=0.5707068575753106
Epoch #55: loss=0.6582212696472803
Epoch #56: loss=0.5282718390226364
Epoch #57: loss=0.46012026568253833
Epoch #58: loss=0.4746387451887131
Epoch #59: loss=0.5156260513597064
Epoch #60: loss=0.42857077883349526
Epoch #61: loss=0.4354694055186378
Epoch #62: loss=0.46787909004423356
Epoch #63: loss=0.4636819048060311
Epoch #64: loss=0.4462310042646196
Epoch #65: loss=0.40919941829310524
Epoch #66: loss=0.45648595525158775
Epoch #67: loss=0.5212622417343987
Epoch #68: loss=0.5454732436272833
Epoch #69: loss=0.41943150758743286
Epoch #70: loss=0.4195910096168518
Epoch #71: loss=0.4454461998409695
Epoch #72: loss=0.4343901541497972
Epoch #73: loss=0.45839983059300315
Epoch #74: loss=0.4599704278839959
Epoch #75: loss=0.5057684911621941
Epoch #76: loss=0.42841336958938175
Epoch #77: loss=0.4075330313709047
Epoch #78: loss=0.3772663010491265
Epoch #79: loss=0.42772694180409115
Epoch #80: loss=0.46595166954729295
Epoch #81: loss=0.5388993380798234
Epoch #82: loss=0.404586480723487
Epoch #83: loss=0.38192187084092033
Epoch #84: loss=0.4311164427134726
Epoch #85: loss=0.33922185334894395
Epoch #86: loss=0.4001479653848542
Epoch #87: loss=0.5033093343178431
Epoch #88: loss=0.37716949979464215
Epoch #89: loss=0.36846571167310077
Epoch #90: loss=0.3712761319345898
Epoch #91: loss=0.3575538256102138
Epoch #92: loss=0.39518098284800846
Epoch #93: loss=0.449439588520262
Epoch #94: loss=0.48687288496229386
Epoch #95: loss=0.38675232397185433
Epoch #96: loss=0.4144132625725534
Epoch #97: loss=0.3400314814514584
Epoch #98: loss=0.3090638236867057
Epoch #99: loss=0.3990732580423355
Epoch #100: loss=0.4969663752449883
Epoch #101: loss=0.4334512965546714
Epoch #102: loss=0.36853918847110534
Epoch #103: loss=0.33725785546832615
Epoch #104: loss=0.3230579247077306
Epoch #105: loss=0.2783143925997946
Epoch #106: loss=0.3377693858411577
Epoch #107: loss=0.3815761258204778
Epoch #108: loss=0.32803944995005924
Epoch #109: loss=0.3342339156402482
Epoch #110: loss=0.2930511261026065
Epoch #111: loss=0.23953505522674984
Epoch #112: loss=0.30817510353194344
Epoch #113: loss=0.27865702824460137
Epoch #114: loss=0.23049689249859917
Epoch #115: loss=0.31949225730366176
Epoch #116: loss=0.3004474722676807
Epoch #117: loss=0.3835885243283378
Epoch #118: loss=0.3190146146549119
Epoch #119: loss=0.27087952858871883
Epoch #120: loss=0.25276025715801453
Epoch #121: loss=0.2219739415579372
Epoch #122: loss=0.2492330479953024
Epoch #123: loss=0.35241340017981
Epoch #124: loss=0.3029409630431069
Epoch #125: loss=0.28830260038375854
Epoch #126: loss=0.286608309381538
Epoch #127: loss=0.2386903398566776
Epoch #128: loss=0.22320177737209532
Epoch #129: loss=0.2453194624847836
Epoch #130: loss=0.27327968180179596
Epoch #131: loss=0.2397011849615309
Epoch #132: loss=0.2969297568003337
Epoch #133: loss=0.19472096570663983
Epoch #134: loss=0.16814949777391222
Epoch #135: loss=0.21607288304302427
Epoch #136: loss=0.30758173184262383
Epoch #137: loss=0.27365253617366153
Epoch #138: loss=0.18945647072460917
Epoch #139: loss=0.17408201967676482
Epoch #140: loss=0.1910304580297735
Epoch #141: loss=0.19886581103006998
Epoch #142: loss=0.16921967392166457
Epoch #143: loss=0.17793799191713333
Epoch #144: loss=0.27310728281736374
Epoch #145: loss=0.21265266463160515
Epoch #146: loss=0.18794681504368782
Epoch #147: loss=0.20197909077008566
Epoch #148: loss=0.21236869858370888
Epoch #149: loss=0.17020134131113687
Epoch #150: loss=0.22207386708921856
Epoch #151: loss=0.1925619203183386
Epoch #152: loss=0.21188667457964686
Epoch #153: loss=0.23243486922648218
Epoch #154: loss=0.19222012907266617
Epoch #155: loss=0.17304178782635266
Epoch #156: loss=0.1736478288140562
Epoch #157: loss=0.36044854629370904
Epoch #158: loss=0.18384935872422326
Epoch #159: loss=0.16422737803724077
Epoch #160: loss=0.17699856559435526
Epoch #161: loss=0.17718293517827988
Epoch #162: loss=0.29502764385607505
Epoch #163: loss=0.21996144205331802
Epoch #164: loss=0.18307308190398747
Epoch #165: loss=0.211794037785795
Epoch #166: loss=0.16293028907643425
Epoch #167: loss=0.16487000924017695
Epoch #168: loss=0.15479062663184273
Epoch #169: loss=0.1417638384219673
Epoch #170: loss=0.15289564285841253
Epoch #171: loss=0.18063444023331007
Epoch #172: loss=0.20773249823186132
Epoch #173: loss=0.15372447421153387
Epoch #174: loss=0.18390564247965813
Epoch #175: loss=0.13787814519471592
Epoch #176: loss=0.1825882854561011
Epoch #177: loss=0.14875451723734537
Epoch #178: loss=0.13038460558487308
Epoch #179: loss=0.17500259313318464
Epoch #180: loss=0.14885198718143833
Epoch #181: loss=0.10986889070934719
Epoch #182: loss=0.17433956120577124
Epoch #183: loss=0.16495328976048362
Epoch #184: loss=0.28716689099868137
Epoch #185: loss=0.157984865622388
Epoch #186: loss=0.12520056549045774
Epoch #187: loss=0.13357146249877083
Epoch #188: loss=0.11582096645401584
Epoch #189: loss=0.12463251501321793
Epoch #190: loss=0.16876487765047285
Epoch #191: loss=0.1284445348299212
Epoch #192: loss=0.1428594645112753
Epoch #193: loss=0.12822411374913323
Epoch #194: loss=0.13195900660422114
Epoch #195: loss=0.1487290450475282
Epoch #196: loss=0.13963826621572176
Epoch #197: loss=0.14978289790451527
Epoch #198: loss=0.4311642050743103
Epoch #199: loss=0.2427779965930515
Epoch #200: loss=0.19987946127851805
Epoch #201: loss=0.1492119189351797
Epoch #202: loss=0.12326389116545518
Epoch #203: loss=0.13158238492906094
Epoch #204: loss=0.1340827074729734
Epoch #205: loss=0.17576618223554558
Epoch #206: loss=0.13993011290828386
Epoch #207: loss=0.1567716302557124
Epoch #208: loss=0.17345106870763832
Epoch #209: loss=0.1312551177624199
Epoch #210: loss=0.11509837313658661
Epoch #211: loss=0.1378555130213499
Epoch #212: loss=0.1496980128188928
Epoch #213: loss=0.1620741498967012
Epoch #214: loss=0.15547847975459364
Epoch #215: loss=0.11606312646634048
Epoch #216: loss=0.08844710679517852
Epoch #217: loss=0.08838107654203971
Epoch #218: loss=0.1176537378794617
Epoch #219: loss=0.12948046531528234
Epoch #220: loss=0.08994965524309212
Epoch #221: loss=0.09693226828757259
Epoch #222: loss=0.16146078954140344
Epoch #223: loss=0.10776917181081241
Epoch #224: loss=0.10424912731266683
Epoch #225: loss=0.1068883172960745
Epoch #226: loss=0.09779005207949215
Epoch #227: loss=0.11305981460544798
Epoch #228: loss=0.10395179119788939
Epoch #229: loss=0.1868480628149377
Epoch #230: loss=0.17360525805917051
Epoch #231: loss=0.126530467843016
Epoch #232: loss=0.11508638670461045
Epoch #233: loss=0.08338870894577768
Epoch #234: loss=0.11488350656711394
Epoch #235: loss=0.11326023087733322
Epoch #236: loss=0.1444306116965082
Epoch #237: loss=0.15092986077070236
Epoch #238: loss=0.09784021104375522
Epoch #239: loss=0.185592378800114
Epoch #240: loss=0.2145497076627281
Epoch #241: loss=0.18482233004437554
Epoch #242: loss=0.11347332234597868
Epoch #243: loss=0.08699196473591858
Epoch #244: loss=0.11944151638696592
Epoch #245: loss=0.11346826329827309
Epoch #246: loss=0.0988646431101693
Epoch #247: loss=0.09421692033194834
Epoch #248: loss=0.14812820208155447
Epoch #249: loss=0.1837451397958729

Training time: 0:16:48.578589

Finished.
n2one setting etth2_ettm2 -> electricity
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth2_ettm2_epochs_250_seed_2022/model.pkl', muti_dataset='etth2_ettm2', random_seed=2022, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='electricity')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.47963e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.89844e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.47963e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.37611859271033476, 'MAE': 0.4327168760698795}
Finished.
------------------------- record done -------------------------
n2one setting etth2_ettm2 -> weather
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth2_ettm2_epochs_250_seed_2022/model.pkl', muti_dataset='etth2_ettm2', random_seed=2022, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='weather')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.6178e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=3.02817e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.6178e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.5731172226784328, 'MAE': 0.5627124179142141}
Finished.
------------------------- record done -------------------------
n2one setting etth2_ettm2 -> exchange
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth2_ettm2_epochs_250_seed_2022/model.pkl', muti_dataset='etth2_ettm2', random_seed=2022, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='exchange')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=3.89599e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.23810876953152696, 'MAE': 0.3243185479894845}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='etth2_electricity', random_seed=2022, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/etth2_electricity_epochs_250_seed_2022
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=1.5882498302110812
Epoch #1: loss=0.6273032660346206
Epoch #2: loss=0.43347662732732006
Epoch #3: loss=0.358466824516654
Epoch #4: loss=0.29268609973170406
Epoch #5: loss=0.2486152414277923
Epoch #6: loss=0.22869705236176166
Epoch #7: loss=0.1849918778894878
Epoch #8: loss=0.13971459874656142
Epoch #9: loss=0.15347395538602296
Epoch #10: loss=0.13982750847935677
Epoch #11: loss=0.11970835191583852
Epoch #12: loss=0.09527418372312152
Epoch #13: loss=0.09368901426063442
Epoch #14: loss=0.12101141367925376
Epoch #15: loss=0.07610565689733116
Epoch #16: loss=0.10782909295635253
Epoch #17: loss=0.06642352154183133
Epoch #18: loss=0.0573724976559056
Epoch #19: loss=0.07140530674809181
Epoch #20: loss=0.05128498329417553
Epoch #21: loss=0.047574872930431965
Epoch #22: loss=0.054386888917467395
Epoch #23: loss=0.0594977892157281
Epoch #24: loss=0.0416327373823151
Epoch #25: loss=0.053655000369450666
Epoch #26: loss=0.05374502377845801
Epoch #27: loss=0.04482128305599184
Epoch #28: loss=0.03319744213069675
Epoch #29: loss=0.03256068372156289
Epoch #30: loss=0.039030827955263324
Epoch #31: loss=0.03787746788896961
Epoch #32: loss=0.038833249833082735
Epoch #33: loss=0.045374960189957805
Epoch #34: loss=0.028209823498888533
Epoch #35: loss=0.03312761419546427
Epoch #36: loss=0.03260244263716542
Epoch #37: loss=0.030669204087178336
Epoch #38: loss=0.03399878799015225
Epoch #39: loss=0.01940154521683481
Epoch #40: loss=0.03153828956490587
Epoch #41: loss=0.022965435847986444
Epoch #42: loss=0.033021308189235266
Epoch #43: loss=0.02851179494876017
Epoch #44: loss=0.0307005959535983
Epoch #45: loss=0.025453332360129713
Epoch #46: loss=0.025520503063112633
Epoch #47: loss=0.02283699467612101
Epoch #48: loss=0.019050299214119683
Epoch #49: loss=0.02393100535526883
Epoch #50: loss=0.026708242064980767
Epoch #51: loss=0.02355032672938073
Epoch #52: loss=0.019510996588661973
Epoch #53: loss=0.024170975527717596
Epoch #54: loss=0.02394589901654754
Epoch #55: loss=0.02318287790730232
Epoch #56: loss=0.033332033816827235
Epoch #57: loss=0.02260197838235976
Epoch #58: loss=0.019516507639597943
Epoch #59: loss=0.09569497686163408
Epoch #60: loss=0.01776831944561109
Epoch #61: loss=0.019660114636147825
Epoch #62: loss=0.014552358100645063
Epoch #63: loss=0.02095687536862764
Epoch #64: loss=0.01825387395702575
Epoch #65: loss=0.038448984714128394
Epoch #66: loss=0.02917920267805684
Epoch #67: loss=0.014770866904788219
Epoch #68: loss=0.013148529174497704
Epoch #69: loss=0.013891040700666674
Epoch #70: loss=0.018609086138539857
Epoch #71: loss=0.01660926928587553
Epoch #72: loss=0.018037448980222262
Epoch #73: loss=0.026594656538757745
Epoch #74: loss=0.017631610495921383
Epoch #75: loss=0.023847724724186177
Epoch #76: loss=0.019621115966512618
Epoch #77: loss=0.01968409059208636
Epoch #78: loss=0.019492083824688297
Epoch #79: loss=0.0250113327034874
Epoch #80: loss=0.013728505740135273
Epoch #81: loss=0.013148494576964324
Epoch #82: loss=0.021069086313662232
Epoch #83: loss=0.016671527219154
Epoch #84: loss=0.02431646809791124
Epoch #85: loss=0.02286928871248298
Epoch #86: loss=0.012224368106150749
Epoch #87: loss=0.015549775111494676
Epoch #88: loss=0.012227454169677491
Epoch #89: loss=0.015142318025293828
Epoch #90: loss=0.022572233809602573
Epoch #91: loss=0.016176762202188156
Epoch #92: loss=0.014652625660634907
Epoch #93: loss=0.017665879242973994
Epoch #94: loss=0.016949624100007207
Epoch #95: loss=0.012896103754628269
Epoch #96: loss=0.037777559988970144
Epoch #97: loss=0.015158557435802876
Epoch #98: loss=0.01296238924757146
Epoch #99: loss=0.013513167161585354
Epoch #100: loss=0.019212790924939327
Epoch #101: loss=0.013224591791385604
Epoch #102: loss=0.013910504529698433
Epoch #103: loss=0.014514545627473747
Epoch #104: loss=0.013171805991477435
Epoch #105: loss=0.00932858781086951
Epoch #106: loss=0.009973376493357269
Epoch #107: loss=0.013016254423369324
Epoch #108: loss=0.016230073804497185
Epoch #109: loss=0.023599559690188558
Epoch #110: loss=0.012901212697992067
Epoch #111: loss=0.013476776858779642
Epoch #112: loss=0.010750429108423016
Epoch #113: loss=0.013505154427370943
Epoch #114: loss=0.017836593704620412
Epoch #115: loss=0.0151237849356953
Epoch #116: loss=0.015402041748485517
Epoch #117: loss=0.00956331401255561
Epoch #118: loss=0.010851414227878062
Epoch #119: loss=0.028581471083027016
Epoch #120: loss=0.01805084163327304
Epoch #121: loss=0.011878162525557323
Epoch #122: loss=0.015125384988162452
Epoch #123: loss=0.01041324383115308
Epoch #124: loss=0.01666847681030404
Epoch #125: loss=0.012662688323804858
Epoch #126: loss=0.017580340555712875
Epoch #127: loss=0.01575822898924277
Epoch #128: loss=0.0128464414406264
Epoch #129: loss=0.015111145952598635
Epoch #130: loss=0.01493777584468395
Epoch #131: loss=0.010679346636607079
Epoch #132: loss=0.009678957734516788
Epoch #133: loss=0.015408667981895765
Epoch #134: loss=0.013967505716697437
Epoch #135: loss=0.012760637373192855
Epoch #136: loss=0.008190556631456186
Epoch #137: loss=0.015283776691720093
Epoch #138: loss=0.006436741574139976
Epoch #139: loss=0.012091348477974308
Epoch #140: loss=0.012047362370006939
Epoch #141: loss=0.011711502453669332
Epoch #142: loss=0.01091201593705436
Epoch #143: loss=0.014221791155272868
Epoch #144: loss=0.017299276310950518
Epoch #145: loss=0.051553198325564153
Epoch #146: loss=0.011791267355431224
Epoch #147: loss=0.040389677967695894
Epoch #148: loss=0.02060409500256533
Epoch #149: loss=0.012325247326793388
Epoch #150: loss=0.007151247084526936
Epoch #151: loss=0.011710449318254577
Epoch #152: loss=0.010764064701439433
Epoch #153: loss=0.013795822966175165
Epoch #154: loss=0.00867219632842845
Epoch #155: loss=0.013661706861523449
Epoch #156: loss=0.01469745100399539
Epoch #157: loss=0.009149745635180053
Epoch #158: loss=0.010647061629974704
Epoch #159: loss=0.010233111392242391
Epoch #160: loss=0.030289439100714646
Epoch #161: loss=0.013742015154382056
Epoch #162: loss=0.01006100366386108
Epoch #163: loss=0.008491812210394994
Epoch #164: loss=0.012460471240756749
Epoch #165: loss=0.009257421145576233
Epoch #166: loss=0.009581160237824562
Epoch #167: loss=0.021433401198187423
Epoch #168: loss=0.008571925202953181
Epoch #169: loss=0.008863338089257723
Epoch #170: loss=0.009953113488669127
Epoch #171: loss=0.01413225062202489
Epoch #172: loss=0.01678351125298011
Epoch #173: loss=0.029646329866919563
Epoch #174: loss=0.011835512234162108
Epoch #175: loss=0.010172882337206918
Epoch #176: loss=0.011897572138855693
Epoch #177: loss=0.00761761810139344
Epoch #178: loss=0.011035191659885174
Epoch #179: loss=0.01822146093994578
Epoch #180: loss=0.010977479532856506
Epoch #181: loss=0.010234854114525838
Epoch #182: loss=0.007817016865063612
Epoch #183: loss=0.008819490757227091
Epoch #184: loss=0.007744937049166055
Epoch #185: loss=0.004970912299563124
Epoch #186: loss=0.01407529088198605
Epoch #187: loss=0.009631108137022594
Epoch #188: loss=0.00613203798497036
Epoch #189: loss=0.011245634388171489
Epoch #190: loss=0.025001925659059298
Epoch #191: loss=0.013498348404160735
Epoch #192: loss=0.007116629494525913
Epoch #193: loss=0.010886292006833217
Epoch #194: loss=0.006077424649691504
Epoch #195: loss=0.008784609104978011
Epoch #196: loss=0.006743716753567796
Epoch #197: loss=0.015364298185531658
Epoch #198: loss=0.017481140457764523
Epoch #199: loss=0.015521176669941137
Epoch #200: loss=0.008826306057726913
Epoch #201: loss=0.008145624637197726
Epoch #202: loss=0.00881992920483576
Epoch #203: loss=0.010098862614185924
Epoch #204: loss=0.013502246679819087
Epoch #205: loss=0.010512001747773
Epoch #206: loss=0.008573805369939314
Epoch #207: loss=0.00686648094202477
Epoch #208: loss=0.013807168316483645
Epoch #209: loss=0.009807248538691499
Epoch #210: loss=0.0108992730531055
Epoch #211: loss=0.01076488693868713
Epoch #212: loss=0.011158910617778907
Epoch #213: loss=0.00806003569436498
Epoch #214: loss=0.03134631856718021
Epoch #215: loss=0.00854315503194102
Epoch #216: loss=0.004587467556120828
Epoch #217: loss=0.0071821059090569446
Epoch #218: loss=0.013317365466053428
Epoch #219: loss=0.010571463078099995
Epoch #220: loss=0.006067041491139496
Epoch #221: loss=0.009032995873783919
Epoch #222: loss=0.012703294799771024
Epoch #223: loss=0.01113904416493691
Epoch #224: loss=0.007957128919082518
Epoch #225: loss=0.014311688924641292
Epoch #226: loss=0.008928236944484524
Epoch #227: loss=0.008325350311657174
Epoch #228: loss=0.01307765938122984
Epoch #229: loss=0.00950792987980715
Epoch #230: loss=0.010255536997868969
Epoch #231: loss=0.011617160997269732
Epoch #232: loss=0.00917105924165254
Epoch #233: loss=0.009247045201358028
Epoch #234: loss=0.008452470853204963
Epoch #235: loss=0.008705664229548746
Epoch #236: loss=0.018105474378056142
Epoch #237: loss=0.007625911159532445
Epoch #238: loss=0.008401899774581558
Epoch #239: loss=0.005843032383049055
Epoch #240: loss=0.009866434656454126
Epoch #241: loss=0.00911144377850422
Epoch #242: loss=0.010739840747413748
Epoch #243: loss=0.011623474421290534
Epoch #244: loss=0.009345825990616194
Epoch #245: loss=0.007291219119174568
Epoch #246: loss=0.012159732854165557
Epoch #247: loss=0.01633993185834431
Epoch #248: loss=0.015359989739209129
Epoch #249: loss=0.01051867054083254

Training time: 4:30:43.122885

Finished.
n2one setting etth2_electricity -> weather
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth2_electricity_epochs_250_seed_2022/model.pkl', muti_dataset='etth2_electricity', random_seed=2022, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='weather')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=9.76054e-09): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.11699e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=5.02374e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=9.76054e-09): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.804277655920751, 'MAE': 0.670119955818629}
Finished.
------------------------- record done -------------------------
n2one setting etth2_electricity -> exchange
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth2_electricity_epochs_250_seed_2022/model.pkl', muti_dataset='etth2_electricity', random_seed=2022, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='exchange')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=5.47641e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.2859293657894635, 'MAE': 0.3626204453536131}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='etth2_traffic', random_seed=2022, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/etth2_traffic_epochs_250_seed_2022
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=0.9765154137024259
Epoch #1: loss=0.33834884800145487
Epoch #2: loss=0.2489285598089972
Epoch #3: loss=0.17808602478085098
Epoch #4: loss=0.13829680333306243
Epoch #5: loss=0.11866043578786185
Epoch #6: loss=0.09911993368645601
Epoch #7: loss=0.08200484544030198
Epoch #8: loss=0.08437403390597728
Epoch #9: loss=0.06359014884376914
Epoch #10: loss=0.06133039876169196
Epoch #11: loss=0.06680483449415504
Epoch #12: loss=0.05496123447474114
Epoch #13: loss=0.045338948815364405
Epoch #14: loss=0.03745102288787681
Epoch #15: loss=0.03719060155627359
Epoch #16: loss=0.037509983875419005
Epoch #17: loss=0.029251943541727145
Epoch #18: loss=0.03525472099164563
Epoch #19: loss=0.038783827367623445
Epoch #20: loss=0.03148435027085639
Epoch #21: loss=0.02802678210764236
Epoch #22: loss=0.03187645792583876
Epoch #23: loss=0.025048956036261175
Epoch #24: loss=0.026217539284873425
Epoch #25: loss=0.026176210504906174
Epoch #26: loss=0.028195078868656853
Epoch #27: loss=0.028987303360523946
Epoch #28: loss=0.025919619512554552
Epoch #29: loss=0.027518976767256995
Epoch #30: loss=0.023028352881057812
Epoch #31: loss=0.024969768410353515
Epoch #32: loss=0.027420859672965092
Epoch #33: loss=0.02102212801707482
Epoch #34: loss=0.021452464338462218
Epoch #35: loss=0.026750595280372776
Epoch #36: loss=0.02900310893225836
Epoch #37: loss=0.025002525126605998
Epoch #38: loss=0.014722297986903977
Epoch #39: loss=0.015381294996445511
Epoch #40: loss=0.027392491707173727
Epoch #41: loss=0.022096934388991834
Epoch #42: loss=0.014926347381230904
Epoch #43: loss=0.016927928351719464
Epoch #44: loss=0.020678551608583513
Epoch #45: loss=0.01689420544941721
Epoch #46: loss=0.020680391854701417
Epoch #47: loss=0.02678947087824216
Epoch #48: loss=0.017736415769792322
Epoch #49: loss=0.023169205590741766
Epoch #50: loss=0.018785307427141622
Epoch #51: loss=0.019293950842902433
Epoch #52: loss=0.014315304417919766
Epoch #53: loss=0.016545349555643345
Epoch #54: loss=0.02021041669583021
Epoch #55: loss=0.022555716257337138
Epoch #56: loss=0.016292450760391524
Epoch #57: loss=0.016942368881732454
Epoch #58: loss=0.015973455528956404
Epoch #59: loss=0.021021123751325446
Epoch #60: loss=0.011704282856526783
Epoch #61: loss=0.023514181634557773
Epoch #62: loss=0.015074410556571213
Epoch #63: loss=0.01751930523596509
Epoch #64: loss=0.01731797694030322
Epoch #65: loss=0.027183412713755857
Epoch #66: loss=0.016921602484162725
Epoch #67: loss=0.011649264434649384
Epoch #68: loss=0.01570705525358438
Epoch #69: loss=0.014145305581138736
Epoch #70: loss=0.013420759002617544
Epoch #71: loss=0.01642377007094656
Epoch #72: loss=0.01871970057877593
Epoch #73: loss=0.013000377203349638
Epoch #74: loss=0.019970161947022516
Epoch #75: loss=0.016001969636032107
Epoch #76: loss=0.011895490309649004
Epoch #77: loss=0.018695521832808693
Epoch #78: loss=0.009922397001705404
Epoch #79: loss=0.016226604784304603
Epoch #80: loss=0.01726414017811722
Epoch #81: loss=0.012476626270357144
Epoch #82: loss=0.01669848687618315
Epoch #83: loss=0.016727295289029366
Epoch #84: loss=0.012198581026641436
Epoch #85: loss=0.013668980837567313
Epoch #86: loss=0.012273300311785895
Epoch #87: loss=0.017074331340613914
Epoch #88: loss=0.01632592309410274
Epoch #89: loss=0.025717633628536667
Epoch #90: loss=0.011380930131558362
Epoch #91: loss=0.012745919408737783
Epoch #92: loss=0.020620095489374646
Epoch #93: loss=0.009121187669208962
Epoch #94: loss=0.013216367145326155
Epoch #95: loss=0.01785373051488859
Epoch #96: loss=0.015360636184986315
Epoch #97: loss=0.013228448461706378
Epoch #98: loss=0.01235637840904243
Epoch #99: loss=0.01295124916606947
Epoch #100: loss=0.0163386229306976
Epoch #101: loss=0.011109174592248984
Epoch #102: loss=0.016265343246565943
Epoch #103: loss=0.013159323897857383
Epoch #104: loss=0.010013761884552696
Epoch #105: loss=0.011477571885484369
Epoch #106: loss=0.01227606342610034
Epoch #107: loss=0.01240048627603544
Epoch #108: loss=0.01766040421743964
Epoch #109: loss=0.010880761828538836
Epoch #110: loss=0.01149439502489229
Epoch #111: loss=0.015086736244084398
Epoch #112: loss=0.01183820056526196
Epoch #113: loss=0.013358652627775316
Epoch #114: loss=0.01061540222667148
Epoch #115: loss=0.015435827307994407
Epoch #116: loss=0.015018791935641868
Epoch #117: loss=0.011092431223493121
Epoch #118: loss=0.013595834160988541
Epoch #119: loss=0.013554434711361406
Epoch #120: loss=0.011898481555795351
Epoch #121: loss=0.020263510148564215
Epoch #122: loss=0.014236244440893374
Epoch #123: loss=0.009149825976562973
Epoch #124: loss=0.0077217481575749
Epoch #125: loss=0.020217546836890483
Epoch #126: loss=0.016922795144130795
Epoch #127: loss=0.010822936439641802
Epoch #128: loss=0.012316054745600854
Epoch #129: loss=0.008687282713361607
Epoch #130: loss=0.0191235629732772
Epoch #131: loss=0.012616842389244733
Epoch #132: loss=0.011331752939281693
Epoch #133: loss=0.009674627965656983
Epoch #134: loss=0.012621892486407955
Epoch #135: loss=0.011182074304905414
Epoch #136: loss=0.015175839910149452
Epoch #137: loss=0.013744611698437896
Epoch #138: loss=0.008454918259217846
Epoch #139: loss=0.019826666675343246
Epoch #140: loss=0.010081549301563056
Epoch #141: loss=0.007768432097573137
Epoch #142: loss=0.013478485645669521
Epoch #143: loss=0.012932598955234806
Epoch #144: loss=0.011940974177278511
Epoch #145: loss=0.008626912569389245
Epoch #146: loss=0.012881897859635973
Epoch #147: loss=0.010839017141205543
Epoch #148: loss=0.01092470794073113
Epoch #149: loss=0.015803300300322676
Epoch #150: loss=0.01309591805945784
Epoch #151: loss=0.01624718761256166
Epoch #152: loss=0.00946105293735596
Epoch #153: loss=0.01180079696672916
Epoch #154: loss=0.007799241462513704
Epoch #155: loss=0.022725339591909324
Epoch #156: loss=0.011715180005124358
Epoch #157: loss=0.01515195711674726
Epoch #158: loss=0.008084471516239632
Epoch #159: loss=0.011667705589756144
Epoch #160: loss=0.010640501434762317
Epoch #161: loss=0.009324271626147828
Epoch #162: loss=0.010914919482003784
Epoch #163: loss=0.012927170935539304
Epoch #164: loss=0.010486381870716195
Epoch #165: loss=0.012211812024368605
Epoch #166: loss=0.011103165363784402
Epoch #167: loss=0.008232522925092186
Epoch #168: loss=0.011238606499563753
Epoch #169: loss=0.01178071151792768
Epoch #170: loss=0.010631517820596514
Epoch #171: loss=0.011658012068510791
Epoch #172: loss=0.007544306019478559
Epoch #173: loss=0.007694911932978233
Epoch #174: loss=0.018629756690551162
Epoch #175: loss=0.007769994616771858
Epoch #176: loss=0.02424483546865335
Epoch #177: loss=0.0089425749180413
Epoch #178: loss=0.009521901170152183
Epoch #179: loss=0.008945701300163264
Epoch #180: loss=0.008928943709656542
Epoch #181: loss=0.01211032188981833
Epoch #182: loss=0.013588429957328705
Epoch #183: loss=0.012385568420964822
Epoch #184: loss=0.012306937788401074
Epoch #185: loss=0.008169404885915036
Epoch #186: loss=0.017762465019878673
Epoch #187: loss=0.008142467640097585
Epoch #188: loss=0.009289626058839985
Epoch #189: loss=0.01877022950954374
Epoch #190: loss=0.011144238849639651
Epoch #191: loss=0.00890801110225451
Epoch #192: loss=0.009366781500508951
Epoch #193: loss=0.012401557166012265
Epoch #194: loss=0.009655509191976889
Epoch #195: loss=0.00970040293165198
Epoch #196: loss=0.012558468340091477
Epoch #197: loss=0.010435290714609035
Epoch #198: loss=0.014228931619091521
Epoch #199: loss=0.016964691034874527
Epoch #200: loss=0.011950973405158983
Epoch #201: loss=0.010147355752722473
Epoch #202: loss=0.009943934056849752
Epoch #203: loss=0.0056301539610619615
Epoch #204: loss=0.009666245066139754
Epoch #205: loss=0.013076091770235969
Epoch #206: loss=0.00769333183960397
Epoch #207: loss=0.010125859667867491
Epoch #208: loss=0.008328312696683809
Epoch #209: loss=0.008226760366109148
Epoch #210: loss=0.008340087011743843
Epoch #211: loss=0.010345232603838466
Epoch #212: loss=0.009935116651896666
Epoch #213: loss=0.00961350334543784
Epoch #214: loss=0.00875078520564908
Epoch #215: loss=0.005817239978156617
Epoch #216: loss=0.010331051100620955
Epoch #217: loss=0.010259467074007194
Epoch #218: loss=0.0070276010715348
Epoch #219: loss=0.006536864809973165
Epoch #220: loss=0.011554557584420783
Epoch #221: loss=0.00866518621125026
Epoch #222: loss=0.009584173067051957
Epoch #223: loss=0.007217337761546664
Epoch #224: loss=0.015066076602278518
Epoch #225: loss=0.008709578752865741
Epoch #226: loss=0.009555399828520859
Epoch #227: loss=0.007153779618699673
Epoch #228: loss=0.00979794335260823
Epoch #229: loss=0.01423264317745592
Epoch #230: loss=0.012603315657515924
Epoch #231: loss=0.007855398241986876
Epoch #232: loss=0.007534615147025062
Epoch #233: loss=0.00599081183218245
Epoch #234: loss=0.006658919642559361
Epoch #235: loss=0.012333239324240044
Epoch #236: loss=0.012746785464013366
Epoch #237: loss=0.011167095997705797
Epoch #238: loss=0.007385736997838951
Epoch #239: loss=0.010722044745404691
Epoch #240: loss=0.0063166936036158205
Epoch #241: loss=0.012902657065528629
Epoch #242: loss=0.011026341362740917
Epoch #243: loss=0.009921752532704756
Epoch #244: loss=0.00964504099397319
Epoch #245: loss=0.009542651078526452
Epoch #246: loss=0.024349649582056584
Epoch #247: loss=0.006224354803942624
Epoch #248: loss=0.006275512828134847
Epoch #249: loss=0.009511375373080817

Training time: 10:23:39.133465

Finished.
n2one setting etth2_traffic -> electricity
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth2_traffic_epochs_250_seed_2022/model.pkl', muti_dataset='etth2_traffic', random_seed=2022, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='electricity')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=6.28262e-09): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.70251e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=4.33971e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=6.28262e-09): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.41768867159783174, 'MAE': 0.4591396596157289}
Finished.
------------------------- record done -------------------------
n2one setting etth2_traffic -> weather
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth2_traffic_epochs_250_seed_2022/model.pkl', muti_dataset='etth2_traffic', random_seed=2022, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='weather')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=9.25458e-09): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.91e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=4.89391e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=9.25458e-09): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.8424337967633045, 'MAE': 0.6964023807308874}
Finished.
------------------------- record done -------------------------
n2one setting etth2_traffic -> exchange
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth2_traffic_epochs_250_seed_2022/model.pkl', muti_dataset='etth2_traffic', random_seed=2022, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='exchange')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=4.47529e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.3474532424233223, 'MAE': 0.3841311101995026}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='etth2_weather', random_seed=2022, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/etth2_weather_epochs_250_seed_2022
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=5.506126490506259
Epoch #1: loss=2.6830395207260596
Epoch #2: loss=2.90215621211312
Epoch #3: loss=2.1989956263339883
Epoch #4: loss=2.125512957572937
Epoch #5: loss=1.920335307265773
Epoch #6: loss=1.7662322015473337
Epoch #7: loss=1.6052962577704228
Epoch #8: loss=1.6589696624062278
Epoch #9: loss=1.4753073779019443
Epoch #10: loss=1.390618819178957
Epoch #11: loss=1.2907041022271821
Epoch #12: loss=1.219908154372013
Epoch #13: loss=1.2183293158357793
Epoch #14: loss=1.1946930000276277
Epoch #15: loss=1.1739421811970798
Epoch #16: loss=1.1299045952883633
Epoch #17: loss=1.0891561381744617
Epoch #18: loss=1.0448062979813777
Epoch #19: loss=1.0523433576930652
Epoch #20: loss=0.8746672128186082
Epoch #21: loss=0.8954870339595911
Epoch #22: loss=0.871122992399967
Epoch #23: loss=0.7992001645492784
Epoch #24: loss=0.9402100895390366
Epoch #25: loss=0.8299912105907093
Epoch #26: loss=0.8585988373467417
Epoch #27: loss=0.767795463403066
Epoch #28: loss=0.8174987290844773
Epoch #29: loss=0.7978777704816876
Epoch #30: loss=0.7232388239918333
Epoch #31: loss=0.7036728678327618
Epoch #32: loss=0.6337933323600076
Epoch #33: loss=0.719894907691262
Epoch #34: loss=0.6441158471685468
Epoch #35: loss=0.6694053285049669
Epoch #36: loss=0.7189791672157518
Epoch #37: loss=0.6825680064432549
Epoch #38: loss=0.701143098599983
Epoch #39: loss=0.6621863580111301
Epoch #40: loss=0.7021027153188532
Epoch #41: loss=0.5872999456795779
Epoch #42: loss=0.6034978980367834
Epoch #43: loss=0.5307331789623607
Epoch #44: loss=0.5215470023227461
Epoch #45: loss=0.5041495218421473
Epoch #46: loss=0.48871237671736517
Epoch #47: loss=0.5392303918347214
Epoch #48: loss=0.5085200513854171
Epoch #49: loss=0.546652862519929
Epoch #50: loss=0.531053898009387
Epoch #51: loss=0.5478767572027264
Epoch #52: loss=0.5510493833007235
Epoch #53: loss=0.5399537032300775
Epoch #54: loss=0.7986469864845276
Epoch #55: loss=0.5802111309586149
Epoch #56: loss=0.44796990445165924
Epoch #57: loss=0.43120909008112823
Epoch #58: loss=0.5517405045754982
Epoch #59: loss=0.48026945464538806
Epoch #60: loss=0.4216663137529836
Epoch #61: loss=0.4550644204472051
Epoch #62: loss=0.4054577291914911
Epoch #63: loss=0.4204054777369355
Epoch #64: loss=0.40662153110359656
Epoch #65: loss=0.42769383481054596
Epoch #66: loss=0.3919410696535399
Epoch #67: loss=0.3713151216506958
Epoch #68: loss=0.35987385114034015
Epoch #69: loss=0.3959134659080794
Epoch #70: loss=0.33848968283696607
Epoch #71: loss=0.3683155052589648
Epoch #72: loss=0.40023884809378424
Epoch #73: loss=0.31918967176567425
Epoch #74: loss=0.3145247824264295
Epoch #75: loss=0.2964397957830718
Epoch #76: loss=0.3494393437197714
Epoch #77: loss=0.29820300954760925
Epoch #78: loss=0.40420687830809393
Epoch #79: loss=0.3896591410492406
Epoch #80: loss=0.41861985939921753
Epoch #81: loss=0.36577287406632397
Epoch #82: loss=0.3381065979148402
Epoch #83: loss=0.2825641300190579
Epoch #84: loss=0.24735627233078986
Epoch #85: loss=0.34009860501144873
Epoch #86: loss=0.2650637292500698
Epoch #87: loss=0.26646764395814954
Epoch #88: loss=0.2629555484110659
Epoch #89: loss=0.24294715532750794
Epoch #90: loss=0.29443331627231656
Epoch #91: loss=0.27402632728670584
Epoch #92: loss=0.2931846955960447
Epoch #93: loss=0.24395596506920728
Epoch #94: loss=0.2815980687737465
Epoch #95: loss=0.2476132261482152
Epoch #96: loss=0.25933391465382144
Epoch #97: loss=0.25040572701078473
Epoch #98: loss=0.2887641606908856
Epoch #99: loss=0.3838998955307585
Epoch #100: loss=0.2658887433283257
Epoch #101: loss=0.23838181951732346
Epoch #102: loss=0.2515974141883128
Epoch #103: loss=0.264732047012358
Epoch #104: loss=0.3092958778142929
Epoch #105: loss=0.2947318386850935
Epoch #106: loss=0.24656899973298563
Epoch #107: loss=0.22870648607160105
Epoch #108: loss=0.2118244728807247
Epoch #109: loss=0.20155684220971484
Epoch #110: loss=0.18944204756707855
Epoch #111: loss=0.2503586327939322
Epoch #112: loss=0.17973120578310706
Epoch #113: loss=0.1648850024423816
Epoch #114: loss=0.2281487580727447
Epoch #115: loss=0.19221355269352594
Epoch #116: loss=0.18099591237577525
Epoch #117: loss=0.18251848198247678
Epoch #118: loss=0.19103250607396616
Epoch #119: loss=0.18881012296134775
Epoch #120: loss=0.1929929434801593
Epoch #121: loss=0.14499465048764693
Epoch #122: loss=0.17924751330054167
Epoch #123: loss=0.18376402884270204
Epoch #124: loss=0.1504813658468651
Epoch #125: loss=0.14970506276145126
Epoch #126: loss=0.13920724030696985
Epoch #127: loss=0.22040223318970564
Epoch #128: loss=0.1877728452285131
Epoch #129: loss=0.172784782607447
Epoch #130: loss=0.14622440042369295
Epoch #131: loss=0.12890852220130689
Epoch #132: loss=0.19426279390851656
Epoch #133: loss=0.18687211101253828
Epoch #134: loss=0.17390394696232045
Epoch #135: loss=0.1912561377341097
Epoch #136: loss=0.1537949157257875
Epoch #137: loss=0.13872178282701608
Epoch #138: loss=0.1785286548688556
Epoch #139: loss=0.18633797651890552
Epoch #140: loss=0.24669733350024078
Epoch #141: loss=0.1730476561369318
Epoch #142: loss=0.17453900115056473
Epoch #143: loss=0.14667459961139795
Epoch #144: loss=0.19609733648372418
Epoch #145: loss=0.14285631331078935
Epoch #146: loss=0.16137593437099096
Epoch #147: loss=0.15425098371325116
Epoch #148: loss=0.11387737625927637
Epoch #149: loss=0.17232030122117561
Epoch #150: loss=0.1481151453247576
Epoch #151: loss=0.17004961136615637
Epoch #152: loss=0.2013001808840217
Epoch #153: loss=0.14063821800730444
Epoch #154: loss=0.28918790410865436
Epoch #155: loss=0.17274350887446693
Epoch #156: loss=0.43219515918330714
Epoch #157: loss=0.22809645194898953
Epoch #158: loss=0.17957690979043642
Epoch #159: loss=0.1368580935805133
Epoch #160: loss=0.14575466592655037
Epoch #161: loss=0.15770878879861397
Epoch #162: loss=0.11540937276952194
Epoch #163: loss=0.13953619772060352
Epoch #164: loss=0.13040594570338726
Epoch #165: loss=0.1241928818776752
Epoch #166: loss=0.16542954302646898
Epoch #167: loss=0.15791632833354402
Epoch #168: loss=0.14319850933371167
Epoch #169: loss=0.19154171577908777
Epoch #170: loss=0.1298179840951255
Epoch #171: loss=0.11508847778719483
Epoch #172: loss=0.10039763141310576
Epoch #173: loss=0.13696304477299703
Epoch #174: loss=0.12766604193232275
Epoch #175: loss=0.12308597338922096
Epoch #176: loss=0.13448465773553558
Epoch #177: loss=0.11432024384989883
Epoch #178: loss=0.10539057399287369
Epoch #179: loss=0.10968333725450617
Epoch #180: loss=0.14737773455227865
Epoch #181: loss=0.16485187862858627
Epoch #182: loss=0.12961960617791524
Epoch #183: loss=0.11399662240662357
Epoch #184: loss=0.10495318839270057
Epoch #185: loss=0.10995513341869369
Epoch #186: loss=0.10691805584638407
Epoch #187: loss=0.12090115982926253
Epoch #188: loss=0.10853619117176894
Epoch #189: loss=0.08361068557044773
Epoch #190: loss=0.09555063769221306
Epoch #191: loss=0.08741829374974425
Epoch #192: loss=0.1553357642595515
Epoch #193: loss=0.10968543182719838
Epoch #194: loss=0.09654492879229964
Epoch #195: loss=0.08739531531252644
Epoch #196: loss=0.09455065649341453
Epoch #197: loss=0.15019731517090942
Epoch #198: loss=0.09461117304409995
Epoch #199: loss=0.08813115316584255
Epoch #200: loss=0.06921252311969345
Epoch #201: loss=0.10143621254599455
Epoch #202: loss=0.10535687033199903
Epoch #203: loss=0.08242506545149919
Epoch #204: loss=0.11164042816469164
Epoch #205: loss=0.1052718661499746
Epoch #206: loss=0.09519410737310395
Epoch #207: loss=0.09182200919498097
Epoch #208: loss=0.09576760051828442
Epoch #209: loss=0.07426418685777621
Epoch #210: loss=0.12991179971758163
Epoch #211: loss=0.06835562594686494
Epoch #212: loss=0.09626403631586017
Epoch #213: loss=0.069867367997314
Epoch #214: loss=0.08769466265132933
Epoch #215: loss=0.1271874722883557
Epoch #216: loss=0.09175961357400272
Epoch #217: loss=0.14514614844864066
Epoch #218: loss=0.1158038665625182
Epoch #219: loss=0.07863424430516633
Epoch #220: loss=0.07497742120176554
Epoch #221: loss=0.06322252327068285
Epoch #222: loss=0.08104984383239891
Epoch #223: loss=0.10772353625207236
Epoch #224: loss=0.08657355708154765
Epoch #225: loss=0.08522222191095352
Epoch #226: loss=0.07571076982739297
Epoch #227: loss=0.0682956665810762
Epoch #228: loss=0.0772562547389305
Epoch #229: loss=0.06370738848592296
Epoch #230: loss=0.0851803304288875
Epoch #231: loss=0.0883706755597483
Epoch #232: loss=0.10070949643285888
Epoch #233: loss=0.0723497574923165
Epoch #234: loss=0.10796161134247527
Epoch #235: loss=0.09267296856551459
Epoch #236: loss=0.13555832111248464
Epoch #237: loss=0.0739112708604697
Epoch #238: loss=0.0616646713548989
Epoch #239: loss=0.15864962345045625
Epoch #240: loss=0.09012958082850232
Epoch #241: loss=0.11663204267846816
Epoch #242: loss=0.11006819665657752
Epoch #243: loss=0.1372058508974133
Epoch #244: loss=0.10454483860821435
Epoch #245: loss=0.07140456238817988
Epoch #246: loss=0.08281084279896635
Epoch #247: loss=0.07264117586115997
Epoch #248: loss=0.07901309735395691
Epoch #249: loss=0.11102824082428758

Training time: 0:33:05.324083

Finished.
n2one setting etth2_weather -> electricity
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth2_weather_epochs_250_seed_2022/model.pkl', muti_dataset='etth2_weather', random_seed=2022, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='electricity')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.15435e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.29923e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=5.54431e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.15435e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.3649717219564946, 'MAE': 0.4264767926668779}
Finished.
------------------------- record done -------------------------
n2one setting etth2_weather -> exchange
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth2_weather_epochs_250_seed_2022/model.pkl', muti_dataset='etth2_weather', random_seed=2022, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='exchange')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=3.19323e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.2097100899438477, 'MAE': 0.31637668369140304}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='etth2_exchange', random_seed=2022, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/etth2_exchange_epochs_250_seed_2022
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=4.562200895945231
Epoch #1: loss=2.3670297066370645
Epoch #2: loss=2.016344491640727
Epoch #3: loss=1.9501725832621257
Epoch #4: loss=1.858618680636088
Epoch #5: loss=1.7371663411458333
Epoch #6: loss=1.6197819709777832
Epoch #7: loss=1.5668721596399944
Epoch #8: loss=1.5497837622960409
Epoch #9: loss=1.4473440249760945
Epoch #10: loss=1.4913824160893758
Epoch #11: loss=1.4291822910308838
Epoch #12: loss=1.4257350126902262
Epoch #13: loss=1.3283803780873618
Epoch #14: loss=1.3133264859517415
Epoch #15: loss=1.2132276137669882
Epoch #16: loss=1.2019091208775838
Epoch #17: loss=1.1179049889246622
Epoch #18: loss=1.1983755707740784
Epoch #19: loss=1.0961097677548726
Epoch #20: loss=1.181411095460256
Epoch #21: loss=1.1860732118288675
Epoch #22: loss=1.0335042357444764
Epoch #23: loss=0.9941506226857503
Epoch #24: loss=1.0240734338760376
Epoch #25: loss=0.9817758838335673
Epoch #26: loss=1.0890835682551065
Epoch #27: loss=1.0139663418134053
Epoch #28: loss=0.8570624232292176
Epoch #29: loss=0.9212591687838236
Epoch #30: loss=0.9341998736063639
Epoch #31: loss=0.9847365975379944
Epoch #32: loss=0.9468546072642009
Epoch #33: loss=0.9257757147153218
Epoch #34: loss=0.7961947321891785
Epoch #35: loss=0.837390132745107
Epoch #36: loss=0.7931467493375143
Epoch #37: loss=0.7833076516787211
Epoch #38: loss=0.9121261636416117
Epoch #39: loss=0.7629677255948385
Epoch #40: loss=0.6525826732317607
Epoch #41: loss=0.6830627262592316
Epoch #42: loss=0.7204839368661244
Epoch #43: loss=0.7321500380833944
Epoch #44: loss=0.760862676302592
Epoch #45: loss=0.6588915864626567
Epoch #46: loss=0.6887689928213755
Epoch #47: loss=0.6749540289243062
Epoch #48: loss=0.6106071511904398
Epoch #49: loss=0.5492888530095418
Epoch #50: loss=0.6603978713353474
Epoch #51: loss=0.6864517867565155
Epoch #52: loss=0.5725171566009521
Epoch #53: loss=0.5534673154354095
Epoch #54: loss=0.619933819770813
Epoch #55: loss=0.5580358783404032
Epoch #56: loss=0.5781384766101837
Epoch #57: loss=0.5904338657855988
Epoch #58: loss=0.4807460029919942
Epoch #59: loss=0.6450296560923259
Epoch #60: loss=0.7499499142169952
Epoch #61: loss=0.6858279148737589
Epoch #62: loss=0.4883231441179911
Epoch #63: loss=0.552222783366839
Epoch #64: loss=0.6584105829397837
Epoch #65: loss=0.6165379921595255
Epoch #66: loss=0.6056260267893473
Epoch #67: loss=0.5259182155132294
Epoch #68: loss=0.4943600426117579
Epoch #69: loss=0.48336465656757355
Epoch #70: loss=0.5058537711699803
Epoch #71: loss=0.4815076529979706
Epoch #72: loss=0.41183244784673056
Epoch #73: loss=0.413921982049942
Epoch #74: loss=0.4187788168589274
Epoch #75: loss=0.4438698728879293
Epoch #76: loss=0.5057614892721176
Epoch #77: loss=0.48598078787326815
Epoch #78: loss=0.43387293815612793
Epoch #79: loss=0.5017820219198863
Epoch #80: loss=0.5445236682891845
Epoch #81: loss=0.5002605696519216
Epoch #82: loss=0.4871915509303411
Epoch #83: loss=0.3525756220022837
Epoch #84: loss=0.35723664462566374
Epoch #85: loss=0.40980431288480756
Epoch #86: loss=0.37236321965853375
Epoch #87: loss=0.4622599432865779
Epoch #88: loss=0.3687615970770518
Epoch #89: loss=0.3844270477692286
Epoch #90: loss=0.49942416449387866
Epoch #91: loss=0.4448387106259664
Epoch #92: loss=0.4552285999059677
Epoch #93: loss=0.3508012851079305
Epoch #94: loss=0.47780445317427317
Epoch #95: loss=0.4105732053518295
Epoch #96: loss=0.324534872174263
Epoch #97: loss=0.3942743480205536
Epoch #98: loss=0.38584740459918976
Epoch #99: loss=0.39986731559038163
Epoch #100: loss=0.4808051625887553
Epoch #101: loss=0.3832976222038269
Epoch #102: loss=0.4424462864796321
Epoch #103: loss=0.3130444933970769
Epoch #104: loss=0.3489503790934881
Epoch #105: loss=0.4054222365220388
Epoch #106: loss=0.3510900114973386
Epoch #107: loss=0.26848032921552656
Epoch #108: loss=0.507001967728138
Epoch #109: loss=0.3796996066967646
Epoch #110: loss=0.36864079038302106
Epoch #111: loss=0.34500154356161755
Epoch #112: loss=0.3758265256881714
Epoch #113: loss=0.4593914210796356
Epoch #114: loss=0.3371213138103485
Epoch #115: loss=0.39565406143665316
Epoch #116: loss=0.3281230335434278
Epoch #117: loss=0.37678152273098625
Epoch #118: loss=0.32779080073038735
Epoch #119: loss=0.3970613757769267
Epoch #120: loss=0.5580879122018814
Epoch #121: loss=0.3699710100889206
Epoch #122: loss=0.2826427554090818
Epoch #123: loss=0.2312425583600998
Epoch #124: loss=0.28465240995089214
Epoch #125: loss=0.46996551752090454
Epoch #126: loss=0.38684320549170176
Epoch #127: loss=0.34385676234960555
Epoch #128: loss=0.29436330099900565
Epoch #129: loss=0.30612088143825533
Epoch #130: loss=0.35478756378094356
Epoch #131: loss=0.33187416940927505
Epoch #132: loss=0.3686195204655329
Epoch #133: loss=0.2566030378142993
Epoch #134: loss=0.309074363609155
Epoch #135: loss=0.33027022580305737
Epoch #136: loss=0.34708794951438904
Epoch #137: loss=0.39971877783536913
Epoch #138: loss=0.2447841763496399
Epoch #139: loss=0.3148384282986323
Epoch #140: loss=0.298135835925738
Epoch #141: loss=0.3258493776122729
Epoch #142: loss=0.32053638646999993
Epoch #143: loss=0.24595029999812443
Epoch #144: loss=0.2891515682140986
Epoch #145: loss=0.21988477756579716
Epoch #146: loss=0.2929265369971593
Epoch #147: loss=0.27798330833514534
Epoch #148: loss=0.2829866444071134
Epoch #149: loss=0.29040890286366144
Epoch #150: loss=0.35926131506760917
Epoch #151: loss=0.29977917671203613
Epoch #152: loss=0.2074478194117546
Epoch #153: loss=0.20563444544871648
Epoch #154: loss=0.24967407484849294
Epoch #155: loss=0.25369676649570466
Epoch #156: loss=0.272905732691288
Epoch #157: loss=0.28850239564975105
Epoch #158: loss=0.35043893108765284
Epoch #159: loss=0.4035641238093376
Epoch #160: loss=0.4162931685646375
Epoch #161: loss=0.31751638452212017
Epoch #162: loss=0.2672916320463022
Epoch #163: loss=0.2779377157489459
Epoch #164: loss=0.2995151946942011
Epoch #165: loss=0.300792196393013
Epoch #166: loss=0.2543880353371302
Epoch #167: loss=0.2771444246172905
Epoch #168: loss=0.3445379992326101
Epoch #169: loss=0.2321050062775612
Epoch #170: loss=0.23109499141573905
Epoch #171: loss=0.20955359588066738
Epoch #172: loss=0.2011949121952057
Epoch #173: loss=0.1520839015642802
Epoch #174: loss=0.21981962670882543
Epoch #175: loss=0.18645880793531736
Epoch #176: loss=0.19373687530557315
Epoch #177: loss=0.21350135232011477
Epoch #178: loss=0.2038961097598076
Epoch #179: loss=0.27463680828611053
Epoch #180: loss=0.27537959069013596
Epoch #181: loss=0.2796046510338783
Epoch #182: loss=0.2351681058605512
Epoch #183: loss=0.2654572809735934
Epoch #184: loss=0.22638411372900008
Epoch #185: loss=0.25791451036930085
Epoch #186: loss=0.20313076575597128
Epoch #187: loss=0.1886269693573316
Epoch #188: loss=0.23501577352484068
Epoch #189: loss=0.24143734673659006
Epoch #190: loss=0.2711168761054675
Epoch #191: loss=0.22432773957649868
Epoch #192: loss=0.189887964228789
Epoch #193: loss=0.23752770200371742
Epoch #194: loss=0.2505061852435271
Epoch #195: loss=0.30945053497950237
Epoch #196: loss=0.22137677172819772
Epoch #197: loss=0.22814713716506957
Epoch #198: loss=0.2091058430572351
Epoch #199: loss=0.27805482496817907
Epoch #200: loss=0.1760092258453369
Epoch #201: loss=0.22272203639149665
Epoch #202: loss=0.30847328156232834
Epoch #203: loss=0.2656022643049558
Epoch #204: loss=0.27346843207875887
Epoch #205: loss=0.20410570576786996
Epoch #206: loss=0.1431201457977295
Epoch #207: loss=0.2261030778288841
Epoch #208: loss=0.16645419547955195
Epoch #209: loss=0.22798706541458766
Epoch #210: loss=0.17047455037633577
Epoch #211: loss=0.2114585186044375
Epoch #212: loss=0.24188050739467143
Epoch #213: loss=0.217426219334205
Epoch #214: loss=0.2622162749369939
Epoch #215: loss=0.2166145622730255
Epoch #216: loss=0.20575257961948712
Epoch #217: loss=0.3259706348180771
Epoch #218: loss=0.2537829910715421
Epoch #219: loss=0.27245260576407115
Epoch #220: loss=0.27442284896969793
Epoch #221: loss=0.3824245870113373
Epoch #222: loss=0.4621623009443283
Epoch #223: loss=0.27680509984493257
Epoch #224: loss=0.2485028624534607
Epoch #225: loss=0.24907686511675517
Epoch #226: loss=0.2327096740404765
Epoch #227: loss=0.22998810261487962
Epoch #228: loss=0.1671192112068335
Epoch #229: loss=0.19067644104361534
Epoch #230: loss=0.1883947417140007
Epoch #231: loss=0.18972968434294066
Epoch #232: loss=0.19453853219747544
Epoch #233: loss=0.19310025696953137
Epoch #234: loss=0.20523112018903097
Epoch #235: loss=0.20614387914538385
Epoch #236: loss=0.32409331351518633
Epoch #237: loss=0.27600449124972026
Epoch #238: loss=0.2249643603960673
Epoch #239: loss=0.17171392689148585
Epoch #240: loss=0.2883821611603101
Epoch #241: loss=0.24256101722518603
Epoch #242: loss=0.25591476807991664
Epoch #243: loss=0.21949543530742327
Epoch #244: loss=0.21086696808536848
Epoch #245: loss=0.2028569134573142
Epoch #246: loss=0.1783316229780515
Epoch #247: loss=0.127510038514932
Epoch #248: loss=0.16344860792160035
Epoch #249: loss=0.1891522283355395

Training time: 0:10:31.675299

Finished.
n2one setting etth2_exchange -> electricity
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth2_exchange_epochs_250_seed_2022/model.pkl', muti_dataset='etth2_exchange', random_seed=2022, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='electricity')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.02091e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.99005e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=4.87519e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.02091e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.36882276197127406, 'MAE': 0.4276513455286368}
Finished.
------------------------- record done -------------------------
n2one setting etth2_exchange -> weather
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth2_exchange_epochs_250_seed_2022/model.pkl', muti_dataset='etth2_exchange', random_seed=2022, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='weather')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=8.79335e-09): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.67781e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=4.07493e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.4638914561402454, 'MAE': 0.5029270330042769}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='ettm1_ettm2', random_seed=2022, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/ettm1_ettm2_epochs_250_seed_2022
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=4.982161836190657
Epoch #1: loss=2.724966623566367
Epoch #2: loss=2.2725928263230757
Epoch #3: loss=2.0529825308106164
Epoch #4: loss=1.840441573749889
Epoch #5: loss=1.6818485097451643
Epoch #6: loss=1.5104568112980237
Epoch #7: loss=1.4138154224915938
Epoch #8: loss=1.2506789294156162
Epoch #9: loss=1.1936886175112291
Epoch #10: loss=1.1691501438617706
Epoch #11: loss=1.1394414549524134
Epoch #12: loss=0.9882570396770131
Epoch #13: loss=0.9981535618955438
Epoch #14: loss=0.9633942625739358
Epoch #15: loss=0.8618428680029783
Epoch #16: loss=0.8898286006667397
Epoch #17: loss=0.8510628803209825
Epoch #18: loss=0.7274892926216125
Epoch #19: loss=0.7443512894890525
Epoch #20: loss=0.8286947120319713
Epoch #21: loss=0.8213263993913477
Epoch #22: loss=0.6447181647474115
Epoch #23: loss=0.7386108934879303
Epoch #24: loss=0.6387102576819333
Epoch #25: loss=0.6476072330366481
Epoch #26: loss=0.6261806203560396
Epoch #27: loss=0.56390649757602
Epoch #28: loss=0.5610586689277128
Epoch #29: loss=0.6378890492699363
Epoch #30: loss=0.5990618331865831
Epoch #31: loss=0.5364737740971826
Epoch #32: loss=0.5181962387128309
Epoch #33: loss=0.5167799713936719
Epoch #34: loss=0.6022724590518258
Epoch #35: loss=0.4879942969842391
Epoch #36: loss=0.5033832802013918
Epoch #37: loss=0.5373506938869302
Epoch #38: loss=0.45626264810562134
Epoch #39: loss=0.4501124241135337
Epoch #40: loss=0.4890262403271415
Epoch #41: loss=0.44508878886699677
Epoch #42: loss=0.44246280599724164
Epoch #43: loss=0.38931022719903424
Epoch #44: loss=0.3999424007805911
Epoch #45: loss=0.35505948147990485
Epoch #46: loss=0.38287846744060516
Epoch #47: loss=0.3843089274384759
Epoch #48: loss=0.38711418075995013
Epoch #49: loss=0.42286783727732574
Epoch #50: loss=0.4614927579056133
Epoch #51: loss=0.377148489383134
Epoch #52: loss=0.38887222043492575
Epoch #53: loss=0.30476780100302264
Epoch #54: loss=0.3680975139141083
Epoch #55: loss=0.3542519293048165
Epoch #56: loss=0.3709963797168298
Epoch #57: loss=0.3397193374958905
Epoch #58: loss=0.2651986310427839
Epoch #59: loss=0.28462244908918033
Epoch #60: loss=0.3035378056493672
Epoch #61: loss=0.271287511695515
Epoch #62: loss=0.22612269832329315
Epoch #63: loss=0.22692139920863239
Epoch #64: loss=0.22147487103939056
Epoch #65: loss=0.23688211292028427
Epoch #66: loss=0.20193185420198875
Epoch #67: loss=0.26198113201694057
Epoch #68: loss=0.31162702156738803
Epoch #69: loss=0.29950593547387555
Epoch #70: loss=0.34251136671413074
Epoch #71: loss=0.24368677423758942
Epoch #72: loss=0.32502107186750934
Epoch #73: loss=0.2116730155592615
Epoch #74: loss=0.24040316959673708
Epoch #75: loss=0.19450478120283646
Epoch #76: loss=0.1986014914106239
Epoch #77: loss=0.1786147379739718
Epoch #78: loss=0.1802653023465113
Epoch #79: loss=0.2231296653097326
Epoch #80: loss=0.17941044609655032
Epoch #81: loss=0.3056552254340865
Epoch #82: loss=0.25282288681377063
Epoch #83: loss=0.15796917202797803
Epoch #84: loss=0.16705033657225696
Epoch #85: loss=0.14451736279509284
Epoch #86: loss=0.20212302594022316
Epoch #87: loss=0.167312217368321
Epoch #88: loss=0.1530566323887218
Epoch #89: loss=0.17705239372497256
Epoch #90: loss=0.1869923330166123
Epoch #91: loss=0.16517447121441364
Epoch #92: loss=0.1295354374769059
Epoch #93: loss=0.18536040085283192
Epoch #94: loss=0.11772104674442248
Epoch #95: loss=0.25482796352695336
Epoch #96: loss=0.17459711567922073
Epoch #97: loss=0.14556907591494647
Epoch #98: loss=0.14199895665726878
Epoch #99: loss=0.3570922670716589
Epoch #100: loss=0.19729233262213794
Epoch #101: loss=0.14718546718358994
Epoch #102: loss=0.11706229180774906
Epoch #103: loss=0.1185131472620097
Epoch #104: loss=0.09562414291907441
Epoch #105: loss=0.10359308665448969
Epoch #106: loss=0.14250659900294108
Epoch #107: loss=0.12941053331914273
Epoch #108: loss=0.18001883256841789
Epoch #109: loss=0.10788462429561398
Epoch #110: loss=0.1735727902163159
Epoch #111: loss=0.09016786668110978
Epoch #112: loss=0.09965660820969126
Epoch #113: loss=0.1877265873957764
Epoch #114: loss=0.13442450677129356
Epoch #115: loss=0.10563533414493907
Epoch #116: loss=0.1037825419652191
Epoch #117: loss=0.05735601560974663
Epoch #118: loss=0.09809828727421435
Epoch #119: loss=0.0691929655996236
Epoch #120: loss=0.101600613868372
Epoch #121: loss=0.08498400178822604
Epoch #122: loss=0.07594308722764254
Epoch #123: loss=0.08246780703352256
Epoch #124: loss=0.07218974811786955
Epoch #125: loss=0.07325238751416857
Epoch #126: loss=0.05542380633679303
Epoch #127: loss=0.05803225367245349
Epoch #128: loss=0.09411856125701558
Epoch #129: loss=0.09415936630896547
Epoch #130: loss=0.06613301104781302
Epoch #131: loss=0.11660138305953958
Epoch #132: loss=0.07829416314647956
Epoch #133: loss=0.08010295566848734
Epoch #134: loss=0.0767092365263538
Epoch #135: loss=0.06803301586346193
Epoch #136: loss=0.07597594390707937
Epoch #137: loss=0.06778426967899907
Epoch #138: loss=0.1784483130005273
Epoch #139: loss=0.10801630145446821
Epoch #140: loss=0.07712217682803219
Epoch #141: loss=0.0627747903662649
Epoch #142: loss=0.11519005611030893
Epoch #143: loss=0.10033515874635089
Epoch #144: loss=0.07720306676558474
Epoch #145: loss=0.23911370658739048
Epoch #146: loss=0.09813736870207569
Epoch #147: loss=0.1014657923951745
Epoch #148: loss=0.07140042768283324
Epoch #149: loss=0.06134313023225828
Epoch #150: loss=0.052287291904742066
Epoch #151: loss=0.07180938323621046
Epoch #152: loss=0.0776865469630469
Epoch #153: loss=0.06408187209374526
Epoch #154: loss=0.12894139311869035
Epoch #155: loss=0.07215867335484787
Epoch #156: loss=0.048690384084528145
Epoch #157: loss=0.05163080918348648
Epoch #158: loss=0.06968801815740087
Epoch #159: loss=0.04577959993515502
Epoch #160: loss=0.07945312618870627
Epoch #161: loss=0.13959105033427477
Epoch #162: loss=0.23942437903447586
Epoch #163: loss=0.09344652760773897
Epoch #164: loss=0.06758766181089661
Epoch #165: loss=0.11074548587203026
Epoch #166: loss=0.06575440750880675
Epoch #167: loss=0.056657941207628355
Epoch #168: loss=0.05515954618088224
Epoch #169: loss=0.041620082475922325
Epoch #170: loss=0.050988040695136246
Epoch #171: loss=0.08848162393339655
Epoch #172: loss=0.09467631112784147
Epoch #173: loss=0.07071740163320844
Epoch #174: loss=0.07083764718845487
Epoch #175: loss=0.06273860941556367
Epoch #176: loss=0.08020357626744291
Epoch #177: loss=0.05875694781372493
Epoch #178: loss=0.05014840735715221
Epoch #179: loss=0.04566632761535319
Epoch #180: loss=0.04364701491695913
Epoch #181: loss=0.0616357997906479
Epoch #182: loss=0.039318793368610466
Epoch #183: loss=0.04837201010774483
Epoch #184: loss=0.04799490925771269
Epoch #185: loss=0.04281849380243908
Epoch #186: loss=0.05710579895160415
Epoch #187: loss=0.04038159985264594
Epoch #188: loss=0.040596514897928995
Epoch #189: loss=0.040293162455782294
Epoch #190: loss=0.06090901059691201
Epoch #191: loss=0.04159125550226732
Epoch #192: loss=0.029556183821775696
Epoch #193: loss=0.050676959990100426
Epoch #194: loss=0.06340807807547125
Epoch #195: loss=0.03768970525230874
Epoch #196: loss=0.10185124183242972
Epoch #197: loss=0.09071200882846658
Epoch #198: loss=0.058866765180772
Epoch #199: loss=0.04972273221408779
Epoch #200: loss=0.0450836387462914
Epoch #201: loss=0.08002315072173422
Epoch #202: loss=0.06744433842091398
Epoch #203: loss=0.07029094889929349
Epoch #204: loss=0.05726229124279185
Epoch #205: loss=0.056210477497767315
Epoch #206: loss=0.07200412939048627
Epoch #207: loss=0.07499725600196556
Epoch #208: loss=0.062112672847102986
Epoch #209: loss=0.041563811373304234
Epoch #210: loss=0.07136923354119062
Epoch #211: loss=0.09580660569058223
Epoch #212: loss=0.09082730072127147
Epoch #213: loss=0.12575519952753728
Epoch #214: loss=0.07980672100728209
Epoch #215: loss=0.06015235829082402
Epoch #216: loss=0.03745925329117612
Epoch #217: loss=0.03813021947545084
Epoch #218: loss=0.03384930056265809
Epoch #219: loss=0.12580604823729533
Epoch #220: loss=0.12909943627362902
Epoch #221: loss=0.08205117268318479
Epoch #222: loss=0.049996807324615394
Epoch #223: loss=0.052307504398578945
Epoch #224: loss=0.03568828171542422
Epoch #225: loss=0.0338373893702572
Epoch #226: loss=0.028744601856239817
Epoch #227: loss=0.03449021660807458
Epoch #228: loss=0.033831441923129285
Epoch #229: loss=0.04460375756025314
Epoch #230: loss=0.04227558396417986
Epoch #231: loss=0.024803039998832075
Epoch #232: loss=0.04031760317527435
Epoch #233: loss=0.03188828996975313
Epoch #234: loss=0.023171241945502432
Epoch #235: loss=0.041991846466606315
Epoch #236: loss=0.08804688716984609
Epoch #237: loss=0.04502355032177134
Epoch #238: loss=0.03743325798264281
Epoch #239: loss=0.022255110279233617
Epoch #240: loss=0.04132815761576322
Epoch #241: loss=0.050886160164902154
Epoch #242: loss=0.09574933451685039
Epoch #243: loss=0.04537462468513034
Epoch #244: loss=0.04689385344020345
Epoch #245: loss=0.039549560942263765
Epoch #246: loss=0.04836212746290998
Epoch #247: loss=0.043731309261850336
Epoch #248: loss=0.04440795515917919
Epoch #249: loss=0.027107784820889883

Training time: 0:21:34.338873

Finished.
n2one setting ettm1_ettm2 -> electricity
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/ettm1_ettm2_epochs_250_seed_2022/model.pkl', muti_dataset='ettm1_ettm2', random_seed=2022, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='electricity')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.34328e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.65598e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.34328e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.35847279104255475, 'MAE': 0.42469883837548417}
Finished.
------------------------- record done -------------------------
n2one setting ettm1_ettm2 -> weather
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/ettm1_ettm2_epochs_250_seed_2022/model.pkl', muti_dataset='ettm1_ettm2', random_seed=2022, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='weather')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.47063e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.80078e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.47063e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.3895470469138467, 'MAE': 0.45074812992933233}
Finished.
------------------------- record done -------------------------
n2one setting ettm1_ettm2 -> exchange
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/ettm1_ettm2_epochs_250_seed_2022/model.pkl', muti_dataset='ettm1_ettm2', random_seed=2022, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='exchange')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=4.31324e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.22665858959221918, 'MAE': 0.3209537507252997}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='ettm1_electricity', random_seed=2022, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/ettm1_electricity_epochs_250_seed_2022
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=1.6834479159260372
Epoch #1: loss=0.684882348291127
Epoch #2: loss=0.47705624104443806
Epoch #3: loss=0.3786668422441167
Epoch #4: loss=0.32283409640013455
Epoch #5: loss=0.2924509659993002
Epoch #6: loss=0.23682222535541977
Epoch #7: loss=0.20215897872505417
Epoch #8: loss=0.19356240192989269
Epoch #9: loss=0.18076603839465652
Epoch #10: loss=0.16385384040873452
Epoch #11: loss=0.13146920615515437
Epoch #12: loss=0.11897011630865464
Epoch #13: loss=0.12540404284260717
Epoch #14: loss=0.10670971090579967
Epoch #15: loss=0.11623806973178703
Epoch #16: loss=0.08870690672220774
Epoch #17: loss=0.09866290248885572
Epoch #18: loss=0.0938508915602712
Epoch #19: loss=0.0900335874890976
Epoch #20: loss=0.07039802384012973
Epoch #21: loss=0.07099164621231246
Epoch #22: loss=0.06365421344245593
Epoch #23: loss=0.06075410918803908
Epoch #24: loss=0.0649433635814811
Epoch #25: loss=0.051163140775534854
Epoch #26: loss=0.0652711806701025
Epoch #27: loss=0.0556210210246696
Epoch #28: loss=0.08030789932769061
Epoch #29: loss=0.05311569362087736
Epoch #30: loss=0.052322926609889124
Epoch #31: loss=0.046438730131071734
Epoch #32: loss=0.03664106056753389
Epoch #33: loss=0.04244103909719803
Epoch #34: loss=0.03556670875468078
Epoch #35: loss=0.040212987391951394
Epoch #36: loss=0.029815558567839528
Epoch #37: loss=0.03770138439792489
Epoch #38: loss=0.03920568150176431
Epoch #39: loss=0.037697150650383704
Epoch #40: loss=0.036173300337220306
Epoch #41: loss=0.027191535705779063
Epoch #42: loss=0.034411441381874845
Epoch #43: loss=0.028694228637340783
Epoch #44: loss=0.03294310293367963
Epoch #45: loss=0.03262003632584946
Epoch #46: loss=0.0303581806382933
Epoch #47: loss=0.022847684067309307
Epoch #48: loss=0.030513836796737713
Epoch #49: loss=0.03975545128853711
Epoch #50: loss=0.02755457958127989
Epoch #51: loss=0.026056427041178075
Epoch #52: loss=0.03833371400517546
Epoch #53: loss=0.025915691257715336
Epoch #54: loss=0.02111550961678612
Epoch #55: loss=0.03226392820522369
Epoch #56: loss=0.024594321600806685
Epoch #57: loss=0.02408135036625114
Epoch #58: loss=0.02718821753388697
Epoch #59: loss=0.020169772756151318
Epoch #60: loss=0.023545002888085836
Epoch #61: loss=0.027549588873965598
Epoch #62: loss=0.019895461065335626
Epoch #63: loss=0.017085164100919144
Epoch #64: loss=0.024917053916429854
Epoch #65: loss=0.03021968446190873
Epoch #66: loss=0.02167289695177923
Epoch #67: loss=0.01622382351325289
Epoch #68: loss=0.02005635214396556
Epoch #69: loss=0.018667130339371757
Epoch #70: loss=0.014139888215009179
Epoch #71: loss=0.01771068894086933
Epoch #72: loss=0.022415598094917787
Epoch #73: loss=0.023112748662593316
Epoch #74: loss=0.065472877193074
Epoch #75: loss=0.025600057191366935
Epoch #76: loss=0.019768063673793048
Epoch #77: loss=0.019726185570961896
Epoch #78: loss=0.01654064784588079
Epoch #79: loss=0.015677395118407468
Epoch #80: loss=0.016014925025912643
Epoch #81: loss=0.02812973593595909
Epoch #82: loss=0.020041975329127947
Epoch #83: loss=0.014349923961436802
Epoch #84: loss=0.03805868125258362
Epoch #85: loss=0.021436979543769743
Epoch #86: loss=0.016572805971162483
Epoch #87: loss=0.015321654197830057
Epoch #88: loss=0.01659740558341538
Epoch #89: loss=0.023016901770241963
Epoch #90: loss=0.032449774257558205
Epoch #91: loss=0.020938790315293696
Epoch #92: loss=0.016411380753438662
Epoch #93: loss=0.017559941266859735
Epoch #94: loss=0.014400560613960051
Epoch #95: loss=0.018762769052023853
Epoch #96: loss=0.01245954253778131
Epoch #97: loss=0.019618233281863764
Epoch #98: loss=0.017486978913461005
Epoch #99: loss=0.027098328630434328
Epoch #100: loss=0.014050993104275955
Epoch #101: loss=0.01768844181823219
Epoch #102: loss=0.016393967471976138
Epoch #103: loss=0.026330151157565863
Epoch #104: loss=0.020953843542949734
Epoch #105: loss=0.009710044965007011
Epoch #106: loss=0.013362327516597909
Epoch #107: loss=0.01598880450586306
Epoch #108: loss=0.022236706818564773
Epoch #109: loss=0.01647140028050551
Epoch #110: loss=0.012365221294170493
Epoch #111: loss=0.012948819666509011
Epoch #112: loss=0.010947671322662562
Epoch #113: loss=0.012413080557546136
Epoch #114: loss=0.01570571730220522
Epoch #115: loss=0.0157933024034193
Epoch #116: loss=0.026182341861681654
Epoch #117: loss=0.019081795370382963
Epoch #118: loss=0.01659870049505949
Epoch #119: loss=0.01302633887493116
Epoch #120: loss=0.015953663259524437
Epoch #121: loss=0.017130877634163957
Epoch #122: loss=0.011960117757202586
Epoch #123: loss=0.008612000252644768
Epoch #124: loss=0.013074770635547935
Epoch #125: loss=0.016530325101703932
Epoch #126: loss=0.01259104118947885
Epoch #127: loss=0.014123267976108501
Epoch #128: loss=0.014209808308542819
Epoch #129: loss=0.01743073237015647
Epoch #130: loss=0.017904772805239364
Epoch #131: loss=0.028912840423515034
Epoch #132: loss=0.01738346566103191
Epoch #133: loss=0.012793653517861919
Epoch #134: loss=0.01123584691564488
Epoch #135: loss=0.014932249232997602
Epoch #136: loss=0.01463600919660495
Epoch #137: loss=0.010503580832397368
Epoch #138: loss=0.01401684768845086
Epoch #139: loss=0.013671423315739607
Epoch #140: loss=0.015418948798162684
Epoch #141: loss=0.015273439998865402
Epoch #142: loss=0.024921296883748945
Epoch #143: loss=0.014206994473422227
Epoch #144: loss=0.012208352595724092
Epoch #145: loss=0.013097518544815234
Epoch #146: loss=0.013645895750820076
Epoch #147: loss=0.014329555097978607
Epoch #148: loss=0.014565659666234762
Epoch #149: loss=0.009555526023396817
Epoch #150: loss=0.011593272449471146
Epoch #151: loss=0.011018300053178956
Epoch #152: loss=0.015020479363984013
Epoch #153: loss=0.017447757290719016
Epoch #154: loss=0.01177474395072169
Epoch #155: loss=0.014483924183878116
Epoch #156: loss=0.015550349431402745
Epoch #157: loss=0.01089752166295643
Epoch #158: loss=0.011190153039495337
Epoch #159: loss=0.009250947163725714
Epoch #160: loss=0.007917906364379732
Epoch #161: loss=0.01019032851842664
Epoch #162: loss=0.014799672350040581
Epoch #163: loss=0.010029539081768738
Epoch #164: loss=0.010995541332496108
Epoch #165: loss=0.010109909887180848
Epoch #166: loss=0.013065436891162997
Epoch #167: loss=0.013978867291392904
Epoch #168: loss=0.010683598791575901
Epoch #169: loss=0.010475530320934878
Epoch #170: loss=0.01098654387151985
Epoch #171: loss=0.013459471671165988
Epoch #172: loss=0.010679879211676896
Epoch #173: loss=0.01949386086514085
Epoch #174: loss=0.017941392388272644
Epoch #175: loss=0.01134644714180599
Epoch #176: loss=0.015580493816544852
Epoch #177: loss=0.0058775983898267875
Epoch #178: loss=0.012324940399210676
Epoch #179: loss=0.00993339737708237
Epoch #180: loss=0.01697467520225692
Epoch #181: loss=0.011500231051889069
Epoch #182: loss=0.011699729712493437
Epoch #183: loss=0.022625735525395273
Epoch #184: loss=0.010900657746645606
Epoch #185: loss=0.011380670099527609
Epoch #186: loss=0.009848529352148692
Epoch #187: loss=0.0093360829645002
Epoch #188: loss=0.009506712369154437
Epoch #189: loss=0.009015498336766431
Epoch #190: loss=0.01044662149131427
Epoch #191: loss=0.01575522067523926
Epoch #192: loss=0.015573036717215879
Epoch #193: loss=0.01150550310277655
Epoch #194: loss=0.012700244424502079
Epoch #195: loss=0.01390023297277498
Epoch #196: loss=0.019458358018436043
Epoch #197: loss=0.02705568725264951
Epoch #198: loss=0.019861461495642597
Epoch #199: loss=0.013367331512090021
Epoch #200: loss=0.01103889181104023
Epoch #201: loss=0.012224953036042806
Epoch #202: loss=0.008944144612563919
Epoch #203: loss=0.0068489087549885395
Epoch #204: loss=0.01036907023107156
Epoch #205: loss=0.009105549617589827
Epoch #206: loss=0.008333592491404291
Epoch #207: loss=0.01215014789595005
Epoch #208: loss=0.013417839451695803
Epoch #209: loss=0.017516617806795254
Epoch #210: loss=0.01525634998144969
Epoch #211: loss=0.011975172488439649
Epoch #212: loss=0.011500130365346698
Epoch #213: loss=0.009425019097696087
Epoch #214: loss=0.010181078682138153
Epoch #215: loss=0.0104926320645644
Epoch #216: loss=0.008738039319066146
Epoch #217: loss=0.008930361021102506
Epoch #218: loss=0.008444677647034896
Epoch #219: loss=0.01247433814385286
Epoch #220: loss=0.012221613772639847
Epoch #221: loss=0.008920897527282963
Epoch #222: loss=0.03721616761401262
Epoch #223: loss=0.03051901394424186
Epoch #224: loss=0.0108811946335762
Epoch #225: loss=0.00902808586309779
Epoch #226: loss=0.008419753214819862
Epoch #227: loss=0.007758892813826496
Epoch #228: loss=0.009478550816760595
Epoch #229: loss=0.008839269377924473
Epoch #230: loss=0.006543317850376199
Epoch #231: loss=0.011578899059856327
Epoch #232: loss=0.010876158129543779
Epoch #233: loss=0.014277928861044628
Epoch #234: loss=0.011052219846467516
Epoch #235: loss=0.010653214063057087
Epoch #236: loss=0.010705981544900618
Epoch #237: loss=0.010053839797858172
Epoch #238: loss=0.019714856340850743
Epoch #239: loss=0.008177907251058651
Epoch #240: loss=0.01157103657730538
Epoch #241: loss=0.005802630235410172
Epoch #242: loss=0.006813388422009864
Epoch #243: loss=0.012152854050404437
Epoch #244: loss=0.007523935144468763
Epoch #245: loss=0.007298339649730541
Epoch #246: loss=0.008211752833992787
Epoch #247: loss=0.015331478107437289
Epoch #248: loss=0.01106919619440261
Epoch #249: loss=0.013736450157099522

Training time: 4:32:43.973124

Finished.
n2one setting ettm1_electricity -> weather
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/ettm1_electricity_epochs_250_seed_2022/model.pkl', muti_dataset='ettm1_electricity', random_seed=2022, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='weather')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.00601e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.06257e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=5.23075e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.00601e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.5798921139236592, 'MAE': 0.5723089538425801}
Finished.
------------------------- record done -------------------------
n2one setting ettm1_electricity -> exchange
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/ettm1_electricity_epochs_250_seed_2022/model.pkl', muti_dataset='ettm1_electricity', random_seed=2022, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='exchange')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=4.65907e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=4.65907e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.34879915878725665, 'MAE': 0.38686094480464006}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='ettm1_traffic', random_seed=2022, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/ettm1_traffic_epochs_250_seed_2022
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=0.9976644293753961
Epoch #1: loss=0.37693856547310317
Epoch #2: loss=0.25880011538937603
Epoch #3: loss=0.19577850368970584
Epoch #4: loss=0.14865377768959512
Epoch #5: loss=0.12908291350730097
Epoch #6: loss=0.10523370703744178
Epoch #7: loss=0.08831310550120039
Epoch #8: loss=0.09015837559990696
Epoch #9: loss=0.06713709674182455
Epoch #10: loss=0.07093816119558759
Epoch #11: loss=0.05890397420302375
Epoch #12: loss=0.06156611958260457
Epoch #13: loss=0.051848793737825014
Epoch #14: loss=0.04623540761034599
Epoch #15: loss=0.05081295899213115
Epoch #16: loss=0.03951556263182137
Epoch #17: loss=0.03920762185786103
Epoch #18: loss=0.03937477346846806
Epoch #19: loss=0.044389548685875344
Epoch #20: loss=0.034101289062507936
Epoch #21: loss=0.03169746716658266
Epoch #22: loss=0.033828373999337004
Epoch #23: loss=0.033378632934545335
Epoch #24: loss=0.030144683004037946
Epoch #25: loss=0.031223849188733766
Epoch #26: loss=0.0276368722912481
Epoch #27: loss=0.02984125526785426
Epoch #28: loss=0.028067418503271522
Epoch #29: loss=0.03282907126754904
Epoch #30: loss=0.023721732497364624
Epoch #31: loss=0.026998072816085662
Epoch #32: loss=0.024221192244272296
Epoch #33: loss=0.02776531719859745
Epoch #34: loss=0.03126437937098588
Epoch #35: loss=0.027028622542575154
Epoch #36: loss=0.029294459877603977
Epoch #37: loss=0.03290830201811807
Epoch #38: loss=0.022264437500815755
Epoch #39: loss=0.021850371080955312
Epoch #40: loss=0.032007213134730914
Epoch #41: loss=0.02155873921633413
Epoch #42: loss=0.027012344232929992
Epoch #43: loss=0.021002460106098353
Epoch #44: loss=0.03117820281965577
Epoch #45: loss=0.02631106005553775
Epoch #46: loss=0.020294554539041897
Epoch #47: loss=0.017462971247479274
Epoch #48: loss=0.023033860801035838
Epoch #49: loss=0.028612985267684733
Epoch #50: loss=0.017970677486336538
Epoch #51: loss=0.02426002582209505
Epoch #52: loss=0.014986507506107107
Epoch #53: loss=0.020915321003550178
Epoch #54: loss=0.01709671217831116
Epoch #55: loss=0.019632482962890337
Epoch #56: loss=0.018476807289746967
Epoch #57: loss=0.01820247131389809
Epoch #58: loss=0.018795681179886232
Epoch #59: loss=0.01868929857823396
Epoch #60: loss=0.022847401879992943
Epoch #61: loss=0.019368050460138796
Epoch #62: loss=0.01501894056010124
Epoch #63: loss=0.02184524064818781
Epoch #64: loss=0.013828967333270423
Epoch #65: loss=0.01970239185574379
Epoch #66: loss=0.01625596713672103
Epoch #67: loss=0.015943618780596743
Epoch #68: loss=0.01854612074263249
Epoch #69: loss=0.015615223708341458
Epoch #70: loss=0.028581000664879494
Epoch #71: loss=0.019844322385616335
Epoch #72: loss=0.013196782889827072
Epoch #73: loss=0.011603403009913268
Epoch #74: loss=0.016566359005611868
Epoch #75: loss=0.016211932669191024
Epoch #76: loss=0.019471264256766912
Epoch #77: loss=0.017034110874268316
Epoch #78: loss=0.015715614466536295
Epoch #79: loss=0.014846964631682232
Epoch #80: loss=0.015004709793973761
Epoch #81: loss=0.016179091566392165
Epoch #82: loss=0.020686544149363027
Epoch #83: loss=0.018332437188460577
Epoch #84: loss=0.013640589436164386
Epoch #85: loss=0.013791107197165732
Epoch #86: loss=0.016905810997740314
Epoch #87: loss=0.013680382861349335
Epoch #88: loss=0.01305788129747857
Epoch #89: loss=0.01883396967676898
Epoch #90: loss=0.009882224166523377
Epoch #91: loss=0.011722082080646767
Epoch #92: loss=0.01388246765766823
Epoch #93: loss=0.014983109798222911
Epoch #94: loss=0.01142745093257942
Epoch #95: loss=0.016138523199514435
Epoch #96: loss=0.014733940696540873
Epoch #97: loss=0.018973753082150976
Epoch #98: loss=0.013546751375642967
Epoch #99: loss=0.012546288716892966
Epoch #100: loss=0.025315473700875155
Epoch #101: loss=0.011351479949473104
Epoch #102: loss=0.013108019154163595
Epoch #103: loss=0.015660086646676064
Epoch #104: loss=0.014946164175513855
Epoch #105: loss=0.014449824647369068
Epoch #106: loss=0.01578570862145607
Epoch #107: loss=0.009581959369158444
Epoch #108: loss=0.01819995108336842
Epoch #109: loss=0.01163126124519954
Epoch #110: loss=0.009615003488226985
Epoch #111: loss=0.020020253632241917
Epoch #112: loss=0.015740042602891937
Epoch #113: loss=0.011381094140533511
Epoch #114: loss=0.015344707377876111
Epoch #115: loss=0.015920834551351456
Epoch #116: loss=0.012198722787994309
Epoch #117: loss=0.014160577218038407
Epoch #118: loss=0.013053192429060227
Epoch #119: loss=0.013152278941548733
Epoch #120: loss=0.01381849756544106
Epoch #121: loss=0.012084630269462958
Epoch #122: loss=0.01633198628903373
Epoch #123: loss=0.020060253599310563
Epoch #124: loss=0.011319394226825794
Epoch #125: loss=0.014443854036589472
Epoch #126: loss=0.010488413756459971
Epoch #127: loss=0.017066283887037518
Epoch #128: loss=0.013449610668731538
Epoch #129: loss=0.008178094740383479
Epoch #130: loss=0.011483679488903524
Epoch #131: loss=0.02225050049651796
Epoch #132: loss=0.011956288517923639
Epoch #133: loss=0.010836438236435137
Epoch #134: loss=0.012059436313148411
Epoch #135: loss=0.013974736577869613
Epoch #136: loss=0.013511583374426215
Epoch #137: loss=0.008561306858501549
Epoch #138: loss=0.011533325442126584
Epoch #139: loss=0.009171606030122868
Epoch #140: loss=0.015879762931588853
Epoch #141: loss=0.011379243883281362
Epoch #142: loss=0.013803770030898822
Epoch #143: loss=0.009918382392717095
Epoch #144: loss=0.009860232637974862
Epoch #145: loss=0.010323659933951195
Epoch #146: loss=0.012630603829532636
Epoch #147: loss=0.011223336369224133
Epoch #148: loss=0.013994353643040048
Epoch #149: loss=0.007579889674159594
Epoch #150: loss=0.012449318599550615
Epoch #151: loss=0.008987991362055369
Epoch #152: loss=0.009109875969414066
Epoch #153: loss=0.014062723474448919
Epoch #154: loss=0.014307083432399427
Epoch #155: loss=0.012517868333987152
Epoch #156: loss=0.013256079108617775
Epoch #157: loss=0.0075747330920004794
Epoch #158: loss=0.011676649466801844
Epoch #159: loss=0.00869935159515128
Epoch #160: loss=0.014734328898300128
Epoch #161: loss=0.014664873308474134
Epoch #162: loss=0.006856860668887971
Epoch #163: loss=0.013669860731795129
Epoch #164: loss=0.011949895603646538
Epoch #165: loss=0.010156488678687502
Epoch #166: loss=0.011824737092997998
Epoch #167: loss=0.011801037897014076
Epoch #168: loss=0.008526289153249107
Epoch #169: loss=0.009789906750072566
Epoch #170: loss=0.01048678001682946
Epoch #171: loss=0.014772276833327007
Epoch #172: loss=0.01199244742180655
Epoch #173: loss=0.008878335675370948
Epoch #174: loss=0.011843676280983984
Epoch #175: loss=0.00854837908076492
Epoch #176: loss=0.010342759152474032
Epoch #177: loss=0.007878922838841715
Epoch #178: loss=0.01242153553059058
Epoch #179: loss=0.010852946558586382
Epoch #180: loss=0.014095733907483924
Epoch #181: loss=0.008940842652382416
Epoch #182: loss=0.010439952558370836
Epoch #183: loss=0.013855114590643917
Epoch #184: loss=0.00840943302583121
Epoch #185: loss=0.012201623592012587
Epoch #186: loss=0.00889945839460577
Epoch #187: loss=0.015139106904246651
Epoch #188: loss=0.013732247205613418
Epoch #189: loss=0.010419728752745173
Epoch #190: loss=0.0075383392105163455
Epoch #191: loss=0.010203423907559335
Epoch #192: loss=0.01183623594770241
Epoch #193: loss=0.011024939028782512
Epoch #194: loss=0.008172888822947845
Epoch #195: loss=0.010941471861577236
Epoch #196: loss=0.014268762752347607
Epoch #197: loss=0.008862928831455764
Epoch #198: loss=0.009827603176364169
Epoch #199: loss=0.009225068278942745
Epoch #200: loss=0.011383293495381615
Epoch #201: loss=0.008785437749142219
Epoch #202: loss=0.009829460644639866
Epoch #203: loss=0.014137630643226252
Epoch #204: loss=0.008331898393402546
Epoch #205: loss=0.01897542837064154
Epoch #206: loss=0.006366208813500986
Epoch #207: loss=0.010786566751541566
Epoch #208: loss=0.01023749795338013
Epoch #209: loss=0.009769963737919779
Epoch #210: loss=0.014597640501129636
Epoch #211: loss=0.013093342460512705
Epoch #212: loss=0.007390980017716854
Epoch #213: loss=0.006868059187314078
Epoch #214: loss=0.010696556285949944
Epoch #215: loss=0.012284364289637998
Epoch #216: loss=0.007101261591996714
Epoch #217: loss=0.007186174993514817
Epoch #218: loss=0.011050244519809866
Epoch #219: loss=0.007564306140856371
Epoch #220: loss=0.007328606420720412
Epoch #221: loss=0.011004963664421738
Epoch #222: loss=0.009782149680820933
Epoch #223: loss=0.01503681676991829
Epoch #224: loss=0.008273031043315083
Epoch #225: loss=0.009758795390864631
Epoch #226: loss=0.008460132763842398
Epoch #227: loss=0.009263242134595498
Epoch #228: loss=0.009833716549066495
Epoch #229: loss=0.010109957765237368
Epoch #230: loss=0.008977108800413438
Epoch #231: loss=0.007228872190739795
Epoch #232: loss=0.010531429813511751
Epoch #233: loss=0.009838623382955367
Epoch #234: loss=0.008358319639278291
Epoch #235: loss=0.01596736574709291
Epoch #236: loss=0.00976258343293874
Epoch #237: loss=0.009566719929042527
Epoch #238: loss=0.010278002749857806
Epoch #239: loss=0.009782422741988634
Epoch #240: loss=0.010588759080188137
Epoch #241: loss=0.006673967524932778
Epoch #242: loss=0.008779424847654928
Epoch #243: loss=0.009257317596988087
Epoch #244: loss=0.008672789117687554
Epoch #245: loss=0.010019320897578825
Epoch #246: loss=0.010584471917903708
Epoch #247: loss=0.008232849810639436
Epoch #248: loss=0.009908000936002333
Epoch #249: loss=0.010792202199942605

Training time: 10:18:47.596428

Finished.
n2one setting ettm1_traffic -> electricity
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/ettm1_traffic_epochs_250_seed_2022/model.pkl', muti_dataset='ettm1_traffic', random_seed=2022, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='electricity')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=7.49996e-09): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.86302e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=4.56301e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=7.49996e-09): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.39814677386111685, 'MAE': 0.44961869057566756}
Finished.
------------------------- record done -------------------------
n2one setting ettm1_traffic -> weather
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/ettm1_traffic_epochs_250_seed_2022/model.pkl', muti_dataset='ettm1_traffic', random_seed=2022, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='weather')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.31644e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.61422e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.5564181289868828, 'MAE': 0.5703243632013314}
Finished.
------------------------- record done -------------------------
n2one setting ettm1_traffic -> exchange
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/ettm1_traffic_epochs_250_seed_2022/model.pkl', muti_dataset='ettm1_traffic', random_seed=2022, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='exchange')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=5.56958e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.37463119901699776, 'MAE': 0.39168400981793233}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='ettm1_weather', random_seed=2022, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/ettm1_weather_epochs_250_seed_2022
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=5.000543046641994
Epoch #1: loss=2.5393280209721745
Epoch #2: loss=2.604514830821269
Epoch #3: loss=2.1624835761817724
Epoch #4: loss=1.8742080798020233
Epoch #5: loss=1.6975914465414512
Epoch #6: loss=1.5115209366824176
Epoch #7: loss=1.3709698432200663
Epoch #8: loss=1.4278128372656334
Epoch #9: loss=1.274041754168433
Epoch #10: loss=1.2215549897503208
Epoch #11: loss=1.2668682079057436
Epoch #12: loss=1.15630194625339
Epoch #13: loss=1.049646164919879
Epoch #14: loss=1.0739993291932184
Epoch #15: loss=0.9991156507182766
Epoch #16: loss=0.8915852888210399
Epoch #17: loss=0.9780369578181086
Epoch #18: loss=0.8394382676562747
Epoch #19: loss=0.7836479931264311
Epoch #20: loss=0.8115037131953884
Epoch #21: loss=0.7346957744778814
Epoch #22: loss=0.7286964284407126
Epoch #23: loss=0.6968652847650889
Epoch #24: loss=0.6539808141218649
Epoch #25: loss=0.7880313799187944
Epoch #26: loss=0.6704664480041813
Epoch #27: loss=0.5974996122154029
Epoch #28: loss=0.6823153737429026
Epoch #29: loss=0.5811879699294632
Epoch #30: loss=0.6281544123146985
Epoch #31: loss=0.6728090651937433
Epoch #32: loss=0.5516306902911212
Epoch #33: loss=0.5451319757345561
Epoch #34: loss=0.5248157841128271
Epoch #35: loss=0.682512234997105
Epoch #36: loss=0.5773252804537077
Epoch #37: loss=0.543841643107904
Epoch #38: loss=0.5603267018859451
Epoch #39: loss=0.4593363339836533
Epoch #40: loss=0.48284316948942235
Epoch #41: loss=0.4402353103096421
Epoch #42: loss=0.4918735486430091
Epoch #43: loss=0.4504911674035562
Epoch #44: loss=0.40957013336387843
Epoch #45: loss=0.3679492594422521
Epoch #46: loss=0.37770743748626195
Epoch #47: loss=0.4095053330466554
Epoch #48: loss=0.42980008350836263
Epoch #49: loss=0.40892682043281764
Epoch #50: loss=0.3739120134630719
Epoch #51: loss=0.40244767512824087
Epoch #52: loss=0.36264045818431956
Epoch #53: loss=0.316812132258673
Epoch #54: loss=0.3746433781610953
Epoch #55: loss=0.46403849567915945
Epoch #56: loss=0.44537018642232223
Epoch #57: loss=0.3374951252260724
Epoch #58: loss=0.31055697355721446
Epoch #59: loss=0.3116814985468581
Epoch #60: loss=0.33029333038910014
Epoch #61: loss=0.2937785491750047
Epoch #62: loss=0.35734944810738434
Epoch #63: loss=0.33454765540522496
Epoch #64: loss=0.3655864765515199
Epoch #65: loss=0.30360456656765294
Epoch #66: loss=0.251274278035035
Epoch #67: loss=0.279282430337893
Epoch #68: loss=0.2552127908613231
Epoch #69: loss=0.26528485119342804
Epoch #70: loss=0.2636614000877818
Epoch #71: loss=0.23229370185652295
Epoch #72: loss=0.2694843878617158
Epoch #73: loss=0.2802913809144819
Epoch #74: loss=0.2559005418742025
Epoch #75: loss=0.2693563176167978
Epoch #76: loss=0.23899672663695104
Epoch #77: loss=0.20639965501991478
Epoch #78: loss=0.20745652269672704
Epoch #79: loss=0.26907142473233714
Epoch #80: loss=0.19267907235268
Epoch #81: loss=0.1906886306163427
Epoch #82: loss=0.20585093808335228
Epoch #83: loss=0.18602060083601926
Epoch #84: loss=0.1871885798267416
Epoch #85: loss=0.20733219908701406
Epoch #86: loss=0.19578380963286837
Epoch #87: loss=0.19033820866732984
Epoch #88: loss=0.20413384848349803
Epoch #89: loss=0.1566896458735337
Epoch #90: loss=0.17111537998189796
Epoch #91: loss=0.19923293912732923
Epoch #92: loss=0.21987702294781403
Epoch #93: loss=0.17694934246105118
Epoch #94: loss=0.20442987233400345
Epoch #95: loss=0.12004489199938001
Epoch #96: loss=0.1816127295228275
Epoch #97: loss=0.16585227112109596
Epoch #98: loss=0.11966768803225981
Epoch #99: loss=0.14498453486610102
Epoch #100: loss=0.12622118610385302
Epoch #101: loss=0.1964625606464373
Epoch #102: loss=0.18711543727565455
Epoch #103: loss=0.18768317393354467
Epoch #104: loss=0.18835465599958962
Epoch #105: loss=0.1852892774182397
Epoch #106: loss=0.19497476699384483
Epoch #107: loss=0.15791228353171735
Epoch #108: loss=0.15226514264941216
Epoch #109: loss=0.15435087811705228
Epoch #110: loss=0.1554355133022811
Epoch #111: loss=0.1437858289762123
Epoch #112: loss=0.10310511844786438
Epoch #113: loss=0.15084073365338752
Epoch #114: loss=0.13335808054418177
Epoch #115: loss=0.11470258689007244
Epoch #116: loss=0.20573795033065048
Epoch #117: loss=0.14993082601073626
Epoch #118: loss=0.13754310821359222
Epoch #119: loss=0.2079317416693713
Epoch #120: loss=0.10433688769872124
Epoch #121: loss=0.1772842713304468
Epoch #122: loss=0.14528889047938423
Epoch #123: loss=0.16109743734469284
Epoch #124: loss=0.21064197322404063
Epoch #125: loss=0.31746361777186394
Epoch #126: loss=0.2398658570002865
Epoch #127: loss=0.12447151179249222
Epoch #128: loss=0.10846910061868462
Epoch #129: loss=0.11541857469726254
Epoch #130: loss=0.08886498872291397
Epoch #131: loss=0.09026457918052738
Epoch #132: loss=0.10196345596498735
Epoch #133: loss=0.13476298819925334
Epoch #134: loss=0.12006204177600306
Epoch #135: loss=0.16079315976113887
Epoch #136: loss=0.2850886656827218
Epoch #137: loss=0.174821947776788
Epoch #138: loss=0.13357680587953813
Epoch #139: loss=0.09210268428196779
Epoch #140: loss=0.10295629763119929
Epoch #141: loss=0.0804356425195127
Epoch #142: loss=0.10022942283870401
Epoch #143: loss=0.10988405034751506
Epoch #144: loss=0.09549154446938553
Epoch #145: loss=0.08500100903817125
Epoch #146: loss=0.10816870745573495
Epoch #147: loss=0.15385208012083093
Epoch #148: loss=0.07795525317055148
Epoch #149: loss=0.10242803460238753
Epoch #150: loss=0.10624772078684859
Epoch #151: loss=0.08620275129136201
Epoch #152: loss=0.06736876432960098
Epoch #153: loss=0.06813096723242386
Epoch #154: loss=0.06684775610227843
Epoch #155: loss=0.07297874398126795
Epoch #156: loss=0.07611804244083327
Epoch #157: loss=0.11150574034734352
Epoch #158: loss=0.07627106946263765
Epoch #159: loss=0.11701335083391215
Epoch #160: loss=0.10984551010502351
Epoch #161: loss=0.082340432330966
Epoch #162: loss=0.06118136463133064
Epoch #163: loss=0.06231305013234551
Epoch #164: loss=0.06034534575568663
Epoch #165: loss=0.06854966295429983
Epoch #166: loss=0.10069980880094541
Epoch #167: loss=0.066276777737044
Epoch #168: loss=0.0632432424840895
Epoch #169: loss=0.08883886857310662
Epoch #170: loss=0.07627991415761612
Epoch #171: loss=0.08465999573771213
Epoch #172: loss=0.07947005623498478
Epoch #173: loss=0.07508309469935862
Epoch #174: loss=0.09430796607724719
Epoch #175: loss=0.0770857735864214
Epoch #176: loss=0.056031559166070576
Epoch #177: loss=0.05079509841429221
Epoch #178: loss=0.1006920916607251
Epoch #179: loss=0.06649452075362206
Epoch #180: loss=0.05539380597907144
Epoch #181: loss=0.05309558033037025
Epoch #182: loss=0.058994696475565434
Epoch #183: loss=0.04648112472952218
Epoch #184: loss=0.057320476006213074
Epoch #185: loss=0.053940013755817674
Epoch #186: loss=0.04898847644594875
Epoch #187: loss=0.07110814405353488
Epoch #188: loss=0.11719656599735892
Epoch #189: loss=0.09339406932830005
Epoch #190: loss=0.05909140465932118
Epoch #191: loss=0.07840061806947798
Epoch #192: loss=0.08307511920763834
Epoch #193: loss=0.05662648928527896
Epoch #194: loss=0.037042254154142494
Epoch #195: loss=0.06406932412269148
Epoch #196: loss=0.055771237138558076
Epoch #197: loss=0.04297088011092431
Epoch #198: loss=0.039278431896220996
Epoch #199: loss=0.04225308410319928
Epoch #200: loss=0.05003006436635513
Epoch #201: loss=0.06443411879543517
Epoch #202: loss=0.08472310736574032
Epoch #203: loss=0.06639134712718628
Epoch #204: loss=0.06310126177865912
Epoch #205: loss=0.04510951845126378
Epoch #206: loss=0.04674273296385198
Epoch #207: loss=0.04957552620436292
Epoch #208: loss=0.06045392092720077
Epoch #209: loss=0.0668536936612548
Epoch #210: loss=0.05040948100488733
Epoch #211: loss=0.050222296519456686
Epoch #212: loss=0.0392012922344981
Epoch #213: loss=0.048960532540002384
Epoch #214: loss=0.0448638005182147
Epoch #215: loss=0.042183996505431226
Epoch #216: loss=0.040897850576486136
Epoch #217: loss=0.054940161370747795
Epoch #218: loss=0.04240621847880853
Epoch #219: loss=0.05758717291157793
Epoch #220: loss=0.07098655893492538
Epoch #221: loss=0.03642809564700803
Epoch #222: loss=0.0494325413444155
Epoch #223: loss=0.04830029226738859
Epoch #224: loss=0.03824191862666929
Epoch #225: loss=0.036963844445307516
Epoch #226: loss=0.035387428653602664
Epoch #227: loss=0.042489760202934615
Epoch #228: loss=0.05958210613981292
Epoch #229: loss=0.039070459057551785
Epoch #230: loss=0.03527031686610064
Epoch #231: loss=0.13129369768540602
Epoch #232: loss=0.07590111470907121
Epoch #233: loss=0.14040831521757552
Epoch #234: loss=0.06791444545662081
Epoch #235: loss=0.17742997035384178
Epoch #236: loss=0.06394433204990786
Epoch #237: loss=0.040145926454381364
Epoch #238: loss=0.04120261073615905
Epoch #239: loss=0.04844067757949233
Epoch #240: loss=0.04987480428472564
Epoch #241: loss=0.03897936737819298
Epoch #242: loss=0.045310155438208904
Epoch #243: loss=0.0328276297203391
Epoch #244: loss=0.05779729894286877
Epoch #245: loss=0.06567974094100096
Epoch #246: loss=0.03379549420866612
Epoch #247: loss=0.04660158509992667
Epoch #248: loss=0.037462150514427875
Epoch #249: loss=0.028868929407483822

Training time: 0:38:10.914264

Finished.
n2one setting ettm1_weather -> electricity
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/ettm1_weather_epochs_250_seed_2022/model.pkl', muti_dataset='ettm1_weather', random_seed=2022, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='electricity')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.2268e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.4657e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.2268e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.3588297366310251, 'MAE': 0.42265026449443355}
Finished.
------------------------- record done -------------------------
n2one setting ettm1_weather -> exchange
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/ettm1_weather_epochs_250_seed_2022/model.pkl', muti_dataset='ettm1_weather', random_seed=2022, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='exchange')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=3.27708e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.24034743784335266, 'MAE': 0.33665289728487496}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='ettm1_exchange', random_seed=2022, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/ettm1_exchange_epochs_250_seed_2022
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=4.824226868780036
Epoch #1: loss=2.649638351641203
Epoch #2: loss=2.253688881271764
Epoch #3: loss=2.0709401306353117
Epoch #4: loss=2.0197148009350427
Epoch #5: loss=1.8559768576371043
Epoch #6: loss=1.7284980698635704
Epoch #7: loss=1.6323555519706325
Epoch #8: loss=1.574778581920423
Epoch #9: loss=1.51135810425407
Epoch #10: loss=1.4303276978040997
Epoch #11: loss=1.3297825110586066
Epoch #12: loss=1.3634003087093955
Epoch #13: loss=1.2314919358805607
Epoch #14: loss=1.2447952000718367
Epoch #15: loss=1.278618640021274
Epoch #16: loss=1.1730402645311857
Epoch #17: loss=1.1834897994995117
Epoch #18: loss=1.1052282076132924
Epoch #19: loss=1.0804751829097146
Epoch #20: loss=1.0235655621478432
Epoch #21: loss=1.050794620262949
Epoch #22: loss=1.0272147780970524
Epoch #23: loss=1.058476171995464
Epoch #24: loss=1.043645441532135
Epoch #25: loss=1.0325090226374174
Epoch #26: loss=0.9476144658891779
Epoch #27: loss=0.8893044810546072
Epoch #28: loss=0.9160811493271276
Epoch #29: loss=0.951014590890784
Epoch #30: loss=0.9528898879101402
Epoch #31: loss=0.8954572207049319
Epoch #32: loss=0.8308886289596558
Epoch #33: loss=0.84050095395038
Epoch #34: loss=0.7837160668875042
Epoch #35: loss=0.859898146830107
Epoch #36: loss=0.7680188135096901
Epoch #37: loss=0.9149024423800016
Epoch #38: loss=0.7449267930106113
Epoch #39: loss=0.777041821103347
Epoch #40: loss=0.7664825194760373
Epoch #41: loss=0.8489110846268503
Epoch #42: loss=0.7783312734804655
Epoch #43: loss=0.7173784437932467
Epoch #44: loss=0.7431933911223161
Epoch #45: loss=0.6959400867160997
Epoch #46: loss=0.6943069837595287
Epoch #47: loss=0.6431610741113362
Epoch #48: loss=0.6727449297904968
Epoch #49: loss=0.6519674298010374
Epoch #50: loss=0.7704540271508066
Epoch #51: loss=0.6078659437204662
Epoch #52: loss=0.7046300376716413
Epoch #53: loss=0.8179574969567751
Epoch #54: loss=0.7394293923127023
Epoch #55: loss=0.6468320394817152
Epoch #56: loss=0.5735415690823605
Epoch #57: loss=0.5898663087895042
Epoch #58: loss=0.5413309116112558
Epoch #59: loss=0.6490806121575204
Epoch #60: loss=0.564942290908412
Epoch #61: loss=0.5775323996418401
Epoch #62: loss=0.5209856315662986
Epoch #63: loss=0.5435732054082971
Epoch #64: loss=0.7339860028342197
Epoch #65: loss=0.5451392719620153
Epoch #66: loss=0.5311100224131032
Epoch #67: loss=0.44933825103860153
Epoch #68: loss=0.4578209213520351
Epoch #69: loss=0.49220405362154307
Epoch #70: loss=0.5016679348129975
Epoch #71: loss=0.5539240884153467
Epoch #72: loss=0.537555771438699
Epoch #73: loss=0.5043785807333494
Epoch #74: loss=0.4947660628118013
Epoch #75: loss=0.5141405478904122
Epoch #76: loss=0.46900183903543574
Epoch #77: loss=0.4532525790365119
Epoch #78: loss=0.45005669484013006
Epoch #79: loss=0.4690956732160167
Epoch #80: loss=0.4584077973114817
Epoch #81: loss=0.5102631230103342
Epoch #82: loss=0.46047286140291316
Epoch #83: loss=0.4500108024007396
Epoch #84: loss=0.4500287237920259
Epoch #85: loss=0.4042641026409049
Epoch #86: loss=0.4097426431743722
Epoch #87: loss=0.41300374896902786
Epoch #88: loss=0.4425930741586183
Epoch #89: loss=0.4203438986288874
Epoch #90: loss=0.4434331890783812
Epoch #91: loss=0.44387916593175186
Epoch #92: loss=0.39064753133999675
Epoch #93: loss=0.41651332848950434
Epoch #94: loss=0.3447600140383369
Epoch #95: loss=0.34280727411571305
Epoch #96: loss=0.4241638528673272
Epoch #97: loss=0.36848891173538406
Epoch #98: loss=0.34284904128626775
Epoch #99: loss=0.45231818446987554
Epoch #100: loss=0.30845715027106435
Epoch #101: loss=0.33037366051422923
Epoch #102: loss=0.3261563683810987
Epoch #103: loss=0.35439086901514155
Epoch #104: loss=0.3462343772775249
Epoch #105: loss=0.39173840849023117
Epoch #106: loss=0.3096019366854115
Epoch #107: loss=0.30023357468216044
Epoch #108: loss=0.32628340078027623
Epoch #109: loss=0.34001063673119797
Epoch #110: loss=0.33028412806360347
Epoch #111: loss=0.3673648053878232
Epoch #112: loss=0.3123888412588521
Epoch #113: loss=0.39243181128250926
Epoch #114: loss=0.40531684222974274
Epoch #115: loss=0.3333334389485811
Epoch #116: loss=0.32685333333517375
Epoch #117: loss=0.3205182273921214
Epoch #118: loss=0.37536343540015976
Epoch #119: loss=0.4154522301335084
Epoch #120: loss=0.3023779682422939
Epoch #121: loss=0.27704303084235443
Epoch #122: loss=0.2817862873014651
Epoch #123: loss=0.27807001063698217
Epoch #124: loss=0.26307689751449387
Epoch #125: loss=0.24800924447021985
Epoch #126: loss=0.25363139572896454
Epoch #127: loss=0.292087028685369
Epoch #128: loss=0.23530526498430654
Epoch #129: loss=0.16951476900201096
Epoch #130: loss=0.24320887695801885
Epoch #131: loss=0.2610423137482844
Epoch #132: loss=0.2178678061617048
Epoch #133: loss=0.2760591271676515
Epoch #134: loss=0.2750488503983146
Epoch #135: loss=0.31489210732673345
Epoch #136: loss=0.31790045962521907
Epoch #137: loss=0.22881773663194557
Epoch #138: loss=0.2547267926366706
Epoch #139: loss=0.23025299961629667
Epoch #140: loss=0.295438256702925
Epoch #141: loss=0.22468903150997663
Epoch #142: loss=0.2600195678441148
Epoch #143: loss=0.22791313615284467
Epoch #144: loss=0.3115767734615426
Epoch #145: loss=0.2867617513004102
Epoch #146: loss=0.4170883450853197
Epoch #147: loss=0.2856454810029582
Epoch #148: loss=0.2453984375062742
Epoch #149: loss=0.23623814002463692
Epoch #150: loss=0.37908408594758886
Epoch #151: loss=0.3127452307625821
Epoch #152: loss=0.19267783196348892
Epoch #153: loss=0.2717434442357013
Epoch #154: loss=0.1833012633417782
Epoch #155: loss=0.258933713953746
Epoch #156: loss=0.2070104577823689
Epoch #157: loss=0.19453653536344828
Epoch #158: loss=0.2502049713542587
Epoch #159: loss=0.1561206014532792
Epoch #160: loss=0.17009632995254115
Epoch #161: loss=0.18330553056378113
Epoch #162: loss=0.1506838363252188
Epoch #163: loss=0.22505331823700353
Epoch #164: loss=0.39211033323877736
Epoch #165: loss=0.27995718427394567
Epoch #166: loss=0.26303955481240626
Epoch #167: loss=0.22288676429735987
Epoch #168: loss=0.2751451185659358
Epoch #169: loss=0.2495316270934908
Epoch #170: loss=0.17009163530249344
Epoch #171: loss=0.13830177015379855
Epoch #172: loss=0.12241104833389584
Epoch #173: loss=0.16633796574253784
Epoch #174: loss=0.2271456375325981
Epoch #175: loss=0.15128713454070844
Epoch #176: loss=0.2042662615054532
Epoch #177: loss=0.1431526543670579
Epoch #178: loss=0.16793635448342875
Epoch #179: loss=0.18851842652810247
Epoch #180: loss=0.2595940675390394
Epoch #181: loss=0.23709368764569885
Epoch #182: loss=0.1880420842453053
Epoch #183: loss=0.21838611050655968
Epoch #184: loss=0.20735806540439003
Epoch #185: loss=0.18156784733659342
Epoch #186: loss=0.14929360151290894
Epoch #187: loss=0.11648691425982274
Epoch #188: loss=0.23149296151180015
Epoch #189: loss=0.1947634235808724
Epoch #190: loss=0.19778236903642354
Epoch #191: loss=0.2045881920739224
Epoch #192: loss=0.15797367221430728
Epoch #193: loss=0.22748016409183802
Epoch #194: loss=0.2590601189356101
Epoch #195: loss=0.2143807850385967
Epoch #196: loss=0.18477144876593038
Epoch #197: loss=0.1821192036333837
Epoch #198: loss=0.22027597772447685
Epoch #199: loss=0.1642746199902735
Epoch #200: loss=0.2599875875993779
Epoch #201: loss=0.19523025108011147
Epoch #202: loss=0.19532246064198644
Epoch #203: loss=0.2861800336916196
Epoch #204: loss=0.2515373998566678
Epoch #205: loss=0.1612112010387998
Epoch #206: loss=0.15678509285575465
Epoch #207: loss=0.14006734502158666
Epoch #208: loss=0.11927885483754308
Epoch #209: loss=0.156122206661262
Epoch #210: loss=0.15014715335871043
Epoch #211: loss=0.15970979238811292
Epoch #212: loss=0.13712941266988454
Epoch #213: loss=0.21531559428886363
Epoch #214: loss=0.1331872398916044
Epoch #215: loss=0.14406466444856242
Epoch #216: loss=0.15312088653445244
Epoch #217: loss=0.13313592342953934
Epoch #218: loss=0.11538180061861088
Epoch #219: loss=0.14255410079893313
Epoch #220: loss=0.1944784680871587
Epoch #221: loss=0.14681845334799667
Epoch #222: loss=0.12121373003250674
Epoch #223: loss=0.10478125257711661
Epoch #224: loss=0.09271601548320368
Epoch #225: loss=0.2347227495751883
Epoch #226: loss=0.2403189579122945
Epoch #227: loss=0.13933447081791728
Epoch #228: loss=0.15748003164404317
Epoch #229: loss=0.20318104503186127
Epoch #230: loss=0.13250597213443957
Epoch #231: loss=0.12372116293562085
Epoch #232: loss=0.12692180138669515
Epoch #233: loss=0.16079251370147654
Epoch #234: loss=0.12148054453887437
Epoch #235: loss=0.09317350485607197
Epoch #236: loss=0.1165163422885694
Epoch #237: loss=0.10637088570939868
Epoch #238: loss=0.0819012001156807
Epoch #239: loss=0.07578925219805617
Epoch #240: loss=0.12531750708034164
Epoch #241: loss=0.13175615119306663
Epoch #242: loss=0.11523198748105451
Epoch #243: loss=0.19152452933945155
Epoch #244: loss=0.14112703423751027
Epoch #245: loss=0.13945584822642176
Epoch #246: loss=0.11224392134892314
Epoch #247: loss=0.10299224269233252
Epoch #248: loss=0.10659846428193544
Epoch #249: loss=0.13975678884277218

Training time: 0:16:48.313078

Finished.
n2one setting ettm1_exchange -> electricity
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/ettm1_exchange_epochs_250_seed_2022/model.pkl', muti_dataset='ettm1_exchange', random_seed=2022, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='electricity')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.2094e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.42765e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=5.76299e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.2094e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.35777421990793673, 'MAE': 0.42596773695168666}
Finished.
------------------------- record done -------------------------
n2one setting ettm1_exchange -> weather
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/ettm1_exchange_epochs_250_seed_2022/model.pkl', muti_dataset='ettm1_exchange', random_seed=2022, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='weather')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.15521e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.21793e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=5.44818e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.5095811536863899, 'MAE': 0.5277162708377567}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='ettm2_electricity', random_seed=2022, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/ettm2_electricity_epochs_250_seed_2022
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=1.760812242526606
Epoch #1: loss=0.7201609165374055
Epoch #2: loss=0.4792319512869938
Epoch #3: loss=0.3985880437385605
Epoch #4: loss=0.33036126398358
Epoch #5: loss=0.2987845590315669
Epoch #6: loss=0.2520350478589535
Epoch #7: loss=0.204537188970899
Epoch #8: loss=0.19868539166019625
Epoch #9: loss=0.183129456432828
Epoch #10: loss=0.15797270180276
Epoch #11: loss=0.1375098279284605
Epoch #12: loss=0.11206728197544454
Epoch #13: loss=0.1197143690885011
Epoch #14: loss=0.10825308506006756
Epoch #15: loss=0.10831421923947084
Epoch #16: loss=0.09035446291722088
Epoch #17: loss=0.1000384989374104
Epoch #18: loss=0.09681340387786727
Epoch #19: loss=0.09478655200541379
Epoch #20: loss=0.07346045089807048
Epoch #21: loss=0.07232194825030684
Epoch #22: loss=0.06212140150727278
Epoch #23: loss=0.05673114459888718
Epoch #24: loss=0.060837281573316956
Epoch #25: loss=0.05489140157363411
Epoch #26: loss=0.06700790710522826
Epoch #27: loss=0.07012564199275326
Epoch #28: loss=0.07039334958121854
Epoch #29: loss=0.04734596316085522
Epoch #30: loss=0.04760650918027947
Epoch #31: loss=0.05237984015179955
Epoch #32: loss=0.03774740962109652
Epoch #33: loss=0.04306216829060858
Epoch #34: loss=0.03539537564228592
Epoch #35: loss=0.043426438104305194
Epoch #36: loss=0.03733089953066265
Epoch #37: loss=0.0381466534580609
Epoch #38: loss=0.04257071601557938
Epoch #39: loss=0.03746937874056601
Epoch #40: loss=0.03675783827833562
Epoch #41: loss=0.027310986693524924
Epoch #42: loss=0.03187157372742083
Epoch #43: loss=0.029021890393559294
Epoch #44: loss=0.02861563150885975
Epoch #45: loss=0.038042949553977426
Epoch #46: loss=0.03370738553278799
Epoch #47: loss=0.019094079518297977
Epoch #48: loss=0.03226909229141963
Epoch #49: loss=0.03755228003839042
Epoch #50: loss=0.031592698227431826
Epoch #51: loss=0.03150937258786976
Epoch #52: loss=0.03236937433052023
Epoch #53: loss=0.028831236115278092
Epoch #54: loss=0.021395402898765112
Epoch #55: loss=0.03767687496020885
Epoch #56: loss=0.02921948365233165
Epoch #57: loss=0.025091223263024657
Epoch #58: loss=0.023719261954426318
Epoch #59: loss=0.02304908067022355
Epoch #60: loss=0.021942007234768874
Epoch #61: loss=0.02693114433008392
Epoch #62: loss=0.02272494642431455
Epoch #63: loss=0.018295938876865668
Epoch #64: loss=0.021656514828497297
Epoch #65: loss=0.02619501864411363
Epoch #66: loss=0.021051403553034347
Epoch #67: loss=0.02352401035057987
Epoch #68: loss=0.020816411083516475
Epoch #69: loss=0.01810942440439599
Epoch #70: loss=0.021529445817928303
Epoch #71: loss=0.01834414012804459
Epoch #72: loss=0.02150071885086826
Epoch #73: loss=0.02222171619184545
Epoch #74: loss=0.079518363953439
Epoch #75: loss=0.022928150885469133
Epoch #76: loss=0.02085092972861659
Epoch #77: loss=0.020180280680813074
Epoch #78: loss=0.01675377696761665
Epoch #79: loss=0.020257143247122203
Epoch #80: loss=0.014565359440411957
Epoch #81: loss=0.02250624512895908
Epoch #82: loss=0.023996126105172943
Epoch #83: loss=0.015045545321464236
Epoch #84: loss=0.029003256361045122
Epoch #85: loss=0.01913478864193473
Epoch #86: loss=0.0157256695145253
Epoch #87: loss=0.017833413573049278
Epoch #88: loss=0.015861172618096168
Epoch #89: loss=0.019724453134421564
Epoch #90: loss=0.03267515014333886
Epoch #91: loss=0.019333753525840502
Epoch #92: loss=0.014965886449131341
Epoch #93: loss=0.021470624863742346
Epoch #94: loss=0.02017173916522234
Epoch #95: loss=0.01946697142276722
Epoch #96: loss=0.011413682353856737
Epoch #97: loss=0.01701553460650275
Epoch #98: loss=0.020146795684199623
Epoch #99: loss=0.017828960572502644
Epoch #100: loss=0.011664823745377362
Epoch #101: loss=0.014746606793515497
Epoch #102: loss=0.016376906148044403
Epoch #103: loss=0.029795599572223063
Epoch #104: loss=0.026012832228745144
Epoch #105: loss=0.012979772162107548
Epoch #106: loss=0.013640939915154398
Epoch #107: loss=0.014628847229753127
Epoch #108: loss=0.017745923151504753
Epoch #109: loss=0.012261255546606214
Epoch #110: loss=0.013985904185585285
Epoch #111: loss=0.016398296576990545
Epoch #112: loss=0.01370180960823866
Epoch #113: loss=0.013412386686621789
Epoch #114: loss=0.011252007502497942
Epoch #115: loss=0.015863857968571506
Epoch #116: loss=0.022806436361510765
Epoch #117: loss=0.01513731833975145
Epoch #118: loss=0.013991417036130912
Epoch #119: loss=0.01465231531505832
Epoch #120: loss=0.02219492362525331
Epoch #121: loss=0.014158678507479329
Epoch #122: loss=0.015735281016838916
Epoch #123: loss=0.012425010744938122
Epoch #124: loss=0.013292248210209108
Epoch #125: loss=0.010212055418159286
Epoch #126: loss=0.015073262662707452
Epoch #127: loss=0.015404097050363871
Epoch #128: loss=0.01315413828916758
Epoch #129: loss=0.01543372759493124
Epoch #130: loss=0.019504180498013036
Epoch #131: loss=0.027623483776858256
Epoch #132: loss=0.015921593251173857
Epoch #133: loss=0.011910111744974387
Epoch #134: loss=0.00988596742080331
Epoch #135: loss=0.013540027159229707
Epoch #136: loss=0.016602420225649714
Epoch #137: loss=0.013621793267056233
Epoch #138: loss=0.01405153061801158
Epoch #139: loss=0.015036187739201248
Epoch #140: loss=0.018125531496830757
Epoch #141: loss=0.014174213613304927
Epoch #142: loss=0.02020311819437501
Epoch #143: loss=0.013243764744884284
Epoch #144: loss=0.01577012970169946
Epoch #145: loss=0.014371368180397977
Epoch #146: loss=0.013724109884156881
Epoch #147: loss=0.011789284780809477
Epoch #148: loss=0.012608371818429756
Epoch #149: loss=0.009079295264277186
Epoch #150: loss=0.013832435472265267
Epoch #151: loss=0.010321102277687835
Epoch #152: loss=0.009743151941521833
Epoch #153: loss=0.018522520029408216
Epoch #154: loss=0.01090883056491863
Epoch #155: loss=0.011342089880325871
Epoch #156: loss=0.012426395156208274
Epoch #157: loss=0.010843996481208344
Epoch #158: loss=0.018609477812459366
Epoch #159: loss=0.007586776662084235
Epoch #160: loss=0.00842121233904162
Epoch #161: loss=0.0186552371903735
Epoch #162: loss=0.013169538519953249
Epoch #163: loss=0.007829886761232573
Epoch #164: loss=0.019022560196894318
Epoch #165: loss=0.01150419511956777
Epoch #166: loss=0.010234040617893719
Epoch #167: loss=0.017878995248234372
Epoch #168: loss=0.00856842532277413
Epoch #169: loss=0.007719767235390595
Epoch #170: loss=0.01012694683323575
Epoch #171: loss=0.010570184531958609
Epoch #172: loss=0.008507139604021122
Epoch #173: loss=0.017090986516110667
Epoch #174: loss=0.019288247026024802
Epoch #175: loss=0.011263061710352818
Epoch #176: loss=0.012901387823302388
Epoch #177: loss=0.010178664444099988
Epoch #178: loss=0.008838451282662642
Epoch #179: loss=0.012558881642755558
Epoch #180: loss=0.021606666327163813
Epoch #181: loss=0.009387593132290142
Epoch #182: loss=0.012921640147484491
Epoch #183: loss=0.015517776858984459
Epoch #184: loss=0.0109957117877139
Epoch #185: loss=0.010767792563423296
Epoch #186: loss=0.008754140528670023
Epoch #187: loss=0.005046570218092459
Epoch #188: loss=0.009740613858181849
Epoch #189: loss=0.008679618705381396
Epoch #190: loss=0.007535561153790366
Epoch #191: loss=0.013595185970013725
Epoch #192: loss=0.016549135210113152
Epoch #193: loss=0.009879402892561685
Epoch #194: loss=0.007566913048135739
Epoch #195: loss=0.012958332401073008
Epoch #196: loss=0.03368474986296882
Epoch #197: loss=0.02654944070182174
Epoch #198: loss=0.020147926927823294
Epoch #199: loss=0.011386361772510789
Epoch #200: loss=0.009626510138157053
Epoch #201: loss=0.014318165022803397
Epoch #202: loss=0.006871525186649804
Epoch #203: loss=0.0071192938968639525
Epoch #204: loss=0.013906545024673757
Epoch #205: loss=0.00992137679994297
Epoch #206: loss=0.008182181760341599
Epoch #207: loss=0.008636707231119775
Epoch #208: loss=0.012247998683982674
Epoch #209: loss=0.009237090140319548
Epoch #210: loss=0.010278225265911791
Epoch #211: loss=0.012697365005740459
Epoch #212: loss=0.015612260602868347
Epoch #213: loss=0.012274303511840555
Epoch #214: loss=0.008135123414261664
Epoch #215: loss=0.008977916116751899
Epoch #216: loss=0.008411062303744677
Epoch #217: loss=0.009862354436117163
Epoch #218: loss=0.01735242706964056
Epoch #219: loss=0.015670561267614243
Epoch #220: loss=0.008457104751913169
Epoch #221: loss=0.00878677644513699
Epoch #222: loss=0.03630328523303685
Epoch #223: loss=0.021595702090732617
Epoch #224: loss=0.009737234214046922
Epoch #225: loss=0.007061743297746001
Epoch #226: loss=0.009469815987566941
Epoch #227: loss=0.010984031964357835
Epoch #228: loss=0.009013173006284219
Epoch #229: loss=0.007738339939121023
Epoch #230: loss=0.0058491377488072505
Epoch #231: loss=0.007462062908975402
Epoch #232: loss=0.011007504933408506
Epoch #233: loss=0.01476674084376308
Epoch #234: loss=0.007972457731322874
Epoch #235: loss=0.012221642289922926
Epoch #236: loss=0.010921715220977504
Epoch #237: loss=0.008668029558559437
Epoch #238: loss=0.021999731811544722
Epoch #239: loss=0.00869065017556191
Epoch #240: loss=0.011413062666069453
Epoch #241: loss=0.0069873736080249315
Epoch #242: loss=0.007450035662938706
Epoch #243: loss=0.009085979405843164
Epoch #244: loss=0.008794543703461166
Epoch #245: loss=0.006376154942831434
Epoch #246: loss=0.009122410426912156
Epoch #247: loss=0.011249310645051806
Epoch #248: loss=0.012871518103105679
Epoch #249: loss=0.011951856725131785

Training time: 4:41:04.504411

Finished.
n2one setting ettm2_electricity -> weather
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/ettm2_electricity_epochs_250_seed_2022/model.pkl', muti_dataset='ettm2_electricity', random_seed=2022, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='weather')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.26796e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.39166e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=5.86641e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.26796e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.8713915245997939, 'MAE': 0.7132663899077071}
Finished.
------------------------- record done -------------------------
n2one setting ettm2_electricity -> exchange
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/ettm2_electricity_epochs_250_seed_2022/model.pkl', muti_dataset='ettm2_electricity', random_seed=2022, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='exchange')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=4.5559e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=4.5559e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.37002292114169444, 'MAE': 0.39665979009843805}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='ettm2_traffic', random_seed=2022, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/ettm2_traffic_epochs_250_seed_2022
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=1.0293352594353762
Epoch #1: loss=0.37899018231809206
Epoch #2: loss=0.259449570969959
Epoch #3: loss=0.19995445700496742
Epoch #4: loss=0.15120182984763542
Epoch #5: loss=0.12709366558983767
Epoch #6: loss=0.10393019910274502
Epoch #7: loss=0.0986737451896942
Epoch #8: loss=0.08221044579365068
Epoch #9: loss=0.06763211725213547
Epoch #10: loss=0.0719619831689052
Epoch #11: loss=0.060257780011465936
Epoch #12: loss=0.059092262676593774
Epoch #13: loss=0.04829434802481544
Epoch #14: loss=0.050700564153454045
Epoch #15: loss=0.051189944578520316
Epoch #16: loss=0.04235148990824435
Epoch #17: loss=0.038640075195730074
Epoch #18: loss=0.04010388153079227
Epoch #19: loss=0.04633483959733693
Epoch #20: loss=0.03153658039424619
Epoch #21: loss=0.03290992100999924
Epoch #22: loss=0.03588845229023301
Epoch #23: loss=0.03293856158655218
Epoch #24: loss=0.02717869200494102
Epoch #25: loss=0.03583370605008922
Epoch #26: loss=0.02661729131943684
Epoch #27: loss=0.030242959158339047
Epoch #28: loss=0.030617841696171974
Epoch #29: loss=0.0329081313431216
Epoch #30: loss=0.027523585560652646
Epoch #31: loss=0.022586801845245894
Epoch #32: loss=0.024937989162217147
Epoch #33: loss=0.02602091658500931
Epoch #34: loss=0.031959209447798086
Epoch #35: loss=0.028274352244877034
Epoch #36: loss=0.03186430889673857
Epoch #37: loss=0.03200353498403666
Epoch #38: loss=0.019211416012799007
Epoch #39: loss=0.022763657637020755
Epoch #40: loss=0.034701587112896774
Epoch #41: loss=0.020876032034658484
Epoch #42: loss=0.018942802619852577
Epoch #43: loss=0.020142827352697193
Epoch #44: loss=0.031940978139778656
Epoch #45: loss=0.02404909644234298
Epoch #46: loss=0.016192994990670374
Epoch #47: loss=0.021710984400521008
Epoch #48: loss=0.01684924019159235
Epoch #49: loss=0.030373461485839314
Epoch #50: loss=0.020800506352126397
Epoch #51: loss=0.02727783038042206
Epoch #52: loss=0.01634243844604527
Epoch #53: loss=0.02226619627242716
Epoch #54: loss=0.013699036952579407
Epoch #55: loss=0.016287726360696708
Epoch #56: loss=0.017837700753337793
Epoch #57: loss=0.02601822166345461
Epoch #58: loss=0.01943438502032852
Epoch #59: loss=0.01927936413118809
Epoch #60: loss=0.02232779682282399
Epoch #61: loss=0.01831674209937511
Epoch #62: loss=0.019932699170193496
Epoch #63: loss=0.020017747628251314
Epoch #64: loss=0.01638751587997706
Epoch #65: loss=0.02123832886600931
Epoch #66: loss=0.014655093460280837
Epoch #67: loss=0.016460743877336113
Epoch #68: loss=0.022174461750993194
Epoch #69: loss=0.013511279343211606
Epoch #70: loss=0.024805188466720647
Epoch #71: loss=0.019479250148451328
Epoch #72: loss=0.013739185741775939
Epoch #73: loss=0.012106493897794748
Epoch #74: loss=0.0195597656058051
Epoch #75: loss=0.018455055484015874
Epoch #76: loss=0.02061792794594494
Epoch #77: loss=0.01757206521678411
Epoch #78: loss=0.018467204734421136
Epoch #79: loss=0.015617062041757604
Epoch #80: loss=0.012322914872614811
Epoch #81: loss=0.014141480005320901
Epoch #82: loss=0.021257714316262763
Epoch #83: loss=0.01985023626296238
Epoch #84: loss=0.015710775003447662
Epoch #85: loss=0.016228544107180373
Epoch #86: loss=0.01615069635989157
Epoch #87: loss=0.012563374046952904
Epoch #88: loss=0.01302064316592557
Epoch #89: loss=0.012200748256306598
Epoch #90: loss=0.014466508759589991
Epoch #91: loss=0.01277551519989067
Epoch #92: loss=0.016384119693993164
Epoch #93: loss=0.013059564499944056
Epoch #94: loss=0.014359893621466115
Epoch #95: loss=0.013704570465197654
Epoch #96: loss=0.017805564422863147
Epoch #97: loss=0.02227480101406058
Epoch #98: loss=0.012091033010900112
Epoch #99: loss=0.012318561840046998
Epoch #100: loss=0.01982179663705726
Epoch #101: loss=0.013474110595536293
Epoch #102: loss=0.01169510258644801
Epoch #103: loss=0.015953127622188583
Epoch #104: loss=0.019734190860821867
Epoch #105: loss=0.014028915293786231
Epoch #106: loss=0.015991831179998948
Epoch #107: loss=0.012016666459525325
Epoch #108: loss=0.013051714737376065
Epoch #109: loss=0.01076535238514474
Epoch #110: loss=0.013170025597181372
Epoch #111: loss=0.013581232059813065
Epoch #112: loss=0.013886461524748353
Epoch #113: loss=0.01407513927885295
Epoch #114: loss=0.013183109265201932
Epoch #115: loss=0.014104079467505547
Epoch #116: loss=0.014686456576223354
Epoch #117: loss=0.013603772694817856
Epoch #118: loss=0.01278829223252054
Epoch #119: loss=0.012346116172752256
Epoch #120: loss=0.016727961613995344
Epoch #121: loss=0.013266176910020928
Epoch #122: loss=0.017850227872575692
Epoch #123: loss=0.017023745285583668
Epoch #124: loss=0.012856502581406028
Epoch #125: loss=0.01651798919782593
Epoch #126: loss=0.011176056364329291
Epoch #127: loss=0.016821329304176347
Epoch #128: loss=0.011036283231315945
Epoch #129: loss=0.009626328646587282
Epoch #130: loss=0.011503856428440144
Epoch #131: loss=0.021333123136547533
Epoch #132: loss=0.00983423155151443
Epoch #133: loss=0.013492812208054141
Epoch #134: loss=0.013166573851151693
Epoch #135: loss=0.015418285987947193
Epoch #136: loss=0.013004117016607196
Epoch #137: loss=0.008061110843407718
Epoch #138: loss=0.011271018878977376
Epoch #139: loss=0.00919472475075007
Epoch #140: loss=0.014323383374375031
Epoch #141: loss=0.012700032158420723
Epoch #142: loss=0.012344312857811682
Epoch #143: loss=0.011445085611374396
Epoch #144: loss=0.010783289820153185
Epoch #145: loss=0.011203166852801993
Epoch #146: loss=0.010380869862157584
Epoch #147: loss=0.012952615369433108
Epoch #148: loss=0.01148686946829368
Epoch #149: loss=0.009788121482407257
Epoch #150: loss=0.011469029484844509
Epoch #151: loss=0.010830639965187749
Epoch #152: loss=0.01024175705633554
Epoch #153: loss=0.013042059134035068
Epoch #154: loss=0.014315818389670982
Epoch #155: loss=0.007220231578818694
Epoch #156: loss=0.01438531828929541
Epoch #157: loss=0.011556797698225488
Epoch #158: loss=0.011770013977297692
Epoch #159: loss=0.009337267823366503
Epoch #160: loss=0.015316217307598497
Epoch #161: loss=0.01462641620446708
Epoch #162: loss=0.008937167292438796
Epoch #163: loss=0.015434342631351423
Epoch #164: loss=0.008813081248441337
Epoch #165: loss=0.007690141629954407
Epoch #166: loss=0.0127940988940222
Epoch #167: loss=0.009828633989387237
Epoch #168: loss=0.010102151593406506
Epoch #169: loss=0.007159944964220208
Epoch #170: loss=0.014230407190671562
Epoch #171: loss=0.010466161002705695
Epoch #172: loss=0.009048024463754804
Epoch #173: loss=0.009364983819924686
Epoch #174: loss=0.010644552985747385
Epoch #175: loss=0.010651905514039302
Epoch #176: loss=0.014802037624498634
Epoch #177: loss=0.008083019103413637
Epoch #178: loss=0.012364553615735163
Epoch #179: loss=0.011016778619095837
Epoch #180: loss=0.009498923097848215
Epoch #181: loss=0.011527903126526698
Epoch #182: loss=0.012387911836875529
Epoch #183: loss=0.01038185637839643
Epoch #184: loss=0.007892199445513937
Epoch #185: loss=0.011524153145295242
Epoch #186: loss=0.010219552340120317
Epoch #187: loss=0.017388226272465494
Epoch #188: loss=0.014339209926696168
Epoch #189: loss=0.012594200331201545
Epoch #190: loss=0.013841101383275106
Epoch #191: loss=0.009985058813783653
Epoch #192: loss=0.008050158812239326
Epoch #193: loss=0.01092165837988907
Epoch #194: loss=0.011254996916620975
Epoch #195: loss=0.014195007075038754
Epoch #196: loss=0.016781164386698
Epoch #197: loss=0.008394571790683834
Epoch #198: loss=0.007821223061472574
Epoch #199: loss=0.008462492171638962
Epoch #200: loss=0.013782019768902157
Epoch #201: loss=0.00859140968209759
Epoch #202: loss=0.009667388040924185
Epoch #203: loss=0.011969940469421304
Epoch #204: loss=0.007178532964101767
Epoch #205: loss=0.012646947112629182
Epoch #206: loss=0.00614985731612509
Epoch #207: loss=0.011975216145971513
Epoch #208: loss=0.011400918093716532
Epoch #209: loss=0.009918901056563588
Epoch #210: loss=0.015186992490935894
Epoch #211: loss=0.013067816724131424
Epoch #212: loss=0.009403735169315953
Epoch #213: loss=0.006465395304067748
Epoch #214: loss=0.008657655445692148
Epoch #215: loss=0.012285226155754541
Epoch #216: loss=0.008644910191861461
Epoch #217: loss=0.0075945492602422305
Epoch #218: loss=0.009141881439482387
Epoch #219: loss=0.0097647334933769
Epoch #220: loss=0.008157992096048026
Epoch #221: loss=0.012618816195921022
Epoch #222: loss=0.011901337128646116
Epoch #223: loss=0.010302697686027902
Epoch #224: loss=0.008277034152094872
Epoch #225: loss=0.010126313997404853
Epoch #226: loss=0.005812553591987696
Epoch #227: loss=0.01019239662400338
Epoch #228: loss=0.010429921683672398
Epoch #229: loss=0.01047945260174278
Epoch #230: loss=0.007370104321385245
Epoch #231: loss=0.009128910670104145
Epoch #232: loss=0.011794227709151476
Epoch #233: loss=0.009559361550797344
Epoch #234: loss=0.009749551554317698
Epoch #235: loss=0.013458928762668933
Epoch #236: loss=0.007775730069597755
Epoch #237: loss=0.010472073997185541
Epoch #238: loss=0.008515358243560587
Epoch #239: loss=0.00994568216968605
Epoch #240: loss=0.010052247834881957
Epoch #241: loss=0.007985656701274647
Epoch #242: loss=0.00821870826007893
Epoch #243: loss=0.008475926682085118
Epoch #244: loss=0.008468069328284686
Epoch #245: loss=0.0103081206048571
Epoch #246: loss=0.010267502176757386
Epoch #247: loss=0.006925330644393823
Epoch #248: loss=0.009372296255330724
Epoch #249: loss=0.013912121475494055

Training time: 10:33:34.564214

Finished.
n2one setting ettm2_traffic -> electricity
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/ettm2_traffic_epochs_250_seed_2022/model.pkl', muti_dataset='ettm2_traffic', random_seed=2022, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='electricity')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.03652e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.94498e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=5.29772e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.03652e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.40623144307647074, 'MAE': 0.4528123784866998}
Finished.
------------------------- record done -------------------------
n2one setting ettm2_traffic -> weather
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/ettm2_traffic_epochs_250_seed_2022/model.pkl', muti_dataset='ettm2_traffic', random_seed=2022, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='weather')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.305e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.61259e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.305e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.5138455089692177, 'MAE': 0.5445311848206517}
Finished.
------------------------- record done -------------------------
n2one setting ettm2_traffic -> exchange
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/ettm2_traffic_epochs_250_seed_2022/model.pkl', muti_dataset='ettm2_traffic', random_seed=2022, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='exchange')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=5.15889e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.27010853590394324, 'MAE': 0.34892469226082834}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='ettm2_weather', random_seed=2022, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/ettm2_weather_epochs_250_seed_2022
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=5.255980987806578
Epoch #1: loss=2.565979751380714
Epoch #2: loss=2.648457224304612
Epoch #3: loss=2.206229996036839
Epoch #4: loss=1.9171576274407875
Epoch #5: loss=1.7418820245845898
Epoch #6: loss=1.5546903384698403
Epoch #7: loss=1.414368819546055
Epoch #8: loss=1.4338550567626953
Epoch #9: loss=1.2813944494402087
Epoch #10: loss=1.238924252020346
Epoch #11: loss=1.2896415107959025
Epoch #12: loss=1.1948697856954626
Epoch #13: loss=1.0623203161600474
Epoch #14: loss=1.0878766259631596
Epoch #15: loss=1.029571365665745
Epoch #16: loss=0.9005266460212501
Epoch #17: loss=0.96142379335455
Epoch #18: loss=0.8390572683231251
Epoch #19: loss=0.7980681271166414
Epoch #20: loss=0.7872946938952884
Epoch #21: loss=0.7419914812655062
Epoch #22: loss=0.712391692238885
Epoch #23: loss=0.7082955966124663
Epoch #24: loss=0.6623962134928316
Epoch #25: loss=0.792112914291588
Epoch #26: loss=0.671686729869327
Epoch #27: loss=0.5934404158914411
Epoch #28: loss=0.6710204256547464
Epoch #29: loss=0.5662177422562161
Epoch #30: loss=0.6154371612780803
Epoch #31: loss=0.6594463715682158
Epoch #32: loss=0.5577397604246397
Epoch #33: loss=0.5407403804160453
Epoch #34: loss=0.5245088209977021
Epoch #35: loss=0.6766772922631856
Epoch #36: loss=0.578312038569837
Epoch #37: loss=0.5381064125009485
Epoch #38: loss=0.6326503600623157
Epoch #39: loss=0.5003497068946426
Epoch #40: loss=0.539042176427068
Epoch #41: loss=0.4833540626474329
Epoch #42: loss=0.5203830801151894
Epoch #43: loss=0.4960442636464093
Epoch #44: loss=0.4051821755396353
Epoch #45: loss=0.3588743588408908
Epoch #46: loss=0.38176966317602107
Epoch #47: loss=0.47618146482351664
Epoch #48: loss=0.48869711805034327
Epoch #49: loss=0.4227428843040724
Epoch #50: loss=0.37892093086564865
Epoch #51: loss=0.41433548524573044
Epoch #52: loss=0.42545897131030624
Epoch #53: loss=0.3304559632733062
Epoch #54: loss=0.3765060551263191
Epoch #55: loss=0.4706680641786472
Epoch #56: loss=0.47410905401448944
Epoch #57: loss=0.3506000750773662
Epoch #58: loss=0.36767853796482086
Epoch #59: loss=0.37186599059684855
Epoch #60: loss=0.35793866217136383
Epoch #61: loss=0.49573874554118597
Epoch #62: loss=0.3810925145406981
Epoch #63: loss=0.3326757224024953
Epoch #64: loss=0.3926779531143807
Epoch #65: loss=0.31523689426280355
Epoch #66: loss=0.26040528150829106
Epoch #67: loss=0.32048223711348867
Epoch #68: loss=0.2800471295376082
Epoch #69: loss=0.34951395803206675
Epoch #70: loss=0.3349189649562578
Epoch #71: loss=0.38759258027012283
Epoch #72: loss=0.4464786632640942
Epoch #73: loss=0.3633366343942848
Epoch #74: loss=0.3549562865817869
Epoch #75: loss=0.3447638162084528
Epoch #76: loss=0.27717348389528895
Epoch #77: loss=0.2383359692789413
Epoch #78: loss=0.23750518544300184
Epoch #79: loss=0.2868423631062379
Epoch #80: loss=0.20537605358136668
Epoch #81: loss=0.19700942490551923
Epoch #82: loss=0.23092795183529724
Epoch #83: loss=0.2228330824826215
Epoch #84: loss=0.22085299181777077
Epoch #85: loss=0.23076420116263466
Epoch #86: loss=0.18307079374790192
Epoch #87: loss=0.1857629120349884
Epoch #88: loss=0.21681681053863988
Epoch #89: loss=0.17157280948516485
Epoch #90: loss=0.18346653012810527
Epoch #91: loss=0.2035344063832953
Epoch #92: loss=0.22343817071334734
Epoch #93: loss=0.18123518826591
Epoch #94: loss=0.222326253314276
Epoch #95: loss=0.1413554767700466
Epoch #96: loss=0.20972585214956388
Epoch #97: loss=0.1883785768940642
Epoch #98: loss=0.14026941228154544
Epoch #99: loss=0.1544446862629942
Epoch #100: loss=0.1304997266144366
Epoch #101: loss=0.216596597855961
Epoch #102: loss=0.21501373650657163
Epoch #103: loss=0.19982137108171308
Epoch #104: loss=0.1973087773532481
Epoch #105: loss=0.19368229403689102
Epoch #106: loss=0.20567719396707174
Epoch #107: loss=0.1750382362185298
Epoch #108: loss=0.17768853822269956
Epoch #109: loss=0.1882634350174182
Epoch #110: loss=0.16606420127523913
Epoch #111: loss=0.16407840417043582
Epoch #112: loss=0.12301171581084663
Epoch #113: loss=0.1815314503336275
Epoch #114: loss=0.1532065684126841
Epoch #115: loss=0.14439763662380142
Epoch #116: loss=0.2492310900140453
Epoch #117: loss=0.16663086897618062
Epoch #118: loss=0.15036450125075676
Epoch #119: loss=0.24200538757282333
Epoch #120: loss=0.13426635776822632
Epoch #121: loss=0.14925331370653333
Epoch #122: loss=0.10554168584781724
Epoch #123: loss=0.14982641115784645
Epoch #124: loss=0.14805863848006404
Epoch #125: loss=0.13115457490690657
Epoch #126: loss=0.15414236657120087
Epoch #127: loss=0.10259374744586043
Epoch #128: loss=0.1011818752095506
Epoch #129: loss=0.11718112693445103
Epoch #130: loss=0.10008410565756462
Epoch #131: loss=0.11589913331979029
Epoch #132: loss=0.12845018727553859
Epoch #133: loss=0.15078476383476644
Epoch #134: loss=0.10273890720831381
Epoch #135: loss=0.09789676657198249
Epoch #136: loss=0.17642661335097776
Epoch #137: loss=0.1251898980825334
Epoch #138: loss=0.11446643812028137
Epoch #139: loss=0.09386288817669894
Epoch #140: loss=0.09697917899167216
Epoch #141: loss=0.07695442417988906
Epoch #142: loss=0.11526093178906956
Epoch #143: loss=0.12253702210413443
Epoch #144: loss=0.10828570623856944
Epoch #145: loss=0.11022754124290235
Epoch #146: loss=0.12448238848230324
Epoch #147: loss=0.1670649388956057
Epoch #148: loss=0.09590338840073831
Epoch #149: loss=0.12046966998762376
Epoch #150: loss=0.1287975222677798
Epoch #151: loss=0.09715323432071789
Epoch #152: loss=0.07654689725589107
Epoch #153: loss=0.0778131027378746
Epoch #154: loss=0.07413430255208467
Epoch #155: loss=0.09455759153776877
Epoch #156: loss=0.08885134816975207
Epoch #157: loss=0.12261641745430392
Epoch #158: loss=0.08458910283406039
Epoch #159: loss=0.12540757469832897
Epoch #160: loss=0.11899696622748633
Epoch #161: loss=0.09548385280209619
Epoch #162: loss=0.07187531319623058
Epoch #163: loss=0.07409469063418943
Epoch #164: loss=0.06416400527027813
Epoch #165: loss=0.07821801093381804
Epoch #166: loss=0.10457580284895124
Epoch #167: loss=0.07361683994531631
Epoch #168: loss=0.07148160352497487
Epoch #169: loss=0.11392244147891933
Epoch #170: loss=0.10272427311016095
Epoch #171: loss=0.11308474219530015
Epoch #172: loss=0.08838523924350739
Epoch #173: loss=0.08409419897440318
Epoch #174: loss=0.11484144325997378
Epoch #175: loss=0.08450675982277135
Epoch #176: loss=0.06620706411430964
Epoch #177: loss=0.06612033746834542
Epoch #178: loss=0.11304883017028505
Epoch #179: loss=0.0754222203348134
Epoch #180: loss=0.12713752712148266
Epoch #181: loss=0.15675736500604734
Epoch #182: loss=0.13159763913702321
Epoch #183: loss=0.07565344276057707
Epoch #184: loss=0.08739651840280842
Epoch #185: loss=0.07576665468513966
Epoch #186: loss=0.04808080649456462
Epoch #187: loss=0.08222186303622014
Epoch #188: loss=0.1291122570935939
Epoch #189: loss=0.11409531005129621
Epoch #190: loss=0.08415798384796928
Epoch #191: loss=0.09144308750291129
Epoch #192: loss=0.09070461646125123
Epoch #193: loss=0.06637065238445192
Epoch #194: loss=0.043726674960674464
Epoch #195: loss=0.06952558915961433
Epoch #196: loss=0.06013418318754112
Epoch #197: loss=0.049614976537791455
Epoch #198: loss=0.04422641442333524
Epoch #199: loss=0.04460200307437697
Epoch #200: loss=0.0516865547947787
Epoch #201: loss=0.07061330724910304
Epoch #202: loss=0.09036410287828059
Epoch #203: loss=0.07020583351117533
Epoch #204: loss=0.06340033420034356
Epoch #205: loss=0.045457190431251716
Epoch #206: loss=0.05300478337684999
Epoch #207: loss=0.04769590940024402
Epoch #208: loss=0.07108216767979635
Epoch #209: loss=0.11394912719323828
Epoch #210: loss=0.06280764889576144
Epoch #211: loss=0.1441991531063576
Epoch #212: loss=0.08193894596518697
Epoch #213: loss=0.07136959866998163
Epoch #214: loss=0.07205390917590342
Epoch #215: loss=0.17901227316139517
Epoch #216: loss=0.19900028860649546
Epoch #217: loss=0.2716811636613833
Epoch #218: loss=0.08348076055581505
Epoch #219: loss=0.0684345602938855
Epoch #220: loss=0.09913150948547833
Epoch #221: loss=0.04594077806718446
Epoch #222: loss=0.06344564102992818
Epoch #223: loss=0.06296708540537872
Epoch #224: loss=0.04459997301770223
Epoch #225: loss=0.047723022314745026
Epoch #226: loss=0.04555614434837087
Epoch #227: loss=0.05179123331264064
Epoch #228: loss=0.06729893323436782
Epoch #229: loss=0.05732961497395425
Epoch #230: loss=0.05153570860322263
Epoch #231: loss=0.13531043332674214
Epoch #232: loss=0.05882890872355249
Epoch #233: loss=0.11938012454256013
Epoch #234: loss=0.05642672389040928
Epoch #235: loss=0.11868185406500423
Epoch #236: loss=0.048208820985982545
Epoch #237: loss=0.03934493371461694
Epoch #238: loss=0.03876007272786385
Epoch #239: loss=0.05761723902479217
Epoch #240: loss=0.054506153447201125
Epoch #241: loss=0.04406305732255852
Epoch #242: loss=0.04364122714645959
Epoch #243: loss=0.03872410360270658
Epoch #244: loss=0.05741594361795767
Epoch #245: loss=0.06757872367932184
Epoch #246: loss=0.03274174618559915
Epoch #247: loss=0.04592930169373348
Epoch #248: loss=0.03640706542677976
Epoch #249: loss=0.03563128726405872

Training time: 0:38:09.760540

Finished.
n2one setting ettm2_weather -> electricity
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/ettm2_weather_epochs_250_seed_2022/model.pkl', muti_dataset='ettm2_weather', random_seed=2022, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='electricity')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.3226e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.51171e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.3226e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.36409564195414085, 'MAE': 0.4249779613394606}
Finished.
------------------------- record done -------------------------
n2one setting ettm2_weather -> exchange
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/ettm2_weather_epochs_250_seed_2022/model.pkl', muti_dataset='ettm2_weather', random_seed=2022, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='exchange')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=3.15552e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.21864996466917724, 'MAE': 0.3195156646667012}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='ettm2_exchange', random_seed=2022, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/ettm2_exchange_epochs_250_seed_2022
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=5.212452624973498
Epoch #1: loss=2.811783608637358
Epoch #2: loss=2.337703560528002
Epoch #3: loss=2.161136533084669
Epoch #4: loss=2.0923983674300346
Epoch #5: loss=1.9204494012029547
Epoch #6: loss=1.8248729643068815
Epoch #7: loss=1.7131775868566412
Epoch #8: loss=1.6614645217594348
Epoch #9: loss=1.5851699992230064
Epoch #10: loss=1.5076598493676436
Epoch #11: loss=1.3998831698769016
Epoch #12: loss=1.4358667072496916
Epoch #13: loss=1.3089412074340017
Epoch #14: loss=1.321737251783672
Epoch #15: loss=1.348674736524883
Epoch #16: loss=1.2388653566962795
Epoch #17: loss=1.2353565818385075
Epoch #18: loss=1.1462548437871432
Epoch #19: loss=1.1170922455034757
Epoch #20: loss=1.0769060410951312
Epoch #21: loss=1.0899528139515926
Epoch #22: loss=1.0727483096875643
Epoch #23: loss=1.1261976078936928
Epoch #24: loss=1.0964141142995734
Epoch #25: loss=1.0631196812579506
Epoch #26: loss=1.0099474003440456
Epoch #27: loss=1.0402995065638894
Epoch #28: loss=1.0304891341610958
Epoch #29: loss=1.0143626520508213
Epoch #30: loss=1.0152513729898553
Epoch #31: loss=0.9402924744706405
Epoch #32: loss=0.8875942010628549
Epoch #33: loss=0.9035408841936212
Epoch #34: loss=0.8301557522071036
Epoch #35: loss=0.8797554248257687
Epoch #36: loss=0.8039475742139315
Epoch #37: loss=0.9313702614683854
Epoch #38: loss=0.7843792250281886
Epoch #39: loss=0.8359000525976482
Epoch #40: loss=0.7397841403358861
Epoch #41: loss=0.8271041951681438
Epoch #42: loss=0.7859084480687192
Epoch #43: loss=0.7478476354950353
Epoch #44: loss=0.7844867078881514
Epoch #45: loss=0.7178034437330145
Epoch #46: loss=0.7202811962679813
Epoch #47: loss=0.675567029338134
Epoch #48: loss=0.6961575652423658
Epoch #49: loss=0.6891236634630906
Epoch #50: loss=0.8153182911245447
Epoch #51: loss=0.6455951631069183
Epoch #52: loss=0.7366383781558589
Epoch #53: loss=0.7956311420390481
Epoch #54: loss=0.676179554901625
Epoch #55: loss=0.615561958990599
Epoch #56: loss=0.5904479795380643
Epoch #57: loss=0.6415487261194932
Epoch #58: loss=0.5747723548035872
Epoch #59: loss=0.6571328812523892
Epoch #60: loss=0.5978109538555145
Epoch #61: loss=0.6029499549614755
Epoch #62: loss=0.5238093655360373
Epoch #63: loss=0.5348569672358664
Epoch #64: loss=0.7110110897766916
Epoch #65: loss=0.533529489448196
Epoch #66: loss=0.5626755780295322
Epoch #67: loss=0.5513326541373604
Epoch #68: loss=0.5787610521442011
Epoch #69: loss=0.532601469441464
Epoch #70: loss=0.5199975834081048
Epoch #71: loss=0.5770620170392489
Epoch #72: loss=0.5546455555840543
Epoch #73: loss=0.5051778303949457
Epoch #74: loss=0.4906321716936011
Epoch #75: loss=0.5236705933746538
Epoch #76: loss=0.45442694425582886
Epoch #77: loss=0.49617166268198115
Epoch #78: loss=0.5025431976506585
Epoch #79: loss=0.5552594795038825
Epoch #80: loss=0.4942935109138489
Epoch #81: loss=0.5368119133146185
Epoch #82: loss=0.4482914977952054
Epoch #83: loss=0.4503406019587266
Epoch #84: loss=0.4821305353390543
Epoch #85: loss=0.4240797789473283
Epoch #86: loss=0.4260802229768352
Epoch #87: loss=0.425055098376776
Epoch #88: loss=0.4706629099030244
Epoch #89: loss=0.44604157303508957
Epoch #90: loss=0.45174058801249456
Epoch #91: loss=0.4627806081583625
Epoch #92: loss=0.4221921837643573
Epoch #93: loss=0.44423342221661616
Epoch #94: loss=0.3595751320060931
Epoch #95: loss=0.3612560622001949
Epoch #96: loss=0.4166043424292615
Epoch #97: loss=0.37166764430309596
Epoch #98: loss=0.36617505158248703
Epoch #99: loss=0.4729441208274741
Epoch #100: loss=0.32573024536433975
Epoch #101: loss=0.3592696503589028
Epoch #102: loss=0.3534047682034342
Epoch #103: loss=0.3582368508765572
Epoch #104: loss=0.3380702334014993
Epoch #105: loss=0.41594022198727254
Epoch #106: loss=0.3274046849263342
Epoch #107: loss=0.3421579222930105
Epoch #108: loss=0.3619457844056581
Epoch #109: loss=0.36356897730576365
Epoch #110: loss=0.3609116469558917
Epoch #111: loss=0.3822007520418418
Epoch #112: loss=0.3223253586574605
Epoch #113: loss=0.40138200788121475
Epoch #114: loss=0.3776404347858931
Epoch #115: loss=0.3427068951882814
Epoch #116: loss=0.3664471809017031
Epoch #117: loss=0.3889328249190983
Epoch #118: loss=0.4219559415390617
Epoch #119: loss=0.47918404089777095
Epoch #120: loss=0.3561953549322329
Epoch #121: loss=0.30868424514406606
Epoch #122: loss=0.3175331393354817
Epoch #123: loss=0.31657012904945175
Epoch #124: loss=0.3090413424529527
Epoch #125: loss=0.27230900526046753
Epoch #126: loss=0.28583961137031255
Epoch #127: loss=0.3086375849027383
Epoch #128: loss=0.26572978104415695
Epoch #129: loss=0.18642868532946236
Epoch #130: loss=0.27129698662381424
Epoch #131: loss=0.2873156866744945
Epoch #132: loss=0.28101060892406265
Epoch #133: loss=0.3379070170615849
Epoch #134: loss=0.35392779347143677
Epoch #135: loss=0.40098900700870316
Epoch #136: loss=0.3825267992521587
Epoch #137: loss=0.25140816600699173
Epoch #138: loss=0.2928650688968207
Epoch #139: loss=0.25138716399669647
Epoch #140: loss=0.30708123508252594
Epoch #141: loss=0.2277748294566807
Epoch #142: loss=0.2647915011958072
Epoch #143: loss=0.21382856329804972
Epoch #144: loss=0.23280670572268336
Epoch #145: loss=0.21799529265416295
Epoch #146: loss=0.33527444616744395
Epoch #147: loss=0.24719621162665517
Epoch #148: loss=0.2321232204374514
Epoch #149: loss=0.21290336353214165
Epoch #150: loss=0.341337241624531
Epoch #151: loss=0.27649132162332535
Epoch #152: loss=0.20859937467857412
Epoch #153: loss=0.31392644894750493
Epoch #154: loss=0.2018384470751411
Epoch #155: loss=0.3077887077080576
Epoch #156: loss=0.22710864285105153
Epoch #157: loss=0.22866547695900263
Epoch #158: loss=0.2776544686210783
Epoch #159: loss=0.1681356445739144
Epoch #160: loss=0.18158675730228424
Epoch #161: loss=0.22776113489740774
Epoch #162: loss=0.16386513921775317
Epoch #163: loss=0.25267702380293294
Epoch #164: loss=0.4145006506066573
Epoch #165: loss=0.29180480696653066
Epoch #166: loss=0.31517964013312993
Epoch #167: loss=0.26314275398066167
Epoch #168: loss=0.289215103183922
Epoch #169: loss=0.2723378668490209
Epoch #170: loss=0.1782797551468799
Epoch #171: loss=0.15104441658446663
Epoch #172: loss=0.12789396078963028
Epoch #173: loss=0.17413001703588585
Epoch #174: loss=0.21805932137526965
Epoch #175: loss=0.15230777075416163
Epoch #176: loss=0.20481129186718086
Epoch #177: loss=0.14199865609407425
Epoch #178: loss=0.17261813698630585
Epoch #179: loss=0.20076519937107437
Epoch #180: loss=0.27236919967751755
Epoch #181: loss=0.25929047716291326
Epoch #182: loss=0.20471382219540446
Epoch #183: loss=0.23874840218769877
Epoch #184: loss=0.21379101472465614
Epoch #185: loss=0.19719223560471283
Epoch #186: loss=0.17004367787587016
Epoch #187: loss=0.1362449350325685
Epoch #188: loss=0.25374758439628703
Epoch #189: loss=0.21347004567321978
Epoch #190: loss=0.21754322318654312
Epoch #191: loss=0.22453644616823448
Epoch #192: loss=0.17814408125061737
Epoch #193: loss=0.26037881954720143
Epoch #194: loss=0.25542050678479045
Epoch #195: loss=0.19509297021125493
Epoch #196: loss=0.1757225276608216
Epoch #197: loss=0.18400388191405095
Epoch #198: loss=0.21330017714123978
Epoch #199: loss=0.1741819467983748
Epoch #200: loss=0.26025934909519394
Epoch #201: loss=0.14943646639585495
Epoch #202: loss=0.14831690098109998
Epoch #203: loss=0.2483859754314548
Epoch #204: loss=0.21434207064540764
Epoch #205: loss=0.16819194469012713
Epoch #206: loss=0.18158063645425596
Epoch #207: loss=0.17023387531700887
Epoch #208: loss=0.1387176533278666
Epoch #209: loss=0.17569941419519877
Epoch #210: loss=0.1718196055214656
Epoch #211: loss=0.15197113509240903
Epoch #212: loss=0.11227070736257654
Epoch #213: loss=0.21853123271935865
Epoch #214: loss=0.1292525847491465
Epoch #215: loss=0.16311932021850034
Epoch #216: loss=0.17919734492897987
Epoch #217: loss=0.16935310767669426
Epoch #218: loss=0.15462679553188777
Epoch #219: loss=0.15850047061317846
Epoch #220: loss=0.21166885703017838
Epoch #221: loss=0.18406840256954493
Epoch #222: loss=0.16397782826894208
Epoch #223: loss=0.13445823325922615
Epoch #224: loss=0.13764266748177378
Epoch #225: loss=0.2588344300655942
Epoch #226: loss=0.24057196257145783
Epoch #227: loss=0.15249214674297132
Epoch #228: loss=0.1477508417477733
Epoch #229: loss=0.19831803166552595
Epoch #230: loss=0.14816587849667198
Epoch #231: loss=0.1340224323304076
Epoch #232: loss=0.11801858618855476
Epoch #233: loss=0.1630900500244216
Epoch #234: loss=0.1258048826926633
Epoch #235: loss=0.09877857016889673
Epoch #236: loss=0.12462572243652846
Epoch #237: loss=0.1263164270081018
Epoch #238: loss=0.09290493093430996
Epoch #239: loss=0.08618838516505141
Epoch #240: loss=0.13921187690606243
Epoch #241: loss=0.1508959033771565
Epoch #242: loss=0.15284335338755659
Epoch #243: loss=0.2149837391549035
Epoch #244: loss=0.15149135142564774
Epoch #245: loss=0.14453965033355512
Epoch #246: loss=0.10523864391602968
Epoch #247: loss=0.11334502206821191
Epoch #248: loss=0.11998631040516652
Epoch #249: loss=0.12425500370169941

Training time: 0:16:52.694458

Finished.
n2one setting ettm2_exchange -> electricity
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/ettm2_exchange_epochs_250_seed_2022/model.pkl', muti_dataset='ettm2_exchange', random_seed=2022, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='electricity')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.28165e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.5371e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.28165e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.3599659006862348, 'MAE': 0.4281541682950346}
Finished.
------------------------- record done -------------------------
n2one setting ettm2_exchange -> weather
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/ettm2_exchange_epochs_250_seed_2022/model.pkl', muti_dataset='ettm2_exchange', random_seed=2022, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='weather')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.28387e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.46972e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=5.84242e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.5299214560431325, 'MAE': 0.5478362086238914}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='electricity_traffic', random_seed=2022, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/electricity_traffic_epochs_250_seed_2022
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=0.7991148752750025
Epoch #1: loss=0.2882325354009388
Epoch #2: loss=0.18936187180667638
Epoch #3: loss=0.1419814010973412
Epoch #4: loss=0.10528955547987709
Epoch #5: loss=0.08815107499788549
Epoch #6: loss=0.0768941636815264
Epoch #7: loss=0.06766811663769846
Epoch #8: loss=0.057210432477720514
Epoch #9: loss=0.048205266594068034
Epoch #10: loss=0.04052830048772213
Epoch #11: loss=0.038976332312882805
Epoch #12: loss=0.034926505583210116
Epoch #13: loss=0.032717182503899815
Epoch #14: loss=0.03201918420086623
Epoch #15: loss=0.03040673918289936
Epoch #16: loss=0.03003288129714986
Epoch #17: loss=0.031093038699258235
Epoch #18: loss=0.02543191191805676
Epoch #19: loss=0.026025259669512725
Epoch #20: loss=0.022948125346938884
Epoch #21: loss=0.02614689199214554
Epoch #22: loss=0.027830468044395457
Epoch #23: loss=0.02550695816390311
Epoch #24: loss=0.024281625075494858
Epoch #25: loss=0.021944304706153064
Epoch #26: loss=0.022803533912832366
Epoch #27: loss=0.02393091512721548
Epoch #28: loss=0.019129067983776282
Epoch #29: loss=0.02120867731393687
Epoch #30: loss=0.02126444154645341
Epoch #31: loss=0.019882113363955265
Epoch #32: loss=0.02280680806738571
Epoch #33: loss=0.01810366773066825
Epoch #34: loss=0.021074184837260236
Epoch #35: loss=0.024240894629368095
Epoch #36: loss=0.01596180844587228
Epoch #37: loss=0.0208783367041944
Epoch #38: loss=0.01755131924871197
Epoch #39: loss=0.023653532401993176
Epoch #40: loss=0.018945571900831907
Epoch #41: loss=0.017534712349581875
Epoch #42: loss=0.01595835615486668
Epoch #43: loss=0.01978471660789237
Epoch #44: loss=0.015648820925490584
Epoch #45: loss=0.01840050953099585
Epoch #46: loss=0.013928774201681378
Epoch #47: loss=0.01620089638379831
Epoch #48: loss=0.012857306874872325
Epoch #49: loss=0.01593091939902681
Epoch #50: loss=0.014324215395767352
Epoch #51: loss=0.01322420535532172
Epoch #52: loss=0.019723521271909076
Epoch #53: loss=0.017213220770585146
Epoch #54: loss=0.014593210287291469
Epoch #55: loss=0.01355386067156043
Epoch #56: loss=0.013117358926501612
Epoch #57: loss=0.016122810345376107
Epoch #58: loss=0.017034422199120578
Epoch #59: loss=0.01221019482207705
Epoch #60: loss=0.014283728826700908
Epoch #61: loss=0.014543435820351794
Epoch #62: loss=0.01334792268532348
Epoch #63: loss=0.022788536396113383
Epoch #64: loss=0.01200502587292211
Epoch #65: loss=0.017844046109622865
Epoch #66: loss=0.013943458294821864
Epoch #67: loss=0.012703122451078951
Epoch #68: loss=0.015140264604285136
Epoch #69: loss=0.011877298336663553
Epoch #70: loss=0.012404906755969258
Epoch #71: loss=0.018671466401096792
Epoch #72: loss=0.011536084197576544
Epoch #73: loss=0.016288325491349315
Epoch #74: loss=0.01422952803490752
Epoch #75: loss=0.012201371137815428
Epoch #76: loss=0.016400732258641786
Epoch #77: loss=0.0121945922271036
Epoch #78: loss=0.01602417599694559
Epoch #79: loss=0.013587322707658174
Epoch #80: loss=0.01423496286901915
Epoch #81: loss=0.01722006368534368
Epoch #82: loss=0.017117813518877038
Epoch #83: loss=0.008735062651458611
Epoch #84: loss=0.012018288903566005
Epoch #85: loss=0.012943611000242368
Epoch #86: loss=0.012172950419994104
Epoch #87: loss=0.013055083803744556
Epoch #88: loss=0.01194013097974045
Epoch #89: loss=0.015013138377113584
Epoch #90: loss=0.01252550496037572
Epoch #91: loss=0.009684868378091674
Epoch #92: loss=0.012554965384481852
Epoch #93: loss=0.01037827901550277
Epoch #94: loss=0.013111236929417795
Epoch #95: loss=0.010866470248739542
Epoch #96: loss=0.018723368294213703
Epoch #97: loss=0.010531288266108583
Epoch #98: loss=0.012668883945794488
Epoch #99: loss=0.01343200519003052
Epoch #100: loss=0.010814822955759256
Epoch #101: loss=0.012419800806110796
Epoch #102: loss=0.010378973192668386
Epoch #103: loss=0.011956937351573401
Epoch #104: loss=0.011894962771784914
Epoch #105: loss=0.010364986018621932
Epoch #106: loss=0.016962415299009114
Epoch #107: loss=0.009431192367954654
Epoch #108: loss=0.011621388705503015
Epoch #109: loss=0.013026411183893744
Epoch #110: loss=0.008779718561114414
Epoch #111: loss=0.014376180508406833
Epoch #112: loss=0.01133614323687065
Epoch #113: loss=0.010304504859024842
Epoch #114: loss=0.013179166360224093
Epoch #115: loss=0.017318910565160352
Epoch #116: loss=0.009908671422922725
Epoch #117: loss=0.009057881717162427
Epoch #118: loss=0.013148012313702863
Epoch #119: loss=0.01642420481465224
Epoch #120: loss=0.010776189775175643
Epoch #121: loss=0.007657752213458884
Epoch #122: loss=0.01073671907531129
Epoch #123: loss=0.009542888160171566
Epoch #124: loss=0.0114919952149221
Epoch #125: loss=0.008933913286452133
Epoch #126: loss=0.011641608527305537
Epoch #127: loss=0.008884632833472166
Epoch #128: loss=0.010896799018941151
Epoch #129: loss=0.009837424280187364
Epoch #130: loss=0.010394562249243518
Epoch #131: loss=0.009379627248188162
Epoch #132: loss=0.008601575357226046
Epoch #133: loss=0.014300201829808819
Epoch #134: loss=0.010964655921202951
Epoch #135: loss=0.011385081942777353
Epoch #136: loss=0.009553018206790415
Epoch #137: loss=0.012681294938011902
Epoch #138: loss=0.007812389331860531
Epoch #139: loss=0.010624689063808049
Epoch #140: loss=0.008101723819947173
Epoch #141: loss=0.01023055156112016
Epoch #142: loss=0.011941213568219177
Epoch #143: loss=0.010092863438899738
Epoch #144: loss=0.012367720320075229
Epoch #145: loss=0.00848769964220958
Epoch #146: loss=0.008118858063036376
Epoch #147: loss=0.008913814652513749
Epoch #148: loss=0.00978886637732285
Epoch #149: loss=0.007278838297348258
Epoch #150: loss=0.009060515316470116
Epoch #151: loss=0.011239436003986202
Epoch #152: loss=0.010135908474997497
Epoch #153: loss=0.008533868197397846
Epoch #154: loss=0.010805115528273438
Epoch #155: loss=0.010096745028490471
Epoch #156: loss=0.006656877822121268
Epoch #157: loss=0.009920288882603882
Epoch #158: loss=0.009809425381463555
Epoch #159: loss=0.0117847143336052
Epoch #160: loss=0.008493270846249084
Epoch #161: loss=0.009558177888709288
Epoch #162: loss=0.0067382964865834545
Epoch #163: loss=0.009168303221780437
Epoch #164: loss=0.009338918495990197
Epoch #165: loss=0.008676074350180467
Epoch #166: loss=0.009756561578113651
Epoch #167: loss=0.0101348444139891
Epoch #168: loss=0.008610784123309187
Epoch #169: loss=0.007097376653744401
Epoch #170: loss=0.01173537017672562
Epoch #171: loss=0.0070550033206702506
Epoch #172: loss=0.00820773943504106
Epoch #173: loss=0.011455635047101308
Epoch #174: loss=0.008620771501936702
Epoch #175: loss=0.00780542161958178
Epoch #176: loss=0.008722676270355416
Epoch #177: loss=0.011695516412343203
Epoch #178: loss=0.009707220659840915
Epoch #179: loss=0.008591438829758604
Epoch #180: loss=0.010579453855556623
Epoch #181: loss=0.0071444055392407965
Epoch #182: loss=0.008735285800478117
Epoch #183: loss=0.014089299269396324
Epoch #184: loss=0.008180060028025949
Epoch #185: loss=0.01100956628269623
Epoch #186: loss=0.009684749926565739
Epoch #187: loss=0.008432232920383984
Epoch #188: loss=0.006532531377737011
Epoch #189: loss=0.011174322867330376
Epoch #190: loss=0.006785484300895822
Epoch #191: loss=0.0072037069461971335
Epoch #192: loss=0.01492436814197872
Epoch #193: loss=0.00863540000169306
Epoch #194: loss=0.0053796776432142685
Epoch #195: loss=0.008875879240939662
Epoch #196: loss=0.009456949543387182
Epoch #197: loss=0.008856320439664972
Epoch #198: loss=0.008262941741660795
Epoch #199: loss=0.009053361441970378
Epoch #200: loss=0.010653300903617883
Epoch #201: loss=0.008865734495053604
Epoch #202: loss=0.006996143330578661
Epoch #203: loss=0.008296248730527568
Epoch #204: loss=0.01149765277468475
Epoch #205: loss=0.007500544193063785
Epoch #206: loss=0.007616965293440266
Epoch #207: loss=0.00993758970300316
Epoch #208: loss=0.006983162974406123
Epoch #209: loss=0.007167366540021758
Epoch #210: loss=0.008242628570804334
Epoch #211: loss=0.007955748046584395
Epoch #212: loss=0.009278179506001922
Epoch #213: loss=0.0073209979169852855
Epoch #214: loss=0.008354219946577682
Epoch #215: loss=0.006931053014468781
Epoch #216: loss=0.011570719484607268
Epoch #217: loss=0.007162161485397263
Epoch #218: loss=0.009531859113820136
Epoch #219: loss=0.006150133495414832
Epoch #220: loss=0.010044357745834592
Epoch #221: loss=0.009161570931419522
Epoch #222: loss=0.006270124863872601
Epoch #223: loss=0.008707352062155487
Epoch #224: loss=0.011669206677508488
Epoch #225: loss=0.007360936620823086
Epoch #226: loss=0.007112124995644864
Epoch #227: loss=0.008342473367753858
Epoch #228: loss=0.008603173674301775
Epoch #229: loss=0.009471372741918904
Epoch #230: loss=0.007459054951426523
Epoch #231: loss=0.009057018368158353
Epoch #232: loss=0.0076364584600626464
Epoch #233: loss=0.010094640468009484
Epoch #234: loss=0.008309148757124234
Epoch #235: loss=0.012692683268524086
Epoch #236: loss=0.00714734272186328
Epoch #237: loss=0.006748386723559796
Epoch #238: loss=0.006311115996397651
Epoch #239: loss=0.011290351582828548
Epoch #240: loss=0.005445911616532694
Epoch #241: loss=0.010556416262762597
Epoch #242: loss=0.007102079486919883
Epoch #243: loss=0.011212854396758698
Epoch #244: loss=0.008945498659860206
Epoch #245: loss=0.007248227912534917
Epoch #246: loss=0.013082839431569735
Epoch #247: loss=0.008207404810958089
Epoch #248: loss=0.0079022828662627
Epoch #249: loss=0.00994347034334949

Training time: 14:20:33.928430

Finished.
n2one setting electricity_traffic -> weather
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/electricity_traffic_epochs_250_seed_2022/model.pkl', muti_dataset='electricity_traffic', random_seed=2022, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='weather')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.40525e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.75394e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.40525e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.6400659796014915, 'MAE': 0.6147065273988542}
Finished.
------------------------- record done -------------------------
n2one setting electricity_traffic -> exchange
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/electricity_traffic_epochs_250_seed_2022/model.pkl', muti_dataset='electricity_traffic', random_seed=2022, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='exchange')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=5.91732e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.37873927605735724, 'MAE': 0.40076039966009097}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='electricity_weather', random_seed=2022, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/electricity_weather_epochs_250_seed_2022
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=1.7079170487112545
Epoch #1: loss=0.7871003004075471
Epoch #2: loss=0.5758371235486066
Epoch #3: loss=0.42296388114048356
Epoch #4: loss=0.3935751412117516
Epoch #5: loss=0.32697463108586305
Epoch #6: loss=0.30335271148070136
Epoch #7: loss=0.26910931857501397
Epoch #8: loss=0.246622838788486
Epoch #9: loss=0.23763321155429917
Epoch #10: loss=0.21841039824210945
Epoch #11: loss=0.20789459429316287
Epoch #12: loss=0.1893144253888288
Epoch #13: loss=0.1500599391500785
Epoch #14: loss=0.16393708259468465
Epoch #15: loss=0.1335783649117871
Epoch #16: loss=0.1293577577070647
Epoch #17: loss=0.13437398660509317
Epoch #18: loss=0.1120934320790881
Epoch #19: loss=0.09977332908413253
Epoch #20: loss=0.10419895006370562
Epoch #21: loss=0.1195987233682737
Epoch #22: loss=0.08485888659428022
Epoch #23: loss=0.07913238856061133
Epoch #24: loss=0.071462709354693
Epoch #25: loss=0.0674568869533173
Epoch #26: loss=0.07027449138913443
Epoch #27: loss=0.06369732443769315
Epoch #28: loss=0.0867059138289489
Epoch #29: loss=0.07889492574123724
Epoch #30: loss=0.08555841109492636
Epoch #31: loss=0.06212669084256552
Epoch #32: loss=0.05034119961514897
Epoch #33: loss=0.05719998440911902
Epoch #34: loss=0.04263811661279425
Epoch #35: loss=0.0387588011292789
Epoch #36: loss=0.05731451853449518
Epoch #37: loss=0.05243094357519655
Epoch #38: loss=0.0646226316722652
Epoch #39: loss=0.04099749531717676
Epoch #40: loss=0.03635566199053433
Epoch #41: loss=0.04982291437049399
Epoch #42: loss=0.0403649427542887
Epoch #43: loss=0.03505801178162021
Epoch #44: loss=0.04537767277069669
Epoch #45: loss=0.033081554158212303
Epoch #46: loss=0.03288454091933258
Epoch #47: loss=0.02922622852432689
Epoch #48: loss=0.033114838688996666
Epoch #49: loss=0.04032387107312787
Epoch #50: loss=0.02746470222989049
Epoch #51: loss=0.03874323709488119
Epoch #52: loss=0.03971760859511664
Epoch #53: loss=0.03269192269132325
Epoch #54: loss=0.04976992589966407
Epoch #55: loss=0.038146036901841374
Epoch #56: loss=0.030814606773865647
Epoch #57: loss=0.033918896283060634
Epoch #58: loss=0.03227495548103316
Epoch #59: loss=0.029409100768566992
Epoch #60: loss=0.03416290192130781
Epoch #61: loss=0.029964839805773746
Epoch #62: loss=0.020405994227990745
Epoch #63: loss=0.0337332301036393
Epoch #64: loss=0.03576499761735768
Epoch #65: loss=0.02411263725020512
Epoch #66: loss=0.026781625260990812
Epoch #67: loss=0.018994148922627872
Epoch #68: loss=0.01901477670279581
Epoch #69: loss=0.03618193128225407
Epoch #70: loss=0.0164540531380459
Epoch #71: loss=0.023745674027837223
Epoch #72: loss=0.026093773081863798
Epoch #73: loss=0.02289426385395508
Epoch #74: loss=0.024597050455534397
Epoch #75: loss=0.0336252556367635
Epoch #76: loss=0.026169865820067063
Epoch #77: loss=0.04169709748965594
Epoch #78: loss=0.04376219525552148
Epoch #79: loss=0.02858071639980874
Epoch #80: loss=0.02076371613657102
Epoch #81: loss=0.020650066073174704
Epoch #82: loss=0.02535664706634524
Epoch #83: loss=0.017622570077632353
Epoch #84: loss=0.014922603722581217
Epoch #85: loss=0.02459920346414837
Epoch #86: loss=0.019952649834045138
Epoch #87: loss=0.022089112771611918
Epoch #88: loss=0.02076098512101956
Epoch #89: loss=0.04125189151556529
Epoch #90: loss=0.021845834769888297
Epoch #91: loss=0.019866795283950252
Epoch #92: loss=0.021521292736898755
Epoch #93: loss=0.02030334525734256
Epoch #94: loss=0.01644249963916312
Epoch #95: loss=0.016968004594715338
Epoch #96: loss=0.02435268225116113
Epoch #97: loss=0.02981574444458565
Epoch #98: loss=0.01870951356795034
Epoch #99: loss=0.038189012985018964
Epoch #100: loss=0.02295270818721782
Epoch #101: loss=0.021101095246168534
Epoch #102: loss=0.02409485230569242
Epoch #103: loss=0.017251872520132917
Epoch #104: loss=0.02192995542867018
Epoch #105: loss=0.021727474651605384
Epoch #106: loss=0.019177783234145557
Epoch #107: loss=0.01646352343506821
Epoch #108: loss=0.023583055589776164
Epoch #109: loss=0.026065044025373872
Epoch #110: loss=0.018871596154336808
Epoch #111: loss=0.024537705689848477
Epoch #112: loss=0.02128441514976065
Epoch #113: loss=0.01802694871625796
Epoch #114: loss=0.015589275370017822
Epoch #115: loss=0.014798585886420194
Epoch #116: loss=0.019922200160105083
Epoch #117: loss=0.02093796372913181
Epoch #118: loss=0.019478460405288958
Epoch #119: loss=0.028710382475392888
Epoch #120: loss=0.04608699263181532
Epoch #121: loss=0.022552726506315957
Epoch #122: loss=0.02913746687044156
Epoch #123: loss=0.015737766087973108
Epoch #124: loss=0.011429735805446106
Epoch #125: loss=0.013997677665321902
Epoch #126: loss=0.015504021236977623
Epoch #127: loss=0.018417854965812063
Epoch #128: loss=0.01594264966084136
Epoch #129: loss=0.017593311949737737
Epoch #130: loss=0.023833705782984257
Epoch #131: loss=0.017235612713125258
Epoch #132: loss=0.025198232186944773
Epoch #133: loss=0.013179924518711361
Epoch #134: loss=0.012933951654264794
Epoch #135: loss=0.014790152001878385
Epoch #136: loss=0.019334512810868287
Epoch #137: loss=0.019816009108442143
Epoch #138: loss=0.010858870420980023
Epoch #139: loss=0.012876656216432365
Epoch #140: loss=0.01407814113483952
Epoch #141: loss=0.015681593944935894
Epoch #142: loss=0.014430208104328449
Epoch #143: loss=0.020629641955741213
Epoch #144: loss=0.01595507433332009
Epoch #145: loss=0.015801391854005937
Epoch #146: loss=0.015012259244843673
Epoch #147: loss=0.015879821857844777
Epoch #148: loss=0.014631864772633158
Epoch #149: loss=0.020288156998215526
Epoch #150: loss=0.022102398002924135
Epoch #151: loss=0.020084395133994213
Epoch #152: loss=0.018364679119489302
Epoch #153: loss=0.02103228568618108
Epoch #154: loss=0.015267332482704576
Epoch #155: loss=0.021844905179956128
Epoch #156: loss=0.012780001839577228
Epoch #157: loss=0.015498616903757284
Epoch #158: loss=0.016543346169416093
Epoch #159: loss=0.019112027240543013
Epoch #160: loss=0.014200291303217932
Epoch #161: loss=0.011768959964201476
Epoch #162: loss=0.011828836161134662
Epoch #163: loss=0.012218279985228543
Epoch #164: loss=0.020851365252000733
Epoch #165: loss=0.01152826672733925
Epoch #166: loss=0.016123905223029363
Epoch #167: loss=0.024468683348988756
Epoch #168: loss=0.01597784900622978
Epoch #169: loss=0.020634575244136588
Epoch #170: loss=0.019782187814286825
Epoch #171: loss=0.009893543072184414
Epoch #172: loss=0.015069537441214675
Epoch #173: loss=0.01797775629259007
Epoch #174: loss=0.010985437198122694
Epoch #175: loss=0.008192380715123987
Epoch #176: loss=0.016726261080794242
Epoch #177: loss=0.017799197741440613
Epoch #178: loss=0.0161244088965193
Epoch #179: loss=0.010259694396568495
Epoch #180: loss=0.011356710151853765
Epoch #181: loss=0.02626601349873071
Epoch #182: loss=0.026500046915798756
Epoch #183: loss=0.011161315328171826
Epoch #184: loss=0.012973283944998382
Epoch #185: loss=0.014678704401549295
Epoch #186: loss=0.012981438635907509
Epoch #187: loss=0.007822255025499015
Epoch #188: loss=0.008152278903345054
Epoch #189: loss=0.013165287523005716
Epoch #190: loss=0.014520586272855946
Epoch #191: loss=0.016584540021663496
Epoch #192: loss=0.014001793267006747
Epoch #193: loss=0.007829390580112366
Epoch #194: loss=0.011646465547035599
Epoch #195: loss=0.01268331546589121
Epoch #196: loss=0.014287337810166787
Epoch #197: loss=0.04776103381451316
Epoch #198: loss=0.011917473606078186
Epoch #199: loss=0.011613235336034599
Epoch #200: loss=0.01071026323142432
Epoch #201: loss=0.01327137522587636
Epoch #202: loss=0.015431380620268536
Epoch #203: loss=0.008419482768878947
Epoch #204: loss=0.01586692576143644
Epoch #205: loss=0.015630421992954974
Epoch #206: loss=0.01487504596134543
Epoch #207: loss=0.009589099574006117
Epoch #208: loss=0.010065802003281133
Epoch #209: loss=0.013756004015523243
Epoch #210: loss=0.012241041504674637
Epoch #211: loss=0.01803002859095396
Epoch #212: loss=0.009748017537774604
Epoch #213: loss=0.013768947225665238
Epoch #214: loss=0.014863580280545192
Epoch #215: loss=0.01713911862230339
Epoch #216: loss=0.01896248029916338
Epoch #217: loss=0.03096375385331532
Epoch #218: loss=0.017927569295001226
Epoch #219: loss=0.012893112189128393
Epoch #220: loss=0.008539197978198593
Epoch #221: loss=0.013156770545685991
Epoch #222: loss=0.011842175320796306
Epoch #223: loss=0.01998517959548418
Epoch #224: loss=0.010948343809626282
Epoch #225: loss=0.021627681355259814
Epoch #226: loss=0.014114696845584697
Epoch #227: loss=0.010723658460412129
Epoch #228: loss=0.013341357360782857
Epoch #229: loss=0.015254130159398697
Epoch #230: loss=0.009595771685429856
Epoch #231: loss=0.01101830695067688
Epoch #232: loss=0.011735696588295326
Epoch #233: loss=0.011121956992361652
Epoch #234: loss=0.009894436211791561
Epoch #235: loss=0.01107345034380246
Epoch #236: loss=0.013443457578498097
Epoch #237: loss=0.01720960002448893
Epoch #238: loss=0.013093723412868177
Epoch #239: loss=0.01327161714446366
Epoch #240: loss=0.011775460377703247
Epoch #241: loss=0.009422340870063928
Epoch #242: loss=0.01222437543834604
Epoch #243: loss=0.01013604988698292
Epoch #244: loss=0.014114994708527824
Epoch #245: loss=0.019514485521584895
Epoch #246: loss=0.042543787208267674
Epoch #247: loss=0.014298060822860212
Epoch #248: loss=0.011692796634699376
Epoch #249: loss=0.014376746877921901

Training time: 4:56:30.016808

Finished.
n2one setting electricity_weather -> exchange
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/electricity_weather_epochs_250_seed_2022/model.pkl', muti_dataset='electricity_weather', random_seed=2022, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='exchange')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=4.73905e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=4.73905e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.300914539088647, 'MAE': 0.36890877748892037}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='electricity_exchange', random_seed=2022, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/electricity_exchange_epochs_250_seed_2022
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=1.6310705883162362
Epoch #1: loss=0.6619092389080662
Epoch #2: loss=0.4638150523679959
Epoch #3: loss=0.3541420456305101
Epoch #4: loss=0.33432477680926626
Epoch #5: loss=0.27567500591640415
Epoch #6: loss=0.23594195114895927
Epoch #7: loss=0.21782718268208953
Epoch #8: loss=0.19501955580747599
Epoch #9: loss=0.18246605374077532
Epoch #10: loss=0.17951665818691254
Epoch #11: loss=0.1579220579501162
Epoch #12: loss=0.13950762807145306
Epoch #13: loss=0.14025492506458404
Epoch #14: loss=0.11572174436347644
Epoch #15: loss=0.11443505446372786
Epoch #16: loss=0.11402413959538502
Epoch #17: loss=0.09273347517866132
Epoch #18: loss=0.08303182052963591
Epoch #19: loss=0.07953000855051819
Epoch #20: loss=0.08011121877772015
Epoch #21: loss=0.08413445706674574
Epoch #22: loss=0.0894080145216595
Epoch #23: loss=0.07088395497424806
Epoch #24: loss=0.08607579306571951
Epoch #25: loss=0.0694221861654819
Epoch #26: loss=0.05214242211976131
Epoch #27: loss=0.05246921540252117
Epoch #28: loss=0.053513589408531737
Epoch #29: loss=0.054654211350815725
Epoch #30: loss=0.05369618953570814
Epoch #31: loss=0.05097658618135338
Epoch #32: loss=0.05193291664672704
Epoch #33: loss=0.0704293680408443
Epoch #34: loss=0.05035261271484649
Epoch #35: loss=0.04556659509719414
Epoch #36: loss=0.04210320853849018
Epoch #37: loss=0.048330108279866196
Epoch #38: loss=0.04419222842522622
Epoch #39: loss=0.04600906825700837
Epoch #40: loss=0.03788795879557955
Epoch #41: loss=0.04082554512258087
Epoch #42: loss=0.04613022419686307
Epoch #43: loss=0.05181406275227689
Epoch #44: loss=0.04934297351302483
Epoch #45: loss=0.03642183395688589
Epoch #46: loss=0.03281988069842538
Epoch #47: loss=0.03652900365134559
Epoch #48: loss=0.03408613323664298
Epoch #49: loss=0.038599106456976846
Epoch #50: loss=0.03886149358782793
Epoch #51: loss=0.04080036372062396
Epoch #52: loss=0.0327507268841234
Epoch #53: loss=0.03578175716583354
Epoch #54: loss=0.030918789281591908
Epoch #55: loss=0.032724987986174875
Epoch #56: loss=0.03478178594935131
Epoch #57: loss=0.038431340288922845
Epoch #58: loss=0.027672772238449984
Epoch #59: loss=0.028016982666969774
Epoch #60: loss=0.02865980529526424
Epoch #61: loss=0.02564895004678042
Epoch #62: loss=0.03170907582470575
Epoch #63: loss=0.027108160802941646
Epoch #64: loss=0.021631150963959653
Epoch #65: loss=0.03622130821048534
Epoch #66: loss=0.029234002755125447
Epoch #67: loss=0.02903222516858469
Epoch #68: loss=0.026470788813615077
Epoch #69: loss=0.0291613069986415
Epoch #70: loss=0.03375468004130798
Epoch #71: loss=0.023289747198613354
Epoch #72: loss=0.024461968837300244
Epoch #73: loss=0.016916046356237394
Epoch #74: loss=0.030454749433609648
Epoch #75: loss=0.02441283924035207
Epoch #76: loss=0.02931738091954016
Epoch #77: loss=0.03733734846062743
Epoch #78: loss=0.02225206222220276
Epoch #79: loss=0.027981348082743086
Epoch #80: loss=0.02381611490936378
Epoch #81: loss=0.02465962877781569
Epoch #82: loss=0.027569286069849725
Epoch #83: loss=0.0266353292120012
Epoch #84: loss=0.03391362723539156
Epoch #85: loss=0.027086584225486637
Epoch #86: loss=0.02372334524670667
Epoch #87: loss=0.025302564024841726
Epoch #88: loss=0.026029950647195942
Epoch #89: loss=0.027622754953442123
Epoch #90: loss=0.025939537533190907
Epoch #91: loss=0.02716347975493621
Epoch #92: loss=0.020478710670896034
Epoch #93: loss=0.018154891843852335
Epoch #94: loss=0.01783228559401113
Epoch #95: loss=0.02362689318235873
Epoch #96: loss=0.020930502314253578
Epoch #97: loss=0.016186546084738556
Epoch #98: loss=0.02659096404886026
Epoch #99: loss=0.020957788078667806
Epoch #100: loss=0.02980023475666743
Epoch #101: loss=0.029088001587147905
Epoch #102: loss=0.0456035205603246
Epoch #103: loss=0.0269826555282789
Epoch #104: loss=0.018286292823696777
Epoch #105: loss=0.029710644109303408
Epoch #106: loss=0.015504880825809194
Epoch #107: loss=0.023288819431482977
Epoch #108: loss=0.013069696004426753
Epoch #109: loss=0.02000044310931824
Epoch #110: loss=0.02283007877303491
Epoch #111: loss=0.027734387869448443
Epoch #112: loss=0.019322848422162652
Epoch #113: loss=0.016457188570529427
Epoch #114: loss=0.022900814877675795
Epoch #115: loss=0.014917690222244285
Epoch #116: loss=0.02076891023580036
Epoch #117: loss=0.019613946915662914
Epoch #118: loss=0.03586211591222292
Epoch #119: loss=0.024109867150013512
Epoch #120: loss=0.020222164522771553
Epoch #121: loss=0.026033104177618168
Epoch #122: loss=0.025090618786546417
Epoch #123: loss=0.01775820450828974
Epoch #124: loss=0.021264535405016348
Epoch #125: loss=0.01829945727042347
Epoch #126: loss=0.01550027129249534
Epoch #127: loss=0.02557177117429136
Epoch #128: loss=0.0335967345831995
Epoch #129: loss=0.027497842388459703
Epoch #130: loss=0.02236535361016843
Epoch #131: loss=0.020258301036965985
Epoch #132: loss=0.018954201592599525
Epoch #133: loss=0.016288561260647573
Epoch #134: loss=0.019491465019711822
Epoch #135: loss=0.03337931397139579
Epoch #136: loss=0.023993779672309756
Epoch #137: loss=0.027304251041011404
Epoch #138: loss=0.020544322815831285
Epoch #139: loss=0.020540715761834437
Epoch #140: loss=0.020679205439497906
Epoch #141: loss=0.017351371116368638
Epoch #142: loss=0.01276996051259254
Epoch #143: loss=0.014314050685793394
Epoch #144: loss=0.026143974233397472
Epoch #145: loss=0.01794563594686807
Epoch #146: loss=0.017649278787069452
Epoch #147: loss=0.014239659347489341
Epoch #148: loss=0.019204017303627138
Epoch #149: loss=0.02855347157233058
Epoch #150: loss=0.018120540787025685
Epoch #151: loss=0.02143326449868946
Epoch #152: loss=0.01589759939992772
Epoch #153: loss=0.01397945490979949
Epoch #154: loss=0.0203307231771052
Epoch #155: loss=0.019685433763518115
Epoch #156: loss=0.012169767560817352
Epoch #157: loss=0.013756869954027315
Epoch #158: loss=0.013585187993456446
Epoch #159: loss=0.012401149785913963
Epoch #160: loss=0.026956273906469333
Epoch #161: loss=0.021140664951071867
Epoch #162: loss=0.014646320137772766
Epoch #163: loss=0.010647909684456308
Epoch #164: loss=0.013471714764979235
Epoch #165: loss=0.016674685269919023
Epoch #166: loss=0.020247412166108113
Epoch #167: loss=0.022477578348320933
Epoch #168: loss=0.019856699409053907
Epoch #169: loss=0.017599712921756584
Epoch #170: loss=0.017846270749737485
Epoch #171: loss=0.028630594411188322
Epoch #172: loss=0.024719866416572756
Epoch #173: loss=0.01568146334220811
Epoch #174: loss=0.018827203793011005
Epoch #175: loss=0.022623265949240672
Epoch #176: loss=0.013413539879401937
Epoch #177: loss=0.025486590288364325
Epoch #178: loss=0.024726797632149796
Epoch #179: loss=0.015148834762019092
Epoch #180: loss=0.01155350016407728
Epoch #181: loss=0.011055707221172785
Epoch #182: loss=0.01015015514056187
Epoch #183: loss=0.009197856707253923
Epoch #184: loss=0.020134203792351518
Epoch #185: loss=0.016327060685467576
Epoch #186: loss=0.016471236402814286
Epoch #187: loss=0.017493204210140376
Epoch #188: loss=0.013002815226264196
Epoch #189: loss=0.011385084399235178
Epoch #190: loss=0.014356900369932472
Epoch #191: loss=0.014304668012911618
Epoch #192: loss=0.013399044264781666
Epoch #193: loss=0.011445134072097466
Epoch #194: loss=0.010511598897658042
Epoch #195: loss=0.008954708712135858
Epoch #196: loss=0.016164946189984583
Epoch #197: loss=0.012860015020964066
Epoch #198: loss=0.009312968016166868
Epoch #199: loss=0.01507851969028574
Epoch #200: loss=0.017868923285454485
Epoch #201: loss=0.020086706474534515
Epoch #202: loss=0.019651948241959254
Epoch #203: loss=0.043445642462204066
Epoch #204: loss=0.015011187381409023
Epoch #205: loss=0.012757742127274816
Epoch #206: loss=0.013586305298309597
Epoch #207: loss=0.01557363448048854
Epoch #208: loss=0.014875984697345798
Epoch #209: loss=0.012624015742353813
Epoch #210: loss=0.016857006312503642
Epoch #211: loss=0.01615851831922837
Epoch #212: loss=0.011715361312043069
Epoch #213: loss=0.012897312162744057
Epoch #214: loss=0.014072131515302948
Epoch #215: loss=0.009410946947695138
Epoch #216: loss=0.01110544141684871
Epoch #217: loss=0.01318928114199405
Epoch #218: loss=0.025122162100691264
Epoch #219: loss=0.015664796586878072
Epoch #220: loss=0.019588718632565907
Epoch #221: loss=0.01304838755516559
Epoch #222: loss=0.013499094688688833
Epoch #223: loss=0.02128937460201167
Epoch #224: loss=0.021022028254173798
Epoch #225: loss=0.02207196236847784
Epoch #226: loss=0.012155188275338362
Epoch #227: loss=0.012688877917316637
Epoch #228: loss=0.01378239803178955
Epoch #229: loss=0.015382664639514794
Epoch #230: loss=0.017308340522996363
Epoch #231: loss=0.016392677517977225
Epoch #232: loss=0.01576616672707345
Epoch #233: loss=0.011508975902679843
Epoch #234: loss=0.013912644753010383
Epoch #235: loss=0.011277803851381414
Epoch #236: loss=0.010753153583850842
Epoch #237: loss=0.01852189252042639
Epoch #238: loss=0.012349750028974815
Epoch #239: loss=0.015023851117698085
Epoch #240: loss=0.01757262520820092
Epoch #241: loss=0.02984573391119675
Epoch #242: loss=0.018488577502198646
Epoch #243: loss=0.01506051668061487
Epoch #244: loss=0.016546714883035836
Epoch #245: loss=0.012162512330084297
Epoch #246: loss=0.010907285302820103
Epoch #247: loss=0.012378719173361884
Epoch #248: loss=0.009273936223726307
Epoch #249: loss=0.010938357348324898

Training time: 4:34:50.512441

Finished.
n2one setting electricity_exchange -> weather
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/electricity_exchange_epochs_250_seed_2022/model.pkl', muti_dataset='electricity_exchange', random_seed=2022, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='weather')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.33471e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.66268e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.33471e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.8749262311778022, 'MAE': 0.7133364314504936}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='traffic_weather', random_seed=2022, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/traffic_weather_epochs_250_seed_2022
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=1.0231039714370225
Epoch #1: loss=0.3784130900796201
Epoch #2: loss=0.3021710333791939
Epoch #3: loss=0.22374667304039403
Epoch #4: loss=0.1780620812279013
Epoch #5: loss=0.14130861067154385
Epoch #6: loss=0.12951794958899956
Epoch #7: loss=0.1177313257583526
Epoch #8: loss=0.1184530802226315
Epoch #9: loss=0.09910508377796358
Epoch #10: loss=0.08018355217955268
Epoch #11: loss=0.06935451737854646
Epoch #12: loss=0.06038927623943367
Epoch #13: loss=0.06514259719768085
Epoch #14: loss=0.06393339741804117
Epoch #15: loss=0.06012369738577085
Epoch #16: loss=0.05025164848130599
Epoch #17: loss=0.04389259547780704
Epoch #18: loss=0.052129294054408616
Epoch #19: loss=0.04507578739913197
Epoch #20: loss=0.041752256818015025
Epoch #21: loss=0.05075420970427552
Epoch #22: loss=0.032296885510535306
Epoch #23: loss=0.040126611468663254
Epoch #24: loss=0.03769656800235483
Epoch #25: loss=0.03287874061395603
Epoch #26: loss=0.032846427106755775
Epoch #27: loss=0.0316304132437516
Epoch #28: loss=0.0331178749758102
Epoch #29: loss=0.03460302248296668
Epoch #30: loss=0.036557931885876284
Epoch #31: loss=0.03048178066223176
Epoch #32: loss=0.022447578985176388
Epoch #33: loss=0.030745561315737316
Epoch #34: loss=0.03363092764236687
Epoch #35: loss=0.023853118895597103
Epoch #36: loss=0.03470119120509087
Epoch #37: loss=0.02856210258041838
Epoch #38: loss=0.023668567480509974
Epoch #39: loss=0.022310109031758993
Epoch #40: loss=0.02173762069022003
Epoch #41: loss=0.02740243124925827
Epoch #42: loss=0.0282022887292876
Epoch #43: loss=0.03296901193446887
Epoch #44: loss=0.019754832875241025
Epoch #45: loss=0.022106976060640264
Epoch #46: loss=0.025276716205684484
Epoch #47: loss=0.02629685199064772
Epoch #48: loss=0.02606046619339477
Epoch #49: loss=0.024138402417438472
Epoch #50: loss=0.01902363191340129
Epoch #51: loss=0.021420916318298247
Epoch #52: loss=0.020127360449174653
Epoch #53: loss=0.02185326159094263
Epoch #54: loss=0.01934548081454023
Epoch #55: loss=0.02550896741347576
Epoch #56: loss=0.021371763753327163
Epoch #57: loss=0.018305232585171815
Epoch #58: loss=0.021251062069561222
Epoch #59: loss=0.014764324620898094
Epoch #60: loss=0.024479287851267278
Epoch #61: loss=0.018961609619263735
Epoch #62: loss=0.017920526220968732
Epoch #63: loss=0.023001364515129442
Epoch #64: loss=0.018303106006987126
Epoch #65: loss=0.01816570605972031
Epoch #66: loss=0.0200173294818399
Epoch #67: loss=0.014914626727388807
Epoch #68: loss=0.020605083867062727
Epoch #69: loss=0.02370554471797748
Epoch #70: loss=0.02355583839550843
Epoch #71: loss=0.02663034833429199
Epoch #72: loss=0.019458241921129556
Epoch #73: loss=0.015666394412275277
Epoch #74: loss=0.01984722255367963
Epoch #75: loss=0.01928449332289191
Epoch #76: loss=0.02558675764452641
Epoch #77: loss=0.012147021379450552
Epoch #78: loss=0.02986297872442404
Epoch #79: loss=0.01877853839949501
Epoch #80: loss=0.014636746735377343
Epoch #81: loss=0.013181129715457582
Epoch #82: loss=0.02035533226408334
Epoch #83: loss=0.01799646542359197
Epoch #84: loss=0.016437919356322456
Epoch #85: loss=0.023913127160585235
Epoch #86: loss=0.017783225615169835
Epoch #87: loss=0.01574986619638989
Epoch #88: loss=0.020317495344463643
Epoch #89: loss=0.014159991796511784
Epoch #90: loss=0.018218726555861795
Epoch #91: loss=0.017336286191624955
Epoch #92: loss=0.014069027655250876
Epoch #93: loss=0.018810873187724175
Epoch #94: loss=0.011467367653814427
Epoch #95: loss=0.017965232528867264
Epoch #96: loss=0.016396265121403377
Epoch #97: loss=0.023851731784652493
Epoch #98: loss=0.018013541692231263
Epoch #99: loss=0.026456677447952045
Epoch #100: loss=0.011548816957230458
Epoch #101: loss=0.014180001396763712
Epoch #102: loss=0.01157063582365014
Epoch #103: loss=0.012593751316098287
Epoch #104: loss=0.01958037566909879
Epoch #105: loss=0.014969383456159162
Epoch #106: loss=0.029266615548204665
Epoch #107: loss=0.012757109028874815
Epoch #108: loss=0.015181321733728732
Epoch #109: loss=0.008091495204024721
Epoch #110: loss=0.01545078819770031
Epoch #111: loss=0.013724961628057671
Epoch #112: loss=0.023509371300197264
Epoch #113: loss=0.01689262807452873
Epoch #114: loss=0.0127581348345298
Epoch #115: loss=0.012340948207893563
Epoch #116: loss=0.013177675188352304
Epoch #117: loss=0.019370954257242334
Epoch #118: loss=0.014883288086387003
Epoch #119: loss=0.017866931866477784
Epoch #120: loss=0.013412619772614496
Epoch #121: loss=0.01497713533034265
Epoch #122: loss=0.01812272768589324
Epoch #123: loss=0.010500864160398173
Epoch #124: loss=0.014102793650857388
Epoch #125: loss=0.01444169630321011
Epoch #126: loss=0.011907066841747298
Epoch #127: loss=0.011929005624224045
Epoch #128: loss=0.016250425256936346
Epoch #129: loss=0.025905861363597824
Epoch #130: loss=0.011732593234924722
Epoch #131: loss=0.01869633233585413
Epoch #132: loss=0.014604595904136266
Epoch #133: loss=0.01649629210514176
Epoch #134: loss=0.012784461830059634
Epoch #135: loss=0.012688749070392831
Epoch #136: loss=0.01113906964476612
Epoch #137: loss=0.020631545865480886
Epoch #138: loss=0.012707834430870748
Epoch #139: loss=0.013268951494021233
Epoch #140: loss=0.013128853674896999
Epoch #141: loss=0.010836870796091724
Epoch #142: loss=0.013712242404434218
Epoch #143: loss=0.012362920201101931
Epoch #144: loss=0.012658484427459838
Epoch #145: loss=0.020093139094659908
Epoch #146: loss=0.01626056145736633
Epoch #147: loss=0.01401910975327409
Epoch #148: loss=0.009201806700828555
Epoch #149: loss=0.01085010924655328
Epoch #150: loss=0.009460915899752936
Epoch #151: loss=0.024893441642504784
Epoch #152: loss=0.018692448545308406
Epoch #153: loss=0.008007520013904411
Epoch #154: loss=0.009499380968246679
Epoch #155: loss=0.011820143976651994
Epoch #156: loss=0.01378349859340495
Epoch #157: loss=0.013025562299058161
Epoch #158: loss=0.01153697413336976
Epoch #159: loss=0.009844219278666305
Epoch #160: loss=0.010409179732583275
Epoch #161: loss=0.019184758430288343
Epoch #162: loss=0.013960023474758165
Epoch #163: loss=0.015334334987405659
Epoch #164: loss=0.008513065014404993
Epoch #165: loss=0.013507390442051424
Epoch #166: loss=0.015619938937455011
Epoch #167: loss=0.013457288273744157
Epoch #168: loss=0.009178545232832737
Epoch #169: loss=0.02303698056110423
Epoch #170: loss=0.01345675965023622
Epoch #171: loss=0.010045998540393869
Epoch #172: loss=0.01031067947917256
Epoch #173: loss=0.008833843668505759
Epoch #174: loss=0.011007281899855205
Epoch #175: loss=0.011324971869259104
Epoch #176: loss=0.009347021741198225
Epoch #177: loss=0.01038781297763496
Epoch #178: loss=0.01334595632943092
Epoch #179: loss=0.014391191549277193
Epoch #180: loss=0.012479789760733297
Epoch #181: loss=0.01250041583686349
Epoch #182: loss=0.011897442966844071
Epoch #183: loss=0.008457501795886413
Epoch #184: loss=0.010690138615868532
Epoch #185: loss=0.012093869798968421
Epoch #186: loss=0.013341635083748884
Epoch #187: loss=0.011534431288784437
Epoch #188: loss=0.010528653533132915
Epoch #189: loss=0.01598134403262553
Epoch #190: loss=0.012018378064182413
Epoch #191: loss=0.013786297906170732
Epoch #192: loss=0.009224007512776545
Epoch #193: loss=0.007610481305156091
Epoch #194: loss=0.012236820923168157
Epoch #195: loss=0.021844882674404206
Epoch #196: loss=0.01126030695891828
Epoch #197: loss=0.010107886621971464
Epoch #198: loss=0.009119305341989053
Epoch #199: loss=0.014045829763344411
Epoch #200: loss=0.013671135402054878
Epoch #201: loss=0.008320528432057734
Epoch #202: loss=0.00941094623962075
Epoch #203: loss=0.011970218586701865
Epoch #204: loss=0.013460496295617
Epoch #205: loss=0.0106668565413939
Epoch #206: loss=0.010628161340309406
Epoch #207: loss=0.01236890304340983
Epoch #208: loss=0.01531397101014689
Epoch #209: loss=0.013282169187863605
Epoch #210: loss=0.012598021076694561
Epoch #211: loss=0.010756986645139625
Epoch #212: loss=0.008812751455389754
Epoch #213: loss=0.010603342781119077
Epoch #214: loss=0.008802194578874076
Epoch #215: loss=0.012587690520819239
Epoch #216: loss=0.013761873350756458
Epoch #217: loss=0.007499274058424923
Epoch #218: loss=0.01007503332483146
Epoch #219: loss=0.011770596882752626
Epoch #220: loss=0.007520391994074924
Epoch #221: loss=0.00924834183812383
Epoch #222: loss=0.01390123793459458
Epoch #223: loss=0.012865665799983765
Epoch #224: loss=0.009703089375938083
Epoch #225: loss=0.010423783945575983
Epoch #226: loss=0.010403621327322407
Epoch #227: loss=0.009993345754730137
Epoch #228: loss=0.016424371715779137
Epoch #229: loss=0.015338626583986485
Epoch #230: loss=0.006444861163431228
Epoch #231: loss=0.011093208208737312
Epoch #232: loss=0.01798805444064652
Epoch #233: loss=0.009919102206195757
Epoch #234: loss=0.007935524475843923
Epoch #235: loss=0.010605651802285825
Epoch #236: loss=0.01037437774640886
Epoch #237: loss=0.009823921710178823
Epoch #238: loss=0.015446974424292846
Epoch #239: loss=0.010562214289248086
Epoch #240: loss=0.008577032318629921
Epoch #241: loss=0.01026547966417589
Epoch #242: loss=0.008206510550645273
Epoch #243: loss=0.011076528636319406
Epoch #244: loss=0.00999065844615387
Epoch #245: loss=0.008484911151606718
Epoch #246: loss=0.010255269542212444
Epoch #247: loss=0.008914470976198392
Epoch #248: loss=0.019061024683659535
Epoch #249: loss=0.025498758391864686

Training time: 10:33:33.117122

Finished.
n2one setting traffic_weather -> electricity
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/traffic_weather_epochs_250_seed_2022/model.pkl', muti_dataset='traffic_weather', random_seed=2022, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='electricity')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=8.91961e-09): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.98731e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=4.79169e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=8.91961e-09): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.4095418367606453, 'MAE': 0.45434648755101503}
Finished.
------------------------- record done -------------------------
n2one setting traffic_weather -> exchange
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/traffic_weather_epochs_250_seed_2022/model.pkl', muti_dataset='traffic_weather', random_seed=2022, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='exchange')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=5.57982e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.44462963847139464, 'MAE': 0.4299872793530543}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='traffic_exchange', random_seed=2022, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/traffic_exchange_epochs_250_seed_2022
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=0.9990753482299289
Epoch #1: loss=0.36249259802801853
Epoch #2: loss=0.26129442720577634
Epoch #3: loss=0.1902983803944341
Epoch #4: loss=0.15824927339951197
Epoch #5: loss=0.1312163837211228
Epoch #6: loss=0.12479155718980507
Epoch #7: loss=0.09822247134328917
Epoch #8: loss=0.07672614904184794
Epoch #9: loss=0.08045334748698976
Epoch #10: loss=0.09278518198507613
Epoch #11: loss=0.06472049142159093
Epoch #12: loss=0.06708715977468367
Epoch #13: loss=0.05627655166297637
Epoch #14: loss=0.0535528072479834
Epoch #15: loss=0.04985998371567834
Epoch #16: loss=0.04618629743291557
Epoch #17: loss=0.03948787465723681
Epoch #18: loss=0.04217353839207783
Epoch #19: loss=0.04256797818568717
Epoch #20: loss=0.04479653588188384
Epoch #21: loss=0.03560405014746491
Epoch #22: loss=0.03247425967222615
Epoch #23: loss=0.039347393233088196
Epoch #24: loss=0.034328947872078665
Epoch #25: loss=0.03105922815598676
Epoch #26: loss=0.054294533120741235
Epoch #27: loss=0.031689506062793533
Epoch #28: loss=0.03466617368381128
Epoch #29: loss=0.029138641061941056
Epoch #30: loss=0.031879816030221726
Epoch #31: loss=0.03084453460425888
Epoch #32: loss=0.024187708333209584
Epoch #33: loss=0.02660069749001467
Epoch #34: loss=0.029617454256091653
Epoch #35: loss=0.033112234066256546
Epoch #36: loss=0.030474639193010356
Epoch #37: loss=0.024284429423880347
Epoch #38: loss=0.02784234913709659
Epoch #39: loss=0.03708201501780757
Epoch #40: loss=0.035503172131533596
Epoch #41: loss=0.026560118311785976
Epoch #42: loss=0.030659720860154036
Epoch #43: loss=0.02373448355389684
Epoch #44: loss=0.029438194406657726
Epoch #45: loss=0.028518256551088316
Epoch #46: loss=0.021428777684388018
Epoch #47: loss=0.019589721303347542
Epoch #48: loss=0.023321278542668248
Epoch #49: loss=0.01985747247443256
Epoch #50: loss=0.023924936077841182
Epoch #51: loss=0.03812694006382712
Epoch #52: loss=0.026577267615357413
Epoch #53: loss=0.019998204696859267
Epoch #54: loss=0.0201121590065438
Epoch #55: loss=0.019303394632612023
Epoch #56: loss=0.0243139677645569
Epoch #57: loss=0.017831991015847815
Epoch #58: loss=0.024374979876561207
Epoch #59: loss=0.019609308039673187
Epoch #60: loss=0.021377588878185007
Epoch #61: loss=0.020228781491509994
Epoch #62: loss=0.029230087109635337
Epoch #63: loss=0.01727234556141641
Epoch #64: loss=0.020950797562261283
Epoch #65: loss=0.02134328607965566
Epoch #66: loss=0.02234402104339766
Epoch #67: loss=0.01910468332841457
Epoch #68: loss=0.017504212979813395
Epoch #69: loss=0.018214407738300853
Epoch #70: loss=0.02022982425947577
Epoch #71: loss=0.027608529496844322
Epoch #72: loss=0.01936179848334982
Epoch #73: loss=0.015625468446323166
Epoch #74: loss=0.015597269304513854
Epoch #75: loss=0.020389882214039555
Epoch #76: loss=0.0195104485411122
Epoch #77: loss=0.022204147314120234
Epoch #78: loss=0.022955356529296974
Epoch #79: loss=0.022748718688855932
Epoch #80: loss=0.0193357673540227
Epoch #81: loss=0.019796628681621676
Epoch #82: loss=0.01517769456034842
Epoch #83: loss=0.020519872118809915
Epoch #84: loss=0.014635497691804254
Epoch #85: loss=0.017988900648197158
Epoch #86: loss=0.01305711519159778
Epoch #87: loss=0.01937845783902268
Epoch #88: loss=0.024111536496128867
Epoch #89: loss=0.01571641910591045
Epoch #90: loss=0.016814194769506883
Epoch #91: loss=0.01904726598951892
Epoch #92: loss=0.019023261117433122
Epoch #93: loss=0.015357298313355042
Epoch #94: loss=0.011909336552971267
Epoch #95: loss=0.015304836647480662
Epoch #96: loss=0.017453828602882058
Epoch #97: loss=0.01710871943057215
Epoch #98: loss=0.016410580156411142
Epoch #99: loss=0.015462970242406717
Epoch #100: loss=0.012510601238745008
Epoch #101: loss=0.022286859828839614
Epoch #102: loss=0.015206923092054417
Epoch #103: loss=0.016694769992669722
Epoch #104: loss=0.016863538824716293
Epoch #105: loss=0.013411508354088308
Epoch #106: loss=0.01991736494200759
Epoch #107: loss=0.016673337907099496
Epoch #108: loss=0.016384688790602578
Epoch #109: loss=0.012433525234000521
Epoch #110: loss=0.01597503854037763
Epoch #111: loss=0.01647946123631227
Epoch #112: loss=0.015324741083774391
Epoch #113: loss=0.013884288497736152
Epoch #114: loss=0.016680003796709465
Epoch #115: loss=0.017291941867014608
Epoch #116: loss=0.012735132323213365
Epoch #117: loss=0.013768955882693555
Epoch #118: loss=0.01979233750248202
Epoch #119: loss=0.029394088330847244
Epoch #120: loss=0.012075619767660853
Epoch #121: loss=0.016396569978979675
Epoch #122: loss=0.02074580942976406
Epoch #123: loss=0.010854167005765766
Epoch #124: loss=0.01691162660306496
Epoch #125: loss=0.016938373123110945
Epoch #126: loss=0.01732342939699422
Epoch #127: loss=0.011605883097017168
Epoch #128: loss=0.010075568366409734
Epoch #129: loss=0.01561218143016603
Epoch #130: loss=0.01378761499433714
Epoch #131: loss=0.017216898418458488
Epoch #132: loss=0.01393222313651269
Epoch #133: loss=0.01487859209128669
Epoch #134: loss=0.017022240123558983
Epoch #135: loss=0.015705977646993934
Epoch #136: loss=0.01407672244454627
Epoch #137: loss=0.012346446572830726
Epoch #138: loss=0.014323886777003345
Epoch #139: loss=0.011994492076019537
Epoch #140: loss=0.013885434675860037
Epoch #141: loss=0.01373995144437696
Epoch #142: loss=0.014442629749443778
Epoch #143: loss=0.02334993997904547
Epoch #144: loss=0.012954875001675133
Epoch #145: loss=0.02384129420649421
Epoch #146: loss=0.013720154434900152
Epoch #147: loss=0.011850181171732779
Epoch #148: loss=0.01439633608555094
Epoch #149: loss=0.016181363659273917
Epoch #150: loss=0.014669440076057532
Epoch #151: loss=0.01352099409800601
Epoch #152: loss=0.016130394352654842
Epoch #153: loss=0.009904492584233782
Epoch #154: loss=0.018839177288397364
Epoch #155: loss=0.014175941526903851
Epoch #156: loss=0.013853567423286911
Epoch #157: loss=0.010429466264867678
Epoch #158: loss=0.02127688084882898
Epoch #159: loss=0.015168362497030918
Epoch #160: loss=0.013956662375264502
Epoch #161: loss=0.008818512456893022
Epoch #162: loss=0.014736614616831293
Epoch #163: loss=0.011773145129842597
Epoch #164: loss=0.013356407713640116
Epoch #165: loss=0.01591726526545746
Epoch #166: loss=0.013569630587529712
Epoch #167: loss=0.013902165829612146
Epoch #168: loss=0.010871756682473492
Epoch #169: loss=0.013783732068225816
Epoch #170: loss=0.01539498652156239
Epoch #171: loss=0.017613078895006223
Epoch #172: loss=0.015118765473618685
Epoch #173: loss=0.016856974489276363
Epoch #174: loss=0.013300127332197802
Epoch #175: loss=0.01574479592673015
Epoch #176: loss=0.012679514151144152
Epoch #177: loss=0.013675292052566769
Epoch #178: loss=0.011381357942270276
Epoch #179: loss=0.013052029359280185
Epoch #180: loss=0.016600709127586058
Epoch #181: loss=0.014454049537535062
Epoch #182: loss=0.011960676164786471
Epoch #183: loss=0.02015726349798925
Epoch #184: loss=0.012335887222200225
Epoch #185: loss=0.01209156005241876
Epoch #186: loss=0.01679708424553505
Epoch #187: loss=0.010880338253895618
Epoch #188: loss=0.010925082859734404
Epoch #189: loss=0.013367049479602846
Epoch #190: loss=0.01388636096106339
Epoch #191: loss=0.01728845326090767
Epoch #192: loss=0.007681269461778126
Epoch #193: loss=0.013901047743559177
Epoch #194: loss=0.01204489532735362
Epoch #195: loss=0.020215381330364896
Epoch #196: loss=0.009000670948787056
Epoch #197: loss=0.011780248758892914
Epoch #198: loss=0.01269503598738376
Epoch #199: loss=0.011503842851781318
Epoch #200: loss=0.014494280234777406
Epoch #201: loss=0.013368924867814064
Epoch #202: loss=0.010415281096507458
Epoch #203: loss=0.007194404929693007
Epoch #204: loss=0.01492586634406183
Epoch #205: loss=0.011488827267205397
Epoch #206: loss=0.008799986602957948
Epoch #207: loss=0.01265724050784323
Epoch #208: loss=0.014686640853048221
Epoch #209: loss=0.014735275810422515
Epoch #210: loss=0.015067614406503164
Epoch #211: loss=0.010792667998030919
Epoch #212: loss=0.01520115345560587
Epoch #213: loss=0.016066342186324578
Epoch #214: loss=0.011592688835921815
Epoch #215: loss=0.016022146415853894
Epoch #216: loss=0.009381859475511244
Epoch #217: loss=0.012681919103990117
Epoch #218: loss=0.018444416546042012
Epoch #219: loss=0.013586865316162518
Epoch #220: loss=0.014618663336638757
Epoch #221: loss=0.012973518101210791
Epoch #222: loss=0.017054558056063786
Epoch #223: loss=0.02440955811282205
Epoch #224: loss=0.010591107086297232
Epoch #225: loss=0.028537366460341135
Epoch #226: loss=0.009576621855142119
Epoch #227: loss=0.00965753615885336
Epoch #228: loss=0.008634430560920302
Epoch #229: loss=0.0141265978723739
Epoch #230: loss=0.01006208565955896
Epoch #231: loss=0.01753753936169107
Epoch #232: loss=0.012157587921976154
Epoch #233: loss=0.012839717494695844
Epoch #234: loss=0.014508251876646557
Epoch #235: loss=0.014703675899882106
Epoch #236: loss=0.01131700100487596
Epoch #237: loss=0.00832224348492234
Epoch #238: loss=0.011795917885671048
Epoch #239: loss=0.01321981837322344
Epoch #240: loss=0.011015648870177275
Epoch #241: loss=0.022280057550634755
Epoch #242: loss=0.013621372585860617
Epoch #243: loss=0.011381973695122349
Epoch #244: loss=0.009590341202679207
Epoch #245: loss=0.008899853904276615
Epoch #246: loss=0.012527887081716012
Epoch #247: loss=0.012280548677576341
Epoch #248: loss=0.01380170480587022
Epoch #249: loss=0.010145946298244184

Training time: 10:14:51.009242

Finished.
n2one setting traffic_exchange -> electricity
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/traffic_exchange_epochs_250_seed_2022/model.pkl', muti_dataset='traffic_exchange', random_seed=2022, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='electricity')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=8.26857e-09): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.67545e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=4.31083e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=8.26857e-09): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.40224001897367323, 'MAE': 0.45232957409242913}
Finished.
------------------------- record done -------------------------
n2one setting traffic_exchange -> weather
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/traffic_exchange_epochs_250_seed_2022/model.pkl', muti_dataset='traffic_exchange', random_seed=2022, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='weather')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.67766e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=3.42063e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.67766e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.5007486965682182, 'MAE': 0.5451254113225644}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='weather_exchange', random_seed=2022, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/weather_exchange_epochs_250_seed_2022
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=13.655233663671156
Epoch #1: loss=3.8384889118811665
Epoch #2: loss=2.985588203458225
Epoch #3: loss=3.099256554070641
Epoch #4: loss=2.359436943250544
Epoch #5: loss=2.227027311044581
Epoch #6: loss=2.3112102817086613
Epoch #7: loss=2.1067532581441544
Epoch #8: loss=2.0189207371543434
Epoch #9: loss=1.8998937676934635
Epoch #10: loss=1.986519662772908
Epoch #11: loss=1.7582893406643587
Epoch #12: loss=1.763616863419028
Epoch #13: loss=1.7418174322913675
Epoch #14: loss=1.6077024095198686
Epoch #15: loss=1.558286403908449
Epoch #16: loss=1.4982425850980423
Epoch #17: loss=1.378781241529128
Epoch #18: loss=1.4126274200046764
Epoch #19: loss=1.340227063964395
Epoch #20: loss=1.3177307528607987
Epoch #21: loss=1.3127545048208797
Epoch #22: loss=1.201180430019603
Epoch #23: loss=1.2486370486371658
Epoch #24: loss=1.1565928511759813
Epoch #25: loss=1.1054239798994625
Epoch #26: loss=1.1162630810457117
Epoch #27: loss=1.0612371493788326
Epoch #28: loss=1.0639735582996817
Epoch #29: loss=1.071172973688911
Epoch #30: loss=1.0259060544126175
Epoch #31: loss=1.0801735362585854
Epoch #32: loss=1.11786949634552
Epoch #33: loss=1.0427234681213604
Epoch #34: loss=0.9882729702136096
Epoch #35: loss=0.8939521347775179
Epoch #36: loss=1.0878854814697714
Epoch #37: loss=0.9766325389637667
Epoch #38: loss=0.8829865771181443
Epoch #39: loss=0.8986775542006773
Epoch #40: loss=0.8665016381179586
Epoch #41: loss=0.7977268836077522
Epoch #42: loss=0.8318219693268046
Epoch #43: loss=0.7965733899789698
Epoch #44: loss=0.7388249881127301
Epoch #45: loss=0.7090604156255722
Epoch #46: loss=0.91459000285934
Epoch #47: loss=0.8286034570020788
Epoch #48: loss=0.6928443829802906
Epoch #49: loss=0.7674727019141702
Epoch #50: loss=0.7080723003429525
Epoch #51: loss=0.7381144271177404
Epoch #52: loss=0.6350158339037615
Epoch #53: loss=0.6631450109622058
Epoch #54: loss=0.810539697022999
Epoch #55: loss=0.7822315429939943
Epoch #56: loss=0.6964038347496706
Epoch #57: loss=0.8270436148433125
Epoch #58: loss=0.7373191538979026
Epoch #59: loss=0.6826756254715078
Epoch #60: loss=0.6115157525329029
Epoch #61: loss=0.6345526146538117
Epoch #62: loss=0.682864652837024
Epoch #63: loss=0.6577615176930147
Epoch #64: loss=0.6552299874670365
Epoch #65: loss=0.7669835914583767
Epoch #66: loss=0.722395477049491
Epoch #67: loss=0.6023503980215859
Epoch #68: loss=0.6005452319103128
Epoch #69: loss=0.543461437611019
Epoch #70: loss=0.4821605287930545
Epoch #71: loss=0.5848462117068908
Epoch #72: loss=0.5106094768818688
Epoch #73: loss=0.515510775587138
Epoch #74: loss=0.5159340833916384
Epoch #75: loss=0.48970503929783316
Epoch #76: loss=0.5184157464434119
Epoch #77: loss=0.5416161286480287
Epoch #78: loss=0.5684848269995522
Epoch #79: loss=0.4902533134993385
Epoch #80: loss=0.48993218471022215
Epoch #81: loss=0.448550493839909
Epoch #82: loss=0.5126947147004745
Epoch #83: loss=0.4443140380522784
Epoch #84: loss=0.42484426498413086
Epoch #85: loss=0.417361575014451
Epoch #86: loss=0.4023481375154327
Epoch #87: loss=0.3797544902738403
Epoch #88: loss=0.40149467701421065
Epoch #89: loss=0.4218092950827935
Epoch #90: loss=0.4542180461918606
Epoch #91: loss=0.44582410156726837
Epoch #92: loss=0.37932398739983053
Epoch #93: loss=0.38688355859588175
Epoch #94: loss=0.46448273080236774
Epoch #95: loss=0.43451062414576025
Epoch #96: loss=0.38223909981110515
Epoch #97: loss=0.3934524405528517
Epoch #98: loss=0.32954741400830884
Epoch #99: loss=0.3693697229027748
Epoch #100: loss=0.3520213996662813
Epoch #101: loss=0.3584871191312285
Epoch #102: loss=0.3616977250751327
Epoch #103: loss=0.3499638070078457
Epoch #104: loss=0.33738596825038686
Epoch #105: loss=0.3685709785889177
Epoch #106: loss=0.3672906559179811
Epoch #107: loss=0.4085129442460397
Epoch #108: loss=0.2964000307461795
Epoch #109: loss=0.3161308375351569
Epoch #110: loss=0.3905452169916209
Epoch #111: loss=0.28351443063686876
Epoch #112: loss=0.3160761163076934
Epoch #113: loss=0.2839665103922872
Epoch #114: loss=0.25271808049258065
Epoch #115: loss=0.3006444349884987
Epoch #116: loss=0.2779616433031419
Epoch #117: loss=0.24949783634613543
Epoch #118: loss=0.24681845833273494
Epoch #119: loss=0.23477098849766395
Epoch #120: loss=0.24900019300334594
Epoch #121: loss=0.2580921794561779
Epoch #122: loss=0.24626345362733393
Epoch #123: loss=0.2787965266581844
Epoch #124: loss=0.3224206056226702
Epoch #125: loss=0.3124383468838299
Epoch #126: loss=0.2827799410066184
Epoch #127: loss=0.2471463399774888
Epoch #128: loss=0.25038903469548507
Epoch #129: loss=0.25964205613469377
Epoch #130: loss=0.3048000931739807
Epoch #131: loss=0.24861316054182894
Epoch #132: loss=0.24756864480235996
Epoch #133: loss=0.25928495364154086
Epoch #134: loss=0.23538646364913268
Epoch #135: loss=0.2388797325684744
Epoch #136: loss=0.2735764653805424
Epoch #137: loss=0.28587113452308316
Epoch #138: loss=0.3463836419670021
Epoch #139: loss=0.43251966948018356
Epoch #140: loss=0.498333561946364
Epoch #141: loss=0.34240053703679757
Epoch #142: loss=0.37768542985705766
Epoch #143: loss=0.30498550810358105
Epoch #144: loss=0.3074711359599057
Epoch #145: loss=0.22184878763030558
Epoch #146: loss=0.2080303290311028
Epoch #147: loss=0.1491494406672085
Epoch #148: loss=0.3006633941960685
Epoch #149: loss=0.2254675975178971
Epoch #150: loss=0.23035935893216553
Epoch #151: loss=0.21029508508303585
Epoch #152: loss=0.1979342110674171
Epoch #153: loss=0.17358375811839805
Epoch #154: loss=0.2556886907447787
Epoch #155: loss=0.24351592756369533
Epoch #156: loss=0.18887477236635544
Epoch #157: loss=0.1882218777695123
Epoch #158: loss=0.26781128807102933
Epoch #159: loss=0.20162488673539722
Epoch #160: loss=0.2004027889274499
Epoch #161: loss=0.17223233674817226
Epoch #162: loss=0.18278048340888584
Epoch #163: loss=0.18097488856052651
Epoch #164: loss=0.19257998959544828
Epoch #165: loss=0.18978880652609995
Epoch #166: loss=0.15708457810037277
Epoch #167: loss=0.21864652315921643
Epoch #168: loss=0.18307134257081678
Epoch #169: loss=0.19732308256275513
Epoch #170: loss=0.17043158762595234
Epoch #171: loss=0.19251736374024084
Epoch #172: loss=0.13073704972424927
Epoch #173: loss=0.12531618346624515
Epoch #174: loss=0.14828436591607683
Epoch #175: loss=0.12980202464934656
Epoch #176: loss=0.2429144908400143
Epoch #177: loss=0.21623725761823795
Epoch #178: loss=0.1631610118510092
Epoch #179: loss=0.20340660959482193
Epoch #180: loss=0.19743157649303184
Epoch #181: loss=0.23969069112311392
Epoch #182: loss=0.17565411636058023
Epoch #183: loss=0.1217389926314354
Epoch #184: loss=0.11812391997698475
Epoch #185: loss=0.12801803790909402
Epoch #186: loss=0.2804406792582834
Epoch #187: loss=0.14800366156679742
Epoch #188: loss=0.1411834074293866
Epoch #189: loss=0.14249328973100467
Epoch #190: loss=0.18568414646913023
Epoch #191: loss=0.16932303513235905
Epoch #192: loss=0.16488316567505107
Epoch #193: loss=0.21860738514977343
Epoch #194: loss=0.14238105856758707
Epoch #195: loss=0.15151046851978583
Epoch #196: loss=0.1500435517553021
Epoch #197: loss=0.16818306211601286
Epoch #198: loss=0.16697359917794957
Epoch #199: loss=0.14977775393601725
Epoch #200: loss=0.17031613200464668
Epoch #201: loss=0.1466506033697549
Epoch #202: loss=0.1298311233301373
Epoch #203: loss=0.15401778107180314
Epoch #204: loss=0.1677417309187791
Epoch #205: loss=0.2433783049311708
Epoch #206: loss=0.29279765771592364
Epoch #207: loss=0.1379142266643398
Epoch #208: loss=0.09903617893510006
Epoch #209: loss=0.1674224575433661
Epoch #210: loss=0.09622246152995263
Epoch #211: loss=0.13652497053365498
Epoch #212: loss=0.1177438923760372
Epoch #213: loss=0.12021201876375605
Epoch #214: loss=0.11079313606023788
Epoch #215: loss=0.09421372331459732
Epoch #216: loss=0.11486624257967752
Epoch #217: loss=0.08207759781576254
Epoch #218: loss=0.11312950742157067
Epoch #219: loss=0.13176979458726504
Epoch #220: loss=0.09591090870911584
Epoch #221: loss=0.1000654880395707
Epoch #222: loss=0.09325792465139837
Epoch #223: loss=0.10317384643370614
Epoch #224: loss=0.09586658465730793
Epoch #225: loss=0.19036616182283445
Epoch #226: loss=0.13278712282943375
Epoch #227: loss=0.17176297219360576
Epoch #228: loss=0.17839802292120807
Epoch #229: loss=0.17775612565524437
Epoch #230: loss=0.15013933417332523
Epoch #231: loss=0.16244914092342644
Epoch #232: loss=0.15197753457023816
Epoch #233: loss=0.21715733765021844
Epoch #234: loss=0.16349714337026372
Epoch #235: loss=0.19885015728719094
Epoch #236: loss=0.13554302257869175
Epoch #237: loss=0.1321822445401374
Epoch #238: loss=0.10498037205680329
Epoch #239: loss=0.09750463057528524
Epoch #240: loss=0.12408125159495018
Epoch #241: loss=0.08607138003058293
Epoch #242: loss=0.0740722154321916
Epoch #243: loss=0.08051119977608323
Epoch #244: loss=0.06989525685853817
Epoch #245: loss=0.11231196195106297
Epoch #246: loss=0.10576131759101853
Epoch #247: loss=0.12396012794445543
Epoch #248: loss=0.08594062464202151
Epoch #249: loss=0.08982334710548029

Training time: 0:32:25.123246

Finished.
n2one setting weather_exchange -> electricity
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/weather_exchange_epochs_250_seed_2022/model.pkl', muti_dataset='weather_exchange', random_seed=2022, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='electricity')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.33274e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.5563e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.33274e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.3749680650647791, 'MAE': 0.4301120339319825}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='etth1_etth2', random_seed=2023, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/etth1_etth2_epochs_250_seed_2023
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=4.787172947611127
Epoch #1: loss=2.7122524295534407
Epoch #2: loss=2.300441418375288
Epoch #3: loss=2.0130327939987183
Epoch #4: loss=1.7968617592539107
Epoch #5: loss=1.7621307032448905
Epoch #6: loss=1.6526750922203064
Epoch #7: loss=1.535720842225211
Epoch #8: loss=1.4629959974970137
Epoch #9: loss=1.364508501120976
Epoch #10: loss=1.2843746117183141
Epoch #11: loss=1.3749252472605025
Epoch #12: loss=1.1808244245392936
Epoch #13: loss=1.145812336887632
Epoch #14: loss=1.0369614660739899
Epoch #15: loss=1.0636620904718126
Epoch #16: loss=0.9893808577741895
Epoch #17: loss=0.9818002283573151
Epoch #18: loss=0.9505029022693634
Epoch #19: loss=0.9227800624711173
Epoch #20: loss=0.9322397708892822
Epoch #21: loss=0.8349458702972957
Epoch #22: loss=0.7938388884067535
Epoch #23: loss=0.9016260419573102
Epoch #24: loss=0.8570517344134194
Epoch #25: loss=0.8602245024272374
Epoch #26: loss=0.7739431474890027
Epoch #27: loss=0.8101354241371155
Epoch #28: loss=0.7478036667619433
Epoch #29: loss=0.6700499738965716
Epoch #30: loss=0.6373118332454136
Epoch #31: loss=0.6362148970365524
Epoch #32: loss=0.7152315250464848
Epoch #33: loss=0.606820843049458
Epoch #34: loss=0.6255607690129962
Epoch #35: loss=0.6641575332198825
Epoch #36: loss=0.6910580268927983
Epoch #37: loss=0.6154595698629107
Epoch #38: loss=0.5771804537091937
Epoch #39: loss=0.4986929041998727
Epoch #40: loss=0.5870593530791146
Epoch #41: loss=0.48200282880238127
Epoch #42: loss=0.45313097749437603
Epoch #43: loss=0.5661008059978485
Epoch #44: loss=0.4685718183006559
Epoch #45: loss=0.47844134484018597
Epoch #46: loss=0.5873266862971442
Epoch #47: loss=0.6069629447800773
Epoch #48: loss=0.4326994014637811
Epoch #49: loss=0.4369136222771236
Epoch #50: loss=0.43017260091645376
Epoch #51: loss=0.442461011665208
Epoch #52: loss=0.5168840374265399
Epoch #53: loss=0.36485842508929117
Epoch #54: loss=0.4851125052997044
Epoch #55: loss=0.42641845771244596
Epoch #56: loss=0.49462596007755827
Epoch #57: loss=0.5332683495112828
Epoch #58: loss=0.41154952134404865
Epoch #59: loss=0.43750783801078796
Epoch #60: loss=0.4925497089113508
Epoch #61: loss=0.496636301279068
Epoch #62: loss=0.3864630418164389
Epoch #63: loss=0.432960290993963
Epoch #64: loss=0.34258624485560824
Epoch #65: loss=0.500002557677882
Epoch #66: loss=0.3628157952002117
Epoch #67: loss=0.437439637524741
Epoch #68: loss=0.49378147082669394
Epoch #69: loss=0.3639508219701903
Epoch #70: loss=0.34534075430461336
Epoch #71: loss=0.3501406620655741
Epoch #72: loss=0.42345776728221346
Epoch #73: loss=0.3563086465001106
Epoch #74: loss=0.3165508380958012
Epoch #75: loss=0.36290193349123
Epoch #76: loss=0.2975425124168396
Epoch #77: loss=0.320443223629679
Epoch #78: loss=0.4688605772597449
Epoch #79: loss=0.33939690036433084
Epoch #80: loss=0.3382004052400589
Epoch #81: loss=0.34108969462769373
Epoch #82: loss=0.29067813392196384
Epoch #83: loss=0.34069593782935825
Epoch #84: loss=0.4244434354560716
Epoch #85: loss=0.35018005860703333
Epoch #86: loss=0.27680573080267223
Epoch #87: loss=0.37459007437740055
Epoch #88: loss=0.38038266769477297
Epoch #89: loss=0.2844656620706831
Epoch #90: loss=0.30423458026988165
Epoch #91: loss=0.2839089898126466
Epoch #92: loss=0.2992738357612065
Epoch #93: loss=0.38553281128406525
Epoch #94: loss=0.2863037107246263
Epoch #95: loss=0.26739299510206493
Epoch #96: loss=0.26129033097199034
Epoch #97: loss=0.3462830718074526
Epoch #98: loss=0.30341358695711407
Epoch #99: loss=0.29787885397672653
Epoch #100: loss=0.3530327028461865
Epoch #101: loss=0.2968781143426895
Epoch #102: loss=0.2522643059492111
Epoch #103: loss=0.2612274991614478
Epoch #104: loss=0.3134208789893559
Epoch #105: loss=0.3053091966680118
Epoch #106: loss=0.39549114235809874
Epoch #107: loss=0.4333385644214494
Epoch #108: loss=0.31974038055964876
Epoch #109: loss=0.2315053003174918
Epoch #110: loss=0.2338810499225344
Epoch #111: loss=0.2622092291712761
Epoch #112: loss=0.2644811613219125
Epoch #113: loss=0.26193127568278995
Epoch #114: loss=0.23157428205013275
Epoch #115: loss=0.26638048248631613
Epoch #116: loss=0.3011999673077038
Epoch #117: loss=0.2576013739619936
Epoch #118: loss=0.21907806609358108
Epoch #119: loss=0.255375899374485
Epoch #120: loss=0.22498843499592372
Epoch #121: loss=0.26296425823654446
Epoch #122: loss=0.35043792320149286
Epoch #123: loss=0.22977764691625321
Epoch #124: loss=0.2717902213335037
Epoch #125: loss=0.24326777032443456
Epoch #126: loss=0.26449894372906
Epoch #127: loss=0.25760887776102337
Epoch #128: loss=0.22488871216773987
Epoch #129: loss=0.19282282782452448
Epoch #130: loss=0.20727399098021643
Epoch #131: loss=0.25238389734710964
Epoch #132: loss=0.339848378939288
Epoch #133: loss=0.2894881154809679
Epoch #134: loss=0.28180061493601116
Epoch #135: loss=0.22377855969326838
Epoch #136: loss=0.3580925869090216
Epoch #137: loss=0.2146230669958251
Epoch #138: loss=0.23234905941145761
Epoch #139: loss=0.19605818390846252
Epoch #140: loss=0.33861090136425837
Epoch #141: loss=0.3227836883493832
Epoch #142: loss=0.22006084769964218
Epoch #143: loss=0.2682126302804266
Epoch #144: loss=0.2609683211360659
Epoch #145: loss=0.23780234796660288
Epoch #146: loss=0.17121130973100662
Epoch #147: loss=0.27745725001607624
Epoch #148: loss=0.22851816351924623
Epoch #149: loss=0.22790708712169103
Epoch #150: loss=0.21858483765806472
Epoch #151: loss=0.21392442552106722
Epoch #152: loss=0.20903376170567103
Epoch #153: loss=0.2345518959420068
Epoch #154: loss=0.21446587145328522
Epoch #155: loss=0.213810672717435
Epoch #156: loss=0.190471137208598
Epoch #157: loss=0.303738965519837
Epoch #158: loss=0.3117420535002436
Epoch #159: loss=0.24824508598872594
Epoch #160: loss=0.2713567987084389
Epoch #161: loss=0.26448051099266323
Epoch #162: loss=0.23684888545955932
Epoch #163: loss=0.19873999804258347
Epoch #164: loss=0.22350208567721502
Epoch #165: loss=0.18264390262109892
Epoch #166: loss=0.20006329034055984
Epoch #167: loss=0.201826882149492
Epoch #168: loss=0.19119435069816454
Epoch #169: loss=0.26925869498934063
Epoch #170: loss=0.2973767763801983
Epoch #171: loss=0.21065240513001168
Epoch #172: loss=0.19511285796761513
Epoch #173: loss=0.21407998140369142
Epoch #174: loss=0.1818362304142543
Epoch #175: loss=0.21336155278342112
Epoch #176: loss=0.1512697051678385
Epoch #177: loss=0.16530934614794596
Epoch #178: loss=0.1384149422602994
Epoch #179: loss=0.22796926168458803
Epoch #180: loss=0.24575813008206232
Epoch #181: loss=0.22319190523454122
Epoch #182: loss=0.2027461033846651
Epoch #183: loss=0.14979533638272965
Epoch #184: loss=0.17281726747751236
Epoch #185: loss=0.19076624140143394
Epoch #186: loss=0.1594398585813386
Epoch #187: loss=0.2144890794796603
Epoch #188: loss=0.16368292751056807
Epoch #189: loss=0.1736574981893812
Epoch #190: loss=0.1673209677849497
Epoch #191: loss=0.1753318810037204
Epoch #192: loss=0.1440726492021765
Epoch #193: loss=0.16574550792574883
Epoch #194: loss=0.4897953527314322
Epoch #195: loss=0.22178239481789724
Epoch #196: loss=0.20745935663580894
Epoch #197: loss=0.17291056417993136
Epoch #198: loss=0.22667700478008815
Epoch #199: loss=0.16505384977374757
Epoch #200: loss=0.19906889008624212
Epoch #201: loss=0.17774926871061325
Epoch #202: loss=0.1651367380150727
Epoch #203: loss=0.13887724759323256
Epoch #204: loss=0.13018258554594858
Epoch #205: loss=0.15631937075938498
Epoch #206: loss=0.21658711028950556
Epoch #207: loss=0.2462358166064535
Epoch #208: loss=0.14858378576380865
Epoch #209: loss=0.174797851060118
Epoch #210: loss=0.20874212362936564
Epoch #211: loss=0.14755389679755485
Epoch #212: loss=0.13139851604189193
Epoch #213: loss=0.24145718131746566
Epoch #214: loss=0.15930130279489926
Epoch #215: loss=0.15740248560905457
Epoch #216: loss=0.17451426599706923
Epoch #217: loss=0.198248419378485
Epoch #218: loss=0.15843092117990767
Epoch #219: loss=0.1433337059404169
Epoch #220: loss=0.13081909024289676
Epoch #221: loss=0.22255251024450576
Epoch #222: loss=0.164209139666387
Epoch #223: loss=0.2264156937599182
Epoch #224: loss=0.14613778995616095
Epoch #225: loss=0.14384837608252252
Epoch #226: loss=0.12860593519040517
Epoch #227: loss=0.24844184251768248
Epoch #228: loss=0.2025566122361592
Epoch #229: loss=0.2920521896864687
Epoch #230: loss=0.19382682708757265
Epoch #231: loss=0.20459624166999543
Epoch #232: loss=0.1685662062040397
Epoch #233: loss=0.1309974119067192
Epoch #234: loss=0.1877937646848815
Epoch #235: loss=0.15409475352082933
Epoch #236: loss=0.14063708005206926
Epoch #237: loss=0.15092376034174645
Epoch #238: loss=0.1730973039354597
Epoch #239: loss=0.1618801594844886
Epoch #240: loss=0.15027170415435517
Epoch #241: loss=0.13226610661617347
Epoch #242: loss=0.10578940861991473
Epoch #243: loss=0.2148418485053948
Epoch #244: loss=0.20183458551764488
Epoch #245: loss=0.1737376313124384
Epoch #246: loss=0.15083263122609683
Epoch #247: loss=0.13124049508145877
Epoch #248: loss=0.17328984716108867
Epoch #249: loss=0.15674146903412683

Training time: 0:10:04.067111

Finished.
n2one setting etth1_etth2 -> electricity
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth1_etth2_epochs_250_seed_2023/model.pkl', muti_dataset='etth1_etth2', random_seed=2023, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='electricity')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.41549e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.71887e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.41549e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.37935389831558886, 'MAE': 0.4280534786534175}
Finished.
------------------------- record done -------------------------
n2one setting etth1_etth2 -> weather
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth1_etth2_epochs_250_seed_2023/model.pkl', muti_dataset='etth1_etth2', random_seed=2023, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='weather')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.33816e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.56049e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.33816e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.6630022772193769, 'MAE': 0.6126568593662408}
Finished.
------------------------- record done -------------------------
n2one setting etth1_etth2 -> exchange
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth1_etth2_epochs_250_seed_2023/model.pkl', muti_dataset='etth1_etth2', random_seed=2023, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='exchange')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=5.19825e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.1787150554248878, 'MAE': 0.29278825595560354}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='etth1_ettm1', random_seed=2023, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/etth1_ettm1_epochs_250_seed_2023
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=5.367358538839552
Epoch #1: loss=2.823339859644572
Epoch #2: loss=2.4030986030896506
Epoch #3: loss=2.233657287226783
Epoch #4: loss=2.045653912756178
Epoch #5: loss=1.9074417683813307
Epoch #6: loss=1.7511520849333868
Epoch #7: loss=1.6868373155593872
Epoch #8: loss=1.624306129084693
Epoch #9: loss=1.5290883911980524
Epoch #10: loss=1.425053556760152
Epoch #11: loss=1.3352217475573223
Epoch #12: loss=1.275715000099606
Epoch #13: loss=1.2597819831636217
Epoch #14: loss=1.1653535664081573
Epoch #15: loss=1.1302544209692214
Epoch #16: loss=1.2068275809288025
Epoch #17: loss=1.0669863985644445
Epoch #18: loss=0.9892719884713491
Epoch #19: loss=0.9443914989630381
Epoch #20: loss=0.9584659139315287
Epoch #21: loss=0.9407855702771081
Epoch #22: loss=0.986745234992769
Epoch #23: loss=0.9521108137236701
Epoch #24: loss=0.8878818485471938
Epoch #25: loss=0.9327311515808105
Epoch #26: loss=0.8896375960773892
Epoch #27: loss=0.862462172905604
Epoch #28: loss=0.7903514901796976
Epoch #29: loss=0.7864234778616164
Epoch #30: loss=0.9597564803229438
Epoch #31: loss=1.1001655624972448
Epoch #32: loss=0.8763802515135871
Epoch #33: loss=0.7499159971872965
Epoch #34: loss=0.7009291566080518
Epoch #35: loss=0.793953643904792
Epoch #36: loss=0.6895118587546878
Epoch #37: loss=0.6351642045709822
Epoch #38: loss=0.7367291814751096
Epoch #39: loss=0.6784416172239516
Epoch #40: loss=0.714604655901591
Epoch #41: loss=0.7020871208773719
Epoch #42: loss=0.8474702686071396
Epoch #43: loss=0.8084811137782203
Epoch #44: loss=0.7280425826708475
Epoch #45: loss=0.6323040326436361
Epoch #46: loss=0.6087245013978746
Epoch #47: loss=0.6438344584570991
Epoch #48: loss=0.6252222292953067
Epoch #49: loss=0.5359171049462425
Epoch #50: loss=0.5465801490677727
Epoch #51: loss=0.5255898998843299
Epoch #52: loss=0.5095297694206238
Epoch #53: loss=0.5279186533557044
Epoch #54: loss=0.5118517263068093
Epoch #55: loss=0.47335129976272583
Epoch #56: loss=0.5534209907054901
Epoch #57: loss=0.6274182349443436
Epoch #58: loss=0.6594223959578408
Epoch #59: loss=0.6326455937491523
Epoch #60: loss=0.49348101185427773
Epoch #61: loss=0.4763207650846905
Epoch #62: loss=0.46956123411655426
Epoch #63: loss=0.49885104762183297
Epoch #64: loss=0.4725368238157696
Epoch #65: loss=0.48291775749789345
Epoch #66: loss=0.4678890258073807
Epoch #67: loss=0.49096572895844776
Epoch #68: loss=0.5130687571234174
Epoch #69: loss=0.478978607389662
Epoch #70: loss=0.4517888327439626
Epoch #71: loss=0.44178147117296857
Epoch #72: loss=0.39959509174029034
Epoch #73: loss=0.4088434676329295
Epoch #74: loss=0.4704185856713189
Epoch #75: loss=0.4575849307907952
Epoch #76: loss=0.4337448196278678
Epoch #77: loss=0.3996906412972344
Epoch #78: loss=0.38509614765644073
Epoch #79: loss=0.38408907254536945
Epoch #80: loss=0.3535678486029307
Epoch #81: loss=0.4371243582831489
Epoch #82: loss=0.36623039179378086
Epoch #83: loss=0.33842669427394867
Epoch #84: loss=0.35477214058240253
Epoch #85: loss=0.36643731511301464
Epoch #86: loss=0.3300580225057072
Epoch #87: loss=0.35279882947603863
Epoch #88: loss=0.33185955468151307
Epoch #89: loss=0.3052748201621903
Epoch #90: loss=0.4364057994551129
Epoch #91: loss=0.4501838634411494
Epoch #92: loss=0.3861691223250495
Epoch #93: loss=0.31319863183630836
Epoch #94: loss=0.2806407618853781
Epoch #95: loss=0.30300288399060565
Epoch #96: loss=0.3057003327541881
Epoch #97: loss=0.37632157736354405
Epoch #98: loss=0.3886420561207665
Epoch #99: loss=0.3667098805308342
Epoch #100: loss=0.4339954588148329
Epoch #101: loss=0.332295734849241
Epoch #102: loss=0.5919023238950305
Epoch #103: loss=0.4821599870920181
Epoch #104: loss=0.5695849574274487
Epoch #105: loss=0.4153332718544536
Epoch #106: loss=0.37987976604037815
Epoch #107: loss=0.3025210565990872
Epoch #108: loss=0.323087806502978
Epoch #109: loss=0.308005481130547
Epoch #110: loss=0.3548858314752579
Epoch #111: loss=0.26002471148967743
Epoch #112: loss=0.4084615558385849
Epoch #113: loss=0.3590925658742587
Epoch #114: loss=0.31768010142776704
Epoch #115: loss=0.26288848867019016
Epoch #116: loss=0.30918027212222415
Epoch #117: loss=0.28182891921864617
Epoch #118: loss=0.29490912705659866
Epoch #119: loss=0.22391836014058855
Epoch #120: loss=0.2824331166015731
Epoch #121: loss=0.24443006267150244
Epoch #122: loss=0.2580239979757203
Epoch #123: loss=0.23172822015153038
Epoch #124: loss=0.2850412469771173
Epoch #125: loss=0.39251284135712516
Epoch #126: loss=0.30826504942443633
Epoch #127: loss=0.2323232019941012
Epoch #128: loss=0.4916333125697242
Epoch #129: loss=0.3798377364873886
Epoch #130: loss=0.35761533594793743
Epoch #131: loss=0.234046447608206
Epoch #132: loss=0.217210430237982
Epoch #133: loss=0.2680534389283922
Epoch #134: loss=0.27684982783264583
Epoch #135: loss=0.21221775520179006
Epoch #136: loss=0.21183056715461943
Epoch #137: loss=0.22528300889664227
Epoch #138: loss=0.20564649585220549
Epoch #139: loss=0.31215059384703636
Epoch #140: loss=0.2333187564379639
Epoch #141: loss=0.21818833756777975
Epoch #142: loss=0.2478502281010151
Epoch #143: loss=0.22589393456776938
Epoch #144: loss=0.257893859098355
Epoch #145: loss=0.25311188855104977
Epoch #146: loss=0.18740680068731308
Epoch #147: loss=0.1770284842285845
Epoch #148: loss=0.21183367528849179
Epoch #149: loss=0.1748244253297647
Epoch #150: loss=0.17074172447125116
Epoch #151: loss=0.21928764382998148
Epoch #152: loss=0.21827828387419382
Epoch #153: loss=0.15525975368089145
Epoch #154: loss=0.19269848201009962
Epoch #155: loss=0.2242453694343567
Epoch #156: loss=0.17224829478396309
Epoch #157: loss=0.15204056807690197
Epoch #158: loss=0.29002612746424145
Epoch #159: loss=0.19664394814107153
Epoch #160: loss=0.1424338569243749
Epoch #161: loss=0.1983992745065027
Epoch #162: loss=0.20811762743526036
Epoch #163: loss=0.1476193041437202
Epoch #164: loss=0.14852548701067766
Epoch #165: loss=0.14376255642208788
Epoch #166: loss=0.18299048042131794
Epoch #167: loss=0.12915164273646143
Epoch #168: loss=0.2562368083745241
Epoch #169: loss=0.17125340178608894
Epoch #170: loss=0.21384711315234503
Epoch #171: loss=0.24936661827895376
Epoch #172: loss=0.18719441981779206
Epoch #173: loss=0.2433985153006183
Epoch #174: loss=0.20948535410894287
Epoch #175: loss=0.1441719378862116
Epoch #176: loss=0.14998602204852635
Epoch #177: loss=0.16696273369921577
Epoch #178: loss=0.1428220263785786
Epoch #179: loss=0.14952797463370693
Epoch #180: loss=0.11788373730248874
Epoch #181: loss=0.11031050028072463
Epoch #182: loss=0.12446208701779445
Epoch #183: loss=0.113037528263198
Epoch #184: loss=0.1300310173796283
Epoch #185: loss=0.1596712234119574
Epoch #186: loss=0.14533987268805504
Epoch #187: loss=0.17035903512603706
Epoch #188: loss=0.20228869219621023
Epoch #189: loss=0.22659856370753712
Epoch #190: loss=0.13316360571318203
Epoch #191: loss=0.11058029180599584
Epoch #192: loss=0.14953894892500508
Epoch #193: loss=0.12204104951686329
Epoch #194: loss=0.11562735194133388
Epoch #195: loss=0.1497527030814025
Epoch #196: loss=0.08955959044396877
Epoch #197: loss=0.1344226962990231
Epoch #198: loss=0.12695991672161552
Epoch #199: loss=0.11019779534803496
Epoch #200: loss=0.16946424605945745
Epoch #201: loss=0.13029705153571236
Epoch #202: loss=0.13089586690896088
Epoch #203: loss=0.12948935934238964
Epoch #204: loss=0.14163501001894474
Epoch #205: loss=0.13345940452482966
Epoch #206: loss=0.17158775797320736
Epoch #207: loss=0.1433119931154781
Epoch #208: loss=0.12791616717974344
Epoch #209: loss=0.15897183554867902
Epoch #210: loss=0.17976025253948238
Epoch #211: loss=0.15637100384467179
Epoch #212: loss=0.1272690857036246
Epoch #213: loss=0.2089722729805443
Epoch #214: loss=0.1597621486418777
Epoch #215: loss=0.12602113911675084
Epoch #216: loss=0.10922059365030792
Epoch #217: loss=0.25525813342796433
Epoch #218: loss=0.1204163705309232
Epoch #219: loss=0.1159079550868935
Epoch #220: loss=0.11004678232388364
Epoch #221: loss=0.1328380641837915
Epoch #222: loss=0.08388284624864657
Epoch #223: loss=0.11065116359127893
Epoch #224: loss=0.15885540822313893
Epoch #225: loss=0.14714932886676657
Epoch #226: loss=0.12654524410350454
Epoch #227: loss=0.14818562815586725
Epoch #228: loss=0.0971183247036404
Epoch #229: loss=0.1486341609723038
Epoch #230: loss=0.22680411922434965
Epoch #231: loss=0.11225742950207657
Epoch #232: loss=0.10414781752559873
Epoch #233: loss=0.12341824701676767
Epoch #234: loss=0.1628661377148496
Epoch #235: loss=0.12259776724709405
Epoch #236: loss=0.10428389968971412
Epoch #237: loss=0.09597545179227988
Epoch #238: loss=0.1153414442928301
Epoch #239: loss=0.07858723319239086
Epoch #240: loss=0.08958506977392568
Epoch #241: loss=0.0722566047269437
Epoch #242: loss=0.08328584198736483
Epoch #243: loss=0.10665084690683418
Epoch #244: loss=0.0913940164157086
Epoch #245: loss=0.0849849224711458
Epoch #246: loss=0.10984617026729716
Epoch #247: loss=0.07320649663193358
Epoch #248: loss=0.14169564160207906
Epoch #249: loss=0.07344361539516184

Training time: 0:17:02.751334

Finished.
n2one setting etth1_ettm1 -> electricity
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth1_ettm1_epochs_250_seed_2023/model.pkl', muti_dataset='etth1_ettm1', random_seed=2023, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='electricity')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.6724e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=3.23635e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.6724e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.3645538024990659, 'MAE': 0.4206277908368465}
Finished.
------------------------- record done -------------------------
n2one setting etth1_ettm1 -> weather
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth1_ettm1_epochs_250_seed_2023/model.pkl', muti_dataset='etth1_ettm1', random_seed=2023, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='weather')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.64122e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=3.10749e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.64122e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.6053507524442743, 'MAE': 0.612870888908257}
Finished.
------------------------- record done -------------------------
n2one setting etth1_ettm1 -> exchange
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth1_ettm1_epochs_250_seed_2023/model.pkl', muti_dataset='etth1_ettm1', random_seed=2023, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='exchange')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=4.15679e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.21331392162722432, 'MAE': 0.3166414179037917}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='etth1_ettm2', random_seed=2023, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/etth1_ettm2_epochs_250_seed_2023
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=5.33725983566708
Epoch #1: loss=2.8218430942959256
Epoch #2: loss=2.486456255118052
Epoch #3: loss=2.2606822848320007
Epoch #4: loss=2.0705473820368447
Epoch #5: loss=1.9303472373220656
Epoch #6: loss=1.7791226373778448
Epoch #7: loss=1.7097833818859525
Epoch #8: loss=1.645648227797614
Epoch #9: loss=1.5656347076098125
Epoch #10: loss=1.4551107353634305
Epoch #11: loss=1.3497214449776544
Epoch #12: loss=1.323480983575185
Epoch #13: loss=1.3073261843787298
Epoch #14: loss=1.2045658297008939
Epoch #15: loss=1.1742650071779888
Epoch #16: loss=1.2197589443789587
Epoch #17: loss=1.0959407620959811
Epoch #18: loss=1.022006329562929
Epoch #19: loss=0.9806127978695763
Epoch #20: loss=0.9742769367165036
Epoch #21: loss=0.9734562800990211
Epoch #22: loss=0.9889251324865553
Epoch #23: loss=0.9524397717581855
Epoch #24: loss=0.8945839338832431
Epoch #25: loss=0.9453411830796136
Epoch #26: loss=0.8955066369639503
Epoch #27: loss=0.8661924368805356
Epoch #28: loss=0.831264830297894
Epoch #29: loss=0.8138114147716098
Epoch #30: loss=1.0576934980021582
Epoch #31: loss=1.0871326128641765
Epoch #32: loss=0.8464433252811432
Epoch #33: loss=0.7515523632367452
Epoch #34: loss=0.7047236147854063
Epoch #35: loss=0.7925351990593804
Epoch #36: loss=0.6951801114612155
Epoch #37: loss=0.6510187420580122
Epoch #38: loss=0.7216027097569572
Epoch #39: loss=0.6682106124030219
Epoch #40: loss=0.6874741663535436
Epoch #41: loss=0.7682342264387343
Epoch #42: loss=0.9065590848525366
Epoch #43: loss=0.7762305008040534
Epoch #44: loss=0.7111669547028012
Epoch #45: loss=0.6237585461801953
Epoch #46: loss=0.6159400327338113
Epoch #47: loss=0.6727385040786531
Epoch #48: loss=0.6590173426601622
Epoch #49: loss=0.5529615432024002
Epoch #50: loss=0.5553673605124155
Epoch #51: loss=0.526012423965666
Epoch #52: loss=0.5230182326502271
Epoch #53: loss=0.5318047785096698
Epoch #54: loss=0.5091207226117452
Epoch #55: loss=0.4752538816796409
Epoch #56: loss=0.5645635061793857
Epoch #57: loss=0.6502038952377107
Epoch #58: loss=0.6446280429760615
Epoch #59: loss=0.6247871534691917
Epoch #60: loss=0.483235380715794
Epoch #61: loss=0.4714006417327457
Epoch #62: loss=0.4663132627805074
Epoch #63: loss=0.4845656024085151
Epoch #64: loss=0.47670744690630174
Epoch #65: loss=0.47709517346488106
Epoch #66: loss=0.45124955144193435
Epoch #67: loss=0.48588532706101734
Epoch #68: loss=0.5545588665538363
Epoch #69: loss=0.5588748455047607
Epoch #70: loss=0.5318880577882131
Epoch #71: loss=0.4935755150185691
Epoch #72: loss=0.42727983991305035
Epoch #73: loss=0.41584598024686176
Epoch #74: loss=0.4526967737409804
Epoch #75: loss=0.46749771965874565
Epoch #76: loss=0.44356708890861934
Epoch #77: loss=0.3914063589440452
Epoch #78: loss=0.3693818284405602
Epoch #79: loss=0.39401015308168197
Epoch #80: loss=0.35020949608749813
Epoch #81: loss=0.4357837521367603
Epoch #82: loss=0.3643871313995785
Epoch #83: loss=0.3366002125872506
Epoch #84: loss=0.3487476416760021
Epoch #85: loss=0.37043433884779614
Epoch #86: loss=0.3537312489416864
Epoch #87: loss=0.36938874423503876
Epoch #88: loss=0.34124599231614006
Epoch #89: loss=0.31710875944958794
Epoch #90: loss=0.4126921817660332
Epoch #91: loss=0.4482933340801133
Epoch #92: loss=0.4120503117640813
Epoch #93: loss=0.3528047601381938
Epoch #94: loss=0.3219070186217626
Epoch #95: loss=0.3296226246489419
Epoch #96: loss=0.32012315260039437
Epoch #97: loss=0.3731585741043091
Epoch #98: loss=0.33233539594544304
Epoch #99: loss=0.2925459353460206
Epoch #100: loss=0.3866720439659225
Epoch #101: loss=0.30087317691908944
Epoch #102: loss=0.6244192959533797
Epoch #103: loss=0.4802171720398797
Epoch #104: loss=0.5770595471064249
Epoch #105: loss=0.421808135178354
Epoch #106: loss=0.3645266493161519
Epoch #107: loss=0.2799606000383695
Epoch #108: loss=0.3098159051603741
Epoch #109: loss=0.26534375217225814
Epoch #110: loss=0.33917221758100724
Epoch #111: loss=0.27519920468330383
Epoch #112: loss=0.4232285792628924
Epoch #113: loss=0.3843221366405487
Epoch #114: loss=0.3210262672768699
Epoch #115: loss=0.2691390961408615
Epoch #116: loss=0.3409957115848859
Epoch #117: loss=0.3429104785124461
Epoch #118: loss=0.30652619401613873
Epoch #119: loss=0.24909883075290257
Epoch #120: loss=0.3108064333597819
Epoch #121: loss=0.2564638273583518
Epoch #122: loss=0.25599682579437893
Epoch #123: loss=0.221744111014737
Epoch #124: loss=0.23690061436759102
Epoch #125: loss=0.28755930148892933
Epoch #126: loss=0.281333244095246
Epoch #127: loss=0.2134346245891518
Epoch #128: loss=0.5219360871447457
Epoch #129: loss=0.4060641883148087
Epoch #130: loss=0.3725489096509086
Epoch #131: loss=0.24763812124729156
Epoch #132: loss=0.24942710002263388
Epoch #133: loss=0.2918155963222186
Epoch #134: loss=0.305081259045336
Epoch #135: loss=0.22206760860151714
Epoch #136: loss=0.22195636729399362
Epoch #137: loss=0.24756363406777382
Epoch #138: loss=0.21852633522616494
Epoch #139: loss=0.31139828057752716
Epoch #140: loss=0.20060700840420193
Epoch #141: loss=0.21258323556847042
Epoch #142: loss=0.2546311240229342
Epoch #143: loss=0.25059378643830615
Epoch #144: loss=0.26918650791049004
Epoch #145: loss=0.2805584689809216
Epoch #146: loss=0.19892844806114832
Epoch #147: loss=0.19306846294138166
Epoch #148: loss=0.2230176106095314
Epoch #149: loss=0.2005767830544048
Epoch #150: loss=0.1841568528778023
Epoch #151: loss=0.21325271824995676
Epoch #152: loss=0.22541685236824882
Epoch #153: loss=0.16147819513248074
Epoch #154: loss=0.22747353174620205
Epoch #155: loss=0.24147267308500078
Epoch #156: loss=0.19641455800996888
Epoch #157: loss=0.15759732781185043
Epoch #158: loss=0.3002521437075403
Epoch #159: loss=0.1927576536933581
Epoch #160: loss=0.15159917540020412
Epoch #161: loss=0.21364061079091495
Epoch #162: loss=0.222405935327212
Epoch #163: loss=0.15393495601084498
Epoch #164: loss=0.15460025684701073
Epoch #165: loss=0.1457528422276179
Epoch #166: loss=0.17894882129298317
Epoch #167: loss=0.12941640139453942
Epoch #168: loss=0.2803071240584056
Epoch #169: loss=0.17723746597766876
Epoch #170: loss=0.26360560208559036
Epoch #171: loss=0.27431539239154923
Epoch #172: loss=0.17749742915232977
Epoch #173: loss=0.1660821425418059
Epoch #174: loss=0.1774478583700127
Epoch #175: loss=0.16191072513659796
Epoch #176: loss=0.2006265740427706
Epoch #177: loss=0.17770442821913296
Epoch #178: loss=0.1412639566179779
Epoch #179: loss=0.16241460003786617
Epoch #180: loss=0.14848304560614956
Epoch #181: loss=0.13903704078661072
Epoch #182: loss=0.13449891387588447
Epoch #183: loss=0.12128241753412618
Epoch #184: loss=0.13850244414061308
Epoch #185: loss=0.1612558232413398
Epoch #186: loss=0.14568294481270844
Epoch #187: loss=0.15662756375968456
Epoch #188: loss=0.19383974435428777
Epoch #189: loss=0.236550468330582
Epoch #190: loss=0.14205940295424727
Epoch #191: loss=0.12634732760488987
Epoch #192: loss=0.15123187233176497
Epoch #193: loss=0.12730980747275883
Epoch #194: loss=0.13091338032649624
Epoch #195: loss=0.17704273180829155
Epoch #196: loss=0.11198066568209065
Epoch #197: loss=0.13545250913335216
Epoch #198: loss=0.14104857730368772
Epoch #199: loss=0.15633446992271477
Epoch #200: loss=0.21028446550998423
Epoch #201: loss=0.15091394198437533
Epoch #202: loss=0.1444774560837282
Epoch #203: loss=0.14099875030418238
Epoch #204: loss=0.14621678760482204
Epoch #205: loss=0.1082439213577244
Epoch #206: loss=0.10953574834598435
Epoch #207: loss=0.1071362638225158
Epoch #208: loss=0.1108077469592293
Epoch #209: loss=0.14946346243636477
Epoch #210: loss=0.18003688473254442
Epoch #211: loss=0.20050648351510367
Epoch #212: loss=0.1613286371446318
Epoch #213: loss=0.2391741673151652
Epoch #214: loss=0.183931526210573
Epoch #215: loss=0.1275339163839817
Epoch #216: loss=0.11327816349350744
Epoch #217: loss=0.2570010293275118
Epoch #218: loss=0.1250782029496299
Epoch #219: loss=0.11265373581813441
Epoch #220: loss=0.1223412300977442
Epoch #221: loss=0.1294178639849027
Epoch #222: loss=0.07713659976919492
Epoch #223: loss=0.10878601525392798
Epoch #224: loss=0.13870235201385286
Epoch #225: loss=0.10521069293220837
Epoch #226: loss=0.11137781457768546
Epoch #227: loss=0.1573754089574019
Epoch #228: loss=0.11987995128664705
Epoch #229: loss=0.18770904073284733
Epoch #230: loss=0.2399453032347891
Epoch #231: loss=0.12218835577368736
Epoch #232: loss=0.1089050697369708
Epoch #233: loss=0.13721511388818422
Epoch #234: loss=0.16677717947297627
Epoch #235: loss=0.11448678167329894
Epoch #236: loss=0.11145235163470109
Epoch #237: loss=0.09727708778033654
Epoch #238: loss=0.10946173479573594
Epoch #239: loss=0.07025083195832041
Epoch #240: loss=0.09465281592888965
Epoch #241: loss=0.06833694099138181
Epoch #242: loss=0.09975340207003885
Epoch #243: loss=0.13811073835111326
Epoch #244: loss=0.11782659176323149
Epoch #245: loss=0.08813376704023944
Epoch #246: loss=0.12442134765701161
Epoch #247: loss=0.07275007002883488
Epoch #248: loss=0.1542174986874064
Epoch #249: loss=0.06888128734297222

Training time: 0:16:33.395062

Finished.
n2one setting etth1_ettm2 -> electricity
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth1_ettm2_epochs_250_seed_2023/model.pkl', muti_dataset='etth1_ettm2', random_seed=2023, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='electricity')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.76771e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=3.56654e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.76771e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.3704258944936407, 'MAE': 0.4280222113386997}
Finished.
------------------------- record done -------------------------
n2one setting etth1_ettm2 -> weather
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth1_ettm2_epochs_250_seed_2023/model.pkl', muti_dataset='etth1_ettm2', random_seed=2023, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='weather')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.75827e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=3.13521e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.75827e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.6732563984022512, 'MAE': 0.6289622279196575}
Finished.
------------------------- record done -------------------------
n2one setting etth1_ettm2 -> exchange
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth1_ettm2_epochs_250_seed_2023/model.pkl', muti_dataset='etth1_ettm2', random_seed=2023, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='exchange')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=3.69183e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.19156106573854298, 'MAE': 0.302317858912271}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='etth1_electricity', random_seed=2023, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/etth1_electricity_epochs_250_seed_2023
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=1.6038339012643186
Epoch #1: loss=0.6331246885039458
Epoch #2: loss=0.44228127535159995
Epoch #3: loss=0.3655253711906148
Epoch #4: loss=0.3020773766698634
Epoch #5: loss=0.2640790975221047
Epoch #6: loss=0.22103148812382686
Epoch #7: loss=0.2143947706989399
Epoch #8: loss=0.16679122998583606
Epoch #9: loss=0.15667455433299993
Epoch #10: loss=0.14048749169834504
Epoch #11: loss=0.1255070616877297
Epoch #12: loss=0.11715408403244687
Epoch #13: loss=0.11169334385180618
Epoch #14: loss=0.10527116379405303
Epoch #15: loss=0.08734211121208785
Epoch #16: loss=0.08000485656964706
Epoch #17: loss=0.08065602498515169
Epoch #18: loss=0.07750058407815764
Epoch #19: loss=0.07362491647122292
Epoch #20: loss=0.07329740815387084
Epoch #21: loss=0.06606285994384074
Epoch #22: loss=0.05470493744218313
Epoch #23: loss=0.06019900426575232
Epoch #24: loss=0.04831455161556511
Epoch #25: loss=0.050393884981664396
Epoch #26: loss=0.04344065693667068
Epoch #27: loss=0.04730825470448131
Epoch #28: loss=0.041584616146481984
Epoch #29: loss=0.03667990390421459
Epoch #30: loss=0.03795706866757672
Epoch #31: loss=0.03580681536426177
Epoch #32: loss=0.03685042889513325
Epoch #33: loss=0.03481034500802663
Epoch #34: loss=0.03293213496771187
Epoch #35: loss=0.03496467272208708
Epoch #36: loss=0.03307906428434341
Epoch #37: loss=0.030955014880263906
Epoch #38: loss=0.04709036865416437
Epoch #39: loss=0.0252840027565182
Epoch #40: loss=0.0378996456579998
Epoch #41: loss=0.027324685775078607
Epoch #42: loss=0.038805520785234235
Epoch #43: loss=0.02212346025682963
Epoch #44: loss=0.029865958340318372
Epoch #45: loss=0.026617673173526916
Epoch #46: loss=0.022842955272985487
Epoch #47: loss=0.03791380965040156
Epoch #48: loss=0.021673349369737523
Epoch #49: loss=0.02607376394238289
Epoch #50: loss=0.02874613283613765
Epoch #51: loss=0.0255316913208822
Epoch #52: loss=0.024107844697271768
Epoch #53: loss=0.02013715082231523
Epoch #54: loss=0.02314761724527471
Epoch #55: loss=0.020453845432579016
Epoch #56: loss=0.02010401530637088
Epoch #57: loss=0.028979024775652784
Epoch #58: loss=0.01968736561063518
Epoch #59: loss=0.021787529426524642
Epoch #60: loss=0.022373554419950446
Epoch #61: loss=0.026929357859289002
Epoch #62: loss=0.06376410464411204
Epoch #63: loss=0.01899724787245959
Epoch #64: loss=0.027847578847729714
Epoch #65: loss=0.020976017317990203
Epoch #66: loss=0.030727763441263517
Epoch #67: loss=0.021042770436374335
Epoch #68: loss=0.023376497202240522
Epoch #69: loss=0.017823362630889922
Epoch #70: loss=0.01717757518266941
Epoch #71: loss=0.01874454973319477
Epoch #72: loss=0.016563778154682594
Epoch #73: loss=0.02253098497341143
Epoch #74: loss=0.019138963255433582
Epoch #75: loss=0.01800052951201484
Epoch #76: loss=0.014984942549767212
Epoch #77: loss=0.014860170043326302
Epoch #78: loss=0.01610180610743289
Epoch #79: loss=0.026811998661553555
Epoch #80: loss=0.018690594391068217
Epoch #81: loss=0.02002241536719361
Epoch #82: loss=0.011153098854709064
Epoch #83: loss=0.015875759777483792
Epoch #84: loss=0.014745713480274057
Epoch #85: loss=0.0257521070073314
Epoch #86: loss=0.015233350880074928
Epoch #87: loss=0.01574953551386077
Epoch #88: loss=0.015341504466671655
Epoch #89: loss=0.017756237146918798
Epoch #90: loss=0.02046884778140272
Epoch #91: loss=0.016893726416150813
Epoch #92: loss=0.014050926584587148
Epoch #93: loss=0.020100871501021973
Epoch #94: loss=0.020362963289499464
Epoch #95: loss=0.020290829216431053
Epoch #96: loss=0.017427095376904882
Epoch #97: loss=0.01199357167747732
Epoch #98: loss=0.012285808181686883
Epoch #99: loss=0.00931870245918484
Epoch #100: loss=0.03265840746107421
Epoch #101: loss=0.016908966397040343
Epoch #102: loss=0.009352692783556072
Epoch #103: loss=0.017008836133932274
Epoch #104: loss=0.01641217466634017
Epoch #105: loss=0.014101646768700092
Epoch #106: loss=0.013276928688666397
Epoch #107: loss=0.014738359161128756
Epoch #108: loss=0.03271788604651214
Epoch #109: loss=0.015084766200388691
Epoch #110: loss=0.011019026427941465
Epoch #111: loss=0.012425731001357923
Epoch #112: loss=0.01791045036012159
Epoch #113: loss=0.028528810128436152
Epoch #114: loss=0.008598418405177298
Epoch #115: loss=0.013855589436411166
Epoch #116: loss=0.01816833700836358
Epoch #117: loss=0.011051586268877145
Epoch #118: loss=0.01936158578485824
Epoch #119: loss=0.013618781175088171
Epoch #120: loss=0.007896392735835056
Epoch #121: loss=0.01114534174742336
Epoch #122: loss=0.012035814609212471
Epoch #123: loss=0.011574230025694195
Epoch #124: loss=0.01689401245414224
Epoch #125: loss=0.01984509790657662
Epoch #126: loss=0.014059472826618997
Epoch #127: loss=0.010297435930136467
Epoch #128: loss=0.009943413041427691
Epoch #129: loss=0.012108002012920645
Epoch #130: loss=0.010987435029564062
Epoch #131: loss=0.02200534733040003
Epoch #132: loss=0.010916120488329354
Epoch #133: loss=0.013770879243632564
Epoch #134: loss=0.010827586131288227
Epoch #135: loss=0.011200135028047953
Epoch #136: loss=0.01603838300035
Epoch #137: loss=0.012634235771945054
Epoch #138: loss=0.012762144502187035
Epoch #139: loss=0.011364807456230643
Epoch #140: loss=0.007593124270830985
Epoch #141: loss=0.013349248231750651
Epoch #142: loss=0.01847811966670234
Epoch #143: loss=0.011209252756443897
Epoch #144: loss=0.01673700461682662
Epoch #145: loss=0.012514378229353475
Epoch #146: loss=0.026924689694679704
Epoch #147: loss=0.017296053344957395
Epoch #148: loss=0.012554216243166237
Epoch #149: loss=0.022030719685816866
Epoch #150: loss=0.007690695019491392
Epoch #151: loss=0.010367543225229162
Epoch #152: loss=0.0126741762488098
Epoch #153: loss=0.013651352709729975
Epoch #154: loss=0.014379035552116054
Epoch #155: loss=0.014889895968159048
Epoch #156: loss=0.009589197123692961
Epoch #157: loss=0.005074292451687009
Epoch #158: loss=0.010289492617608274
Epoch #159: loss=0.010829992559504207
Epoch #160: loss=0.009411916125472777
Epoch #161: loss=0.012655368465014518
Epoch #162: loss=0.00975748078914277
Epoch #163: loss=0.007842922921820612
Epoch #164: loss=0.008818046465354954
Epoch #165: loss=0.023677251807828547
Epoch #166: loss=0.009771600865195336
Epoch #167: loss=0.010717851707141096
Epoch #168: loss=0.015825432834567547
Epoch #169: loss=0.010264124542140408
Epoch #170: loss=0.013523165775972658
Epoch #171: loss=0.007653920897410433
Epoch #172: loss=0.010172309967790014
Epoch #173: loss=0.008104693239768427
Epoch #174: loss=0.012213137003372383
Epoch #175: loss=0.007978846661633104
Epoch #176: loss=0.00946487997667069
Epoch #177: loss=0.00868328824692914
Epoch #178: loss=0.01379898416345851
Epoch #179: loss=0.010583572522946491
Epoch #180: loss=0.008663804576360967
Epoch #181: loss=0.006144965393238036
Epoch #182: loss=0.01125208486887348
Epoch #183: loss=0.00891027309892418
Epoch #184: loss=0.020444627497920188
Epoch #185: loss=0.015461943757744427
Epoch #186: loss=0.010014464610210824
Epoch #187: loss=0.012204848252955211
Epoch #188: loss=0.01460696953166384
Epoch #189: loss=0.009650530967319798
Epoch #190: loss=0.01807408079309048
Epoch #191: loss=0.01497643857874209
Epoch #192: loss=0.014344641864626305
Epoch #193: loss=0.007812445021976955
Epoch #194: loss=0.010527552038316882
Epoch #195: loss=0.008807860613958881
Epoch #196: loss=0.007480578788534153
Epoch #197: loss=0.009608841112799936
Epoch #198: loss=0.0047642486477831505
Epoch #199: loss=0.009069204447214582
Epoch #200: loss=0.01837935534280316
Epoch #201: loss=0.007882647907802205
Epoch #202: loss=0.005029535803215567
Epoch #203: loss=0.013897095240969154
Epoch #204: loss=0.007234519949500881
Epoch #205: loss=0.007501600054039466
Epoch #206: loss=0.009725933698596569
Epoch #207: loss=0.009562249533301816
Epoch #208: loss=0.008081961380035287
Epoch #209: loss=0.018579150194787737
Epoch #210: loss=0.011253482917509688
Epoch #211: loss=0.006446874883672202
Epoch #212: loss=0.009788782677876553
Epoch #213: loss=0.009848425485493285
Epoch #214: loss=0.009293901444612946
Epoch #215: loss=0.004936440452250436
Epoch #216: loss=0.015504945706641415
Epoch #217: loss=0.010354203905657547
Epoch #218: loss=0.010225224001393455
Epoch #219: loss=0.009819548713893731
Epoch #220: loss=0.011011856950240255
Epoch #221: loss=0.006625506612536122
Epoch #222: loss=0.009147487063562106
Epoch #223: loss=0.007693969227924443
Epoch #224: loss=0.007290627594068987
Epoch #225: loss=0.013293826235451157
Epoch #226: loss=0.01197847368379504
Epoch #227: loss=0.01128768447996685
Epoch #228: loss=0.006395797495515137
Epoch #229: loss=0.00803534889369408
Epoch #230: loss=0.009577025146959718
Epoch #231: loss=0.007931172079513157
Epoch #232: loss=0.009659355821385383
Epoch #233: loss=0.011310716310839691
Epoch #234: loss=0.00995335555827052
Epoch #235: loss=0.00588295444295665
Epoch #236: loss=0.008195138127889799
Epoch #237: loss=0.011221628629555641
Epoch #238: loss=0.005697589582801235
Epoch #239: loss=0.006564855816289243
Epoch #240: loss=0.014285678121512586
Epoch #241: loss=0.008950938812542917
Epoch #242: loss=0.007394739392777144
Epoch #243: loss=0.03367054720991114
Epoch #244: loss=0.007320458533862754
Epoch #245: loss=0.007414856382483111
Epoch #246: loss=0.007526997606363715
Epoch #247: loss=0.005573608026369163
Epoch #248: loss=0.009197812680834364
Epoch #249: loss=0.01099485139985052

Training time: 4:28:57.827349

Finished.
n2one setting etth1_electricity -> weather
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth1_electricity_epochs_250_seed_2023/model.pkl', muti_dataset='etth1_electricity', random_seed=2023, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='weather')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=9.4014e-09): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.80433e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=4.44587e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=4.44587e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 1.330469202023403, 'MAE': 0.8678454602700365}
Finished.
------------------------- record done -------------------------
n2one setting etth1_electricity -> exchange
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth1_electricity_epochs_250_seed_2023/model.pkl', muti_dataset='etth1_electricity', random_seed=2023, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='exchange')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=3.33742e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=3.33742e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.2384755596192837, 'MAE': 0.33336353645835426}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='etth1_traffic', random_seed=2023, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/etth1_traffic_epochs_250_seed_2023
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=0.9686367511749268
Epoch #1: loss=0.34575789408936186
Epoch #2: loss=0.252189896692135
Epoch #3: loss=0.19115897193894013
Epoch #4: loss=0.14233733746207075
Epoch #5: loss=0.12149009338533988
Epoch #6: loss=0.09716090852216795
Epoch #7: loss=0.08919287491289704
Epoch #8: loss=0.0675452553819312
Epoch #9: loss=0.06806973187452219
Epoch #10: loss=0.05832618514774635
Epoch #11: loss=0.05209770536594633
Epoch #12: loss=0.07613247931185763
Epoch #13: loss=0.04885305692463196
Epoch #14: loss=0.045759182622692664
Epoch #15: loss=0.039559014481562
Epoch #16: loss=0.0372542097995227
Epoch #17: loss=0.05011019867205064
Epoch #18: loss=0.03971250745804361
Epoch #19: loss=0.03421223902741556
Epoch #20: loss=0.0392822136031303
Epoch #21: loss=0.03232549143683431
Epoch #22: loss=0.0340944779407173
Epoch #23: loss=0.024725433402042085
Epoch #24: loss=0.0392542054262591
Epoch #25: loss=0.03163549475845001
Epoch #26: loss=0.02799027812090071
Epoch #27: loss=0.028962061493494094
Epoch #28: loss=0.023789022081950627
Epoch #29: loss=0.02212774937879187
Epoch #30: loss=0.022323942786713333
Epoch #31: loss=0.024761889103486045
Epoch #32: loss=0.02830320417838679
Epoch #33: loss=0.02185465664644278
Epoch #34: loss=0.021956712302780776
Epoch #35: loss=0.024971942301042233
Epoch #36: loss=0.026281845383319288
Epoch #37: loss=0.025296762388845814
Epoch #38: loss=0.018074190236911526
Epoch #39: loss=0.024817868050935884
Epoch #40: loss=0.022293468403763388
Epoch #41: loss=0.014517412831408259
Epoch #42: loss=0.021494140326590958
Epoch #43: loss=0.01640920002131066
Epoch #44: loss=0.022114925983927766
Epoch #45: loss=0.019361854250750734
Epoch #46: loss=0.016614215942125143
Epoch #47: loss=0.02264259658399937
Epoch #48: loss=0.032384037560693836
Epoch #49: loss=0.016323193924148407
Epoch #50: loss=0.020981838056773176
Epoch #51: loss=0.01895908060348598
Epoch #52: loss=0.02526397983716657
Epoch #53: loss=0.017004248758040366
Epoch #54: loss=0.01991428840252752
Epoch #55: loss=0.017456262022049474
Epoch #56: loss=0.017299784506364085
Epoch #57: loss=0.011531101134152904
Epoch #58: loss=0.019931239390775766
Epoch #59: loss=0.015308412709511856
Epoch #60: loss=0.019368564454416377
Epoch #61: loss=0.01603316518430404
Epoch #62: loss=0.016156598021161355
Epoch #63: loss=0.03658548696375194
Epoch #64: loss=0.0162311225383315
Epoch #65: loss=0.017159263875538545
Epoch #66: loss=0.017785181327705818
Epoch #67: loss=0.01238201999420866
Epoch #68: loss=0.013687817859191445
Epoch #69: loss=0.02286780328870032
Epoch #70: loss=0.011047637452896828
Epoch #71: loss=0.015480072991069143
Epoch #72: loss=0.016047429020039125
Epoch #73: loss=0.014374792891356535
Epoch #74: loss=0.014386050189210003
Epoch #75: loss=0.018897748939353774
Epoch #76: loss=0.015340378224770393
Epoch #77: loss=0.014299776878862205
Epoch #78: loss=0.0130399404587551
Epoch #79: loss=0.015111302888047398
Epoch #80: loss=0.013131320077585114
Epoch #81: loss=0.014008667608606496
Epoch #82: loss=0.009546391344587698
Epoch #83: loss=0.015725471001951413
Epoch #84: loss=0.018047585103929287
Epoch #85: loss=0.012138575238713692
Epoch #86: loss=0.014455199473899477
Epoch #87: loss=0.013272952622229231
Epoch #88: loss=0.01186407586332985
Epoch #89: loss=0.018658936491181968
Epoch #90: loss=0.02010710385798146
Epoch #91: loss=0.012972836083543923
Epoch #92: loss=0.011786141183680906
Epoch #93: loss=0.014836014019853464
Epoch #94: loss=0.01399999720204388
Epoch #95: loss=0.012525906164424979
Epoch #96: loss=0.013654767062396595
Epoch #97: loss=0.015012932648370256
Epoch #98: loss=0.011375029901811806
Epoch #99: loss=0.013594928552636124
Epoch #100: loss=0.016885183045622636
Epoch #101: loss=0.01333716021164404
Epoch #102: loss=0.019916063169012344
Epoch #103: loss=0.011889562915026781
Epoch #104: loss=0.010491557123388808
Epoch #105: loss=0.0196061935427843
Epoch #106: loss=0.014133441867668533
Epoch #107: loss=0.015573673336856416
Epoch #108: loss=0.016103193921408635
Epoch #109: loss=0.01177702454700841
Epoch #110: loss=0.010286206922290785
Epoch #111: loss=0.011743739259780799
Epoch #112: loss=0.01260750719797701
Epoch #113: loss=0.016703271352599178
Epoch #114: loss=0.011685474336662269
Epoch #115: loss=0.009160833846735746
Epoch #116: loss=0.012242447151056296
Epoch #117: loss=0.0112274747153115
Epoch #118: loss=0.012893939409734547
Epoch #119: loss=0.010759808976667793
Epoch #120: loss=0.0133244847934811
Epoch #121: loss=0.0084405262404675
Epoch #122: loss=0.014170276417926926
Epoch #123: loss=0.009617723464152387
Epoch #124: loss=0.011090218497075031
Epoch #125: loss=0.020404311190325132
Epoch #126: loss=0.008602814839878768
Epoch #127: loss=0.010400356226141208
Epoch #128: loss=0.011136239415343575
Epoch #129: loss=0.021883611572543062
Epoch #130: loss=0.011015788528621326
Epoch #131: loss=0.008794480523115836
Epoch #132: loss=0.008573980920690073
Epoch #133: loss=0.01071317825441343
Epoch #134: loss=0.01062709322314167
Epoch #135: loss=0.01341244461552668
Epoch #136: loss=0.012262930867483025
Epoch #137: loss=0.012596828503396482
Epoch #138: loss=0.007668828871238455
Epoch #139: loss=0.013318702010333354
Epoch #140: loss=0.01381014540323439
Epoch #141: loss=0.015133022091440402
Epoch #142: loss=0.010665035551293058
Epoch #143: loss=0.015535158703598125
Epoch #144: loss=0.010396086412301246
Epoch #145: loss=0.009192439949012553
Epoch #146: loss=0.0108995686713014
Epoch #147: loss=0.013226063805129926
Epoch #148: loss=0.00866535199222075
Epoch #149: loss=0.012799967887793412
Epoch #150: loss=0.008463209839338133
Epoch #151: loss=0.0153975932125863
Epoch #152: loss=0.02189594736970212
Epoch #153: loss=0.008733457832230668
Epoch #154: loss=0.007397662030722985
Epoch #155: loss=0.011574555280082778
Epoch #156: loss=0.009038154516187315
Epoch #157: loss=0.011394698829017016
Epoch #158: loss=0.009114345825217387
Epoch #159: loss=0.01290063880017173
Epoch #160: loss=0.011098768497607965
Epoch #161: loss=0.009825290196542201
Epoch #162: loss=0.010347631601260393
Epoch #163: loss=0.009371291355089788
Epoch #164: loss=0.010312319920702797
Epoch #165: loss=0.015442362684314166
Epoch #166: loss=0.00722783430513226
Epoch #167: loss=0.008588733146079254
Epoch #168: loss=0.010264389880293052
Epoch #169: loss=0.01663231575342527
Epoch #170: loss=0.011101595693516654
Epoch #171: loss=0.007051078580416738
Epoch #172: loss=0.012245015505333839
Epoch #173: loss=0.012687228387556605
Epoch #174: loss=0.009455530600435014
Epoch #175: loss=0.007848280881912265
Epoch #176: loss=0.011472443793136711
Epoch #177: loss=0.010757378407475893
Epoch #178: loss=0.00979394648035962
Epoch #179: loss=0.007418684914765876
Epoch #180: loss=0.013934010973693728
Epoch #181: loss=0.00794490432658169
Epoch #182: loss=0.011445216828566961
Epoch #183: loss=0.008745185722913367
Epoch #184: loss=0.00857232067821976
Epoch #185: loss=0.01181423886049077
Epoch #186: loss=0.009586754960655818
Epoch #187: loss=0.011111953526973629
Epoch #188: loss=0.007156756446158681
Epoch #189: loss=0.010040794031656553
Epoch #190: loss=0.008706029141175088
Epoch #191: loss=0.008401714040486783
Epoch #192: loss=0.009362573574448979
Epoch #193: loss=0.006522039378407218
Epoch #194: loss=0.007319675860093249
Epoch #195: loss=0.011671501617433645
Epoch #196: loss=0.0175780854574059
Epoch #197: loss=0.009357281931761306
Epoch #198: loss=0.005555061851026623
Epoch #199: loss=0.008965663092574737
Epoch #200: loss=0.00830411806565166
Epoch #201: loss=0.011958868327884564
Epoch #202: loss=0.008112330450359554
Epoch #203: loss=0.009334392797989451
Epoch #204: loss=0.007728731692870112
Epoch #205: loss=0.008345059437163644
Epoch #206: loss=0.01003915988140867
Epoch #207: loss=0.013144758185906276
Epoch #208: loss=0.012962415393170754
Epoch #209: loss=0.008604709924035184
Epoch #210: loss=0.010271344169512416
Epoch #211: loss=0.0070276172627509095
Epoch #212: loss=0.009410154296661153
Epoch #213: loss=0.007501643499721461
Epoch #214: loss=0.01232416934894518
Epoch #215: loss=0.009466671603501392
Epoch #216: loss=0.007909877467395467
Epoch #217: loss=0.012316809811225318
Epoch #218: loss=0.008764758038178345
Epoch #219: loss=0.025090078507513623
Epoch #220: loss=0.005783709739523772
Epoch #221: loss=0.011358218946442854
Epoch #222: loss=0.006894488050756436
Epoch #223: loss=0.013027774202925231
Epoch #224: loss=0.009375600503895312
Epoch #225: loss=0.011764287815681928
Epoch #226: loss=0.008070117237739092
Epoch #227: loss=0.010780344057143763
Epoch #228: loss=0.007991612542975776
Epoch #229: loss=0.008803907607417335
Epoch #230: loss=0.011418659601553
Epoch #231: loss=0.008121982421918228
Epoch #232: loss=0.009913617954663107
Epoch #233: loss=0.009391007259946533
Epoch #234: loss=0.013219552373928066
Epoch #235: loss=0.006326667750676451
Epoch #236: loss=0.01217294309608238
Epoch #237: loss=0.01215118310618283
Epoch #238: loss=0.011557365600804727
Epoch #239: loss=0.010577941638752721
Epoch #240: loss=0.005546648101934733
Epoch #241: loss=0.006638798609368452
Epoch #242: loss=0.011416815021434334
Epoch #243: loss=0.005485384680976597
Epoch #244: loss=0.009006783678408035
Epoch #245: loss=0.009987169602318234
Epoch #246: loss=0.011007490099539151
Epoch #247: loss=0.014727188463154559
Epoch #248: loss=0.015322832022731967
Epoch #249: loss=0.006998338737129582

Training time: 10:16:47.584418

Finished.
n2one setting etth1_traffic -> electricity
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth1_traffic_epochs_250_seed_2023/model.pkl', muti_dataset='etth1_traffic', random_seed=2023, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='electricity')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=6.57815e-09): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.46865e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=5.12424e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=6.57815e-09): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.40619967980613286, 'MAE': 0.45199987520910156}
Finished.
------------------------- record done -------------------------
n2one setting etth1_traffic -> weather
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth1_traffic_epochs_250_seed_2023/model.pkl', muti_dataset='etth1_traffic', random_seed=2023, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='weather')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.74724e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=3.44338e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.74724e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.5886294666568946, 'MAE': 0.5859248654222066}
Finished.
------------------------- record done -------------------------
n2one setting etth1_traffic -> exchange
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth1_traffic_epochs_250_seed_2023/model.pkl', muti_dataset='etth1_traffic', random_seed=2023, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='exchange')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=4.84567e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.26680900600046803, 'MAE': 0.3468807228976666}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='etth1_weather', random_seed=2023, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/etth1_weather_epochs_250_seed_2023
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=7.2344507448601
Epoch #1: loss=3.078191222566547
Epoch #2: loss=2.9258288036693227
Epoch #3: loss=2.399298151334127
Epoch #4: loss=2.3349351666190405
Epoch #5: loss=2.1125666300455728
Epoch #6: loss=1.94425972663995
Epoch #7: loss=1.892880374735052
Epoch #8: loss=1.7889865889693752
Epoch #9: loss=1.7000710819706772
Epoch #10: loss=1.6960374586509936
Epoch #11: loss=1.6108441352844238
Epoch #12: loss=1.4277607094157825
Epoch #13: loss=1.434031074697321
Epoch #14: loss=1.3076204097632207
Epoch #15: loss=1.3456883412418943
Epoch #16: loss=1.1886835080204587
Epoch #17: loss=1.0578950426795266
Epoch #18: loss=1.091780671567628
Epoch #19: loss=1.1427604230967434
Epoch #20: loss=1.0427863507559805
Epoch #21: loss=1.0453560009147183
Epoch #22: loss=0.9872963717489531
Epoch #23: loss=1.1608513575611692
Epoch #24: loss=0.9608212637178826
Epoch #25: loss=0.9068228963649634
Epoch #26: loss=0.7992742531227343
Epoch #27: loss=0.7800235206430609
Epoch #28: loss=0.886439272851655
Epoch #29: loss=0.8554342768409036
Epoch #30: loss=0.9843000661243092
Epoch #31: loss=0.8394901535727761
Epoch #32: loss=0.7373127756696759
Epoch #33: loss=0.7806185520056522
Epoch #34: loss=0.7615605430169539
Epoch #35: loss=0.7128151122367743
Epoch #36: loss=0.729643011635
Epoch #37: loss=0.8211840282786976
Epoch #38: loss=0.7041773624492415
Epoch #39: loss=0.6462216187607158
Epoch #40: loss=0.7273829552260312
Epoch #41: loss=0.7170374944354548
Epoch #42: loss=0.6476024714383212
Epoch #43: loss=0.6275684120077075
Epoch #44: loss=0.5942657382199259
Epoch #45: loss=0.6018713008273732
Epoch #46: loss=0.6502576307816939
Epoch #47: loss=0.7332349293159716
Epoch #48: loss=0.6407267198418126
Epoch #49: loss=0.635356119184783
Epoch #50: loss=0.5082967172969471
Epoch #51: loss=0.5043285317493208
Epoch #52: loss=0.514475756522381
Epoch #53: loss=0.5191790037082903
Epoch #54: loss=0.5288191528031321
Epoch #55: loss=0.505004219936602
Epoch #56: loss=0.5032141470547878
Epoch #57: loss=0.49954421773101343
Epoch #58: loss=0.4798500799771511
Epoch #59: loss=0.5331682346083901
Epoch #60: loss=0.5154566692583489
Epoch #61: loss=0.42744207201582013
Epoch #62: loss=0.37432199445637787
Epoch #63: loss=0.425736792159803
Epoch #64: loss=0.38517523714990326
Epoch #65: loss=0.37866554296377936
Epoch #66: loss=0.4112200091282527
Epoch #67: loss=0.33988423058480927
Epoch #68: loss=0.34113131192597473
Epoch #69: loss=0.4397581151940606
Epoch #70: loss=0.36855969329675037
Epoch #71: loss=0.3628218255259774
Epoch #72: loss=0.3518198710499388
Epoch #73: loss=0.3654810126983758
Epoch #74: loss=0.3263563716953451
Epoch #75: loss=0.31427643696467084
Epoch #76: loss=0.371613267696265
Epoch #77: loss=0.3328822151278005
Epoch #78: loss=0.3323878198862076
Epoch #79: loss=0.25756718624721875
Epoch #80: loss=0.3187289215398557
Epoch #81: loss=0.3076344028566823
Epoch #82: loss=0.2908570612921859
Epoch #83: loss=0.3901199224320325
Epoch #84: loss=0.39997465411822003
Epoch #85: loss=0.3518558054259329
Epoch #86: loss=0.2884888676079837
Epoch #87: loss=0.24903738927660565
Epoch #88: loss=0.2706430464079886
Epoch #89: loss=0.33001403420260456
Epoch #90: loss=0.3338286400292859
Epoch #91: loss=0.3494492053534045
Epoch #92: loss=0.35329582925998804
Epoch #93: loss=0.430807349356738
Epoch #94: loss=0.30946265025572345
Epoch #95: loss=0.2574843263084238
Epoch #96: loss=0.2780005559325218
Epoch #97: loss=0.2399083859089649
Epoch #98: loss=0.2750526610197443
Epoch #99: loss=0.20448922749721643
Epoch #100: loss=0.23165746913714844
Epoch #101: loss=0.2461195531668085
Epoch #102: loss=0.20813377224134677
Epoch #103: loss=0.2517752329056913
Epoch #104: loss=0.18852227000576077
Epoch #105: loss=0.16351518847725607
Epoch #106: loss=0.21839380332014777
Epoch #107: loss=0.24407101783788565
Epoch #108: loss=0.26325134165359265
Epoch #109: loss=0.22166264711907416
Epoch #110: loss=0.22349327554305395
Epoch #111: loss=0.20592444051395764
Epoch #112: loss=0.2106832480340293
Epoch #113: loss=0.17398545642693838
Epoch #114: loss=0.1847413798624819
Epoch #115: loss=0.24150704931129108
Epoch #116: loss=0.19763421538201245
Epoch #117: loss=0.3184031348562602
Epoch #118: loss=0.20965046706524762
Epoch #119: loss=0.17050296813249588
Epoch #120: loss=0.2533685467911489
Epoch #121: loss=0.1916661219614925
Epoch #122: loss=0.1948146124680837
Epoch #123: loss=0.15205518867481838
Epoch #124: loss=0.16654905112403812
Epoch #125: loss=0.18411039312680563
Epoch #126: loss=0.18780324799996434
Epoch #127: loss=0.15510226978045522
Epoch #128: loss=0.22941183592333939
Epoch #129: loss=0.1766525050907424
Epoch #130: loss=0.15606412641478307
Epoch #131: loss=0.1503633406351913
Epoch #132: loss=0.1792526256405946
Epoch #133: loss=0.20945911809350504
Epoch #134: loss=0.19494590524471167
Epoch #135: loss=0.15968471426855435
Epoch #136: loss=0.22098456492478197
Epoch #137: loss=0.17394116848255647
Epoch #138: loss=0.1661040261387825
Epoch #139: loss=0.2161691372819019
Epoch #140: loss=0.15155870219071707
Epoch #141: loss=0.16540052764343494
Epoch #142: loss=0.1278605233087684
Epoch #143: loss=0.13687529211694544
Epoch #144: loss=0.11898999017747966
Epoch #145: loss=0.1300347942971822
Epoch #146: loss=0.13308091106062586
Epoch #147: loss=0.17211799707376596
Epoch #148: loss=0.11271083445260019
Epoch #149: loss=0.12269851170254475
Epoch #150: loss=0.1678790139655272
Epoch #151: loss=0.2010378320560311
Epoch #152: loss=0.1954593709246679
Epoch #153: loss=0.26828857895099756
Epoch #154: loss=0.1780851109687126
Epoch #155: loss=0.13790911210305762
Epoch #156: loss=0.16449016208449999
Epoch #157: loss=0.1333432991170522
Epoch #158: loss=0.1421398107301105
Epoch #159: loss=0.11295222050764343
Epoch #160: loss=0.257892862075206
Epoch #161: loss=0.1886998830419598
Epoch #162: loss=0.14954812169978113
Epoch #163: loss=0.15076130182679856
Epoch #164: loss=0.11468242142688144
Epoch #165: loss=0.13119823598500455
Epoch #166: loss=0.12287415315707524
Epoch #167: loss=0.11916895549405705
Epoch #168: loss=0.15514116700400005
Epoch #169: loss=0.14126908564657875
Epoch #170: loss=0.10482346520505169
Epoch #171: loss=0.16264496325994982
Epoch #172: loss=0.1458835048657475
Epoch #173: loss=0.12641650421375578
Epoch #174: loss=0.13744921897622672
Epoch #175: loss=0.12366545804296479
Epoch #176: loss=0.1693968050407641
Epoch #177: loss=0.12097094795017531
Epoch #178: loss=0.20497171343727547
Epoch #179: loss=0.1270652631241264
Epoch #180: loss=0.1180247613652186
Epoch #181: loss=0.11433930858743913
Epoch #182: loss=0.11717133986001665
Epoch #183: loss=0.1253732940125646
Epoch #184: loss=0.10885673998431726
Epoch #185: loss=0.09259986866152647
Epoch #186: loss=0.07468166354704987
Epoch #187: loss=0.13231695765121418
Epoch #188: loss=0.11866486049962766
Epoch #189: loss=0.07910622515235886
Epoch #190: loss=0.11693142134357583
Epoch #191: loss=0.09354556814739198
Epoch #192: loss=0.08570055895005212
Epoch #193: loss=0.09191736649496085
Epoch #194: loss=0.08083433364376877
Epoch #195: loss=0.08723261325874111
Epoch #196: loss=0.13174959746274081
Epoch #197: loss=0.10070653214599147
Epoch #198: loss=0.11104368678096568
Epoch #199: loss=0.09982299782109982
Epoch #200: loss=0.11023727285139488
Epoch #201: loss=0.11794416397584206
Epoch #202: loss=0.10609225144214703
Epoch #203: loss=0.08652115726109708
Epoch #204: loss=0.06621910403059288
Epoch #205: loss=0.07823840633147594
Epoch #206: loss=0.0821073820080721
Epoch #207: loss=0.13926978922928823
Epoch #208: loss=0.07388358207588847
Epoch #209: loss=0.07462197680477843
Epoch #210: loss=0.08458557595131976
Epoch #211: loss=0.07572818747862722
Epoch #212: loss=0.11159033466582045
Epoch #213: loss=0.11960487033833157
Epoch #214: loss=0.11504120633683422
Epoch #215: loss=0.1189276325996175
Epoch #216: loss=0.08292757059362801
Epoch #217: loss=0.08117910463250044
Epoch #218: loss=0.09067825211042707
Epoch #219: loss=0.08760685772832597
Epoch #220: loss=0.1131129323250868
Epoch #221: loss=0.0872057124295018
Epoch #222: loss=0.09490205951486573
Epoch #223: loss=0.08838509477562073
Epoch #224: loss=0.08930031596824076
Epoch #225: loss=0.09603671664654305
Epoch #226: loss=0.08486686390119069
Epoch #227: loss=0.08208432900860455
Epoch #228: loss=0.06710411379621788
Epoch #229: loss=0.09928906804910212
Epoch #230: loss=0.1074013900801991
Epoch #231: loss=0.08122381796552376
Epoch #232: loss=0.07380651197198665
Epoch #233: loss=0.07513609356387999
Epoch #234: loss=0.09816215249399345
Epoch #235: loss=0.0687190702918804
Epoch #236: loss=0.0696394390172579
Epoch #237: loss=0.07249839166461518
Epoch #238: loss=0.11968239949959697
Epoch #239: loss=0.07315044573536425
Epoch #240: loss=0.0550437637068557
Epoch #241: loss=0.07538194025894909
Epoch #242: loss=0.15190471782151496
Epoch #243: loss=0.08141852954797672
Epoch #244: loss=0.22399896812258344
Epoch #245: loss=0.1328248167692712
Epoch #246: loss=0.0975213014718258
Epoch #247: loss=0.14592115417348617
Epoch #248: loss=0.10702742845045798
Epoch #249: loss=0.07027321828134132

Training time: 0:32:07.638284

Finished.
n2one setting etth1_weather -> electricity
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth1_weather_epochs_250_seed_2023/model.pkl', muti_dataset='etth1_weather', random_seed=2023, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='electricity')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.39649e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.92074e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.39649e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.36638092257651045, 'MAE': 0.42362165178762023}
Finished.
------------------------- record done -------------------------
n2one setting etth1_weather -> exchange
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth1_weather_epochs_250_seed_2023/model.pkl', muti_dataset='etth1_weather', random_seed=2023, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='exchange')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.97813e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.21261917793442997, 'MAE': 0.3185184136935926}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='etth1_exchange', random_seed=2023, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/etth1_exchange_epochs_250_seed_2023
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=4.533911291758219
Epoch #1: loss=2.4917834997177124
Epoch #2: loss=2.0833119710286456
Epoch #3: loss=1.8071588198343913
Epoch #4: loss=1.8499510765075684
Epoch #5: loss=1.770011305809021
Epoch #6: loss=1.6227395296096803
Epoch #7: loss=1.6529773712158202
Epoch #8: loss=1.5784374157587686
Epoch #9: loss=1.5875258525212605
Epoch #10: loss=1.4936804533004762
Epoch #11: loss=1.408337410291036
Epoch #12: loss=1.3725328207015992
Epoch #13: loss=1.4234097878138223
Epoch #14: loss=1.3570605675379435
Epoch #15: loss=1.2873833497365317
Epoch #16: loss=1.1979593276977538
Epoch #17: loss=1.2051669518152872
Epoch #18: loss=1.1089259107907614
Epoch #19: loss=1.1240121324857075
Epoch #20: loss=1.2430043856302897
Epoch #21: loss=1.0920029044151307
Epoch #22: loss=1.0481461723645529
Epoch #23: loss=1.0562193393707275
Epoch #24: loss=1.0603830456733703
Epoch #25: loss=1.0166134556134543
Epoch #26: loss=1.1029141902923585
Epoch #27: loss=0.944861892859141
Epoch #28: loss=0.9269342025121053
Epoch #29: loss=0.8492379625638325
Epoch #30: loss=1.0052472670873007
Epoch #31: loss=0.9170256972312927
Epoch #32: loss=0.922053595383962
Epoch #33: loss=0.9446014006932576
Epoch #34: loss=0.9173613429069519
Epoch #35: loss=0.7531376520792643
Epoch #36: loss=0.8751892566680908
Epoch #37: loss=0.8402056256930034
Epoch #38: loss=0.8032449881235758
Epoch #39: loss=0.8029579043388366
Epoch #40: loss=0.8343519826730093
Epoch #41: loss=1.056989586353302
Epoch #42: loss=0.796864906946818
Epoch #43: loss=0.8296450654665629
Epoch #44: loss=0.8337180614471436
Epoch #45: loss=0.7500979026158651
Epoch #46: loss=0.5917978902657827
Epoch #47: loss=0.7323506832122803
Epoch #48: loss=0.6587925434112549
Epoch #49: loss=0.7430546482404073
Epoch #50: loss=0.6805991351604461
Epoch #51: loss=0.6457726041475932
Epoch #52: loss=0.6000778416792552
Epoch #53: loss=0.5809369007746379
Epoch #54: loss=0.6916642824808756
Epoch #55: loss=0.7212318082650503
Epoch #56: loss=0.6710825761159261
Epoch #57: loss=0.6174312015374501
Epoch #58: loss=0.570769586165746
Epoch #59: loss=0.5372885664304098
Epoch #60: loss=0.5470080892244975
Epoch #61: loss=0.5706925769646962
Epoch #62: loss=0.6651242236296336
Epoch #63: loss=0.4724234859148661
Epoch #64: loss=0.48539489408334097
Epoch #65: loss=0.571413912375768
Epoch #66: loss=0.4755800932645798
Epoch #67: loss=0.5215244730313618
Epoch #68: loss=0.4889472166697184
Epoch #69: loss=0.46870385805765785
Epoch #70: loss=0.5082689881324768
Epoch #71: loss=0.5873810052871704
Epoch #72: loss=0.5943659245967865
Epoch #73: loss=0.5477944294611613
Epoch #74: loss=0.44875929355621336
Epoch #75: loss=0.419700425863266
Epoch #76: loss=0.4478090137243271
Epoch #77: loss=0.3742562472820282
Epoch #78: loss=0.5875728130340576
Epoch #79: loss=0.5406068583329519
Epoch #80: loss=0.5224139253298442
Epoch #81: loss=0.5046229978402456
Epoch #82: loss=0.5130865414937337
Epoch #83: loss=0.4205480565627416
Epoch #84: loss=0.3624337732791901
Epoch #85: loss=0.4387112259864807
Epoch #86: loss=0.42293873727321624
Epoch #87: loss=0.539311929543813
Epoch #88: loss=0.42227125763893125
Epoch #89: loss=0.42746854623158775
Epoch #90: loss=0.48160862028598783
Epoch #91: loss=0.4451603939135869
Epoch #92: loss=0.4550476253032684
Epoch #93: loss=0.37482460737228396
Epoch #94: loss=0.34858665664990746
Epoch #95: loss=0.39606203337510426
Epoch #96: loss=0.3625552773475647
Epoch #97: loss=0.38410451461871464
Epoch #98: loss=0.3016979763905207
Epoch #99: loss=0.4547067165374756
Epoch #100: loss=0.4248062789440155
Epoch #101: loss=0.42880100856224695
Epoch #102: loss=0.3156071066856384
Epoch #103: loss=0.42549503842989606
Epoch #104: loss=0.4448673099279404
Epoch #105: loss=0.3438261479139328
Epoch #106: loss=0.4247944196065267
Epoch #107: loss=0.39978717764218646
Epoch #108: loss=0.5506482938925426
Epoch #109: loss=0.3236021985610326
Epoch #110: loss=0.3632144769032796
Epoch #111: loss=0.33304132918516793
Epoch #112: loss=0.38326922605435054
Epoch #113: loss=0.3421442538499832
Epoch #114: loss=0.2706739236911138
Epoch #115: loss=0.282323948542277
Epoch #116: loss=0.24844009727239608
Epoch #117: loss=0.3088184972604116
Epoch #118: loss=0.22581541736920674
Epoch #119: loss=0.34425318042437236
Epoch #120: loss=0.4175639043251673
Epoch #121: loss=0.4849083443482717
Epoch #122: loss=0.35562873582045235
Epoch #123: loss=0.40930401484171547
Epoch #124: loss=0.3597336957852046
Epoch #125: loss=0.3331983337799708
Epoch #126: loss=0.2987362305323283
Epoch #127: loss=0.295551269253095
Epoch #128: loss=0.41516775091489155
Epoch #129: loss=0.3906191905339559
Epoch #130: loss=0.3033292869726817
Epoch #131: loss=0.32081967691580454
Epoch #132: loss=0.2854381223519643
Epoch #133: loss=0.252827891210715
Epoch #134: loss=0.2775660951932271
Epoch #135: loss=0.3703245302041372
Epoch #136: loss=0.3390896225968997
Epoch #137: loss=0.33204915225505827
Epoch #138: loss=0.317258808016777
Epoch #139: loss=0.35868320961793265
Epoch #140: loss=0.3416627526283264
Epoch #141: loss=0.3030156950155894
Epoch #142: loss=0.31711294651031496
Epoch #143: loss=0.24989383021990458
Epoch #144: loss=0.30127907246351243
Epoch #145: loss=0.38751545945803323
Epoch #146: loss=0.42971170544624326
Epoch #147: loss=0.33123838901519775
Epoch #148: loss=0.3212972044944763
Epoch #149: loss=0.3128599454959234
Epoch #150: loss=0.30889950344959893
Epoch #151: loss=0.2867855538924535
Epoch #152: loss=0.2802232305208842
Epoch #153: loss=0.26986323098341625
Epoch #154: loss=0.2621322492758433
Epoch #155: loss=0.22587065200010936
Epoch #156: loss=0.28233325531085335
Epoch #157: loss=0.3052878126502037
Epoch #158: loss=0.34071636696656543
Epoch #159: loss=0.2658691182732582
Epoch #160: loss=0.31982262829939523
Epoch #161: loss=0.3032089779774348
Epoch #162: loss=0.26745346436897915
Epoch #163: loss=0.3130823850631714
Epoch #164: loss=0.2856818988919258
Epoch #165: loss=0.3070058638850848
Epoch #166: loss=0.2988079860806465
Epoch #167: loss=0.2586261883378029
Epoch #168: loss=0.21631522526343663
Epoch #169: loss=0.23549039363861085
Epoch #170: loss=0.2719021434585253
Epoch #171: loss=0.23125550746917725
Epoch #172: loss=0.2825410559773445
Epoch #173: loss=0.23583656946818035
Epoch #174: loss=0.20680054848392804
Epoch #175: loss=0.28373909642299017
Epoch #176: loss=0.2968063898384571
Epoch #177: loss=0.2021265856921673
Epoch #178: loss=0.3393148794770241
Epoch #179: loss=0.3105635608235995
Epoch #180: loss=0.21791807015736897
Epoch #181: loss=0.245849605401357
Epoch #182: loss=0.19484415700038274
Epoch #183: loss=0.23334736203153927
Epoch #184: loss=0.2555930703878403
Epoch #185: loss=0.25244052906831105
Epoch #186: loss=0.21279181589682897
Epoch #187: loss=0.3354447990655899
Epoch #188: loss=0.32053559372822443
Epoch #189: loss=0.21816891779502232
Epoch #190: loss=0.2571328351895014
Epoch #191: loss=0.21635164072116217
Epoch #192: loss=0.21723268628120423
Epoch #193: loss=0.2052316203713417
Epoch #194: loss=0.22610225627819697
Epoch #195: loss=0.19834327151377995
Epoch #196: loss=0.2242149069905281
Epoch #197: loss=0.18331850667794544
Epoch #198: loss=0.2026412402590116
Epoch #199: loss=0.17739596888422965
Epoch #200: loss=0.17955981219808262
Epoch #201: loss=0.19196263800064722
Epoch #202: loss=0.21333540206154186
Epoch #203: loss=0.16788023263216018
Epoch #204: loss=0.16672818536559741
Epoch #205: loss=0.16269584223628045
Epoch #206: loss=0.1863918403784434
Epoch #207: loss=0.1992635063827038
Epoch #208: loss=0.27311807001630467
Epoch #209: loss=0.2998779724041621
Epoch #210: loss=0.30387222866217295
Epoch #211: loss=0.2673663745323817
Epoch #212: loss=0.2712655857205391
Epoch #213: loss=0.26565714478492736
Epoch #214: loss=0.27516936312119167
Epoch #215: loss=0.2657754197716713
Epoch #216: loss=0.22012427672743798
Epoch #217: loss=0.2583110317587852
Epoch #218: loss=0.40045618762572605
Epoch #219: loss=0.28726818958918254
Epoch #220: loss=0.20949958860874177
Epoch #221: loss=0.22008865773677827
Epoch #222: loss=0.20256724059581757
Epoch #223: loss=0.3089024422069391
Epoch #224: loss=0.23308294514815012
Epoch #225: loss=0.20752021769682566
Epoch #226: loss=0.1456964284181595
Epoch #227: loss=0.13845017328858375
Epoch #228: loss=0.16826252366105715
Epoch #229: loss=0.30166030799349147
Epoch #230: loss=0.19202796866496405
Epoch #231: loss=0.270090722044309
Epoch #232: loss=0.2185678447286288
Epoch #233: loss=0.18607448041439056
Epoch #234: loss=0.24245779017607372
Epoch #235: loss=0.18103166272242863
Epoch #236: loss=0.16753467495242755
Epoch #237: loss=0.2349925898015499
Epoch #238: loss=0.24380567347009977
Epoch #239: loss=0.2058316523830096
Epoch #240: loss=0.18707004065314928
Epoch #241: loss=0.22297770182291668
Epoch #242: loss=0.20397217497229575
Epoch #243: loss=0.1634277845422427
Epoch #244: loss=0.22518721967935562
Epoch #245: loss=0.2163515495757262
Epoch #246: loss=0.22222066223621367
Epoch #247: loss=0.23066831181446712
Epoch #248: loss=0.1827146386106809
Epoch #249: loss=0.21194828748703004

Training time: 0:10:38.612093

Finished.
n2one setting etth1_exchange -> electricity
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth1_exchange_epochs_250_seed_2023/model.pkl', muti_dataset='etth1_exchange', random_seed=2023, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='electricity')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.41076e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.81335e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.41076e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.35839166373571674, 'MAE': 0.4259864634380101}
Finished.
------------------------- record done -------------------------
n2one setting etth1_exchange -> weather
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth1_exchange_epochs_250_seed_2023/model.pkl', muti_dataset='etth1_exchange', random_seed=2023, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='weather')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.2522e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.50745e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.2522e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.5669934366071233, 'MAE': 0.5723039904628995}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='etth2_ettm1', random_seed=2023, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/etth2_ettm1_epochs_250_seed_2023
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=5.3051191038555565
Epoch #1: loss=2.8630618386798434
Epoch #2: loss=2.4101036654578314
Epoch #3: loss=2.2432014677259655
Epoch #4: loss=2.067863014009264
Epoch #5: loss=1.9310953285959032
Epoch #6: loss=1.7804745899306402
Epoch #7: loss=1.7228806416193645
Epoch #8: loss=1.6524743305312262
Epoch #9: loss=1.5499398509661357
Epoch #10: loss=1.4415819380018446
Epoch #11: loss=1.342492315504286
Epoch #12: loss=1.3160590330759685
Epoch #13: loss=1.2751604053709242
Epoch #14: loss=1.1819906797673967
Epoch #15: loss=1.1516522963841755
Epoch #16: loss=1.2446184323893652
Epoch #17: loss=1.0822868876987033
Epoch #18: loss=1.028710189792845
Epoch #19: loss=0.9674148857593536
Epoch #20: loss=0.9943857888380686
Epoch #21: loss=0.9659280478954315
Epoch #22: loss=0.9998039404551188
Epoch #23: loss=0.9468463261922201
Epoch #24: loss=0.9008624686135186
Epoch #25: loss=0.9944089982244704
Epoch #26: loss=0.9591595894760556
Epoch #27: loss=0.9238582385910882
Epoch #28: loss=0.8362958365016513
Epoch #29: loss=0.8064647946092818
Epoch #30: loss=0.9144573277897305
Epoch #31: loss=0.8472864064905379
Epoch #32: loss=0.7833634085125394
Epoch #33: loss=0.7200462155871921
Epoch #34: loss=0.6772952907615237
Epoch #35: loss=0.7654556499587165
Epoch #36: loss=0.7082931995391846
Epoch #37: loss=0.6519108447763655
Epoch #38: loss=0.7695587509208255
Epoch #39: loss=0.7211151520411173
Epoch #40: loss=0.748756253057056
Epoch #41: loss=0.7167722682158152
Epoch #42: loss=0.8057636337147819
Epoch #43: loss=0.808648685614268
Epoch #44: loss=0.7417624559667375
Epoch #45: loss=0.613971985048718
Epoch #46: loss=0.6269496546851264
Epoch #47: loss=0.6496788793139987
Epoch #48: loss=0.6411587446928024
Epoch #49: loss=0.5298568656047186
Epoch #50: loss=0.5527141061094072
Epoch #51: loss=0.5322374535931481
Epoch #52: loss=0.5043182406160567
Epoch #53: loss=0.5290654235415988
Epoch #54: loss=0.5517128672864702
Epoch #55: loss=0.4904077284865909
Epoch #56: loss=0.56014427377118
Epoch #57: loss=0.5955879737933477
Epoch #58: loss=0.597470321589046
Epoch #59: loss=0.633657874332534
Epoch #60: loss=0.5051831007003784
Epoch #61: loss=0.4721550941467285
Epoch #62: loss=0.45971877376238507
Epoch #63: loss=0.49763364758756423
Epoch #64: loss=0.46963783933056724
Epoch #65: loss=0.4732550581296285
Epoch #66: loss=0.46140185329649186
Epoch #67: loss=0.4914697855710983
Epoch #68: loss=0.5056643403238721
Epoch #69: loss=0.4449822422530916
Epoch #70: loss=0.41858453386359745
Epoch #71: loss=0.44362326959768933
Epoch #72: loss=0.40787653293874526
Epoch #73: loss=0.4054723580678304
Epoch #74: loss=0.4543283333381017
Epoch #75: loss=0.4702979293134477
Epoch #76: loss=0.4596422248416477
Epoch #77: loss=0.4028017785814073
Epoch #78: loss=0.3564888685941696
Epoch #79: loss=0.36704490416579777
Epoch #80: loss=0.35414640771018135
Epoch #81: loss=0.44968482355276745
Epoch #82: loss=0.36922741598553127
Epoch #83: loss=0.342493437230587
Epoch #84: loss=0.34584279110034305
Epoch #85: loss=0.3513932021127807
Epoch #86: loss=0.31734887262185413
Epoch #87: loss=0.3496441062953737
Epoch #88: loss=0.35404372132486767
Epoch #89: loss=0.32765133182207745
Epoch #90: loss=0.4444753486249182
Epoch #91: loss=0.46580014295048183
Epoch #92: loss=0.4017350773016612
Epoch #93: loss=0.35348373237583375
Epoch #94: loss=0.3117829006579187
Epoch #95: loss=0.3030746993091371
Epoch #96: loss=0.29374079240692985
Epoch #97: loss=0.3134119634826978
Epoch #98: loss=0.3337670713663101
Epoch #99: loss=0.31288426286644405
Epoch #100: loss=0.38688791874382233
Epoch #101: loss=0.3440287742349837
Epoch #102: loss=0.5887651840845743
Epoch #103: loss=0.46961045761903125
Epoch #104: loss=0.6370933204889297
Epoch #105: loss=0.4211469756232368
Epoch #106: loss=0.3282679095864296
Epoch #107: loss=0.27029213474856484
Epoch #108: loss=0.2753273554974132
Epoch #109: loss=0.2609918970200751
Epoch #110: loss=0.31989817652437424
Epoch #111: loss=0.24815567913982603
Epoch #112: loss=0.40804242591063183
Epoch #113: loss=0.36565111246373916
Epoch #114: loss=0.31914374646213317
Epoch #115: loss=0.24564046495490605
Epoch #116: loss=0.31195587250921464
Epoch #117: loss=0.28011168787876767
Epoch #118: loss=0.28623471243513954
Epoch #119: loss=0.20313565184672674
Epoch #120: loss=0.27373752660221523
Epoch #121: loss=0.2271166452103191
Epoch #122: loss=0.21575559137596023
Epoch #123: loss=0.21468541481428677
Epoch #124: loss=0.23271624247233072
Epoch #125: loss=0.3054201296634144
Epoch #126: loss=0.28781776047415203
Epoch #127: loss=0.2210210164388021
Epoch #128: loss=0.5677082170214918
Epoch #129: loss=0.41278378748231465
Epoch #130: loss=0.38347912993696
Epoch #131: loss=0.256420095761617
Epoch #132: loss=0.24498382293515736
Epoch #133: loss=0.27834578769074547
Epoch #134: loss=0.2974761782421006
Epoch #135: loss=0.22766949608922005
Epoch #136: loss=0.21027526135245958
Epoch #137: loss=0.23241300053066677
Epoch #138: loss=0.20925728231668472
Epoch #139: loss=0.30237717347012627
Epoch #140: loss=0.20828965596026844
Epoch #141: loss=0.18565661253200638
Epoch #142: loss=0.23225211848815283
Epoch #143: loss=0.21441834213005173
Epoch #144: loss=0.21956760808825493
Epoch #145: loss=0.22497097237242591
Epoch #146: loss=0.16954099552498925
Epoch #147: loss=0.16475209097067514
Epoch #148: loss=0.1960520533223947
Epoch #149: loss=0.1767543318370978
Epoch #150: loss=0.1883107390668657
Epoch #151: loss=0.21621441716949144
Epoch #152: loss=0.23368479601211017
Epoch #153: loss=0.1556271364291509
Epoch #154: loss=0.18117902986705303
Epoch #155: loss=0.22061548464828068
Epoch #156: loss=0.1688373295797242
Epoch #157: loss=0.15250505030983025
Epoch #158: loss=0.32799985880653065
Epoch #159: loss=0.2496721487906244
Epoch #160: loss=0.24301640647980902
Epoch #161: loss=0.2326534382171101
Epoch #162: loss=0.22419065692358547
Epoch #163: loss=0.16069049719307157
Epoch #164: loss=0.1479951408174303
Epoch #165: loss=0.1606738277607494
Epoch #166: loss=0.21151298036177954
Epoch #167: loss=0.12886687947644127
Epoch #168: loss=0.2638108144617743
Epoch #169: loss=0.1538512363202042
Epoch #170: loss=0.26133545198374325
Epoch #171: loss=0.2686354062623448
Epoch #172: loss=0.17908402987652355
Epoch #173: loss=0.18123622818125618
Epoch #174: loss=0.18045490152306026
Epoch #175: loss=0.12507515586912632
Epoch #176: loss=0.1293910009165605
Epoch #177: loss=0.15438133726517358
Epoch #178: loss=0.14384129126038817
Epoch #179: loss=0.16950550551215807
Epoch #180: loss=0.19965809832016626
Epoch #181: loss=0.1482832214484612
Epoch #182: loss=0.15861743957632118
Epoch #183: loss=0.14726270197166336
Epoch #184: loss=0.15929712334440815
Epoch #185: loss=0.15719628458221754
Epoch #186: loss=0.17680732905864716
Epoch #187: loss=0.18172704676787058
Epoch #188: loss=0.24498530104756355
Epoch #189: loss=0.23851607201827896
Epoch #190: loss=0.13230914519064957
Epoch #191: loss=0.11148209828469488
Epoch #192: loss=0.14935757944153416
Epoch #193: loss=0.11226938954657978
Epoch #194: loss=0.11003881030612522
Epoch #195: loss=0.1511147241625521
Epoch #196: loss=0.10127287440829807
Epoch #197: loss=0.1421384916951259
Epoch #198: loss=0.14374848424146572
Epoch #199: loss=0.12840748267869154
Epoch #200: loss=0.16565185536940893
Epoch #201: loss=0.13296080898079607
Epoch #202: loss=0.14638037917514643
Epoch #203: loss=0.12870442080828878
Epoch #204: loss=0.1352069280627701
Epoch #205: loss=0.11412930736939113
Epoch #206: loss=0.11597935410423411
Epoch #207: loss=0.11043563681758112
Epoch #208: loss=0.10596005546136035
Epoch #209: loss=0.13799447835319573
Epoch #210: loss=0.13231964078214434
Epoch #211: loss=0.11365208588540554
Epoch #212: loss=0.13658243272867468
Epoch #213: loss=0.20953978494637543
Epoch #214: loss=0.17636890295479032
Epoch #215: loss=0.13388756000333363
Epoch #216: loss=0.10918303123778766
Epoch #217: loss=0.25323045408974093
Epoch #218: loss=0.13564961031079292
Epoch #219: loss=0.12611491150326198
Epoch #220: loss=0.10670594829652044
Epoch #221: loss=0.138955550475253
Epoch #222: loss=0.08854081999096605
Epoch #223: loss=0.11074349408348401
Epoch #224: loss=0.1377676853703128
Epoch #225: loss=0.1978922002017498
Epoch #226: loss=0.19914922159579065
Epoch #227: loss=0.16899922986825308
Epoch #228: loss=0.1135235455714994
Epoch #229: loss=0.18464436071614423
Epoch #230: loss=0.2559877261519432
Epoch #231: loss=0.12987888273265627
Epoch #232: loss=0.11147951583067577
Epoch #233: loss=0.13754956289711925
Epoch #234: loss=0.16551453889244133
Epoch #235: loss=0.12353904058949815
Epoch #236: loss=0.11736564710736275
Epoch #237: loss=0.09559671673923731
Epoch #238: loss=0.11024832249515587
Epoch #239: loss=0.07807690629528628
Epoch #240: loss=0.08801854877836174
Epoch #241: loss=0.07192642795335916
Epoch #242: loss=0.09010706262456046
Epoch #243: loss=0.12510224017832014
Epoch #244: loss=0.11385721671912405
Epoch #245: loss=0.12515361110369363
Epoch #246: loss=0.1402208228699035
Epoch #247: loss=0.08565019215974543
Epoch #248: loss=0.14260598892966905
Epoch #249: loss=0.07278500807782014

Training time: 0:16:05.887983

Finished.
n2one setting etth2_ettm1 -> electricity
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth2_ettm1_epochs_250_seed_2023/model.pkl', muti_dataset='etth2_ettm1', random_seed=2023, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='electricity')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.44574e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.85012e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.44574e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.36140535675522806, 'MAE': 0.42210341237500654}
Finished.
------------------------- record done -------------------------
n2one setting etth2_ettm1 -> weather
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth2_ettm1_epochs_250_seed_2023/model.pkl', muti_dataset='etth2_ettm1', random_seed=2023, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='weather')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.47037e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.8104e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.47037e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 1.0475808792883203, 'MAE': 0.8124236540393298}
Finished.
------------------------- record done -------------------------
n2one setting etth2_ettm1 -> exchange
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth2_ettm1_epochs_250_seed_2023/model.pkl', muti_dataset='etth2_ettm1', random_seed=2023, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='exchange')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=4.13073e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.20137988982363847, 'MAE': 0.3084054876044029}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='etth2_ettm2', random_seed=2023, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/etth2_ettm2_epochs_250_seed_2023
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=5.082515623834398
Epoch #1: loss=2.7837292883131237
Epoch #2: loss=2.450013836224874
Epoch #3: loss=2.2326471540662975
Epoch #4: loss=2.0500403311517506
Epoch #5: loss=1.9107113480567932
Epoch #6: loss=1.7656904194090102
Epoch #7: loss=1.695526036951277
Epoch #8: loss=1.6315859887335036
Epoch #9: loss=1.5474633640713162
Epoch #10: loss=1.4276015890969171
Epoch #11: loss=1.3146235214339361
Epoch #12: loss=1.2996495167414348
Epoch #13: loss=1.2783024377293057
Epoch #14: loss=1.1849803527196248
Epoch #15: loss=1.1519637273417578
Epoch #16: loss=1.211841106414795
Epoch #17: loss=1.08731147646904
Epoch #18: loss=1.0129006604353588
Epoch #19: loss=0.962062633699841
Epoch #20: loss=0.9813126987881131
Epoch #21: loss=0.9633254077699449
Epoch #22: loss=0.9821756713920169
Epoch #23: loss=0.9366185731357999
Epoch #24: loss=0.8804645008511014
Epoch #25: loss=1.0003456638918982
Epoch #26: loss=0.9469062818421258
Epoch #27: loss=0.8981998595926497
Epoch #28: loss=0.8267550435331132
Epoch #29: loss=0.7564336823092567
Epoch #30: loss=0.8270982901255289
Epoch #31: loss=0.8119524187511868
Epoch #32: loss=0.7642918460898929
Epoch #33: loss=0.7275875475671556
Epoch #34: loss=0.6677137447728051
Epoch #35: loss=0.7709087232748667
Epoch #36: loss=0.6722764670848846
Epoch #37: loss=0.6233534018198649
Epoch #38: loss=0.704481722580062
Epoch #39: loss=0.6254460430807538
Epoch #40: loss=0.6586590343051486
Epoch #41: loss=0.6896207878986994
Epoch #42: loss=0.7621742304828432
Epoch #43: loss=0.6944739686118232
Epoch #44: loss=0.6707008514139388
Epoch #45: loss=0.5549794650740094
Epoch #46: loss=0.5920010772016313
Epoch #47: loss=0.6410013073020511
Epoch #48: loss=0.6511426766713461
Epoch #49: loss=0.5126008606619306
Epoch #50: loss=0.5358766847186618
Epoch #51: loss=0.5143421540657679
Epoch #52: loss=0.5001918888754315
Epoch #53: loss=0.5161187499761581
Epoch #54: loss=0.5407459454403983
Epoch #55: loss=0.4652320245901744
Epoch #56: loss=0.5216861847374175
Epoch #57: loss=0.5892006506522497
Epoch #58: loss=0.665868866774771
Epoch #59: loss=0.6812369045284059
Epoch #60: loss=0.5112126833862729
Epoch #61: loss=0.4674031370215946
Epoch #62: loss=0.456577534476916
Epoch #63: loss=0.47235046492682564
Epoch #64: loss=0.46288693447907764
Epoch #65: loss=0.460704055097368
Epoch #66: loss=0.452401844991578
Epoch #67: loss=0.46805554462803733
Epoch #68: loss=0.484347951081064
Epoch #69: loss=0.42547479271888733
Epoch #70: loss=0.3949388729201423
Epoch #71: loss=0.4176154269112481
Epoch #72: loss=0.3874360140826967
Epoch #73: loss=0.39734530283345115
Epoch #74: loss=0.45035193529393935
Epoch #75: loss=0.5481569286849763
Epoch #76: loss=0.4609123882320192
Epoch #77: loss=0.4072098566426171
Epoch #78: loss=0.36313770711421967
Epoch #79: loss=0.3775874028603236
Epoch #80: loss=0.3421860138575236
Epoch #81: loss=0.4153892281982634
Epoch #82: loss=0.3341057151556015
Epoch #83: loss=0.3128228477305836
Epoch #84: loss=0.3258293486303753
Epoch #85: loss=0.324615053832531
Epoch #86: loss=0.30667273369100356
Epoch #87: loss=0.36930619676907855
Epoch #88: loss=0.33727022508780163
Epoch #89: loss=0.2910361761848132
Epoch #90: loss=0.42170815500948167
Epoch #91: loss=0.45732990983459687
Epoch #92: loss=0.4059394382768207
Epoch #93: loss=0.33768856442636913
Epoch #94: loss=0.2796984190742175
Epoch #95: loss=0.28230293260680306
Epoch #96: loss=0.2777012454138862
Epoch #97: loss=0.2991495786441697
Epoch #98: loss=0.3082563454906146
Epoch #99: loss=0.28217889699671006
Epoch #100: loss=0.3996792683998744
Epoch #101: loss=0.3367212116718292
Epoch #102: loss=0.5955559031830894
Epoch #103: loss=0.46730438868204754
Epoch #104: loss=0.6506227900584539
Epoch #105: loss=0.42342394093672436
Epoch #106: loss=0.3299940799673398
Epoch #107: loss=0.26307924422952866
Epoch #108: loss=0.2713089966111713
Epoch #109: loss=0.2523660005794631
Epoch #110: loss=0.3244250218073527
Epoch #111: loss=0.29777367495828205
Epoch #112: loss=0.4384837258193228
Epoch #113: loss=0.38755563232633805
Epoch #114: loss=0.32472094976239735
Epoch #115: loss=0.25207096421056324
Epoch #116: loss=0.3237992367810673
Epoch #117: loss=0.2951950364642673
Epoch #118: loss=0.2715068790647719
Epoch #119: loss=0.21777469085322487
Epoch #120: loss=0.2885718560881085
Epoch #121: loss=0.22803307614392704
Epoch #122: loss=0.21961242788367802
Epoch #123: loss=0.21152829958332908
Epoch #124: loss=0.24256551555461353
Epoch #125: loss=0.3482354117764367
Epoch #126: loss=0.2805495104855961
Epoch #127: loss=0.21933410026960903
Epoch #128: loss=0.6126224282715056
Epoch #129: loss=0.4126851252383656
Epoch #130: loss=0.3684636528293292
Epoch #131: loss=0.24699456410275566
Epoch #132: loss=0.2858123928308487
Epoch #133: loss=0.2893923181626532
Epoch #134: loss=0.3009936230050193
Epoch #135: loss=0.23754219089945158
Epoch #136: loss=0.22400075528356764
Epoch #137: loss=0.255054849303431
Epoch #138: loss=0.22348256160815558
Epoch #139: loss=0.3034201529290941
Epoch #140: loss=0.18989322293135855
Epoch #141: loss=0.17787539669209057
Epoch #142: loss=0.19400567395819557
Epoch #143: loss=0.19488288213809332
Epoch #144: loss=0.21339371096756724
Epoch #145: loss=0.20683442387315962
Epoch #146: loss=0.1724502585000462
Epoch #147: loss=0.16136403216256034
Epoch #148: loss=0.193350856916772
Epoch #149: loss=0.17867765120334095
Epoch #150: loss=0.1795726716518402
Epoch #151: loss=0.21856972989108828
Epoch #152: loss=0.24709397678573927
Epoch #153: loss=0.17966301780607966
Epoch #154: loss=0.20560755415095222
Epoch #155: loss=0.2231362917357021
Epoch #156: loss=0.16975587689214283
Epoch #157: loss=0.1315637586845292
Epoch #158: loss=0.29855939621726674
Epoch #159: loss=0.21669369522068235
Epoch #160: loss=0.16010242638488612
Epoch #161: loss=0.2121405257946915
Epoch #162: loss=0.20496170471111932
Epoch #163: loss=0.1504923159049617
Epoch #164: loss=0.13437526238461336
Epoch #165: loss=0.14001214793986744
Epoch #166: loss=0.17062606683207882
Epoch #167: loss=0.11952432265712155
Epoch #168: loss=0.2776230577793386
Epoch #169: loss=0.13634200683898395
Epoch #170: loss=0.21533990879025725
Epoch #171: loss=0.2030066775365008
Epoch #172: loss=0.16422433820035723
Epoch #173: loss=0.14606557517415947
Epoch #174: loss=0.1413079895493057
Epoch #175: loss=0.14784721409281096
Epoch #176: loss=0.17551950468785232
Epoch #177: loss=0.16035881038341257
Epoch #178: loss=0.13656795811322
Epoch #179: loss=0.15913293013970056
Epoch #180: loss=0.14177238506575426
Epoch #181: loss=0.12803444928593105
Epoch #182: loss=0.14906594157218933
Epoch #183: loss=0.14552057120535108
Epoch #184: loss=0.1454982418153021
Epoch #185: loss=0.1879364750865433
Epoch #186: loss=0.15572626122997868
Epoch #187: loss=0.19423736155860954
Epoch #188: loss=0.24626113743417793
Epoch #189: loss=0.25500377515951794
Epoch #190: loss=0.14280885395904383
Epoch #191: loss=0.11386948637664318
Epoch #192: loss=0.15217070747166872
Epoch #193: loss=0.11239021230075094
Epoch #194: loss=0.10719072177178329
Epoch #195: loss=0.15307089955442482
Epoch #196: loss=0.11462943048940764
Epoch #197: loss=0.13411398521727985
Epoch #198: loss=0.1320304183496369
Epoch #199: loss=0.13479126099911
Epoch #200: loss=0.16793491111861336
Epoch #201: loss=0.13433254758516947
Epoch #202: loss=0.1311478681034512
Epoch #203: loss=0.12168566437645091
Epoch #204: loss=0.12502738274633884
Epoch #205: loss=0.11385548042340411
Epoch #206: loss=0.11193812535040909
Epoch #207: loss=0.10601426505794127
Epoch #208: loss=0.11273278223557605
Epoch #209: loss=0.14220071149369082
Epoch #210: loss=0.1428043240060409
Epoch #211: loss=0.1272127725597885
Epoch #212: loss=0.14509492119153342
Epoch #213: loss=0.21400364260706636
Epoch #214: loss=0.16286620414919323
Epoch #215: loss=0.15966414391166633
Epoch #216: loss=0.12182248507936795
Epoch #217: loss=0.26080776275032097
Epoch #218: loss=0.1292785250892242
Epoch #219: loss=0.10926398572822411
Epoch #220: loss=0.1136712779601415
Epoch #221: loss=0.1328750147173802
Epoch #222: loss=0.08731214101943705
Epoch #223: loss=0.1259569740957684
Epoch #224: loss=0.218438769173291
Epoch #225: loss=0.22864662317766082
Epoch #226: loss=0.17145871122678122
Epoch #227: loss=0.17164170731686884
Epoch #228: loss=0.11043246566421455
Epoch #229: loss=0.17822735218538177
Epoch #230: loss=0.2365660203827752
Epoch #231: loss=0.11776432436373499
Epoch #232: loss=0.10515804547402594
Epoch #233: loss=0.13704150822013617
Epoch #234: loss=0.16723654150134987
Epoch #235: loss=0.11757040727469656
Epoch #236: loss=0.12019480609645446
Epoch #237: loss=0.09000958905865748
Epoch #238: loss=0.10901144933369425
Epoch #239: loss=0.07384221162647009
Epoch #240: loss=0.10051064058724377
Epoch #241: loss=0.08613723381939861
Epoch #242: loss=0.11737490735120243
Epoch #243: loss=0.12595818346987167
Epoch #244: loss=0.11389177292585373
Epoch #245: loss=0.08692245702776644
Epoch #246: loss=0.1327883613606294
Epoch #247: loss=0.1311765348331796
Epoch #248: loss=0.18085812115006977
Epoch #249: loss=0.10473020405819018

Training time: 0:16:35.617153

Finished.
n2one setting etth2_ettm2 -> electricity
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth2_ettm2_epochs_250_seed_2023/model.pkl', muti_dataset='etth2_ettm2', random_seed=2023, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='electricity')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.44771e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.90801e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.44771e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.37160741441091066, 'MAE': 0.43093144017901486}
Finished.
------------------------- record done -------------------------
n2one setting etth2_ettm2 -> weather
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth2_ettm2_epochs_250_seed_2023/model.pkl', muti_dataset='etth2_ettm2', random_seed=2023, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='weather')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.46493e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.75403e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.46493e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 1.069282450963351, 'MAE': 0.7981371521278497}
Finished.
------------------------- record done -------------------------
n2one setting etth2_ettm2 -> exchange
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth2_ettm2_epochs_250_seed_2023/model.pkl', muti_dataset='etth2_ettm2', random_seed=2023, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='exchange')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=3.42624e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.20677341885372794, 'MAE': 0.31335819429700884}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='etth2_electricity', random_seed=2023, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/etth2_electricity_epochs_250_seed_2023
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=1.5898908703792385
Epoch #1: loss=0.6266077857737135
Epoch #2: loss=0.4407033307886705
Epoch #3: loss=0.3600340436053712
Epoch #4: loss=0.2983393541014776
Epoch #5: loss=0.25915504687624735
Epoch #6: loss=0.2188171108094294
Epoch #7: loss=0.21844156463516923
Epoch #8: loss=0.16970870580251624
Epoch #9: loss=0.14962563203738594
Epoch #10: loss=0.13734503085838584
Epoch #11: loss=0.12961584104724774
Epoch #12: loss=0.11291062443449003
Epoch #13: loss=0.11079038823841185
Epoch #14: loss=0.09727050921107393
Epoch #15: loss=0.08795780528950073
Epoch #16: loss=0.07985019631592966
Epoch #17: loss=0.08100262780605656
Epoch #18: loss=0.06662373868578182
Epoch #19: loss=0.06269787764744605
Epoch #20: loss=0.07484794738021021
Epoch #21: loss=0.06115758346916154
Epoch #22: loss=0.06329912497525716
Epoch #23: loss=0.0590945433615139
Epoch #24: loss=0.04296966319727689
Epoch #25: loss=0.054405004652103454
Epoch #26: loss=0.045472137276480744
Epoch #27: loss=0.046401663071464566
Epoch #28: loss=0.04244966490375923
Epoch #29: loss=0.03674278553240228
Epoch #30: loss=0.042794912125029394
Epoch #31: loss=0.032385972716771745
Epoch #32: loss=0.02955641351972434
Epoch #33: loss=0.03601487900454142
Epoch #34: loss=0.03560983192845725
Epoch #35: loss=0.03125632473511803
Epoch #36: loss=0.033894308822132964
Epoch #37: loss=0.038687577277348124
Epoch #38: loss=0.04733369615505359
Epoch #39: loss=0.026722565374234918
Epoch #40: loss=0.036506257100150034
Epoch #41: loss=0.023947574848468167
Epoch #42: loss=0.03880570763750456
Epoch #43: loss=0.028102666133923863
Epoch #44: loss=0.028096176052491588
Epoch #45: loss=0.022110038712015945
Epoch #46: loss=0.022309983915098513
Epoch #47: loss=0.033133281105575604
Epoch #48: loss=0.02458484024242441
Epoch #49: loss=0.02840852703070041
Epoch #50: loss=0.02706602745172681
Epoch #51: loss=0.03382543756742962
Epoch #52: loss=0.029135427147394227
Epoch #53: loss=0.015599978041747108
Epoch #54: loss=0.021841202461850694
Epoch #55: loss=0.019930199489264364
Epoch #56: loss=0.020316866693220373
Epoch #57: loss=0.023096289161291746
Epoch #58: loss=0.022177392875963094
Epoch #59: loss=0.03017458554340254
Epoch #60: loss=0.018953454434068123
Epoch #61: loss=0.03391375046765727
Epoch #62: loss=0.0632714566337399
Epoch #63: loss=0.021685528572827645
Epoch #64: loss=0.025717402371455825
Epoch #65: loss=0.016439338954536118
Epoch #66: loss=0.02180954993123234
Epoch #67: loss=0.01793362343188364
Epoch #68: loss=0.02312644306469493
Epoch #69: loss=0.027319937030126046
Epoch #70: loss=0.015502612228951098
Epoch #71: loss=0.017049097942284923
Epoch #72: loss=0.014392910450491089
Epoch #73: loss=0.02203628273385154
Epoch #74: loss=0.019470932632940252
Epoch #75: loss=0.020323465590680403
Epoch #76: loss=0.015176811399968999
Epoch #77: loss=0.011494155363790214
Epoch #78: loss=0.017398720996336523
Epoch #79: loss=0.02185364256374447
Epoch #80: loss=0.017874342182128713
Epoch #81: loss=0.020515389723785606
Epoch #82: loss=0.013886843911278425
Epoch #83: loss=0.015684988830599184
Epoch #84: loss=0.01668185533362259
Epoch #85: loss=0.013890539063495674
Epoch #86: loss=0.01822233284722798
Epoch #87: loss=0.019371876147577096
Epoch #88: loss=0.015564882327098309
Epoch #89: loss=0.01682980091129316
Epoch #90: loss=0.020984580065975558
Epoch #91: loss=0.011304718922474422
Epoch #92: loss=0.019156325854809683
Epoch #93: loss=0.018577382128924222
Epoch #94: loss=0.01921439344324626
Epoch #95: loss=0.01684266147658047
Epoch #96: loss=0.015323634316013251
Epoch #97: loss=0.009743704416284412
Epoch #98: loss=0.011032960417562607
Epoch #99: loss=0.012124185143365161
Epoch #100: loss=0.030823440190898707
Epoch #101: loss=0.019524069881654435
Epoch #102: loss=0.009494403866971502
Epoch #103: loss=0.012359666213763753
Epoch #104: loss=0.012539134730149072
Epoch #105: loss=0.011237041964926813
Epoch #106: loss=0.01872785045785216
Epoch #107: loss=0.013808341030664629
Epoch #108: loss=0.03788374477561575
Epoch #109: loss=0.018922748952385653
Epoch #110: loss=0.014632268818365069
Epoch #111: loss=0.011011724727456856
Epoch #112: loss=0.014788020002281594
Epoch #113: loss=0.0221776140375679
Epoch #114: loss=0.01066555226418309
Epoch #115: loss=0.010322200094290055
Epoch #116: loss=0.013862837070319517
Epoch #117: loss=0.011934464927102586
Epoch #118: loss=0.014345032392778127
Epoch #119: loss=0.015543098164482395
Epoch #120: loss=0.009687126865440116
Epoch #121: loss=0.008014027608601836
Epoch #122: loss=0.011630753393635642
Epoch #123: loss=0.013351911623985649
Epoch #124: loss=0.020261752273743296
Epoch #125: loss=0.016457124131415098
Epoch #126: loss=0.012452266224633005
Epoch #127: loss=0.008831433207577481
Epoch #128: loss=0.00673850671674012
Epoch #129: loss=0.007776902282962621
Epoch #130: loss=0.011756895425184289
Epoch #131: loss=0.030362786206263457
Epoch #132: loss=0.013297312458368052
Epoch #133: loss=0.014273835467841908
Epoch #134: loss=0.01413813032257433
Epoch #135: loss=0.009598712111566295
Epoch #136: loss=0.010598317458238964
Epoch #137: loss=0.00896041800731881
Epoch #138: loss=0.015829919073794593
Epoch #139: loss=0.01035840185352022
Epoch #140: loss=0.009995075527801486
Epoch #141: loss=0.011767754051018018
Epoch #142: loss=0.017073405400435327
Epoch #143: loss=0.011266213830112531
Epoch #144: loss=0.01485394093807128
Epoch #145: loss=0.010219414977011804
Epoch #146: loss=0.023611637265364662
Epoch #147: loss=0.0174215408963103
Epoch #148: loss=0.01180810048940546
Epoch #149: loss=0.02179351918649201
Epoch #150: loss=0.008071098230153083
Epoch #151: loss=0.01032502828829598
Epoch #152: loss=0.014958131202303509
Epoch #153: loss=0.016326962992202536
Epoch #154: loss=0.01452975364772123
Epoch #155: loss=0.014736246459772413
Epoch #156: loss=0.011677958607345695
Epoch #157: loss=0.01227282074597031
Epoch #158: loss=0.014203984203937224
Epoch #159: loss=0.010397702146114969
Epoch #160: loss=0.012727086025889305
Epoch #161: loss=0.008134934509942512
Epoch #162: loss=0.009059864867506293
Epoch #163: loss=0.007305959019637468
Epoch #164: loss=0.008330934258904053
Epoch #165: loss=0.030138165938452158
Epoch #166: loss=0.011551865089043021
Epoch #167: loss=0.009891549823894421
Epoch #168: loss=0.014566443921755375
Epoch #169: loss=0.00932154306260267
Epoch #170: loss=0.010458283466105946
Epoch #171: loss=0.011189417925978466
Epoch #172: loss=0.007784745970151029
Epoch #173: loss=0.009677380743995876
Epoch #174: loss=0.010780660964572723
Epoch #175: loss=0.01298058841206423
Epoch #176: loss=0.0087827959179732
Epoch #177: loss=0.006651083885627464
Epoch #178: loss=0.014789490891860445
Epoch #179: loss=0.012003411801010881
Epoch #180: loss=0.009181005606500904
Epoch #181: loss=0.008528715020674565
Epoch #182: loss=0.010633933776705639
Epoch #183: loss=0.020300518651066985
Epoch #184: loss=0.03139043468772576
Epoch #185: loss=0.014926084120106237
Epoch #186: loss=0.006313114953294777
Epoch #187: loss=0.0077726675384527455
Epoch #188: loss=0.01466250546679764
Epoch #189: loss=0.012416794702869261
Epoch #190: loss=0.018577670030492478
Epoch #191: loss=0.015139714461040014
Epoch #192: loss=0.01410834453684113
Epoch #193: loss=0.008751180881872864
Epoch #194: loss=0.014132680748511253
Epoch #195: loss=0.008918637255857636
Epoch #196: loss=0.00871801977824894
Epoch #197: loss=0.00948533747746629
Epoch #198: loss=0.0061797763376422905
Epoch #199: loss=0.012760604806982616
Epoch #200: loss=0.012616239942694736
Epoch #201: loss=0.014843710585764933
Epoch #202: loss=0.009829293689907034
Epoch #203: loss=0.01155665579568538
Epoch #204: loss=0.006755106027723253
Epoch #205: loss=0.009058532342683842
Epoch #206: loss=0.014507785090955155
Epoch #207: loss=0.0098423053668961
Epoch #208: loss=0.010132951954224931
Epoch #209: loss=0.018783909836823046
Epoch #210: loss=0.008010064999369916
Epoch #211: loss=0.007840120308484496
Epoch #212: loss=0.009125025143719807
Epoch #213: loss=0.014762844910764237
Epoch #214: loss=0.011392959423552028
Epoch #215: loss=0.008523797074214998
Epoch #216: loss=0.010654664385627967
Epoch #217: loss=0.011452598514685034
Epoch #218: loss=0.010154097879954284
Epoch #219: loss=0.009069599065519358
Epoch #220: loss=0.00829411459616073
Epoch #221: loss=0.006317533165956109
Epoch #222: loss=0.014270705938720422
Epoch #223: loss=0.006495602221459593
Epoch #224: loss=0.00786212752720486
Epoch #225: loss=0.013961772142132891
Epoch #226: loss=0.006775053385531575
Epoch #227: loss=0.009411749201163042
Epoch #228: loss=0.011961254440038607
Epoch #229: loss=0.015709693230391855
Epoch #230: loss=0.006208244636981384
Epoch #231: loss=0.004984958344632504
Epoch #232: loss=0.007248316341757323
Epoch #233: loss=0.013327421767427019
Epoch #234: loss=0.01277549298183436
Epoch #235: loss=0.006700747419417006
Epoch #236: loss=0.00729828884311978
Epoch #237: loss=0.010525692232033357
Epoch #238: loss=0.00826188157491454
Epoch #239: loss=0.012637343819688994
Epoch #240: loss=0.007472364964597524
Epoch #241: loss=0.005581366927488977
Epoch #242: loss=0.008994582529827844
Epoch #243: loss=0.04281235958833571
Epoch #244: loss=0.00879177218941949
Epoch #245: loss=0.006301582582126706
Epoch #246: loss=0.0055370845857840145
Epoch #247: loss=0.006688011940352181
Epoch #248: loss=0.008279968897645963
Epoch #249: loss=0.009974692678053721

Training time: 4:27:42.961211

Finished.
n2one setting etth2_electricity -> weather
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth2_electricity_epochs_250_seed_2023/model.pkl', muti_dataset='etth2_electricity', random_seed=2023, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='weather')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=8.77425e-09): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.9395e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=4.59538e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.8713428727999343, 'MAE': 0.741045145438152}
Finished.
------------------------- record done -------------------------
n2one setting etth2_electricity -> exchange
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth2_electricity_epochs_250_seed_2023/model.pkl', muti_dataset='etth2_electricity', random_seed=2023, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='exchange')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=3.23393e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=3.23393e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.27567036005568224, 'MAE': 0.3523393401025379}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='etth2_traffic', random_seed=2023, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/etth2_traffic_epochs_250_seed_2023
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=0.9770074691644883
Epoch #1: loss=0.34237689542893845
Epoch #2: loss=0.2493897973548008
Epoch #3: loss=0.1898221447806638
Epoch #4: loss=0.14290833104963407
Epoch #5: loss=0.12083394273095287
Epoch #6: loss=0.0988447649653045
Epoch #7: loss=0.08621514649253652
Epoch #8: loss=0.06628076756004377
Epoch #9: loss=0.06698794292406071
Epoch #10: loss=0.05596340980843756
Epoch #11: loss=0.05310000617258568
Epoch #12: loss=0.07726553959988477
Epoch #13: loss=0.04462075611025575
Epoch #14: loss=0.04814490553268132
Epoch #15: loss=0.041607082263331846
Epoch #16: loss=0.038252956411287925
Epoch #17: loss=0.049087975192139886
Epoch #18: loss=0.03963119940019359
Epoch #19: loss=0.03561267008393
Epoch #20: loss=0.04285846356172822
Epoch #21: loss=0.02867878048270255
Epoch #22: loss=0.03098033988000064
Epoch #23: loss=0.023389125735685443
Epoch #24: loss=0.042177756710280206
Epoch #25: loss=0.028114110155441553
Epoch #26: loss=0.028769925029860473
Epoch #27: loss=0.03190087714212142
Epoch #28: loss=0.024341621052771722
Epoch #29: loss=0.022670891066939196
Epoch #30: loss=0.023325149171706503
Epoch #31: loss=0.01999458076509511
Epoch #32: loss=0.024065462530106177
Epoch #33: loss=0.027936583642485597
Epoch #34: loss=0.021362969829589427
Epoch #35: loss=0.01959137341650085
Epoch #36: loss=0.023381738066919774
Epoch #37: loss=0.022214857431975814
Epoch #38: loss=0.01812025728672732
Epoch #39: loss=0.028224339337737627
Epoch #40: loss=0.019725884537301504
Epoch #41: loss=0.017728745028731854
Epoch #42: loss=0.018991549283683233
Epoch #43: loss=0.019744447686589433
Epoch #44: loss=0.02278351903979059
Epoch #45: loss=0.018779823742485397
Epoch #46: loss=0.01896830143525412
Epoch #47: loss=0.02255741845243273
Epoch #48: loss=0.022756952763670655
Epoch #49: loss=0.01883911703187025
Epoch #50: loss=0.021521948468381104
Epoch #51: loss=0.015798172425223897
Epoch #52: loss=0.019193837072250676
Epoch #53: loss=0.016482558382473568
Epoch #54: loss=0.02475380925909092
Epoch #55: loss=0.017517624911678027
Epoch #56: loss=0.017766552887072195
Epoch #57: loss=0.01065917438687462
Epoch #58: loss=0.01499522495098515
Epoch #59: loss=0.015656843171529985
Epoch #60: loss=0.0181960453497277
Epoch #61: loss=0.014599004244352546
Epoch #62: loss=0.016252138977307692
Epoch #63: loss=0.03964035645190528
Epoch #64: loss=0.015422550040150251
Epoch #65: loss=0.015434078525101296
Epoch #66: loss=0.012769973026084852
Epoch #67: loss=0.018492822863147632
Epoch #68: loss=0.013420753154790584
Epoch #69: loss=0.020612142738694342
Epoch #70: loss=0.011906122538808763
Epoch #71: loss=0.01790194586763358
Epoch #72: loss=0.016896476368961043
Epoch #73: loss=0.014043293212093274
Epoch #74: loss=0.016554840105134024
Epoch #75: loss=0.015491133721517742
Epoch #76: loss=0.013089695968591502
Epoch #77: loss=0.016781687972289096
Epoch #78: loss=0.014787242750656524
Epoch #79: loss=0.01705244125413062
Epoch #80: loss=0.01250806953767379
Epoch #81: loss=0.010071640652254753
Epoch #82: loss=0.01495724877216521
Epoch #83: loss=0.015278897225501506
Epoch #84: loss=0.017623071750610963
Epoch #85: loss=0.012950707808247728
Epoch #86: loss=0.014877435209148155
Epoch #87: loss=0.014552687646304574
Epoch #88: loss=0.011236896676585287
Epoch #89: loss=0.013195390443176126
Epoch #90: loss=0.02497071678636615
Epoch #91: loss=0.013678005469697168
Epoch #92: loss=0.01329397688070805
Epoch #93: loss=0.015389228723569471
Epoch #94: loss=0.013845872463791344
Epoch #95: loss=0.012312559486284822
Epoch #96: loss=0.016166280322762846
Epoch #97: loss=0.012426620481176232
Epoch #98: loss=0.011014084858674892
Epoch #99: loss=0.01373358886869532
Epoch #100: loss=0.011347503180833163
Epoch #101: loss=0.013878768158101844
Epoch #102: loss=0.0147365682568295
Epoch #103: loss=0.012110947454112345
Epoch #104: loss=0.009534808278383346
Epoch #105: loss=0.017186525184989365
Epoch #106: loss=0.011068723077153824
Epoch #107: loss=0.014393924285733528
Epoch #108: loss=0.015299349993763929
Epoch #109: loss=0.011348219599484512
Epoch #110: loss=0.008690255338314575
Epoch #111: loss=0.013496515746614215
Epoch #112: loss=0.01402798255946904
Epoch #113: loss=0.013702580190023355
Epoch #114: loss=0.013548436469006726
Epoch #115: loss=0.008422905051639605
Epoch #116: loss=0.01384864237081925
Epoch #117: loss=0.012336919033385211
Epoch #118: loss=0.010942365526578306
Epoch #119: loss=0.012945653212064357
Epoch #120: loss=0.015260164303707373
Epoch #121: loss=0.008715687827412784
Epoch #122: loss=0.010058488427711999
Epoch #123: loss=0.011449002718114347
Epoch #124: loss=0.017581666764892718
Epoch #125: loss=0.016874363774818747
Epoch #126: loss=0.009125220543791735
Epoch #127: loss=0.009168414295305658
Epoch #128: loss=0.011805385660773446
Epoch #129: loss=0.015376316908555657
Epoch #130: loss=0.012343423629239346
Epoch #131: loss=0.012399764934734075
Epoch #132: loss=0.008334262375261687
Epoch #133: loss=0.009180325990526293
Epoch #134: loss=0.010655818644087704
Epoch #135: loss=0.011092722760548256
Epoch #136: loss=0.010160755888833953
Epoch #137: loss=0.014566305413944282
Epoch #138: loss=0.01098583339253613
Epoch #139: loss=0.013507757730393803
Epoch #140: loss=0.012303869380179999
Epoch #141: loss=0.00862524724437577
Epoch #142: loss=0.013931579614815916
Epoch #143: loss=0.009590474235805413
Epoch #144: loss=0.011918284373581747
Epoch #145: loss=0.013035221698922206
Epoch #146: loss=0.010980303990484512
Epoch #147: loss=0.01205931348442885
Epoch #148: loss=0.010723517577465832
Epoch #149: loss=0.009437413007883415
Epoch #150: loss=0.01014101914548389
Epoch #151: loss=0.014497432317761084
Epoch #152: loss=0.019256642150889035
Epoch #153: loss=0.01370683859504456
Epoch #154: loss=0.006545569125872498
Epoch #155: loss=0.010239954424474893
Epoch #156: loss=0.010546742117188344
Epoch #157: loss=0.008421591605349568
Epoch #158: loss=0.009943192517138391
Epoch #159: loss=0.011025127696271054
Epoch #160: loss=0.012739082272449869
Epoch #161: loss=0.008539561213809199
Epoch #162: loss=0.012298534093878818
Epoch #163: loss=0.008154144715759974
Epoch #164: loss=0.012683679721355293
Epoch #165: loss=0.011454149500111342
Epoch #166: loss=0.007671512656331264
Epoch #167: loss=0.010007879807625445
Epoch #168: loss=0.00992693420621739
Epoch #169: loss=0.016231044934273745
Epoch #170: loss=0.013701992131492523
Epoch #171: loss=0.008278631940856469
Epoch #172: loss=0.008825093585399266
Epoch #173: loss=0.010157317452335524
Epoch #174: loss=0.008961886774470051
Epoch #175: loss=0.00914476009755464
Epoch #176: loss=0.009195609340240983
Epoch #177: loss=0.00988818530405371
Epoch #178: loss=0.01114013039900694
Epoch #179: loss=0.012187690750864025
Epoch #180: loss=0.011532015149711076
Epoch #181: loss=0.006716719407277311
Epoch #182: loss=0.013083439440023308
Epoch #183: loss=0.007775299467275555
Epoch #184: loss=0.010185407524739828
Epoch #185: loss=0.009563224701399345
Epoch #186: loss=0.009589219897469293
Epoch #187: loss=0.010815502485018294
Epoch #188: loss=0.010551401246789204
Epoch #189: loss=0.010257223606444212
Epoch #190: loss=0.007842060267989967
Epoch #191: loss=0.007481996621079186
Epoch #192: loss=0.007740696025156267
Epoch #193: loss=0.006474811782895923
Epoch #194: loss=0.0054442951941937184
Epoch #195: loss=0.014010870636901518
Epoch #196: loss=0.014359212883300538
Epoch #197: loss=0.009309243143834089
Epoch #198: loss=0.006829038878087312
Epoch #199: loss=0.011397272736064274
Epoch #200: loss=0.007050633599678369
Epoch #201: loss=0.008127381126470708
Epoch #202: loss=0.008897138730323014
Epoch #203: loss=0.0089065128224437
Epoch #204: loss=0.009060597466150839
Epoch #205: loss=0.010835522783497042
Epoch #206: loss=0.008654471945669448
Epoch #207: loss=0.010439642179010906
Epoch #208: loss=0.010279741981733781
Epoch #209: loss=0.009089292216736622
Epoch #210: loss=0.009527359215924607
Epoch #211: loss=0.01005261758692301
Epoch #212: loss=0.008195015278121593
Epoch #213: loss=0.008711529630237332
Epoch #214: loss=0.00976471174152657
Epoch #215: loss=0.009663144824691482
Epoch #216: loss=0.009257545354806896
Epoch #217: loss=0.010736945255664076
Epoch #218: loss=0.009968341402252664
Epoch #219: loss=0.02934691846199624
Epoch #220: loss=0.005888245675635258
Epoch #221: loss=0.011162350506751377
Epoch #222: loss=0.007672668418754455
Epoch #223: loss=0.012905927263299416
Epoch #224: loss=0.009273836852881865
Epoch #225: loss=0.009839186028147897
Epoch #226: loss=0.008884146326735683
Epoch #227: loss=0.011101770311258027
Epoch #228: loss=0.006879617829228509
Epoch #229: loss=0.010559758295272788
Epoch #230: loss=0.012264137906484883
Epoch #231: loss=0.008564860379248074
Epoch #232: loss=0.008132767930118608
Epoch #233: loss=0.012584158206636931
Epoch #234: loss=0.009351593989856572
Epoch #235: loss=0.007686660123125048
Epoch #236: loss=0.011814733421781248
Epoch #237: loss=0.011451308675053003
Epoch #238: loss=0.011582736605817186
Epoch #239: loss=0.008997506426402953
Epoch #240: loss=0.008164809488545234
Epoch #241: loss=0.0070109898852432516
Epoch #242: loss=0.009862288216158954
Epoch #243: loss=0.005953452025750464
Epoch #244: loss=0.011120518724612519
Epoch #245: loss=0.007471510966194411
Epoch #246: loss=0.010895477185957937
Epoch #247: loss=0.01179231524585893
Epoch #248: loss=0.009745807775867765
Epoch #249: loss=0.009253368203334847

Training time: 10:14:10.051529

Finished.
n2one setting etth2_traffic -> electricity
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth2_traffic_epochs_250_seed_2023/model.pkl', muti_dataset='etth2_traffic', random_seed=2023, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='electricity')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=8.9866e-09): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.86216e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=4.90629e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=8.9866e-09): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.3976068480268768, 'MAE': 0.4481086451153009}
Finished.
------------------------- record done -------------------------
n2one setting etth2_traffic -> weather
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth2_traffic_epochs_250_seed_2023/model.pkl', muti_dataset='etth2_traffic', random_seed=2023, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='weather')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.19166e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.49956e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.19166e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.4135577750159497, 'MAE': 0.479553719390426}
Finished.
------------------------- record done -------------------------
n2one setting etth2_traffic -> exchange
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth2_traffic_epochs_250_seed_2023/model.pkl', muti_dataset='etth2_traffic', random_seed=2023, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='exchange')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=4.13222e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.2738914579999981, 'MAE': 0.34535883070179274}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='etth2_weather', random_seed=2023, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/etth2_weather_epochs_250_seed_2023
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=7.27968708674113
Epoch #1: loss=3.0669073552796333
Epoch #2: loss=2.945836796905055
Epoch #3: loss=2.400130047942653
Epoch #4: loss=2.338750983729507
Epoch #5: loss=2.1184105439619585
Epoch #6: loss=1.9465196096535884
Epoch #7: loss=1.8928165291294907
Epoch #8: loss=1.791613499323527
Epoch #9: loss=1.7020895806225864
Epoch #10: loss=1.704628839637294
Epoch #11: loss=1.6331722375118372
Epoch #12: loss=1.4336454651572488
Epoch #13: loss=1.4184485240416094
Epoch #14: loss=1.3173023426171504
Epoch #15: loss=1.3527518041206128
Epoch #16: loss=1.188409888383114
Epoch #17: loss=1.0621843934059143
Epoch #18: loss=1.1103322921377239
Epoch #19: loss=1.1323692762490474
Epoch #20: loss=1.0464685830202969
Epoch #21: loss=1.041187873392394
Epoch #22: loss=1.00447775739612
Epoch #23: loss=1.1163593602902961
Epoch #24: loss=1.006023134246017
Epoch #25: loss=0.9653757250670231
Epoch #26: loss=0.8224767970316338
Epoch #27: loss=0.7979452790636005
Epoch #28: loss=0.8928241549116193
Epoch #29: loss=0.859546852834297
Epoch #30: loss=1.0368943991083088
Epoch #31: loss=0.8708765271938208
Epoch #32: loss=0.7417980342200308
Epoch #33: loss=0.7858015452370499
Epoch #34: loss=0.779421374653325
Epoch #35: loss=0.716967449946837
Epoch #36: loss=0.7554135087764624
Epoch #37: loss=0.848943414110126
Epoch #38: loss=0.7270039509643208
Epoch #39: loss=0.6631784989978328
Epoch #40: loss=0.7354203110391443
Epoch #41: loss=0.7220654415361809
Epoch #42: loss=0.6490590482047109
Epoch #43: loss=0.6395195939324119
Epoch #44: loss=0.6099436346328619
Epoch #45: loss=0.6472331827337091
Epoch #46: loss=0.6849784968477307
Epoch #47: loss=0.7627541693774137
Epoch #48: loss=0.6410009391380079
Epoch #49: loss=0.6199715245853771
Epoch #50: loss=0.4985106506130912
Epoch #51: loss=0.5108992105180566
Epoch #52: loss=0.5129423574967817
Epoch #53: loss=0.5154009000821547
Epoch #54: loss=0.5151740592537504
Epoch #55: loss=0.4789661205176151
Epoch #56: loss=0.485071394479636
Epoch #57: loss=0.5133746627605322
Epoch #58: loss=0.49688453204704053
Epoch #59: loss=0.5496665781194513
Epoch #60: loss=0.5400736837676077
Epoch #61: loss=0.4463405925216097
Epoch #62: loss=0.41342449549472693
Epoch #63: loss=0.44540867480364715
Epoch #64: loss=0.3867305593960213
Epoch #65: loss=0.3789083126819495
Epoch #66: loss=0.4072469012303786
Epoch #67: loss=0.3533710679321578
Epoch #68: loss=0.3545916694583315
Epoch #69: loss=0.44041514125737274
Epoch #70: loss=0.38202805455886957
Epoch #71: loss=0.37188370435526874
Epoch #72: loss=0.365631618734562
Epoch #73: loss=0.37074347427397064
Epoch #74: loss=0.3290592513301156
Epoch #75: loss=0.3163591433655132
Epoch #76: loss=0.36692246614080487
Epoch #77: loss=0.35096193488800165
Epoch #78: loss=0.3184825735110225
Epoch #79: loss=0.25991400082906085
Epoch #80: loss=0.29735182316014264
Epoch #81: loss=0.2914513180201704
Epoch #82: loss=0.2733115653196971
Epoch #83: loss=0.561694141590234
Epoch #84: loss=0.5543679608540102
Epoch #85: loss=0.3763646862723611
Epoch #86: loss=0.29618698900396173
Epoch #87: loss=0.2712669119690404
Epoch #88: loss=0.3802899853749709
Epoch #89: loss=0.45494654368270526
Epoch #90: loss=0.34209305770469434
Epoch #91: loss=0.31019002921653516
Epoch #92: loss=0.23309788130449527
Epoch #93: loss=0.28792077099735086
Epoch #94: loss=0.22583805205244006
Epoch #95: loss=0.23034926707094366
Epoch #96: loss=0.2859399151621443
Epoch #97: loss=0.23283231168082266
Epoch #98: loss=0.27874750102108176
Epoch #99: loss=0.21848361690839133
Epoch #100: loss=0.2659980979832736
Epoch #101: loss=0.30313593578158005
Epoch #102: loss=0.2723087440385963
Epoch #103: loss=0.27870146320624783
Epoch #104: loss=0.18986764256701325
Epoch #105: loss=0.18184571487433981
Epoch #106: loss=0.22826650377475854
Epoch #107: loss=0.26155197530081775
Epoch #108: loss=0.31253302187630627
Epoch #109: loss=0.25005641308697785
Epoch #110: loss=0.23538477470477423
Epoch #111: loss=0.20561143056009756
Epoch #112: loss=0.2062874725370696
Epoch #113: loss=0.1855554266860991
Epoch #114: loss=0.1895064807525187
Epoch #115: loss=0.24323367846734595
Epoch #116: loss=0.20795944423386545
Epoch #117: loss=0.30903360665296065
Epoch #118: loss=0.22269530594348907
Epoch #119: loss=0.17375529941284296
Epoch #120: loss=0.25435661818041944
Epoch #121: loss=0.1967746140153119
Epoch #122: loss=0.1999785823352409
Epoch #123: loss=0.16474070332267068
Epoch #124: loss=0.16620263129924284
Epoch #125: loss=0.19773669902122382
Epoch #126: loss=0.202016464462786
Epoch #127: loss=0.1853283771285505
Epoch #128: loss=0.2503313348826134
Epoch #129: loss=0.19631059896765332
Epoch #130: loss=0.16228529065847397
Epoch #131: loss=0.15253138993725632
Epoch #132: loss=0.15416394794980684
Epoch #133: loss=0.19043298867164235
Epoch #134: loss=0.1505729498511011
Epoch #135: loss=0.14579706571318887
Epoch #136: loss=0.22951256348328156
Epoch #137: loss=0.178801355375485
Epoch #138: loss=0.1583371078877738
Epoch #139: loss=0.21927543887586304
Epoch #140: loss=0.16331180523742328
Epoch #141: loss=0.17054832297744174
Epoch #142: loss=0.1321787838682984
Epoch #143: loss=0.14633719668243872
Epoch #144: loss=0.13332676898800966
Epoch #145: loss=0.12944208012160027
Epoch #146: loss=0.13719766993414273
Epoch #147: loss=0.17404498938809743
Epoch #148: loss=0.11763604177218495
Epoch #149: loss=0.1357472501695156
Epoch #150: loss=0.17263394503882437
Epoch #151: loss=0.19426271138769208
Epoch #152: loss=0.1341864284918164
Epoch #153: loss=0.2010770860043439
Epoch #154: loss=0.13874007772767183
Epoch #155: loss=0.12775410305369983
Epoch #156: loss=0.1712391089760896
Epoch #157: loss=0.12100676486663746
Epoch #158: loss=0.1528579859119473
Epoch #159: loss=0.10238386340665095
Epoch #160: loss=0.09666806583603223
Epoch #161: loss=0.1422089532469258
Epoch #162: loss=0.1228108692236922
Epoch #163: loss=0.1414885576814413
Epoch #164: loss=0.10333794908541621
Epoch #165: loss=0.11874173739642808
Epoch #166: loss=0.14144954175660104
Epoch #167: loss=0.1278438867831772
Epoch #168: loss=0.16282643310048364
Epoch #169: loss=0.14175242682298025
Epoch #170: loss=0.12073590233922005
Epoch #171: loss=0.11349999103130716
Epoch #172: loss=0.10326096127656373
Epoch #173: loss=0.10474666710378545
Epoch #174: loss=0.10324670453414772
Epoch #175: loss=0.12559689812813746
Epoch #176: loss=0.17642203481359917
Epoch #177: loss=0.1246771751479669
Epoch #178: loss=0.21299504962834445
Epoch #179: loss=0.12602756561880762
Epoch #180: loss=0.1308105613358996
Epoch #181: loss=0.12423251224963953
Epoch #182: loss=0.12474805918155295
Epoch #183: loss=0.14342509345574814
Epoch #184: loss=0.12949104076533607
Epoch #185: loss=0.11855391458128438
Epoch #186: loss=0.0872511092121854
Epoch #187: loss=0.1101242672990669
Epoch #188: loss=0.10105975119001938
Epoch #189: loss=0.0791553846475753
Epoch #190: loss=0.11440316220801888
Epoch #191: loss=0.08193964590177391
Epoch #192: loss=0.092194939032197
Epoch #193: loss=0.0924154564625386
Epoch #194: loss=0.0704025041142648
Epoch #195: loss=0.07285191192094123
Epoch #196: loss=0.12961500262220702
Epoch #197: loss=0.1099246817669182
Epoch #198: loss=0.10573405868402033
Epoch #199: loss=0.08507865723786932
Epoch #200: loss=0.09482307534551981
Epoch #201: loss=0.10207183777608654
Epoch #202: loss=0.11696598311942635
Epoch #203: loss=0.11443361363401919
Epoch #204: loss=0.1044155815334031
Epoch #205: loss=0.09372393860284126
Epoch #206: loss=0.08648467848472523
Epoch #207: loss=0.12818622002095886
Epoch #208: loss=0.07555376870952772
Epoch #209: loss=0.08052600880689693
Epoch #210: loss=0.09636871866656072
Epoch #211: loss=0.07748322600893902
Epoch #212: loss=0.11438699402479512
Epoch #213: loss=0.1338792279024016
Epoch #214: loss=0.16144967750843728
Epoch #215: loss=0.14489171665274736
Epoch #216: loss=0.09729092788289893
Epoch #217: loss=0.09826753926322315
Epoch #218: loss=0.09502723863856359
Epoch #219: loss=0.06582703128118407
Epoch #220: loss=0.1011403538286686
Epoch #221: loss=0.10806996967982162
Epoch #222: loss=0.09030637856234204
Epoch #223: loss=0.20673636739339793
Epoch #224: loss=0.15054067201686627
Epoch #225: loss=0.24825361946767027
Epoch #226: loss=0.25362429846868373
Epoch #227: loss=0.16455402947736508
Epoch #228: loss=0.0895112178316622
Epoch #229: loss=0.10271133657431963
Epoch #230: loss=0.112502196803689
Epoch #231: loss=0.0820491048084064
Epoch #232: loss=0.07394899650843757
Epoch #233: loss=0.07959989325679613
Epoch #234: loss=0.08858528744542238
Epoch #235: loss=0.06518586533087672
Epoch #236: loss=0.07420003306910847
Epoch #237: loss=0.06650144206077764
Epoch #238: loss=0.12251001771426562
Epoch #239: loss=0.09157057117783662
Epoch #240: loss=0.055442532072916176
Epoch #241: loss=0.07491073794098514
Epoch #242: loss=0.11601975323124365
Epoch #243: loss=0.07874830338087949
Epoch #244: loss=0.18792254082632787
Epoch #245: loss=0.11045435195167859
Epoch #246: loss=0.08555592455421433
Epoch #247: loss=0.1421074393402898
Epoch #248: loss=0.09574705526006944
Epoch #249: loss=0.06765848251454758

Training time: 0:29:38.523124

Finished.
n2one setting etth2_weather -> electricity
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth2_weather_epochs_250_seed_2023/model.pkl', muti_dataset='etth2_weather', random_seed=2023, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='electricity')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.27486e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.59114e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.27486e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.3756745798424322, 'MAE': 0.42910052532712967}
Finished.
------------------------- record done -------------------------
n2one setting etth2_weather -> exchange
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth2_weather_epochs_250_seed_2023/model.pkl', muti_dataset='etth2_weather', random_seed=2023, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='exchange')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=3.46447e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.20651761303146748, 'MAE': 0.3119298107796164}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='etth2_exchange', random_seed=2023, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/etth2_exchange_epochs_250_seed_2023
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=4.262055253982544
Epoch #1: loss=2.510687764485677
Epoch #2: loss=2.070224340756734
Epoch #3: loss=1.7848359982172648
Epoch #4: loss=1.8504031419754028
Epoch #5: loss=1.754647970199585
Epoch #6: loss=1.5991121053695678
Epoch #7: loss=1.6424074570337932
Epoch #8: loss=1.5592928091684977
Epoch #9: loss=1.5541117906570434
Epoch #10: loss=1.4607118368148804
Epoch #11: loss=1.3811457395553588
Epoch #12: loss=1.3643701712290446
Epoch #13: loss=1.383940577507019
Epoch #14: loss=1.321357043584188
Epoch #15: loss=1.2711129665374756
Epoch #16: loss=1.1820748368899028
Epoch #17: loss=1.2053778251012166
Epoch #18: loss=1.0976021806399028
Epoch #19: loss=1.1100600997606913
Epoch #20: loss=1.2850427707036336
Epoch #21: loss=1.0908902645111085
Epoch #22: loss=0.9970062692960103
Epoch #23: loss=0.9932774305343628
Epoch #24: loss=1.0002614736557007
Epoch #25: loss=0.9908257881800334
Epoch #26: loss=1.0751586159070332
Epoch #27: loss=0.9492513616879781
Epoch #28: loss=0.9310032010078431
Epoch #29: loss=0.8879819949467976
Epoch #30: loss=1.0557682474454244
Epoch #31: loss=0.9048788865407308
Epoch #32: loss=0.8887178460756938
Epoch #33: loss=0.8825605869293213
Epoch #34: loss=0.9063387155532837
Epoch #35: loss=0.7500391801198324
Epoch #36: loss=0.9040923237800598
Epoch #37: loss=0.8742305715878804
Epoch #38: loss=0.8169597466786702
Epoch #39: loss=0.7792043288548788
Epoch #40: loss=0.8232499877611796
Epoch #41: loss=1.0712296485900878
Epoch #42: loss=0.7876405994097392
Epoch #43: loss=0.8372730612754822
Epoch #44: loss=0.8130390564600627
Epoch #45: loss=0.7250901897748311
Epoch #46: loss=0.590276449918747
Epoch #47: loss=0.7377098520596822
Epoch #48: loss=0.6767714500427247
Epoch #49: loss=0.7127172291278839
Epoch #50: loss=0.7020983815193176
Epoch #51: loss=0.6807368377844493
Epoch #52: loss=0.6536143024762472
Epoch #53: loss=0.6284643391768138
Epoch #54: loss=0.7315265536308289
Epoch #55: loss=0.67518443663915
Epoch #56: loss=0.564218654235204
Epoch #57: loss=0.601228819290797
Epoch #58: loss=0.5763404448827107
Epoch #59: loss=0.5186372816562652
Epoch #60: loss=0.5502229193846385
Epoch #61: loss=0.561612089474996
Epoch #62: loss=0.6484177231788635
Epoch #63: loss=0.46422038475672406
Epoch #64: loss=0.4945788989464442
Epoch #65: loss=0.5952255705992381
Epoch #66: loss=0.5273048003514608
Epoch #67: loss=0.5266255676746369
Epoch #68: loss=0.5040217856566112
Epoch #69: loss=0.4904062976439794
Epoch #70: loss=0.47369287510712943
Epoch #71: loss=0.5456834097703298
Epoch #72: loss=0.48940404256184894
Epoch #73: loss=0.46125834981600444
Epoch #74: loss=0.3856956760088603
Epoch #75: loss=0.40662830670674643
Epoch #76: loss=0.41021353205045064
Epoch #77: loss=0.40245913962523144
Epoch #78: loss=0.5562769671281179
Epoch #79: loss=0.5020017961661021
Epoch #80: loss=0.49012768069903057
Epoch #81: loss=0.47998787959416706
Epoch #82: loss=0.49232133825620017
Epoch #83: loss=0.3994458576043447
Epoch #84: loss=0.35458807746569315
Epoch #85: loss=0.42138966023921964
Epoch #86: loss=0.43337322970231373
Epoch #87: loss=0.5202017625172933
Epoch #88: loss=0.3776069074869156
Epoch #89: loss=0.4019144982099533
Epoch #90: loss=0.4109194502234459
Epoch #91: loss=0.42218813896179197
Epoch #92: loss=0.3942735413710276
Epoch #93: loss=0.33555842538674674
Epoch #94: loss=0.3422401547431946
Epoch #95: loss=0.38347394367059073
Epoch #96: loss=0.3568217635154724
Epoch #97: loss=0.3634690925478935
Epoch #98: loss=0.30890233914057413
Epoch #99: loss=0.40107010503609974
Epoch #100: loss=0.40388439198335013
Epoch #101: loss=0.4313187837600708
Epoch #102: loss=0.3259211093187332
Epoch #103: loss=0.4034014860788981
Epoch #104: loss=0.44174224535624185
Epoch #105: loss=0.3472240577141444
Epoch #106: loss=0.4104617794354757
Epoch #107: loss=0.3961178739865621
Epoch #108: loss=0.5297522962093353
Epoch #109: loss=0.310488689939181
Epoch #110: loss=0.37299447456995644
Epoch #111: loss=0.3322291195392609
Epoch #112: loss=0.38035864333311714
Epoch #113: loss=0.33441460331281025
Epoch #114: loss=0.2840354080001513
Epoch #115: loss=0.2982856621344884
Epoch #116: loss=0.26987183292706807
Epoch #117: loss=0.36750863095124564
Epoch #118: loss=0.2715103129545848
Epoch #119: loss=0.37113871723413466
Epoch #120: loss=0.4926469629009565
Epoch #121: loss=0.4297676314910253
Epoch #122: loss=0.32652515868345894
Epoch #123: loss=0.402567329009374
Epoch #124: loss=0.3538854161898295
Epoch #125: loss=0.3095004071791967
Epoch #126: loss=0.3069970478614171
Epoch #127: loss=0.31806177695592247
Epoch #128: loss=0.4318334311246872
Epoch #129: loss=0.4396970719099045
Epoch #130: loss=0.33771737217903136
Epoch #131: loss=0.41651124358177183
Epoch #132: loss=0.3584810882806778
Epoch #133: loss=0.30364716053009033
Epoch #134: loss=0.3307258814573288
Epoch #135: loss=0.3733609884977341
Epoch #136: loss=0.34270579516887667
Epoch #137: loss=0.4204726442694664
Epoch #138: loss=0.3190713713566462
Epoch #139: loss=0.3579638118545214
Epoch #140: loss=0.3137863685687383
Epoch #141: loss=0.27607010006904603
Epoch #142: loss=0.29491707930962247
Epoch #143: loss=0.2228780508041382
Epoch #144: loss=0.25794832905133563
Epoch #145: loss=0.31971057057380675
Epoch #146: loss=0.3486820469299952
Epoch #147: loss=0.2729355583588282
Epoch #148: loss=0.2778954769174258
Epoch #149: loss=0.28574171861012776
Epoch #150: loss=0.29558458427588147
Epoch #151: loss=0.32833966662486397
Epoch #152: loss=0.3006324698527654
Epoch #153: loss=0.2957156578699748
Epoch #154: loss=0.26548075725634895
Epoch #155: loss=0.25329644878705343
Epoch #156: loss=0.2623054265975952
Epoch #157: loss=0.29864545663197833
Epoch #158: loss=0.30000369548797606
Epoch #159: loss=0.23731681903203328
Epoch #160: loss=0.3013019616405169
Epoch #161: loss=0.2672107157607873
Epoch #162: loss=0.23747499187787374
Epoch #163: loss=0.2978343014915784
Epoch #164: loss=0.2540836215019226
Epoch #165: loss=0.27093408902486166
Epoch #166: loss=0.26781821846961973
Epoch #167: loss=0.23765004674593607
Epoch #168: loss=0.20720041245222093
Epoch #169: loss=0.24105914384126664
Epoch #170: loss=0.2726704557736715
Epoch #171: loss=0.22536927262941997
Epoch #172: loss=0.24892269646128018
Epoch #173: loss=0.20761691282192865
Epoch #174: loss=0.18304602801799774
Epoch #175: loss=0.2615992600719134
Epoch #176: loss=0.28786583145459493
Epoch #177: loss=0.1823177029689153
Epoch #178: loss=0.31151481419801713
Epoch #179: loss=0.330535089969635
Epoch #180: loss=0.21343322495619457
Epoch #181: loss=0.2527068371574084
Epoch #182: loss=0.2487474431594213
Epoch #183: loss=0.24899838268756866
Epoch #184: loss=0.29199555814266204
Epoch #185: loss=0.3028551027178764
Epoch #186: loss=0.2413343315323194
Epoch #187: loss=0.35504203935464224
Epoch #188: loss=0.30435728629430137
Epoch #189: loss=0.20474509075284003
Epoch #190: loss=0.2516538734237353
Epoch #191: loss=0.2205749548971653
Epoch #192: loss=0.20195039560397465
Epoch #193: loss=0.19315125147501627
Epoch #194: loss=0.19318737337986627
Epoch #195: loss=0.18171849151452382
Epoch #196: loss=0.2129263440767924
Epoch #197: loss=0.182892282307148
Epoch #198: loss=0.1887573371330897
Epoch #199: loss=0.18354120676716168
Epoch #200: loss=0.18907496283451716
Epoch #201: loss=0.20169191683332124
Epoch #202: loss=0.21720086112618447
Epoch #203: loss=0.16313469012578327
Epoch #204: loss=0.16720009769002597
Epoch #205: loss=0.15959949046373367
Epoch #206: loss=0.18930500323573748
Epoch #207: loss=0.1977186863621076
Epoch #208: loss=0.2351312170426051
Epoch #209: loss=0.2530389132599036
Epoch #210: loss=0.2051696226000786
Epoch #211: loss=0.2206615721186002
Epoch #212: loss=0.2387744039297104
Epoch #213: loss=0.24101323237021763
Epoch #214: loss=0.23957244977355002
Epoch #215: loss=0.24597580780585607
Epoch #216: loss=0.21191992834210396
Epoch #217: loss=0.2659470843772093
Epoch #218: loss=0.4168458099166552
Epoch #219: loss=0.2688472037514051
Epoch #220: loss=0.21048895120620728
Epoch #221: loss=0.2254006087779999
Epoch #222: loss=0.19778336708744368
Epoch #223: loss=0.32275839721163113
Epoch #224: loss=0.2401236412425836
Epoch #225: loss=0.20047994231184324
Epoch #226: loss=0.15311593214670818
Epoch #227: loss=0.14556735704342524
Epoch #228: loss=0.1748972234626611
Epoch #229: loss=0.29518134544293084
Epoch #230: loss=0.22295099447170894
Epoch #231: loss=0.28053476959466933
Epoch #232: loss=0.20139582405487697
Epoch #233: loss=0.1996159200867017
Epoch #234: loss=0.22602749168872832
Epoch #235: loss=0.17071657131115595
Epoch #236: loss=0.17141673068205515
Epoch #237: loss=0.22060985763867696
Epoch #238: loss=0.22548276434342066
Epoch #239: loss=0.19751097162564596
Epoch #240: loss=0.1633247509598732
Epoch #241: loss=0.19052051256100336
Epoch #242: loss=0.19143281728029252
Epoch #243: loss=0.15846165642142296
Epoch #244: loss=0.22881506209572156
Epoch #245: loss=0.18429704482356707
Epoch #246: loss=0.19048125445842742
Epoch #247: loss=0.21824738706151645
Epoch #248: loss=0.1767213632663091
Epoch #249: loss=0.23139418549835683

Training time: 0:10:16.875906

Finished.
n2one setting etth2_exchange -> electricity
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth2_exchange_epochs_250_seed_2023/model.pkl', muti_dataset='etth2_exchange', random_seed=2023, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='electricity')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.5561e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=3.12665e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.5561e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.3553638784881307, 'MAE': 0.42357203771424073}
Finished.
------------------------- record done -------------------------
n2one setting etth2_exchange -> weather
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth2_exchange_epochs_250_seed_2023/model.pkl', muti_dataset='etth2_exchange', random_seed=2023, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='weather')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.31747e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.44219e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.31747e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.6325957797147828, 'MAE': 0.5929332667809067}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='ettm1_ettm2', random_seed=2023, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/ettm1_ettm2_epochs_250_seed_2023
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=5.096848531202837
Epoch #1: loss=2.706327297470786
Epoch #2: loss=2.3769496462561865
Epoch #3: loss=2.0409033677794715
Epoch #4: loss=1.8271443031050942
Epoch #5: loss=1.6459332325241782
Epoch #6: loss=1.513408276167783
Epoch #7: loss=1.3287163701924412
Epoch #8: loss=1.2324762696569616
Epoch #9: loss=1.2045161642811515
Epoch #10: loss=1.117993181402033
Epoch #11: loss=1.0864150795069607
Epoch #12: loss=1.0156979262828827
Epoch #13: loss=0.9432615773244337
Epoch #14: loss=0.9998530989343469
Epoch #15: loss=0.9885544966567646
Epoch #16: loss=0.8549913791092959
Epoch #17: loss=0.7579919831319288
Epoch #18: loss=0.7867779704657468
Epoch #19: loss=0.8011749278415333
Epoch #20: loss=0.7237053107131611
Epoch #21: loss=0.7081777236678384
Epoch #22: loss=0.7892111919142983
Epoch #23: loss=0.6453563272953033
Epoch #24: loss=0.6834663559090007
Epoch #25: loss=0.6740892962975935
Epoch #26: loss=0.7483461648225784
Epoch #27: loss=0.8396947559985247
Epoch #28: loss=0.6719788963144476
Epoch #29: loss=0.551868430592797
Epoch #30: loss=0.6377850838682868
Epoch #31: loss=0.5798696116967634
Epoch #32: loss=0.5302238613367081
Epoch #33: loss=0.4942801066420295
Epoch #34: loss=0.5972233658487146
Epoch #35: loss=0.5511423782868818
Epoch #36: loss=0.5015247491273013
Epoch #37: loss=0.563298917629502
Epoch #38: loss=0.5092535601420836
Epoch #39: loss=0.43391245332631195
Epoch #40: loss=0.439792661504312
Epoch #41: loss=0.45592524924061517
Epoch #42: loss=0.4002929140220989
Epoch #43: loss=0.33913357284936035
Epoch #44: loss=0.438213892958381
Epoch #45: loss=0.43086551265283063
Epoch #46: loss=0.38823603093624115
Epoch #47: loss=0.399333119392395
Epoch #48: loss=0.4330188672650944
Epoch #49: loss=0.4170446206222881
Epoch #50: loss=0.3437016958540136
Epoch #51: loss=0.3753254230726849
Epoch #52: loss=0.31374400650913065
Epoch #53: loss=0.31364724852822046
Epoch #54: loss=0.2882267467000268
Epoch #55: loss=0.28797130354426126
Epoch #56: loss=0.24994608692147516
Epoch #57: loss=0.37698832425204193
Epoch #58: loss=0.3496817018498074
Epoch #59: loss=0.24539650705727664
Epoch #60: loss=0.39445910535075446
Epoch #61: loss=0.28438232974572614
Epoch #62: loss=0.43032602898099204
Epoch #63: loss=0.330864241854711
Epoch #64: loss=0.29246936399828305
Epoch #65: loss=0.2619092681191184
Epoch #66: loss=0.3245341804894534
Epoch #67: loss=0.1903137540952726
Epoch #68: loss=0.21632947942072694
Epoch #69: loss=0.2130884937942028
Epoch #70: loss=0.2797192534939809
Epoch #71: loss=0.26430563831871207
Epoch #72: loss=0.23655468022281473
Epoch #73: loss=0.2353073460134593
Epoch #74: loss=0.24600027264519173
Epoch #75: loss=0.24625985764644362
Epoch #76: loss=0.27263442359187384
Epoch #77: loss=0.1809375655244697
Epoch #78: loss=0.21697474812919443
Epoch #79: loss=0.20634976469657637
Epoch #80: loss=0.1570917217230255
Epoch #81: loss=0.17362050034783102
Epoch #82: loss=0.26063336296515033
Epoch #83: loss=0.19784395836971022
Epoch #84: loss=0.19589276205409656
Epoch #85: loss=0.11740828000686386
Epoch #86: loss=0.11995357156477192
Epoch #87: loss=0.1647877534004775
Epoch #88: loss=0.1470421969213269
Epoch #89: loss=0.14809651232578538
Epoch #90: loss=0.15519668128002773
Epoch #91: loss=0.1868858886036006
Epoch #92: loss=0.10280699672346766
Epoch #93: loss=0.08742074990137057
Epoch #94: loss=0.09089771573516456
Epoch #95: loss=0.09161325459453193
Epoch #96: loss=0.10323480872268026
Epoch #97: loss=0.11328581026331945
Epoch #98: loss=0.13415449976243757
Epoch #99: loss=0.2199146771295504
Epoch #100: loss=0.09782136570323598
Epoch #101: loss=0.11855764009735802
Epoch #102: loss=0.1006727647036314
Epoch #103: loss=0.1750444234772162
Epoch #104: loss=0.17519609494642777
Epoch #105: loss=0.07706990804184567
Epoch #106: loss=0.09281160140579398
Epoch #107: loss=0.09709490175274285
Epoch #108: loss=0.10125182907689702
Epoch #109: loss=0.19685105640779843
Epoch #110: loss=0.12299942259084094
Epoch #111: loss=0.09587067704309117
Epoch #112: loss=0.08993706174872139
Epoch #113: loss=0.13740948316725818
Epoch #114: loss=0.1812450997531414
Epoch #115: loss=0.1281521665779027
Epoch #116: loss=0.07852320152927529
Epoch #117: loss=0.08448388148099184
Epoch #118: loss=0.10302530093626543
Epoch #119: loss=0.14491593719206072
Epoch #120: loss=0.15130764245986938
Epoch #121: loss=0.10799702781845223
Epoch #122: loss=0.10480622185224836
Epoch #123: loss=0.10318551500412551
Epoch #124: loss=0.11751460808921944
Epoch #125: loss=0.10890071796761318
Epoch #126: loss=0.07466676221652464
Epoch #127: loss=0.08519395711747083
Epoch #128: loss=0.06836329959332943
Epoch #129: loss=0.07868979558009993
Epoch #130: loss=0.1192769320173697
Epoch #131: loss=0.07938832764259794
Epoch #132: loss=0.10043684935027902
Epoch #133: loss=0.09289212084629318
Epoch #134: loss=0.0695277787744999
Epoch #135: loss=0.06283891582014886
Epoch #136: loss=0.06138230978765271
Epoch #137: loss=0.0724240080030127
Epoch #138: loss=0.25921861590309575
Epoch #139: loss=0.13782991147176785
Epoch #140: loss=0.09160304526713761
Epoch #141: loss=0.11130767036229372
Epoch #142: loss=0.07816486661745743
Epoch #143: loss=0.12039826530963182
Epoch #144: loss=0.06553230324590748
Epoch #145: loss=0.07323858920823444
Epoch #146: loss=0.059799926694143905
Epoch #147: loss=0.07972840016538446
Epoch #148: loss=0.11858992761170323
Epoch #149: loss=0.10288368317891251
Epoch #150: loss=0.09498768951743841
Epoch #151: loss=0.06036936576393517
Epoch #152: loss=0.06172760537910191
Epoch #153: loss=0.05013578084551475
Epoch #154: loss=0.04807134120809761
Epoch #155: loss=0.05422302360900424
Epoch #156: loss=0.04313874443654309
Epoch #157: loss=0.05647617769004269
Epoch #158: loss=0.056171353737061676
Epoch #159: loss=0.11777401563118804
Epoch #160: loss=0.09256422223353927
Epoch #161: loss=0.06910977233201265
Epoch #162: loss=0.17003140615468676
Epoch #163: loss=0.1990586549720981
Epoch #164: loss=0.0928791095926003
Epoch #165: loss=0.060665915720164776
Epoch #166: loss=0.05083204636519605
Epoch #167: loss=0.052163666833869436
Epoch #168: loss=0.054841263135048474
Epoch #169: loss=0.0435414633882994
Epoch #170: loss=0.05083289149809967
Epoch #171: loss=0.062345412729138676
Epoch #172: loss=0.060601611537012184
Epoch #173: loss=0.05926699699325995
Epoch #174: loss=0.09745933547277343
Epoch #175: loss=0.10165751357139512
Epoch #176: loss=0.07477572086182507
Epoch #177: loss=0.06202709480104121
Epoch #178: loss=0.0974112532778897
Epoch #179: loss=0.07434895693917166
Epoch #180: loss=0.08440303142097863
Epoch #181: loss=0.05998051153834571
Epoch #182: loss=0.06019297589293935
Epoch #183: loss=0.05426090646704489
Epoch #184: loss=0.04840421939099377
Epoch #185: loss=0.05174247941679575
Epoch #186: loss=0.07974182691594417
Epoch #187: loss=0.0950821636413986
Epoch #188: loss=0.047299215189096605
Epoch #189: loss=0.04637655929069628
Epoch #190: loss=0.03148087410425598
Epoch #191: loss=0.03660205880772661
Epoch #192: loss=0.04226215405982326
Epoch #193: loss=0.036008051596581936
Epoch #194: loss=0.027986276683143595
Epoch #195: loss=0.06899528400125829
Epoch #196: loss=0.04798172096806494
Epoch #197: loss=0.1865952047942714
Epoch #198: loss=0.06949040005830201
Epoch #199: loss=0.03966860341924158
Epoch #200: loss=0.045019626024771824
Epoch #201: loss=0.05751644485545429
Epoch #202: loss=0.06021026530387727
Epoch #203: loss=0.07698760139332576
Epoch #204: loss=0.15743429781022397
Epoch #205: loss=0.06119347173212604
Epoch #206: loss=0.0435800756835802
Epoch #207: loss=0.16021563569930466
Epoch #208: loss=0.0674022853967141
Epoch #209: loss=0.08517492096871138
Epoch #210: loss=0.05706308676268567
Epoch #211: loss=0.05342321669344197
Epoch #212: loss=0.060177179984748363
Epoch #213: loss=0.07874375729906288
Epoch #214: loss=0.0738527904772623
Epoch #215: loss=0.06040417762811889
Epoch #216: loss=0.049260811329903925
Epoch #217: loss=0.06548987100408836
Epoch #218: loss=0.047098188872703096
Epoch #219: loss=0.03912827918644656
Epoch #220: loss=0.07141125832938335
Epoch #221: loss=0.038576904430308125
Epoch #222: loss=0.03589202654124661
Epoch #223: loss=0.28052558546716516
Epoch #224: loss=0.12366620603610169
Epoch #225: loss=0.07718436216766184
Epoch #226: loss=0.05107872865416787
Epoch #227: loss=0.047816130205650224
Epoch #228: loss=0.07206331206146967
Epoch #229: loss=0.06623103156347167
Epoch #230: loss=0.06947598657147451
Epoch #231: loss=0.04729647608473897
Epoch #232: loss=0.051827526185661554
Epoch #233: loss=0.06150896732949398
Epoch #234: loss=0.05306183745745908
Epoch #235: loss=0.044600209390575234
Epoch #236: loss=0.04675740338015286
Epoch #237: loss=0.03323644852604379
Epoch #238: loss=0.046402949204837736
Epoch #239: loss=0.03747629395432093
Epoch #240: loss=0.06117845980704508
Epoch #241: loss=0.03649648659947244
Epoch #242: loss=0.04223243350332433
Epoch #243: loss=0.046412542356516824
Epoch #244: loss=0.032230202023955906
Epoch #245: loss=0.0397815185345032
Epoch #246: loss=0.05668521452356468
Epoch #247: loss=0.04014222463592887
Epoch #248: loss=0.0408910149496726
Epoch #249: loss=0.037206245797940275

Training time: 0:21:42.668039

Finished.
n2one setting ettm1_ettm2 -> electricity
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/ettm1_ettm2_epochs_250_seed_2023/model.pkl', muti_dataset='ettm1_ettm2', random_seed=2023, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='electricity')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.33247e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.84825e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.33247e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.37487549295386713, 'MAE': 0.4267969686102011}
Finished.
------------------------- record done -------------------------
n2one setting ettm1_ettm2 -> weather
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/ettm1_ettm2_epochs_250_seed_2023/model.pkl', muti_dataset='ettm1_ettm2', random_seed=2023, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='weather')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.62837e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.95424e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.62837e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.6038502685824754, 'MAE': 0.5956710613219222}
Finished.
------------------------- record done -------------------------
n2one setting ettm1_ettm2 -> exchange
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/ettm1_ettm2_epochs_250_seed_2023/model.pkl', muti_dataset='ettm1_ettm2', random_seed=2023, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='exchange')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=3.60162e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.20173607665800428, 'MAE': 0.3120510304224533}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='ettm1_electricity', random_seed=2023, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/ettm1_electricity_epochs_250_seed_2023
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=1.6786906358348317
Epoch #1: loss=0.6837450878088733
Epoch #2: loss=0.48155863443950575
Epoch #3: loss=0.3727819165102689
Epoch #4: loss=0.37069272061428393
Epoch #5: loss=0.2907263587666563
Epoch #6: loss=0.26121230670307055
Epoch #7: loss=0.23283994350447712
Epoch #8: loss=0.1821694570672081
Epoch #9: loss=0.16946449042129588
Epoch #10: loss=0.16737610324992833
Epoch #11: loss=0.14420206052621445
Epoch #12: loss=0.1344099552538919
Epoch #13: loss=0.113848413767136
Epoch #14: loss=0.10579561473554876
Epoch #15: loss=0.11437115970864353
Epoch #16: loss=0.08998159238933978
Epoch #17: loss=0.08029954230794346
Epoch #18: loss=0.09963732549965561
Epoch #19: loss=0.0795509973344539
Epoch #20: loss=0.08096002510760024
Epoch #21: loss=0.05731833790125976
Epoch #22: loss=0.07102019481742418
Epoch #23: loss=0.06721230853679983
Epoch #24: loss=0.05938456571346755
Epoch #25: loss=0.05090512969413301
Epoch #26: loss=0.048022549470370435
Epoch #27: loss=0.045502085599161864
Epoch #28: loss=0.04494412902725793
Epoch #29: loss=0.049505920984630795
Epoch #30: loss=0.042349726411917935
Epoch #31: loss=0.0559460723482308
Epoch #32: loss=0.04463355780048969
Epoch #33: loss=0.04915774030209777
Epoch #34: loss=0.03529789026739659
Epoch #35: loss=0.03433993382411672
Epoch #36: loss=0.0391360219274872
Epoch #37: loss=0.029232194388764968
Epoch #38: loss=0.030231469074482972
Epoch #39: loss=0.02808701374367474
Epoch #40: loss=0.04117833372522482
Epoch #41: loss=0.03647398671025904
Epoch #42: loss=0.028534254608311164
Epoch #43: loss=0.035341576552467356
Epoch #44: loss=0.02568456181921121
Epoch #45: loss=0.03428385884035379
Epoch #46: loss=0.026792377550491547
Epoch #47: loss=0.029163531050175787
Epoch #48: loss=0.030990186016249234
Epoch #49: loss=0.034711256192671694
Epoch #50: loss=0.021924165596467364
Epoch #51: loss=0.025266195952482747
Epoch #52: loss=0.03094341533843439
Epoch #53: loss=0.039761977377544014
Epoch #54: loss=0.026531700269285453
Epoch #55: loss=0.020977139958259033
Epoch #56: loss=0.019531497243702053
Epoch #57: loss=0.025163727973066045
Epoch #58: loss=0.01813854273452132
Epoch #59: loss=0.020741500450278554
Epoch #60: loss=0.025106553341744536
Epoch #61: loss=0.0232505123050328
Epoch #62: loss=0.024848200247205052
Epoch #63: loss=0.02378566943442853
Epoch #64: loss=0.022505092125663713
Epoch #65: loss=0.02943257916913682
Epoch #66: loss=0.018097202667366176
Epoch #67: loss=0.02165104429229129
Epoch #68: loss=0.02106807604172626
Epoch #69: loss=0.020038047870649136
Epoch #70: loss=0.022773348069216787
Epoch #71: loss=0.02120684942473231
Epoch #72: loss=0.02637434244984817
Epoch #73: loss=0.01870813728811595
Epoch #74: loss=0.013532035845943379
Epoch #75: loss=0.016520378504724084
Epoch #76: loss=0.01853039440753623
Epoch #77: loss=0.01620800852079887
Epoch #78: loss=0.0308055754944525
Epoch #79: loss=0.016690656282158472
Epoch #80: loss=0.03177909046311229
Epoch #81: loss=0.026071299389229124
Epoch #82: loss=0.01186530424912421
Epoch #83: loss=0.017231485408770078
Epoch #84: loss=0.030895616814314603
Epoch #85: loss=0.014284589531949261
Epoch #86: loss=0.018390027045080134
Epoch #87: loss=0.020108083126111325
Epoch #88: loss=0.023288044361790352
Epoch #89: loss=0.012820850730437438
Epoch #90: loss=0.021567139201776508
Epoch #91: loss=0.015321674071889124
Epoch #92: loss=0.015194469700281295
Epoch #93: loss=0.016947977437360033
Epoch #94: loss=0.02666008464780956
Epoch #95: loss=0.02346244055415919
Epoch #96: loss=0.02456905141315726
Epoch #97: loss=0.013245647884302223
Epoch #98: loss=0.014483085633620215
Epoch #99: loss=0.015136770823590334
Epoch #100: loss=0.022189046068647203
Epoch #101: loss=0.01501930871763104
Epoch #102: loss=0.013947391454396904
Epoch #103: loss=0.017219148389889905
Epoch #104: loss=0.015885732792204257
Epoch #105: loss=0.025555693510960483
Epoch #106: loss=0.02547014762502983
Epoch #107: loss=0.016045725869164938
Epoch #108: loss=0.01683559070017991
Epoch #109: loss=0.014105360882590996
Epoch #110: loss=0.012979995555395866
Epoch #111: loss=0.01088824694412017
Epoch #112: loss=0.012165774732929128
Epoch #113: loss=0.011209051218374733
Epoch #114: loss=0.01572537343249185
Epoch #115: loss=0.015176230957394982
Epoch #116: loss=0.017300350023577334
Epoch #117: loss=0.02055982084568248
Epoch #118: loss=0.012356722200656147
Epoch #119: loss=0.01969867884703906
Epoch #120: loss=0.06476787438476599
Epoch #121: loss=0.01142547991746021
Epoch #122: loss=0.011586342164187647
Epoch #123: loss=0.012290750470128842
Epoch #124: loss=0.018857510251810785
Epoch #125: loss=0.01720063171979932
Epoch #126: loss=0.011119064934206677
Epoch #127: loss=0.008889657351979425
Epoch #128: loss=0.009791515811230056
Epoch #129: loss=0.01354751625962286
Epoch #130: loss=0.014492442933698774
Epoch #131: loss=0.009843858910836027
Epoch #132: loss=0.04017629396154248
Epoch #133: loss=0.02015249070140578
Epoch #134: loss=0.01570501268602184
Epoch #135: loss=0.01290982441485057
Epoch #136: loss=0.016551767325124155
Epoch #137: loss=0.02056531239467875
Epoch #138: loss=0.013190062733259086
Epoch #139: loss=0.008592711345198654
Epoch #140: loss=0.010354372688252844
Epoch #141: loss=0.011560214658079148
Epoch #142: loss=0.011329652334141604
Epoch #143: loss=0.016177134670763198
Epoch #144: loss=0.017027254166405548
Epoch #145: loss=0.012754560832540134
Epoch #146: loss=0.012589935731936628
Epoch #147: loss=0.010725131358040344
Epoch #148: loss=0.011489580445347848
Epoch #149: loss=0.01386470124402493
Epoch #150: loss=0.015389453256095996
Epoch #151: loss=0.025494456240705336
Epoch #152: loss=0.017871894867892982
Epoch #153: loss=0.011906706845596101
Epoch #154: loss=0.00801599768321072
Epoch #155: loss=0.010419500436713072
Epoch #156: loss=0.015204918556527587
Epoch #157: loss=0.01386925386093453
Epoch #158: loss=0.016820791944933647
Epoch #159: loss=0.008370455758399275
Epoch #160: loss=0.009935304976779552
Epoch #161: loss=0.020741820540342265
Epoch #162: loss=0.012831817557723177
Epoch #163: loss=0.01399791107358024
Epoch #164: loss=0.009737696162974097
Epoch #165: loss=0.014568354344575424
Epoch #166: loss=0.01354226339061419
Epoch #167: loss=0.008398298794352478
Epoch #168: loss=0.013049586340753469
Epoch #169: loss=0.01782463681733331
Epoch #170: loss=0.008846311363796927
Epoch #171: loss=0.014954099903080938
Epoch #172: loss=0.011118790764815359
Epoch #173: loss=0.010086409731295397
Epoch #174: loss=0.015440668217334278
Epoch #175: loss=0.018819150551017485
Epoch #176: loss=0.008786614934306491
Epoch #177: loss=0.017844541995847143
Epoch #178: loss=0.01938233542286213
Epoch #179: loss=0.012046547921557479
Epoch #180: loss=0.01394627411740745
Epoch #181: loss=0.008203938214181283
Epoch #182: loss=0.010532322440437373
Epoch #183: loss=0.009856374085132532
Epoch #184: loss=0.01338375543460884
Epoch #185: loss=0.01281776410002957
Epoch #186: loss=0.009900137814093544
Epoch #187: loss=0.007404218845809978
Epoch #188: loss=0.00789646227375593
Epoch #189: loss=0.010001702848397593
Epoch #190: loss=0.02174756135416346
Epoch #191: loss=0.010354864522649946
Epoch #192: loss=0.004216535383979552
Epoch #193: loss=0.012602871222262332
Epoch #194: loss=0.008226127295890902
Epoch #195: loss=0.0126860702745385
Epoch #196: loss=0.012261967855142374
Epoch #197: loss=0.007414189081674818
Epoch #198: loss=0.014889672500255281
Epoch #199: loss=0.006595457815063741
Epoch #200: loss=0.008721106027815588
Epoch #201: loss=0.014707962976833834
Epoch #202: loss=0.012408680549026075
Epoch #203: loss=0.010297779277256685
Epoch #204: loss=0.00896367827489627
Epoch #205: loss=0.011964137410370357
Epoch #206: loss=0.008873042022509232
Epoch #207: loss=0.01048262484465636
Epoch #208: loss=0.009131152716592228
Epoch #209: loss=0.011397334300377827
Epoch #210: loss=0.009462052642217703
Epoch #211: loss=0.006444581895945692
Epoch #212: loss=0.0062870905048291745
Epoch #213: loss=0.0376825445198036
Epoch #214: loss=0.010610573050209758
Epoch #215: loss=0.00842382545968313
Epoch #216: loss=0.0185341277969398
Epoch #217: loss=0.012089673394662699
Epoch #218: loss=0.012373721792207644
Epoch #219: loss=0.00681434425395452
Epoch #220: loss=0.007952877927760183
Epoch #221: loss=0.010398478558338219
Epoch #222: loss=0.010877366780853236
Epoch #223: loss=0.009041010768395422
Epoch #224: loss=0.004654879114460495
Epoch #225: loss=0.008700465556892113
Epoch #226: loss=0.008725263872983339
Epoch #227: loss=0.010695025048992548
Epoch #228: loss=0.015390649970890168
Epoch #229: loss=0.013751091381902414
Epoch #230: loss=0.008931163214189169
Epoch #231: loss=0.010065281272414196
Epoch #232: loss=0.008853826179615845
Epoch #233: loss=0.007158281622596186
Epoch #234: loss=0.0101436258609535
Epoch #235: loss=0.010385522895288535
Epoch #236: loss=0.009223449151315698
Epoch #237: loss=0.01924676539170197
Epoch #238: loss=0.010737471002138625
Epoch #239: loss=0.00653281530562763
Epoch #240: loss=0.008485574132015266
Epoch #241: loss=0.008640366842576123
Epoch #242: loss=0.017256280554406603
Epoch #243: loss=0.009584514860937608
Epoch #244: loss=0.009960817719959788
Epoch #245: loss=0.005684205979958783
Epoch #246: loss=0.004616929732355309
Epoch #247: loss=0.004757687113840759
Epoch #248: loss=0.01038014661990238
Epoch #249: loss=0.00829951191308392

Training time: 4:44:39.759442

Finished.
n2one setting ettm1_electricity -> weather
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/ettm1_electricity_epochs_250_seed_2023/model.pkl', muti_dataset='ettm1_electricity', random_seed=2023, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='weather')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=8.24146e-09): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.69879e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=4.16874e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=8.24146e-09): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.5874369953914673, 'MAE': 0.5908774928039687}
Finished.
------------------------- record done -------------------------
n2one setting ettm1_electricity -> exchange
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/ettm1_electricity_epochs_250_seed_2023/model.pkl', muti_dataset='ettm1_electricity', random_seed=2023, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='exchange')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=3.76171e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=3.76171e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.2977342063032586, 'MAE': 0.3666313884921853}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='ettm1_traffic', random_seed=2023, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/ettm1_traffic_epochs_250_seed_2023
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=0.9836043504505223
Epoch #1: loss=0.3694593582093238
Epoch #2: loss=0.2714971921237064
Epoch #3: loss=0.1894886285911516
Epoch #4: loss=0.16101150044836401
Epoch #5: loss=0.13124244390663617
Epoch #6: loss=0.1094542382986958
Epoch #7: loss=0.08910411577842456
Epoch #8: loss=0.08488217003873944
Epoch #9: loss=0.07620287432152893
Epoch #10: loss=0.05887272336679209
Epoch #11: loss=0.05701977538237218
Epoch #12: loss=0.05650874391466994
Epoch #13: loss=0.04912008917655791
Epoch #14: loss=0.050621575777251425
Epoch #15: loss=0.048699345826123334
Epoch #16: loss=0.04376752461218206
Epoch #17: loss=0.041461855994552405
Epoch #18: loss=0.041790127969973154
Epoch #19: loss=0.04192405655578191
Epoch #20: loss=0.034403210812950245
Epoch #21: loss=0.049126990458993124
Epoch #22: loss=0.029548726639073482
Epoch #23: loss=0.033062242031100614
Epoch #24: loss=0.035822953351689044
Epoch #25: loss=0.032742423106215356
Epoch #26: loss=0.03282548118791279
Epoch #27: loss=0.027127638952337198
Epoch #28: loss=0.034645351411253085
Epoch #29: loss=0.032087912870104825
Epoch #30: loss=0.025384120595799323
Epoch #31: loss=0.029227397058565568
Epoch #32: loss=0.021690448723815568
Epoch #33: loss=0.023360385304799076
Epoch #34: loss=0.025434150463738238
Epoch #35: loss=0.021920854094737496
Epoch #36: loss=0.0241788310206971
Epoch #37: loss=0.03016474165195996
Epoch #38: loss=0.021993133583493352
Epoch #39: loss=0.0221536103389994
Epoch #40: loss=0.023371836895382893
Epoch #41: loss=0.019368819015291024
Epoch #42: loss=0.020005085938268757
Epoch #43: loss=0.01943800846655626
Epoch #44: loss=0.02063623117396335
Epoch #45: loss=0.02159902293421488
Epoch #46: loss=0.02205699557232599
Epoch #47: loss=0.02400464623042874
Epoch #48: loss=0.02414335779520872
Epoch #49: loss=0.01374388668737625
Epoch #50: loss=0.01806915023814586
Epoch #51: loss=0.01958026372310055
Epoch #52: loss=0.020259925159415014
Epoch #53: loss=0.018472259600843085
Epoch #54: loss=0.01641134729703806
Epoch #55: loss=0.01725252925214024
Epoch #56: loss=0.020347618222305176
Epoch #57: loss=0.013989189059611192
Epoch #58: loss=0.015707452807235154
Epoch #59: loss=0.03958987490121859
Epoch #60: loss=0.01809127311034764
Epoch #61: loss=0.015116196335506715
Epoch #62: loss=0.015828064333305844
Epoch #63: loss=0.017046980152438947
Epoch #64: loss=0.02005856209005179
Epoch #65: loss=0.016517678840928775
Epoch #66: loss=0.014227676918064185
Epoch #67: loss=0.015275713535522314
Epoch #68: loss=0.012479162577020681
Epoch #69: loss=0.019696052353179727
Epoch #70: loss=0.017196501133450597
Epoch #71: loss=0.016464770232558778
Epoch #72: loss=0.01810343618753936
Epoch #73: loss=0.016941178503114503
Epoch #74: loss=0.02173671416483845
Epoch #75: loss=0.013253139718020984
Epoch #76: loss=0.013632557154566096
Epoch #77: loss=0.018415657638394534
Epoch #78: loss=0.013919328282957145
Epoch #79: loss=0.013782604466625015
Epoch #80: loss=0.014532230346104107
Epoch #81: loss=0.015756245492444133
Epoch #82: loss=0.013055541794043356
Epoch #83: loss=0.013775974612346746
Epoch #84: loss=0.017353200347081172
Epoch #85: loss=0.01441703953737295
Epoch #86: loss=0.012183428652779778
Epoch #87: loss=0.013546191261891107
Epoch #88: loss=0.0167042373573631
Epoch #89: loss=0.023288545456672523
Epoch #90: loss=0.01811472140065899
Epoch #91: loss=0.0116237079690865
Epoch #92: loss=0.015440723264928884
Epoch #93: loss=0.012774220397277177
Epoch #94: loss=0.013118662342617453
Epoch #95: loss=0.009031002879551348
Epoch #96: loss=0.02103632418626817
Epoch #97: loss=0.016306191187450355
Epoch #98: loss=0.01805103568172793
Epoch #99: loss=0.014591897710600925
Epoch #100: loss=0.012411984006864529
Epoch #101: loss=0.011868121542976924
Epoch #102: loss=0.020945361293991015
Epoch #103: loss=0.01108017073905406
Epoch #104: loss=0.015012648522317514
Epoch #105: loss=0.010094833723392753
Epoch #106: loss=0.013047218949869776
Epoch #107: loss=0.010786826358275981
Epoch #108: loss=0.015981297209853026
Epoch #109: loss=0.02358175630240892
Epoch #110: loss=0.017449871753575815
Epoch #111: loss=0.01416417728175536
Epoch #112: loss=0.01231296556678527
Epoch #113: loss=0.014703387856666616
Epoch #114: loss=0.014319805949487651
Epoch #115: loss=0.009762617796178073
Epoch #116: loss=0.025295417337120433
Epoch #117: loss=0.0136236026290714
Epoch #118: loss=0.008580418303308855
Epoch #119: loss=0.012017501454255229
Epoch #120: loss=0.012459052470492685
Epoch #121: loss=0.012263042933426537
Epoch #122: loss=0.013264901535134797
Epoch #123: loss=0.00974757533602071
Epoch #124: loss=0.013580915640137758
Epoch #125: loss=0.014945557722129741
Epoch #126: loss=0.010690285107728285
Epoch #127: loss=0.010967750961194615
Epoch #128: loss=0.01370072130766326
Epoch #129: loss=0.012842504810127147
Epoch #130: loss=0.015866466054223736
Epoch #131: loss=0.012828555409964557
Epoch #132: loss=0.01160184107864204
Epoch #133: loss=0.010661152151096486
Epoch #134: loss=0.010750091103504184
Epoch #135: loss=0.01187055334156776
Epoch #136: loss=0.011531076777505317
Epoch #137: loss=0.008101071619593316
Epoch #138: loss=0.016613753467629146
Epoch #139: loss=0.010650827843453314
Epoch #140: loss=0.012743664995540747
Epoch #141: loss=0.011988114436793932
Epoch #142: loss=0.012806800229956032
Epoch #143: loss=0.014108331027368184
Epoch #144: loss=0.014323555673868636
Epoch #145: loss=0.013642230130785266
Epoch #146: loss=0.00794644870578181
Epoch #147: loss=0.008655184825225162
Epoch #148: loss=0.012235801272139723
Epoch #149: loss=0.0130799926669331
Epoch #150: loss=0.014889863431509711
Epoch #151: loss=0.013797806193077305
Epoch #152: loss=0.010829493042453911
Epoch #153: loss=0.01039343366302328
Epoch #154: loss=0.010075225718364755
Epoch #155: loss=0.011543323575549163
Epoch #156: loss=0.014199579373881634
Epoch #157: loss=0.009528746573153253
Epoch #158: loss=0.01177179181361827
Epoch #159: loss=0.014059888539383218
Epoch #160: loss=0.009684843452691867
Epoch #161: loss=0.009974819037040887
Epoch #162: loss=0.016969841948427277
Epoch #163: loss=0.007709256089991892
Epoch #164: loss=0.00888746593061905
Epoch #165: loss=0.011296748143829083
Epoch #166: loss=0.008694363297207306
Epoch #167: loss=0.01249896667920771
Epoch #168: loss=0.010390038136192806
Epoch #169: loss=0.01026076621726424
Epoch #170: loss=0.014369145807745318
Epoch #171: loss=0.00983274564449902
Epoch #172: loss=0.014450456569746912
Epoch #173: loss=0.010444802437113643
Epoch #174: loss=0.014182068836592022
Epoch #175: loss=0.010381397441344123
Epoch #176: loss=0.014027196809843894
Epoch #177: loss=0.008325866630107787
Epoch #178: loss=0.008194899306491093
Epoch #179: loss=0.011886723153916548
Epoch #180: loss=0.00865061434108779
Epoch #181: loss=0.010769198403015913
Epoch #182: loss=0.009605491287343018
Epoch #183: loss=0.013193436131356838
Epoch #184: loss=0.01020879424598499
Epoch #185: loss=0.011365398601047096
Epoch #186: loss=0.013655964286252082
Epoch #187: loss=0.009827484309736385
Epoch #188: loss=0.01294001771511601
Epoch #189: loss=0.006663482578355688
Epoch #190: loss=0.010760326023724433
Epoch #191: loss=0.010305407962153472
Epoch #192: loss=0.010780919349567276
Epoch #193: loss=0.01822589669746127
Epoch #194: loss=0.011931145020850208
Epoch #195: loss=0.012763512002257924
Epoch #196: loss=0.011014301694799182
Epoch #197: loss=0.00978268444885362
Epoch #198: loss=0.008470669750818708
Epoch #199: loss=0.01006139011801309
Epoch #200: loss=0.009409592889470955
Epoch #201: loss=0.01115082447803098
Epoch #202: loss=0.007822856884355708
Epoch #203: loss=0.01830945586407491
Epoch #204: loss=0.01124980644326799
Epoch #205: loss=0.008986150695382136
Epoch #206: loss=0.011717406215906551
Epoch #207: loss=0.008046852689775674
Epoch #208: loss=0.009734080856248448
Epoch #209: loss=0.013224005903784647
Epoch #210: loss=0.008472545411580969
Epoch #211: loss=0.008150375919012628
Epoch #212: loss=0.008745662979354238
Epoch #213: loss=0.009616096696787148
Epoch #214: loss=0.009718773279064773
Epoch #215: loss=0.008432517137886788
Epoch #216: loss=0.008741711809593659
Epoch #217: loss=0.01394439311399373
Epoch #218: loss=0.016493793391898107
Epoch #219: loss=0.009188927161050716
Epoch #220: loss=0.00739352463351429
Epoch #221: loss=0.010982362096783847
Epoch #222: loss=0.013583615750292015
Epoch #223: loss=0.011444706581037849
Epoch #224: loss=0.009264915378325281
Epoch #225: loss=0.00645983065014776
Epoch #226: loss=0.011544948682205956
Epoch #227: loss=0.011305545973875545
Epoch #228: loss=0.010773080572395111
Epoch #229: loss=0.008405430800592705
Epoch #230: loss=0.00762197138191939
Epoch #231: loss=0.008249567662034224
Epoch #232: loss=0.0097848344727091
Epoch #233: loss=0.012998536554759797
Epoch #234: loss=0.009625657543611077
Epoch #235: loss=0.011770877235997516
Epoch #236: loss=0.007780009911682714
Epoch #237: loss=0.017462407576509235
Epoch #238: loss=0.00787541258472825
Epoch #239: loss=0.008040914210725046
Epoch #240: loss=0.00863990897717997
Epoch #241: loss=0.007494192935695993
Epoch #242: loss=0.007399675095739763
Epoch #243: loss=0.01930260364188789
Epoch #244: loss=0.01130873253663686
Epoch #245: loss=0.007987286521014245
Epoch #246: loss=0.00739367653699017
Epoch #247: loss=0.012079845564347881
Epoch #248: loss=0.0077631944889128834
Epoch #249: loss=0.007632522686182391

Training time: 10:36:30.906916

Finished.
n2one setting ettm1_traffic -> electricity
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/ettm1_traffic_epochs_250_seed_2023/model.pkl', muti_dataset='ettm1_traffic', random_seed=2023, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='electricity')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=6.30117e-09): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.53397e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=4.05886e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=6.30117e-09): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.3862774304987158, 'MAE': 0.4397566244108323}
Finished.
------------------------- record done -------------------------
n2one setting ettm1_traffic -> weather
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/ettm1_traffic_epochs_250_seed_2023/model.pkl', muti_dataset='ettm1_traffic', random_seed=2023, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='weather')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.08002e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.23224e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=5.48069e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.23224e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.4635456955743327, 'MAE': 0.5117878128019828}
Finished.
------------------------- record done -------------------------
n2one setting ettm1_traffic -> exchange
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/ettm1_traffic_epochs_250_seed_2023/model.pkl', muti_dataset='ettm1_traffic', random_seed=2023, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='exchange')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
Evaluation result: {'MSE': 0.27615463746184377, 'MAE': 0.34956552079515646}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='ettm1_weather', random_seed=2023, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/ettm1_weather_epochs_250_seed_2023
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=5.271303924354347
Epoch #1: loss=2.6424800318640633
Epoch #2: loss=2.5360660230791248
Epoch #3: loss=2.21596174626737
Epoch #4: loss=1.9886180291304718
Epoch #5: loss=1.8343644722087964
Epoch #6: loss=1.6531283919875686
Epoch #7: loss=1.473683747085365
Epoch #8: loss=1.4520051801526868
Epoch #9: loss=1.26073358348898
Epoch #10: loss=1.1820911459020667
Epoch #11: loss=1.1796348755424086
Epoch #12: loss=1.019694270314397
Epoch #13: loss=1.1268601884713043
Epoch #14: loss=0.949048217889425
Epoch #15: loss=0.9655527053652583
Epoch #16: loss=0.8260457612372734
Epoch #17: loss=0.9468923871581619
Epoch #18: loss=0.9442181667766055
Epoch #19: loss=0.8172738648749687
Epoch #20: loss=0.7096608635541555
Epoch #21: loss=0.7800215901555242
Epoch #22: loss=0.9091106894853953
Epoch #23: loss=0.7918363084664216
Epoch #24: loss=0.7651549352181924
Epoch #25: loss=0.743307883675034
Epoch #26: loss=0.7056353688240051
Epoch #27: loss=0.6776036893999254
Epoch #28: loss=0.6311150225433143
Epoch #29: loss=0.6420441592061842
Epoch #30: loss=0.5984613597393036
Epoch #31: loss=0.5523730073426221
Epoch #32: loss=0.5469224952362679
Epoch #33: loss=0.6513507752805143
Epoch #34: loss=0.5366357419941876
Epoch #35: loss=0.5668889195532412
Epoch #36: loss=0.5007996011424709
Epoch #37: loss=0.5438308804421812
Epoch #38: loss=0.6020127687905286
Epoch #39: loss=0.4694410465859078
Epoch #40: loss=0.46200218957823674
Epoch #41: loss=0.549106585818368
Epoch #42: loss=0.46779097012571386
Epoch #43: loss=0.5656825134883056
Epoch #44: loss=0.43495916112049204
Epoch #45: loss=0.4972265249974019
Epoch #46: loss=0.43411894223174535
Epoch #47: loss=0.41155513839141744
Epoch #48: loss=0.44290334348743027
Epoch #49: loss=0.3778279721736908
Epoch #50: loss=0.3474875495240495
Epoch #51: loss=0.36538012809044607
Epoch #52: loss=0.32696591760661153
Epoch #53: loss=0.3369207490940352
Epoch #54: loss=0.3546134229447391
Epoch #55: loss=0.34052455022528366
Epoch #56: loss=0.29214839234545426
Epoch #57: loss=0.3891613777424838
Epoch #58: loss=0.3522942859578777
Epoch #59: loss=0.32590019702911377
Epoch #60: loss=0.30841559775777766
Epoch #61: loss=0.30117084005394495
Epoch #62: loss=0.32993269651322754
Epoch #63: loss=0.2906668282843925
Epoch #64: loss=0.2966368089656572
Epoch #65: loss=0.3205448968990429
Epoch #66: loss=0.25969057872488693
Epoch #67: loss=0.2555115676006755
Epoch #68: loss=0.28504212361735265
Epoch #69: loss=0.2409418130645881
Epoch #70: loss=0.2268406484980841
Epoch #71: loss=0.20753054965186762
Epoch #72: loss=0.22902115273314552
Epoch #73: loss=0.21316814805204803
Epoch #74: loss=0.22594168077449542
Epoch #75: loss=0.2818637647338816
Epoch #76: loss=0.19739190348096797
Epoch #77: loss=0.2577791610682333
Epoch #78: loss=0.2189759144911895
Epoch #79: loss=0.21508757085413546
Epoch #80: loss=0.21438817377831484
Epoch #81: loss=0.1903393020903742
Epoch #82: loss=0.23044281130706942
Epoch #83: loss=0.2484544654553001
Epoch #84: loss=0.22274984600576195
Epoch #85: loss=0.25680539495236165
Epoch #86: loss=0.19094497166775368
Epoch #87: loss=0.20280856357232943
Epoch #88: loss=0.22399018040379962
Epoch #89: loss=0.38005327534031225
Epoch #90: loss=0.20443577661707596
Epoch #91: loss=0.1782259226248071
Epoch #92: loss=0.21695336498118736
Epoch #93: loss=0.22250816960995262
Epoch #94: loss=0.19528838156445608
Epoch #95: loss=0.26504425140651494
Epoch #96: loss=0.23340583783951965
Epoch #97: loss=0.17849565247023427
Epoch #98: loss=0.13382229158604467
Epoch #99: loss=0.13962553220020757
Epoch #100: loss=0.2748384524036098
Epoch #101: loss=0.16542272863758578
Epoch #102: loss=0.13989305697582863
Epoch #103: loss=0.11832991292750514
Epoch #104: loss=0.17959334087130185
Epoch #105: loss=0.15754156438885508
Epoch #106: loss=0.1331904206324268
Epoch #107: loss=0.13358685495080175
Epoch #108: loss=0.12051849349125011
Epoch #109: loss=0.15117581450455897
Epoch #110: loss=0.12555138857380763
Epoch #111: loss=0.13133703061455004
Epoch #112: loss=0.13107330493024877
Epoch #113: loss=0.09738674412506658
Epoch #114: loss=0.11359596745790662
Epoch #115: loss=0.12627029660585765
Epoch #116: loss=0.10511593812623539
Epoch #117: loss=0.10028712723303486
Epoch #118: loss=0.10862898786325713
Epoch #119: loss=0.1270688207366982
Epoch #120: loss=0.10474496656978452
Epoch #121: loss=0.09550098974157024
Epoch #122: loss=0.11825840084536655
Epoch #123: loss=0.12177742514255885
Epoch #124: loss=0.10454192455555941
Epoch #125: loss=0.10262506384704564
Epoch #126: loss=0.10126419299961747
Epoch #127: loss=0.08221565025883752
Epoch #128: loss=0.1708961637136904
Epoch #129: loss=0.12675482729399526
Epoch #130: loss=0.11116626747959368
Epoch #131: loss=0.11359027758039333
Epoch #132: loss=0.11271492014261517
Epoch #133: loss=0.09200168763463562
Epoch #134: loss=0.09865073074360152
Epoch #135: loss=0.09604513901915099
Epoch #136: loss=0.08892870538339422
Epoch #137: loss=0.09193334879504668
Epoch #138: loss=0.0654721377870521
Epoch #139: loss=0.06821930655152411
Epoch #140: loss=0.10972750247330279
Epoch #141: loss=0.08119638838075302
Epoch #142: loss=0.07216374968757501
Epoch #143: loss=0.0906256763012828
Epoch #144: loss=0.09734136988786427
Epoch #145: loss=0.07420370914041996
Epoch #146: loss=0.08764517931519328
Epoch #147: loss=0.08278838358819485
Epoch #148: loss=0.08927578302855427
Epoch #149: loss=0.1584225669503212
Epoch #150: loss=0.09828779292670456
Epoch #151: loss=0.06252471712493413
Epoch #152: loss=0.05078487863411774
Epoch #153: loss=0.08413086710749446
Epoch #154: loss=0.07917207554989569
Epoch #155: loss=0.07636001859665723
Epoch #156: loss=0.057872458084209544
Epoch #157: loss=0.06289671829624756
Epoch #158: loss=0.05810588834857618
Epoch #159: loss=0.07108693431458764
Epoch #160: loss=0.06068708943958218
Epoch #161: loss=0.15340710572294286
Epoch #162: loss=0.11645718261196807
Epoch #163: loss=0.08112138066742872
Epoch #164: loss=0.07440326040661013
Epoch #165: loss=0.07337635362873206
Epoch #166: loss=0.06682006395547777
Epoch #167: loss=0.05452910493555907
Epoch #168: loss=0.0433436118670412
Epoch #169: loss=0.192787871469517
Epoch #170: loss=0.08672077722243361
Epoch #171: loss=0.060777420127714005
Epoch #172: loss=0.09046682217032523
Epoch #173: loss=0.08691234544322297
Epoch #174: loss=0.07037360253869682
Epoch #175: loss=0.06767435832502874
Epoch #176: loss=0.11320685655684085
Epoch #177: loss=0.05136354614955348
Epoch #178: loss=0.0514792481538009
Epoch #179: loss=0.11968931955964984
Epoch #180: loss=0.09779137086022545
Epoch #181: loss=0.09377511266014867
Epoch #182: loss=0.06988107616937644
Epoch #183: loss=0.08158194243505194
Epoch #184: loss=0.19798781667408105
Epoch #185: loss=0.22470876564447945
Epoch #186: loss=0.2731758077201006
Epoch #187: loss=0.3858072971673431
Epoch #188: loss=0.12907604133156506
Epoch #189: loss=0.06397232871401955
Epoch #190: loss=0.06440407588977266
Epoch #191: loss=0.054225003296459046
Epoch #192: loss=0.05985509314750497
Epoch #193: loss=0.055966967314079
Epoch #194: loss=0.050044063881442356
Epoch #195: loss=0.06600338922863877
Epoch #196: loss=0.03700895808838509
Epoch #197: loss=0.07035096538429325
Epoch #198: loss=0.08033147716038935
Epoch #199: loss=0.05575598051419129
Epoch #200: loss=0.05579625286564634
Epoch #201: loss=0.06086279069249694
Epoch #202: loss=0.047796130809630896
Epoch #203: loss=0.05707787412747338
Epoch #204: loss=0.05031052196549403
Epoch #205: loss=0.03840265282102533
Epoch #206: loss=0.07096018239453032
Epoch #207: loss=0.07917813475973703
Epoch #208: loss=0.059044036460486615
Epoch #209: loss=0.06335590603585178
Epoch #210: loss=0.09617926084713356
Epoch #211: loss=0.12136992812156677
Epoch #212: loss=0.053700175941795915
Epoch #213: loss=0.04521491423853346
Epoch #214: loss=0.047766315851461245
Epoch #215: loss=0.08553730834879585
Epoch #216: loss=0.053608551747291476
Epoch #217: loss=0.044463793545760015
Epoch #218: loss=0.0547127095428673
Epoch #219: loss=0.042286873568554185
Epoch #220: loss=0.03759871243272681
Epoch #221: loss=0.061861304515922394
Epoch #222: loss=0.05852550485901333
Epoch #223: loss=0.030067591839847533
Epoch #224: loss=0.042769423409088236
Epoch #225: loss=0.0309118136914598
Epoch #226: loss=0.04423917827473299
Epoch #227: loss=0.08300269609065475
Epoch #228: loss=0.06880953638638193
Epoch #229: loss=0.029881667421274894
Epoch #230: loss=0.04978266628962513
Epoch #231: loss=0.0626642912928317
Epoch #232: loss=0.05670790920487127
Epoch #233: loss=0.04999871927036627
Epoch #234: loss=0.03381482544480949
Epoch #235: loss=0.04435627232934978
Epoch #236: loss=0.1482303576997003
Epoch #237: loss=0.05716338099257366
Epoch #238: loss=0.04491182799274857
Epoch #239: loss=0.06498126532076984
Epoch #240: loss=0.032082399076505286
Epoch #241: loss=0.03893815879584164
Epoch #242: loss=0.03724460735464016
Epoch #243: loss=0.03170774688289778
Epoch #244: loss=0.04229003763631792
Epoch #245: loss=0.05830321414396167
Epoch #246: loss=0.04644349693144496
Epoch #247: loss=0.03513514601650673
Epoch #248: loss=0.03581524505657521
Epoch #249: loss=0.05879875098833361

Training time: 0:39:42.048691

Finished.
n2one setting ettm1_weather -> electricity
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/ettm1_weather_epochs_250_seed_2023/model.pkl', muti_dataset='ettm1_weather', random_seed=2023, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='electricity')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.65846e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=3.2924e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.65846e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.34686753584427865, 'MAE': 0.41671375601453425}
Finished.
------------------------- record done -------------------------
n2one setting ettm1_weather -> exchange
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/ettm1_weather_epochs_250_seed_2023/model.pkl', muti_dataset='ettm1_weather', random_seed=2023, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='exchange')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=3.33972e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.2042247349658038, 'MAE': 0.3113975212393101}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='ettm1_exchange', random_seed=2023, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/ettm1_exchange_epochs_250_seed_2023
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=4.938558239685862
Epoch #1: loss=2.692422866821289
Epoch #2: loss=2.222694102086519
Epoch #3: loss=2.080796536646391
Epoch #4: loss=2.0389165188136853
Epoch #5: loss=1.9065388252860622
Epoch #6: loss=1.8231329792424251
Epoch #7: loss=1.7717623647890592
Epoch #8: loss=1.637094121230276
Epoch #9: loss=1.5595777724918567
Epoch #10: loss=1.51909895947105
Epoch #11: loss=1.4036970264033268
Epoch #12: loss=1.4511733839386387
Epoch #13: loss=1.3206000390805697
Epoch #14: loss=1.2992699397237677
Epoch #15: loss=1.2824274050562006
Epoch #16: loss=1.2096311042183323
Epoch #17: loss=1.1309918792624223
Epoch #18: loss=1.19270364234322
Epoch #19: loss=1.0922863420687223
Epoch #20: loss=1.0722078053574813
Epoch #21: loss=1.1145472934371548
Epoch #22: loss=1.0262715502789146
Epoch #23: loss=1.1252774721697758
Epoch #24: loss=1.0257340763744556
Epoch #25: loss=0.969702799069254
Epoch #26: loss=0.9135561867764121
Epoch #27: loss=0.8883317646227384
Epoch #28: loss=0.8515879041270206
Epoch #29: loss=0.9350690371111819
Epoch #30: loss=0.9104551171001635
Epoch #31: loss=0.9722155207081845
Epoch #32: loss=0.8642054106059828
Epoch #33: loss=0.8978016596091422
Epoch #34: loss=0.7613655124839983
Epoch #35: loss=0.7656785469306143
Epoch #36: loss=0.8258869742092333
Epoch #37: loss=0.749553172211898
Epoch #38: loss=0.8554200906502573
Epoch #39: loss=0.8233856966620997
Epoch #40: loss=0.7872828386331859
Epoch #41: loss=0.7936600415330184
Epoch #42: loss=0.6979191036600816
Epoch #43: loss=0.6968181290124592
Epoch #44: loss=0.6996271218124189
Epoch #45: loss=0.694531478379902
Epoch #46: loss=0.8670098577675066
Epoch #47: loss=0.7069024427940971
Epoch #48: loss=0.5794651774983657
Epoch #49: loss=0.6163857218466307
Epoch #50: loss=0.6735930129101402
Epoch #51: loss=0.54291948048692
Epoch #52: loss=0.6376900139607882
Epoch #53: loss=0.6123203729328356
Epoch #54: loss=0.6792268062892713
Epoch #55: loss=0.6732993063173796
Epoch #56: loss=0.5672283392203482
Epoch #57: loss=0.5153027772903442
Epoch #58: loss=0.5895335533116993
Epoch #59: loss=0.5967788225726077
Epoch #60: loss=0.5553466175731859
Epoch #61: loss=0.4815150067994469
Epoch #62: loss=0.520636869104285
Epoch #63: loss=0.46785743456137807
Epoch #64: loss=0.5087742256490808
Epoch #65: loss=0.507096034131552
Epoch #66: loss=0.5699722986472281
Epoch #67: loss=0.6206920790044885
Epoch #68: loss=0.7577023051286998
Epoch #69: loss=0.5574876917035956
Epoch #70: loss=0.5480604885440123
Epoch #71: loss=0.4421369088323493
Epoch #72: loss=0.5261700553329367
Epoch #73: loss=0.45524993538856506
Epoch #74: loss=0.45722627561343343
Epoch #75: loss=0.459469094088203
Epoch #76: loss=0.48152120568250356
Epoch #77: loss=0.46648393336095306
Epoch #78: loss=0.5347251460740441
Epoch #79: loss=0.5310351550579071
Epoch #80: loss=0.46167975350430135
Epoch #81: loss=0.4419048326580148
Epoch #82: loss=0.4464000149777061
Epoch #83: loss=0.392635924251456
Epoch #84: loss=0.4096837161402953
Epoch #85: loss=0.4179307057669288
Epoch #86: loss=0.44797368739780624
Epoch #87: loss=0.42955895081946727
Epoch #88: loss=0.4281724067895036
Epoch #89: loss=0.6341190706742438
Epoch #90: loss=0.4244903388776277
Epoch #91: loss=0.4316351460783105
Epoch #92: loss=0.3786794519738147
Epoch #93: loss=0.4081753762929063
Epoch #94: loss=0.36862313433697347
Epoch #95: loss=0.4145401535849822
Epoch #96: loss=0.3209949920051976
Epoch #97: loss=0.29235913369216415
Epoch #98: loss=0.40671677809012563
Epoch #99: loss=0.31266055334555476
Epoch #100: loss=0.359392878256346
Epoch #101: loss=0.3979108059092572
Epoch #102: loss=0.43729378596732493
Epoch #103: loss=0.49709010594769526
Epoch #104: loss=0.4567095966715562
Epoch #105: loss=0.5449494492066534
Epoch #106: loss=0.39058271050453186
Epoch #107: loss=0.37864211946725845
Epoch #108: loss=0.36034936379445226
Epoch #109: loss=0.3608461132175044
Epoch #110: loss=0.3037856432952379
Epoch #111: loss=0.32012171259051875
Epoch #112: loss=0.3282932541088054
Epoch #113: loss=0.28620600621951253
Epoch #114: loss=0.26125440472050715
Epoch #115: loss=0.2940223962068558
Epoch #116: loss=0.2790675959304759
Epoch #117: loss=0.328419649287274
Epoch #118: loss=0.3014810116667497
Epoch #119: loss=0.2707914521819667
Epoch #120: loss=0.2746540484459777
Epoch #121: loss=0.24477097823431618
Epoch #122: loss=0.33039018629412903
Epoch #123: loss=0.26643163357910354
Epoch #124: loss=0.2513155992093839
Epoch #125: loss=0.24874536261746757
Epoch #126: loss=0.23624668348776667
Epoch #127: loss=0.28382548572201477
Epoch #128: loss=0.2881240283972339
Epoch #129: loss=0.41524584630602285
Epoch #130: loss=0.2775117033406308
Epoch #131: loss=0.32928124148594706
Epoch #132: loss=0.29498054479297836
Epoch #133: loss=0.25771389038939224
Epoch #134: loss=0.26422405870337234
Epoch #135: loss=0.2989574051217029
Epoch #136: loss=0.2971349327187789
Epoch #137: loss=0.29395835925089686
Epoch #138: loss=0.27862716348547684
Epoch #139: loss=0.2862281979698884
Epoch #140: loss=0.28944159416775955
Epoch #141: loss=0.33043892289462845
Epoch #142: loss=0.2854719036503842
Epoch #143: loss=0.2581704690268165
Epoch #144: loss=0.30828648923259033
Epoch #145: loss=0.26433347322438894
Epoch #146: loss=0.26700085009399216
Epoch #147: loss=0.273606917575786
Epoch #148: loss=0.19512646174744555
Epoch #149: loss=0.25336626328920064
Epoch #150: loss=0.2089552671501511
Epoch #151: loss=0.2140007771943745
Epoch #152: loss=0.2177354473816721
Epoch #153: loss=0.18661030774053775
Epoch #154: loss=0.18988581393894396
Epoch #155: loss=0.2165508048706933
Epoch #156: loss=0.24256662808750806
Epoch #157: loss=0.18219677280438573
Epoch #158: loss=0.24201924706760206
Epoch #159: loss=0.3540948641143347
Epoch #160: loss=0.24806169145985654
Epoch #161: loss=0.2146329915052966
Epoch #162: loss=0.19121081832992404
Epoch #163: loss=0.2198467803628821
Epoch #164: loss=0.2943434828990384
Epoch #165: loss=0.25536223344112696
Epoch #166: loss=0.20578372086349286
Epoch #167: loss=0.19998184003328023
Epoch #168: loss=0.25484317815617513
Epoch #169: loss=0.18584330654457995
Epoch #170: loss=0.24761295632312172
Epoch #171: loss=0.13929378594222822
Epoch #172: loss=0.18432002102858142
Epoch #173: loss=0.19316991967590233
Epoch #174: loss=0.1764772001065706
Epoch #175: loss=0.22499696282964005
Epoch #176: loss=0.19350888972219668
Epoch #177: loss=0.13281211492262388
Epoch #178: loss=0.17199442496425227
Epoch #179: loss=0.16007064478962044
Epoch #180: loss=0.12637285496059217
Epoch #181: loss=0.12768171453162244
Epoch #182: loss=0.16085752433067874
Epoch #183: loss=0.1498944112344792
Epoch #184: loss=0.14388324300709524
Epoch #185: loss=0.2831921930375852
Epoch #186: loss=0.22240868818603063
Epoch #187: loss=0.18018172269589022
Epoch #188: loss=0.18508611599865712
Epoch #189: loss=0.36643576994538307
Epoch #190: loss=0.27342393523768377
Epoch #191: loss=0.15545817543017237
Epoch #192: loss=0.14647223682780014
Epoch #193: loss=0.0868149601707333
Epoch #194: loss=0.13397178602846047
Epoch #195: loss=0.21967932130945356
Epoch #196: loss=0.19716504667150347
Epoch #197: loss=0.16255022565785207
Epoch #198: loss=0.13032910639518186
Epoch #199: loss=0.21261581013861455
Epoch #200: loss=0.24539189472010262
Epoch #201: loss=0.16488907917549736
Epoch #202: loss=0.16551924085146502
Epoch #203: loss=0.19972116284464536
Epoch #204: loss=0.13697332928055211
Epoch #205: loss=0.16395376661890432
Epoch #206: loss=0.1231155350412193
Epoch #207: loss=0.15646919353227867
Epoch #208: loss=0.17437647400717987
Epoch #209: loss=0.1478085594349786
Epoch #210: loss=0.17683834798241915
Epoch #211: loss=0.1543046220352775
Epoch #212: loss=0.11102477147390968
Epoch #213: loss=0.12024188610283952
Epoch #214: loss=0.1635105515781202
Epoch #215: loss=0.17364610339465894
Epoch #216: loss=0.24029951091659696
Epoch #217: loss=0.2683101825808224
Epoch #218: loss=0.22860461865600787
Epoch #219: loss=0.24838402239899887
Epoch #220: loss=0.18074956967642433
Epoch #221: loss=0.1759325258041683
Epoch #222: loss=0.13891073688864708
Epoch #223: loss=0.13105039710277006
Epoch #224: loss=0.20203915650123044
Epoch #225: loss=0.13843405736904396
Epoch #226: loss=0.1439507082104683
Epoch #227: loss=0.12809510842749947
Epoch #228: loss=0.13225818954800306
Epoch #229: loss=0.11030290040530656
Epoch #230: loss=0.10621260773194463
Epoch #231: loss=0.1093347347096393
Epoch #232: loss=0.11829272912521112
Epoch #233: loss=0.10282385074778606
Epoch #234: loss=0.09616371989250183
Epoch #235: loss=0.15960232776246572
Epoch #236: loss=0.14448542069447667
Epoch #237: loss=0.13758618129711403
Epoch #238: loss=0.14351998504839444
Epoch #239: loss=0.1410653614684155
Epoch #240: loss=0.13151222839951515
Epoch #241: loss=0.12714569564712674
Epoch #242: loss=0.11319231045873542
Epoch #243: loss=0.10089748490013574
Epoch #244: loss=0.08830455847476658
Epoch #245: loss=0.09587591435564191
Epoch #246: loss=0.09762538704825074
Epoch #247: loss=0.15487549904929965
Epoch #248: loss=0.13674013689160347
Epoch #249: loss=0.12946711637471853

Training time: 0:17:08.474647

Finished.
n2one setting ettm1_exchange -> electricity
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/ettm1_exchange_epochs_250_seed_2023/model.pkl', muti_dataset='ettm1_exchange', random_seed=2023, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='electricity')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.58741e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=3.06934e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.58741e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.36071406923389665, 'MAE': 0.42351194642592865}
Finished.
------------------------- record done -------------------------
n2one setting ettm1_exchange -> weather
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/ettm1_exchange_epochs_250_seed_2023/model.pkl', muti_dataset='ettm1_exchange', random_seed=2023, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='weather')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.21168e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.29604e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=5.44074e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.21168e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 1.0887587752822077, 'MAE': 0.7836111318393344}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='ettm2_electricity', random_seed=2023, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/ettm2_electricity_epochs_250_seed_2023
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=1.804158002497202
Epoch #1: loss=0.7260939858584519
Epoch #2: loss=0.5029868819268353
Epoch #3: loss=0.3879440630595368
Epoch #4: loss=0.3794960118830204
Epoch #5: loss=0.29908979513559
Epoch #6: loss=0.27010564712515794
Epoch #7: loss=0.23962659671543593
Epoch #8: loss=0.1877354526869863
Epoch #9: loss=0.1809767730848258
Epoch #10: loss=0.1725883231307548
Epoch #11: loss=0.1481703141008515
Epoch #12: loss=0.13012742530554533
Epoch #13: loss=0.12240088549274278
Epoch #14: loss=0.10328952399228351
Epoch #15: loss=0.11713667979733233
Epoch #16: loss=0.09568946223414268
Epoch #17: loss=0.09155318420941391
Epoch #18: loss=0.10042145869593663
Epoch #19: loss=0.08939875953224857
Epoch #20: loss=0.08182242044516985
Epoch #21: loss=0.060026353910980546
Epoch #22: loss=0.06930153133875006
Epoch #23: loss=0.07213873519416315
Epoch #24: loss=0.05857456524166312
Epoch #25: loss=0.05277062944102898
Epoch #26: loss=0.051319957980100646
Epoch #27: loss=0.04899396245923252
Epoch #28: loss=0.04951235627705029
Epoch #29: loss=0.04729165466493893
Epoch #30: loss=0.04160039728515815
Epoch #31: loss=0.05929472241466528
Epoch #32: loss=0.0468863501839048
Epoch #33: loss=0.04444143765589155
Epoch #34: loss=0.03376505186526015
Epoch #35: loss=0.03654756310048053
Epoch #36: loss=0.034379001397566576
Epoch #37: loss=0.034672828887011405
Epoch #38: loss=0.03460502406057666
Epoch #39: loss=0.026336110946694278
Epoch #40: loss=0.03532677926880543
Epoch #41: loss=0.03554669439551387
Epoch #42: loss=0.02726600043046434
Epoch #43: loss=0.03144027487127986
Epoch #44: loss=0.02410440463874298
Epoch #45: loss=0.03275392272198258
Epoch #46: loss=0.025435072534678346
Epoch #47: loss=0.028973362791486622
Epoch #48: loss=0.0321408431652333
Epoch #49: loss=0.036535421648295596
Epoch #50: loss=0.022012176651701174
Epoch #51: loss=0.02180665410446362
Epoch #52: loss=0.030328496618417976
Epoch #53: loss=0.04426919993964103
Epoch #54: loss=0.029133604181627464
Epoch #55: loss=0.018838356024969975
Epoch #56: loss=0.020460424938096092
Epoch #57: loss=0.029767934114431
Epoch #58: loss=0.02356594291526869
Epoch #59: loss=0.02193400003780409
Epoch #60: loss=0.02532955523824364
Epoch #61: loss=0.017828619242603722
Epoch #62: loss=0.02305782241543407
Epoch #63: loss=0.020931694601032693
Epoch #64: loss=0.02169815688401023
Epoch #65: loss=0.02917325599203223
Epoch #66: loss=0.01845549402410636
Epoch #67: loss=0.027069842374143022
Epoch #68: loss=0.021830021994748063
Epoch #69: loss=0.021221886508178012
Epoch #70: loss=0.020111563486812905
Epoch #71: loss=0.02078031309959563
Epoch #72: loss=0.021915993233686818
Epoch #73: loss=0.0189705343779558
Epoch #74: loss=0.021198787290430402
Epoch #75: loss=0.01619476006777974
Epoch #76: loss=0.019354699509315114
Epoch #77: loss=0.01591543018774421
Epoch #78: loss=0.021968684765841275
Epoch #79: loss=0.02151012327837623
Epoch #80: loss=0.033480429163143734
Epoch #81: loss=0.028385791259590558
Epoch #82: loss=0.014061508031038804
Epoch #83: loss=0.02076386828543562
Epoch #84: loss=0.025276083059129248
Epoch #85: loss=0.014393275332074691
Epoch #86: loss=0.01894807351047611
Epoch #87: loss=0.019485194755350634
Epoch #88: loss=0.02496769967003647
Epoch #89: loss=0.016625002045142296
Epoch #90: loss=0.018472172569901493
Epoch #91: loss=0.014982392188303273
Epoch #92: loss=0.012867669477436063
Epoch #93: loss=0.016396961275892085
Epoch #94: loss=0.02810957325420466
Epoch #95: loss=0.027504531543909472
Epoch #96: loss=0.023120690851419018
Epoch #97: loss=0.010385844800046208
Epoch #98: loss=0.014562946851736955
Epoch #99: loss=0.01728070172073084
Epoch #100: loss=0.01996173637730479
Epoch #101: loss=0.014641730749491513
Epoch #102: loss=0.015392275347938222
Epoch #103: loss=0.018305544370991536
Epoch #104: loss=0.01819136747683634
Epoch #105: loss=0.022899597447718977
Epoch #106: loss=0.019062668117708584
Epoch #107: loss=0.01386932290074309
Epoch #108: loss=0.014836485472582112
Epoch #109: loss=0.017879572212475166
Epoch #110: loss=0.015626051439868612
Epoch #111: loss=0.013306589988242632
Epoch #112: loss=0.010224231491016129
Epoch #113: loss=0.018102512544993007
Epoch #114: loss=0.014175319840608401
Epoch #115: loss=0.01339392584495398
Epoch #116: loss=0.015451482785213458
Epoch #117: loss=0.020650877328891117
Epoch #118: loss=0.016426973968480592
Epoch #119: loss=0.016040518483131597
Epoch #120: loss=0.06311321391353064
Epoch #121: loss=0.013723646503268652
Epoch #122: loss=0.01350052916607539
Epoch #123: loss=0.014672746116201483
Epoch #124: loss=0.015536784122351568
Epoch #125: loss=0.016848361294250935
Epoch #126: loss=0.011962192848479762
Epoch #127: loss=0.008837585328869151
Epoch #128: loss=0.009043694450046604
Epoch #129: loss=0.011407444784270476
Epoch #130: loss=0.014983187768158098
Epoch #131: loss=0.013401489389789604
Epoch #132: loss=0.05195017949417121
Epoch #133: loss=0.02171761339901335
Epoch #134: loss=0.016160047218078715
Epoch #135: loss=0.01473992412467347
Epoch #136: loss=0.014738832800109108
Epoch #137: loss=0.02058243856194474
Epoch #138: loss=0.013171684813225665
Epoch #139: loss=0.01017282354613522
Epoch #140: loss=0.012665616043496888
Epoch #141: loss=0.014792390333920583
Epoch #142: loss=0.01544493135579943
Epoch #143: loss=0.010704102421548296
Epoch #144: loss=0.012786293929163648
Epoch #145: loss=0.01371812536374172
Epoch #146: loss=0.01347211693116907
Epoch #147: loss=0.011703379995909138
Epoch #148: loss=0.01597984050469282
Epoch #149: loss=0.01877142840332607
Epoch #150: loss=0.015490503635356366
Epoch #151: loss=0.020467271050368156
Epoch #152: loss=0.01512204003499271
Epoch #153: loss=0.015468230068119782
Epoch #154: loss=0.011529677628163607
Epoch #155: loss=0.01014570787144721
Epoch #156: loss=0.01119500923100915
Epoch #157: loss=0.012431081264410926
Epoch #158: loss=0.016134012290465936
Epoch #159: loss=0.01420927842316661
Epoch #160: loss=0.012685314977979175
Epoch #161: loss=0.011669488168859444
Epoch #162: loss=0.011592573302265688
Epoch #163: loss=0.013706871253509827
Epoch #164: loss=0.007636242857955379
Epoch #165: loss=0.019797306368267726
Epoch #166: loss=0.014834602382079648
Epoch #167: loss=0.011377444678387347
Epoch #168: loss=0.01661786718377479
Epoch #169: loss=0.01575971895748895
Epoch #170: loss=0.009410335672076045
Epoch #171: loss=0.009309795283839031
Epoch #172: loss=0.012532380553655303
Epoch #173: loss=0.013637453704758221
Epoch #174: loss=0.01625376978379966
Epoch #175: loss=0.019844579105041022
Epoch #176: loss=0.011458394706216977
Epoch #177: loss=0.01243917053707045
Epoch #178: loss=0.012519309953442798
Epoch #179: loss=0.019617127667669296
Epoch #180: loss=0.016398001256390442
Epoch #181: loss=0.009750537715602598
Epoch #182: loss=0.00853147574315403
Epoch #183: loss=0.01474581937956433
Epoch #184: loss=0.012193169537195318
Epoch #185: loss=0.0154610158570871
Epoch #186: loss=0.009564783173600627
Epoch #187: loss=0.006731910472655114
Epoch #188: loss=0.01076311821404477
Epoch #189: loss=0.009398627069327104
Epoch #190: loss=0.022676117876712906
Epoch #191: loss=0.015088074663527736
Epoch #192: loss=0.006493299218569321
Epoch #193: loss=0.009533995842502918
Epoch #194: loss=0.00902435649028581
Epoch #195: loss=0.013860728847128002
Epoch #196: loss=0.007575163731252612
Epoch #197: loss=0.011018155391274738
Epoch #198: loss=0.00814603970002829
Epoch #199: loss=0.010510935231540787
Epoch #200: loss=0.012363003583527918
Epoch #201: loss=0.015237307351055213
Epoch #202: loss=0.015413377590053883
Epoch #203: loss=0.010472483398758874
Epoch #204: loss=0.012061793618893014
Epoch #205: loss=0.009611900279656787
Epoch #206: loss=0.011067281572617372
Epoch #207: loss=0.01242101449640019
Epoch #208: loss=0.013597142481152529
Epoch #209: loss=0.011958080294811312
Epoch #210: loss=0.012451124296373553
Epoch #211: loss=0.006141972314308296
Epoch #212: loss=0.008828930651361588
Epoch #213: loss=0.03422248716492778
Epoch #214: loss=0.009847844407261849
Epoch #215: loss=0.008431530358755109
Epoch #216: loss=0.023264416583066144
Epoch #217: loss=0.010950906269872248
Epoch #218: loss=0.014498290665102673
Epoch #219: loss=0.00832439479028879
Epoch #220: loss=0.011365255235476246
Epoch #221: loss=0.008965909046595019
Epoch #222: loss=0.00930554835151752
Epoch #223: loss=0.011640390092633238
Epoch #224: loss=0.00646757090980389
Epoch #225: loss=0.008487241057586485
Epoch #226: loss=0.007730920696424191
Epoch #227: loss=0.010551440931280867
Epoch #228: loss=0.020304062542595328
Epoch #229: loss=0.011903621884889014
Epoch #230: loss=0.00826603942883835
Epoch #231: loss=0.0075741029828043475
Epoch #232: loss=0.010816052249746386
Epoch #233: loss=0.010796727859821866
Epoch #234: loss=0.01660548181830399
Epoch #235: loss=0.014830055967091122
Epoch #236: loss=0.011328245386206377
Epoch #237: loss=0.013879100953934277
Epoch #238: loss=0.010029514879479173
Epoch #239: loss=0.00951765599770679
Epoch #240: loss=0.009297840066793983
Epoch #241: loss=0.01000169013931587
Epoch #242: loss=0.014827490704718288
Epoch #243: loss=0.011194924550083274
Epoch #244: loss=0.013200846036786708
Epoch #245: loss=0.009174131908059872
Epoch #246: loss=0.00858105832765388
Epoch #247: loss=0.0090706542533849
Epoch #248: loss=0.010563630628803914
Epoch #249: loss=0.009509198170881831

Training time: 4:37:22.310925

Finished.
n2one setting ettm2_electricity -> weather
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/ettm2_electricity_epochs_250_seed_2023/model.pkl', muti_dataset='ettm2_electricity', random_seed=2023, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='weather')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=8.03987e-09): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.64345e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=3.90948e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=8.03987e-09): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 1.141718413745597, 'MAE': 0.8694430274050097}
Finished.
------------------------- record done -------------------------
n2one setting ettm2_electricity -> exchange
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/ettm2_electricity_epochs_250_seed_2023/model.pkl', muti_dataset='ettm2_electricity', random_seed=2023, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='exchange')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=3.96689e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.32855299693108986, 'MAE': 0.38004542735286095}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='ettm2_traffic', random_seed=2023, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/ettm2_traffic_epochs_250_seed_2023
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=1.0321667731354462
Epoch #1: loss=0.38232279306902783
Epoch #2: loss=0.2755503534058798
Epoch #3: loss=0.19650914645481765
Epoch #4: loss=0.1604752296844299
Epoch #5: loss=0.1327786861053686
Epoch #6: loss=0.10791997954181107
Epoch #7: loss=0.09042691561326231
Epoch #8: loss=0.08322194633090108
Epoch #9: loss=0.07437838086717573
Epoch #10: loss=0.058358742784546586
Epoch #11: loss=0.06083357039484849
Epoch #12: loss=0.0552794929270559
Epoch #13: loss=0.05213843467394647
Epoch #14: loss=0.04960395782660023
Epoch #15: loss=0.04519057402841721
Epoch #16: loss=0.04918781243385224
Epoch #17: loss=0.04038609406710277
Epoch #18: loss=0.04434659053213608
Epoch #19: loss=0.03689016007262475
Epoch #20: loss=0.0377058123848065
Epoch #21: loss=0.05494159949572174
Epoch #22: loss=0.027474998614888422
Epoch #23: loss=0.031538280618685334
Epoch #24: loss=0.038324821709451465
Epoch #25: loss=0.02871987453615453
Epoch #26: loss=0.033075305732726656
Epoch #27: loss=0.02457023945046036
Epoch #28: loss=0.035749057899383915
Epoch #29: loss=0.03251066956698963
Epoch #30: loss=0.025966750948347432
Epoch #31: loss=0.026935490957068712
Epoch #32: loss=0.024997927433368938
Epoch #33: loss=0.01907860469190406
Epoch #34: loss=0.02748398057190181
Epoch #35: loss=0.02101569199125962
Epoch #36: loss=0.021777959142433946
Epoch #37: loss=0.026134785451821744
Epoch #38: loss=0.02131042378967391
Epoch #39: loss=0.022965927492362582
Epoch #40: loss=0.021052128166885235
Epoch #41: loss=0.020624906143103167
Epoch #42: loss=0.016923275479568404
Epoch #43: loss=0.019278535092908798
Epoch #44: loss=0.01908806328331563
Epoch #45: loss=0.02613610303691705
Epoch #46: loss=0.023674034782368496
Epoch #47: loss=0.02262121123978644
Epoch #48: loss=0.019308008512608464
Epoch #49: loss=0.015485339456454947
Epoch #50: loss=0.02082686508035731
Epoch #51: loss=0.023936527722547438
Epoch #52: loss=0.01844413401086933
Epoch #53: loss=0.016617286860337963
Epoch #54: loss=0.024963256078059153
Epoch #55: loss=0.01682460815521738
Epoch #56: loss=0.01919242930315853
Epoch #57: loss=0.012101328676531943
Epoch #58: loss=0.015610567750121207
Epoch #59: loss=0.03487538880507388
Epoch #60: loss=0.019155310410044753
Epoch #61: loss=0.014033038891173947
Epoch #62: loss=0.01684127280514347
Epoch #63: loss=0.01949524053586871
Epoch #64: loss=0.015334969526743528
Epoch #65: loss=0.015538011814314523
Epoch #66: loss=0.016006431576374755
Epoch #67: loss=0.019713376501271338
Epoch #68: loss=0.01562923547147852
Epoch #69: loss=0.020664519085367462
Epoch #70: loss=0.013789301935035548
Epoch #71: loss=0.016580231864658946
Epoch #72: loss=0.01944630509343762
Epoch #73: loss=0.01690458026779298
Epoch #74: loss=0.019031233246782783
Epoch #75: loss=0.014467693043324376
Epoch #76: loss=0.014174566341795432
Epoch #77: loss=0.01649060195976258
Epoch #78: loss=0.016213562836943095
Epoch #79: loss=0.01671972095204639
Epoch #80: loss=0.017734618975851848
Epoch #81: loss=0.012957419535655363
Epoch #82: loss=0.014270438597844817
Epoch #83: loss=0.010857630748588983
Epoch #84: loss=0.016355361985524088
Epoch #85: loss=0.012673235775505215
Epoch #86: loss=0.011561515813159734
Epoch #87: loss=0.01756201621129941
Epoch #88: loss=0.015428545402824604
Epoch #89: loss=0.02632322799326918
Epoch #90: loss=0.01652461787008196
Epoch #91: loss=0.011945803401590873
Epoch #92: loss=0.014099861868379268
Epoch #93: loss=0.01120672559726411
Epoch #94: loss=0.015680994870013665
Epoch #95: loss=0.0103080786077798
Epoch #96: loss=0.0312458536259479
Epoch #97: loss=0.011283718400839119
Epoch #98: loss=0.017245912878136156
Epoch #99: loss=0.013336499694518517
Epoch #100: loss=0.014597720367807836
Epoch #101: loss=0.012717857522895839
Epoch #102: loss=0.0160307214080443
Epoch #103: loss=0.013310166798587483
Epoch #104: loss=0.012214722273321597
Epoch #105: loss=0.010837160561102809
Epoch #106: loss=0.015936881818188267
Epoch #107: loss=0.009829784281093644
Epoch #108: loss=0.01692466552706741
Epoch #109: loss=0.02139848172852
Epoch #110: loss=0.017054128207752796
Epoch #111: loss=0.011057687174613648
Epoch #112: loss=0.013331720388797406
Epoch #113: loss=0.01205902140207385
Epoch #114: loss=0.017844805740850005
Epoch #115: loss=0.01175024562854963
Epoch #116: loss=0.02663001295127522
Epoch #117: loss=0.010490522128141796
Epoch #118: loss=0.009218911863642106
Epoch #119: loss=0.013057238624027407
Epoch #120: loss=0.011647200251554479
Epoch #121: loss=0.011151761139549937
Epoch #122: loss=0.014710833689613685
Epoch #123: loss=0.014359824764486941
Epoch #124: loss=0.017206606850640984
Epoch #125: loss=0.014183428472505118
Epoch #126: loss=0.009688364843587751
Epoch #127: loss=0.010356706308891312
Epoch #128: loss=0.016351028704822555
Epoch #129: loss=0.011225708591148876
Epoch #130: loss=0.010234973277982121
Epoch #131: loss=0.013134859196108553
Epoch #132: loss=0.008638190688850878
Epoch #133: loss=0.010155977362303868
Epoch #134: loss=0.014036461255003197
Epoch #135: loss=0.013297797207451159
Epoch #136: loss=0.012842755641639957
Epoch #137: loss=0.00953657488703164
Epoch #138: loss=0.01601287769010362
Epoch #139: loss=0.011041905665470801
Epoch #140: loss=0.0127942287226594
Epoch #141: loss=0.01063220447701704
Epoch #142: loss=0.0129284506698618
Epoch #143: loss=0.015742369018913006
Epoch #144: loss=0.011381512151916115
Epoch #145: loss=0.014705651198361253
Epoch #146: loss=0.009783791004295413
Epoch #147: loss=0.011008183093975433
Epoch #148: loss=0.00841787550043056
Epoch #149: loss=0.01354270754942088
Epoch #150: loss=0.012215501820307997
Epoch #151: loss=0.012610735527896093
Epoch #152: loss=0.013591231334472303
Epoch #153: loss=0.010488490760153029
Epoch #154: loss=0.010665863077843002
Epoch #155: loss=0.013913277519628392
Epoch #156: loss=0.013569219345630937
Epoch #157: loss=0.007888557345856154
Epoch #158: loss=0.010891656290942735
Epoch #159: loss=0.012853632664336222
Epoch #160: loss=0.013217542093560016
Epoch #161: loss=0.014087226942652582
Epoch #162: loss=0.010984005442286952
Epoch #163: loss=0.009780056551236841
Epoch #164: loss=0.015160533790747322
Epoch #165: loss=0.009633584180446757
Epoch #166: loss=0.008034821501117265
Epoch #167: loss=0.010583208878351912
Epoch #168: loss=0.007993925833320651
Epoch #169: loss=0.012368602184756466
Epoch #170: loss=0.010026543511846262
Epoch #171: loss=0.008403957975649074
Epoch #172: loss=0.014943497421816424
Epoch #173: loss=0.011261122883247496
Epoch #174: loss=0.013782801896909504
Epoch #175: loss=0.010987632294162924
Epoch #176: loss=0.011492785411278258
Epoch #177: loss=0.00879918496541968
Epoch #178: loss=0.009616006910607575
Epoch #179: loss=0.007196378513550165
Epoch #180: loss=0.01180962970359507
Epoch #181: loss=0.008182206244296197
Epoch #182: loss=0.011108358427634161
Epoch #183: loss=0.013632283416646827
Epoch #184: loss=0.01046167287439079
Epoch #185: loss=0.01300400815084566
Epoch #186: loss=0.01398962919962289
Epoch #187: loss=0.009293594537039483
Epoch #188: loss=0.013284231596014927
Epoch #189: loss=0.006520669289616834
Epoch #190: loss=0.008693817072733463
Epoch #191: loss=0.013256098063781606
Epoch #192: loss=0.010038456662527616
Epoch #193: loss=0.00961521452475224
Epoch #194: loss=0.01429369966481505
Epoch #195: loss=0.01224291044358593
Epoch #196: loss=0.011647205210457139
Epoch #197: loss=0.010444502586515926
Epoch #198: loss=0.007929488890516312
Epoch #199: loss=0.011372681918371584
Epoch #200: loss=0.009313973434110607
Epoch #201: loss=0.01490911970196136
Epoch #202: loss=0.00879929221185559
Epoch #203: loss=0.016704920116590093
Epoch #204: loss=0.013704306800795861
Epoch #205: loss=0.008724482537188966
Epoch #206: loss=0.011615109208473596
Epoch #207: loss=0.010894778610132064
Epoch #208: loss=0.008468574033077313
Epoch #209: loss=0.013397816182812276
Epoch #210: loss=0.011302756297916551
Epoch #211: loss=0.006644728474062548
Epoch #212: loss=0.01345603891525936
Epoch #213: loss=0.011104708919226285
Epoch #214: loss=0.00846919777386168
Epoch #215: loss=0.011749891042898043
Epoch #216: loss=0.00832764809318903
Epoch #217: loss=0.011605551927998213
Epoch #218: loss=0.018000441441341237
Epoch #219: loss=0.008185001891974481
Epoch #220: loss=0.005659303798647368
Epoch #221: loss=0.011470683252535982
Epoch #222: loss=0.01573096063604924
Epoch #223: loss=0.013177840368993065
Epoch #224: loss=0.009238051120159808
Epoch #225: loss=0.007882807210944155
Epoch #226: loss=0.008634806744561489
Epoch #227: loss=0.007735912321775422
Epoch #228: loss=0.011855326988074217
Epoch #229: loss=0.00865945143683548
Epoch #230: loss=0.009636616988446874
Epoch #231: loss=0.010520907522552969
Epoch #232: loss=0.009600646398790321
Epoch #233: loss=0.0106269924976169
Epoch #234: loss=0.008012759293887313
Epoch #235: loss=0.011699249877654153
Epoch #236: loss=0.009237461617456633
Epoch #237: loss=0.013284440443185367
Epoch #238: loss=0.006218220790397276
Epoch #239: loss=0.009245071731817019
Epoch #240: loss=0.010283934623172362
Epoch #241: loss=0.011713820142330888
Epoch #242: loss=0.008436339101808802
Epoch #243: loss=0.021229356043098092
Epoch #244: loss=0.008788979834061714
Epoch #245: loss=0.00840938047819023
Epoch #246: loss=0.007731902130949738
Epoch #247: loss=0.011754543543546561
Epoch #248: loss=0.0086484916250467
Epoch #249: loss=0.006619747737877312

Training time: 10:19:08.345387

Finished.
n2one setting ettm2_traffic -> electricity
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/ettm2_traffic_epochs_250_seed_2023/model.pkl', muti_dataset='ettm2_traffic', random_seed=2023, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='electricity')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=8.02175e-09): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.83544e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=4.67572e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=8.02175e-09): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.41390740073595444, 'MAE': 0.45948782654113685}
Finished.
------------------------- record done -------------------------
n2one setting ettm2_traffic -> weather
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/ettm2_traffic_epochs_250_seed_2023/model.pkl', muti_dataset='ettm2_traffic', random_seed=2023, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='weather')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.05413e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.3469e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=5.56336e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.5367054198817506, 'MAE': 0.5788699967303804}
Finished.
------------------------- record done -------------------------
n2one setting ettm2_traffic -> exchange
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/ettm2_traffic_epochs_250_seed_2023/model.pkl', muti_dataset='ettm2_traffic', random_seed=2023, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='exchange')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=5.3246e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.27246819643393877, 'MAE': 0.34758525045501204}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='ettm2_weather', random_seed=2023, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/ettm2_weather_epochs_250_seed_2023
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=5.590398840002112
Epoch #1: loss=2.677391039358603
Epoch #2: loss=2.5683376917967924
Epoch #3: loss=2.2479879372828715
Epoch #4: loss=2.0331959595551363
Epoch #5: loss=1.8767632052705094
Epoch #6: loss=1.7029683879903845
Epoch #7: loss=1.5434794103777087
Epoch #8: loss=1.5929992198944092
Epoch #9: loss=1.3224311390438594
Epoch #10: loss=1.2150765979612195
Epoch #11: loss=1.2526244408375509
Epoch #12: loss=1.050616583308658
Epoch #13: loss=1.113112365877306
Epoch #14: loss=0.9699235655166007
Epoch #15: loss=0.9981900727426684
Epoch #16: loss=0.8573159591571705
Epoch #17: loss=0.9698584514695245
Epoch #18: loss=0.957941295327367
Epoch #19: loss=0.8375931672147803
Epoch #20: loss=0.7453614940514436
Epoch #21: loss=0.8148149155281685
Epoch #22: loss=0.9027713360013189
Epoch #23: loss=0.7761595974097381
Epoch #24: loss=0.7375641835702432
Epoch #25: loss=0.7532450846723608
Epoch #26: loss=0.7197031990901844
Epoch #27: loss=0.6804163745931677
Epoch #28: loss=0.6517380661255604
Epoch #29: loss=0.6631866459910934
Epoch #30: loss=0.6156357913403898
Epoch #31: loss=0.568114469180236
Epoch #32: loss=0.5815768185499552
Epoch #33: loss=0.6647998658386437
Epoch #34: loss=0.5450013980672166
Epoch #35: loss=0.5823355687631143
Epoch #36: loss=0.6249529276345227
Epoch #37: loss=0.5949774626139048
Epoch #38: loss=0.6325154634746345
Epoch #39: loss=0.48827654445493546
Epoch #40: loss=0.4807036929839366
Epoch #41: loss=0.5590374163679175
Epoch #42: loss=0.5181606774394577
Epoch #43: loss=0.5868134603307054
Epoch #44: loss=0.47715648686563644
Epoch #45: loss=0.519230912666063
Epoch #46: loss=0.5178824179881328
Epoch #47: loss=0.4446399123282046
Epoch #48: loss=0.4535073883630134
Epoch #49: loss=0.40074804304419337
Epoch #50: loss=0.38073814438806997
Epoch #51: loss=0.3868228652187296
Epoch #52: loss=0.34540126331754634
Epoch #53: loss=0.3556690872521014
Epoch #54: loss=0.36182951161990295
Epoch #55: loss=0.3552590129343239
Epoch #56: loss=0.29445878073975845
Epoch #57: loss=0.38849615164705226
Epoch #58: loss=0.3779023700469249
Epoch #59: loss=0.3269667146173683
Epoch #60: loss=0.3147894683721903
Epoch #61: loss=0.31665391777012797
Epoch #62: loss=0.35766896726311864
Epoch #63: loss=0.32620092501511444
Epoch #64: loss=0.32899859426794825
Epoch #65: loss=0.3499376226115871
Epoch #66: loss=0.2795314776736337
Epoch #67: loss=0.2709255609157923
Epoch #68: loss=0.29904094580057505
Epoch #69: loss=0.2522301188594586
Epoch #70: loss=0.24776227007041107
Epoch #71: loss=0.22052671115946126
Epoch #72: loss=0.24231089027346792
Epoch #73: loss=0.21390605818580938
Epoch #74: loss=0.23143925437250654
Epoch #75: loss=0.29770080906313817
Epoch #76: loss=0.2241221696541116
Epoch #77: loss=0.2879198298260972
Epoch #78: loss=0.2562349011769166
Epoch #79: loss=0.25632307396547216
Epoch #80: loss=0.24111241102218628
Epoch #81: loss=0.2017795536163691
Epoch #82: loss=0.24634785044032173
Epoch #83: loss=0.2410035588451334
Epoch #84: loss=0.2242245964101843
Epoch #85: loss=0.2781292041008537
Epoch #86: loss=0.19854548150623166
Epoch #87: loss=0.233630116324167
Epoch #88: loss=0.24305815249681473
Epoch #89: loss=0.2666020496068774
Epoch #90: loss=0.17672334490595637
Epoch #91: loss=0.17506055815799818
Epoch #92: loss=0.18488574944235184
Epoch #93: loss=0.21143050713313594
Epoch #94: loss=0.31044582801090703
Epoch #95: loss=0.35349439507400665
Epoch #96: loss=0.2791646878058846
Epoch #97: loss=0.20983322005014163
Epoch #98: loss=0.15451927009869265
Epoch #99: loss=0.16522815189248807
Epoch #100: loss=0.36994147220173396
Epoch #101: loss=0.23165611318639806
Epoch #102: loss=0.2427119922799033
Epoch #103: loss=0.1712428955612956
Epoch #104: loss=0.22509781976003904
Epoch #105: loss=0.21815012707500844
Epoch #106: loss=0.1750565785411242
Epoch #107: loss=0.1598249739287673
Epoch #108: loss=0.1408131812271234
Epoch #109: loss=0.15593248454702868
Epoch #110: loss=0.14800733609779462
Epoch #111: loss=0.1559199870035455
Epoch #112: loss=0.145180129924336
Epoch #113: loss=0.11030531271889403
Epoch #114: loss=0.12498494620258743
Epoch #115: loss=0.1309777180085311
Epoch #116: loss=0.11407486621189762
Epoch #117: loss=0.1096257713396807
Epoch #118: loss=0.11751332969681637
Epoch #119: loss=0.1562485149180567
Epoch #120: loss=0.1094965617600325
Epoch #121: loss=0.10197693431699598
Epoch #122: loss=0.12957085236101537
Epoch #123: loss=0.15778698731918592
Epoch #124: loss=0.12139187592106897
Epoch #125: loss=0.10627486516495009
Epoch #126: loss=0.10300534747138217
Epoch #127: loss=0.09385269980978321
Epoch #128: loss=0.17302864014699654
Epoch #129: loss=0.12173988025736164
Epoch #130: loss=0.11754841065487347
Epoch #131: loss=0.12808549766604965
Epoch #132: loss=0.1185498354906166
Epoch #133: loss=0.10715514300642787
Epoch #134: loss=0.09479693273032033
Epoch #135: loss=0.0950848377636961
Epoch #136: loss=0.1039082325894285
Epoch #137: loss=0.11750454767733007
Epoch #138: loss=0.0808658359219899
Epoch #139: loss=0.08084701283557995
Epoch #140: loss=0.12848726856346065
Epoch #141: loss=0.09734936471323709
Epoch #142: loss=0.08831427175853704
Epoch #143: loss=0.0950938754186437
Epoch #144: loss=0.11978006508906146
Epoch #145: loss=0.08704582370213561
Epoch #146: loss=0.08557396346854197
Epoch #147: loss=0.07204315161986931
Epoch #148: loss=0.06552013644092791
Epoch #149: loss=0.14935123870098913
Epoch #150: loss=0.12350764830370208
Epoch #151: loss=0.07642831512399621
Epoch #152: loss=0.058067869948776996
Epoch #153: loss=0.10480655014917657
Epoch #154: loss=0.09296221240750842
Epoch #155: loss=0.08348512911313288
Epoch #156: loss=0.07480580082817657
Epoch #157: loss=0.06728429528507027
Epoch #158: loss=0.061228819946582254
Epoch #159: loss=0.07026898040360696
Epoch #160: loss=0.06320214616386471
Epoch #161: loss=0.1753963951424167
Epoch #162: loss=0.13361385333779696
Epoch #163: loss=0.08775311799065487
Epoch #164: loss=0.07461980483620553
Epoch #165: loss=0.0749469371764241
Epoch #166: loss=0.06972844148608479
Epoch #167: loss=0.05933700894584527
Epoch #168: loss=0.06449931333898692
Epoch #169: loss=0.23008121640698329
Epoch #170: loss=0.09781397984841385
Epoch #171: loss=0.07811743992607335
Epoch #172: loss=0.09503753795414357
Epoch #173: loss=0.09862875671604195
Epoch #174: loss=0.08194974819953377
Epoch #175: loss=0.08218806111127944
Epoch #176: loss=0.13148856087512262
Epoch #177: loss=0.05723613802645657
Epoch #178: loss=0.05929265679741228
Epoch #179: loss=0.13202592347924774
Epoch #180: loss=0.09754444386910747
Epoch #181: loss=0.09918973650279883
Epoch #182: loss=0.07580000293013212
Epoch #183: loss=0.06808568376141626
Epoch #184: loss=0.06304583519797873
Epoch #185: loss=0.09293824310943082
Epoch #186: loss=0.10323762078140233
Epoch #187: loss=0.13495848393319426
Epoch #188: loss=0.09302398396303525
Epoch #189: loss=0.045596340545327275
Epoch #190: loss=0.047671771683805694
Epoch #191: loss=0.049701989452178415
Epoch #192: loss=0.0469491513748024
Epoch #193: loss=0.05236959313923443
Epoch #194: loss=0.05332112901315496
Epoch #195: loss=0.07857153378427029
Epoch #196: loss=0.04321010337790122
Epoch #197: loss=0.06529833582808843
Epoch #198: loss=0.07928480298535244
Epoch #199: loss=0.1977107139757356
Epoch #200: loss=0.0946877117495279
Epoch #201: loss=0.08778522792901541
Epoch #202: loss=0.06413852838749015
Epoch #203: loss=0.07042978218177685
Epoch #204: loss=0.056739817121745764
Epoch #205: loss=0.04132277468169058
Epoch #206: loss=0.0808095437650745
Epoch #207: loss=0.11588244054567169
Epoch #208: loss=0.08438055685444458
Epoch #209: loss=0.09143414235094914
Epoch #210: loss=0.09176613808282323
Epoch #211: loss=0.11114722689160624
Epoch #212: loss=0.055805751097363396
Epoch #213: loss=0.04768041425661461
Epoch #214: loss=0.04665736632572638
Epoch #215: loss=0.09520762066382009
Epoch #216: loss=0.06269875415474982
Epoch #217: loss=0.05072596002168752
Epoch #218: loss=0.07657153750895648
Epoch #219: loss=0.05334192318086688
Epoch #220: loss=0.05481125134974718
Epoch #221: loss=0.06468987900361016
Epoch #222: loss=0.05149931347350011
Epoch #223: loss=0.030129819202262
Epoch #224: loss=0.046189701977513126
Epoch #225: loss=0.032079760136233794
Epoch #226: loss=0.047577051924088516
Epoch #227: loss=0.08956553416981085
Epoch #228: loss=0.07143988550917522
Epoch #229: loss=0.03633219486958272
Epoch #230: loss=0.05532817643236469
Epoch #231: loss=0.06133102182600949
Epoch #232: loss=0.05457210996364419
Epoch #233: loss=0.05488681289795283
Epoch #234: loss=0.03358844506579477
Epoch #235: loss=0.04470552619848702
Epoch #236: loss=0.15675440415538647
Epoch #237: loss=0.06141355477676198
Epoch #238: loss=0.04750374772560757
Epoch #239: loss=0.07783456249917681
Epoch #240: loss=0.03467917792197015
Epoch #241: loss=0.03964709641562926
Epoch #242: loss=0.03996282523951015
Epoch #243: loss=0.03466978438853009
Epoch #244: loss=0.03853291118316151
Epoch #245: loss=0.06519977335591574
Epoch #246: loss=0.05635034798871021
Epoch #247: loss=0.04314797907765653
Epoch #248: loss=0.03256668723062486
Epoch #249: loss=0.07048647826839541

Training time: 0:39:15.012511

Finished.
n2one setting ettm2_weather -> electricity
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/ettm2_weather_epochs_250_seed_2023/model.pkl', muti_dataset='ettm2_weather', random_seed=2023, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='electricity')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.53161e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=3.05838e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.53161e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.3541406030655094, 'MAE': 0.41804795410200757}
Finished.
------------------------- record done -------------------------
n2one setting ettm2_weather -> exchange
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/ettm2_weather_epochs_250_seed_2023/model.pkl', muti_dataset='ettm2_weather', random_seed=2023, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='exchange')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=3.49389e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.2294443090484994, 'MAE': 0.3288749456747908}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='ettm2_exchange', random_seed=2023, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/ettm2_exchange_epochs_250_seed_2023
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=5.7819560828961825
Epoch #1: loss=2.7660111314372013
Epoch #2: loss=2.2780180667576038
Epoch #3: loss=2.1485238514448466
Epoch #4: loss=2.096909629671197
Epoch #5: loss=1.9616621356261403
Epoch #6: loss=1.9057048182738454
Epoch #7: loss=1.865899500093962
Epoch #8: loss=1.732711672782898
Epoch #9: loss=1.6646557167956704
Epoch #10: loss=1.6028024648365222
Epoch #11: loss=1.4805224255511635
Epoch #12: loss=1.5046260388273942
Epoch #13: loss=1.3695670554512425
Epoch #14: loss=1.3824505366777118
Epoch #15: loss=1.3103430773082532
Epoch #16: loss=1.2545299279062372
Epoch #17: loss=1.1586560575585616
Epoch #18: loss=1.2919560482627468
Epoch #19: loss=1.1910485656637895
Epoch #20: loss=1.1298775202349614
Epoch #21: loss=1.1275796796146191
Epoch #22: loss=1.0788236730977108
Epoch #23: loss=1.133295382324018
Epoch #24: loss=1.0600793361663818
Epoch #25: loss=1.0056294045950238
Epoch #26: loss=0.9472069677553678
Epoch #27: loss=0.962684154510498
Epoch #28: loss=1.007476110207407
Epoch #29: loss=1.0199874733623706
Epoch #30: loss=0.9793638273289329
Epoch #31: loss=1.0142678963510614
Epoch #32: loss=0.8616691294469332
Epoch #33: loss=0.9207014661086234
Epoch #34: loss=0.7968611779965853
Epoch #35: loss=0.8189036563823098
Epoch #36: loss=0.8882143340612713
Epoch #37: loss=0.789052781305815
Epoch #38: loss=0.9096958260787161
Epoch #39: loss=0.8616331909832201
Epoch #40: loss=0.86703349100916
Epoch #41: loss=0.8665650737913031
Epoch #42: loss=0.7658651282912806
Epoch #43: loss=0.7377952211781552
Epoch #44: loss=0.7246164789325312
Epoch #45: loss=0.7494669337021677
Epoch #46: loss=0.8797504039187181
Epoch #47: loss=0.7458810100429937
Epoch #48: loss=0.6283575076805917
Epoch #49: loss=0.6570090886793638
Epoch #50: loss=0.754991700774745
Epoch #51: loss=0.6112623873509859
Epoch #52: loss=0.6661283797339389
Epoch #53: loss=0.6295685862240038
Epoch #54: loss=0.697367461104142
Epoch #55: loss=0.7268877060789811
Epoch #56: loss=0.6104193000417006
Epoch #57: loss=0.5670673737400457
Epoch #58: loss=0.6220174149463051
Epoch #59: loss=0.6409598228178526
Epoch #60: loss=0.585604046520434
Epoch #61: loss=0.5130751713326103
Epoch #62: loss=0.5415803727350736
Epoch #63: loss=0.5117978806558409
Epoch #64: loss=0.5513189748713845
Epoch #65: loss=0.5774430287511725
Epoch #66: loss=0.556266219208115
Epoch #67: loss=0.48100082184139054
Epoch #68: loss=0.6417318268826133
Epoch #69: loss=0.5224931240081787
Epoch #70: loss=0.5460664128002367
Epoch #71: loss=0.4783390478083962
Epoch #72: loss=0.560059202344794
Epoch #73: loss=0.5039960734154049
Epoch #74: loss=0.506881688770495
Epoch #75: loss=0.508772432019836
Epoch #76: loss=0.5429042844395888
Epoch #77: loss=0.5793820211761876
Epoch #78: loss=0.5754575619572088
Epoch #79: loss=0.5541642010211945
Epoch #80: loss=0.45906700742872136
Epoch #81: loss=0.4415061944409421
Epoch #82: loss=0.45321998392280777
Epoch #83: loss=0.39709408424402537
Epoch #84: loss=0.4211368552948299
Epoch #85: loss=0.42721550166606903
Epoch #86: loss=0.4204241837325849
Epoch #87: loss=0.41259756684303284
Epoch #88: loss=0.4181788759796243
Epoch #89: loss=0.6588745038760336
Epoch #90: loss=0.4275160620087071
Epoch #91: loss=0.4434418239091572
Epoch #92: loss=0.4001884805528741
Epoch #93: loss=0.4228906937335667
Epoch #94: loss=0.3752361308587225
Epoch #95: loss=0.4419210019864534
Epoch #96: loss=0.3808094953235827
Epoch #97: loss=0.3526348444192033
Epoch #98: loss=0.4763995142359483
Epoch #99: loss=0.3626927121689445
Epoch #100: loss=0.4304239518548313
Epoch #101: loss=0.4651825769167197
Epoch #102: loss=0.4395969996326848
Epoch #103: loss=0.44930219258132736
Epoch #104: loss=0.45591289746133906
Epoch #105: loss=0.5586235562437459
Epoch #106: loss=0.42787125863527
Epoch #107: loss=0.39948359837657527
Epoch #108: loss=0.3792034426802083
Epoch #109: loss=0.37535474488609716
Epoch #110: loss=0.3010987035538021
Epoch #111: loss=0.3337777986338264
Epoch #112: loss=0.3417917639017105
Epoch #113: loss=0.2976272913970445
Epoch #114: loss=0.3164612392061635
Epoch #115: loss=0.37976666500693873
Epoch #116: loss=0.3637718056377612
Epoch #117: loss=0.40889432751818705
Epoch #118: loss=0.38300586138900955
Epoch #119: loss=0.3130695247336438
Epoch #120: loss=0.3263607323169708
Epoch #121: loss=0.33004752977898244
Epoch #122: loss=0.3775516191595479
Epoch #123: loss=0.29699676836791794
Epoch #124: loss=0.2703621905100973
Epoch #125: loss=0.2752910656364341
Epoch #126: loss=0.2386994220708546
Epoch #127: loss=0.2730539641097972
Epoch #128: loss=0.29531630128622055
Epoch #129: loss=0.4250111434804766
Epoch #130: loss=0.2802045008069591
Epoch #131: loss=0.3365074215750945
Epoch #132: loss=0.33755035776841014
Epoch #133: loss=0.30953355447242137
Epoch #134: loss=0.328172001399492
Epoch #135: loss=0.38669930869027186
Epoch #136: loss=0.33542072616125407
Epoch #137: loss=0.3019990999447672
Epoch #138: loss=0.277787236790908
Epoch #139: loss=0.27906633090031774
Epoch #140: loss=0.30248538287062393
Epoch #141: loss=0.3102639348883378
Epoch #142: loss=0.2818838318711833
Epoch #143: loss=0.253932581528237
Epoch #144: loss=0.2910908488066573
Epoch #145: loss=0.2261078185156772
Epoch #146: loss=0.19900629591000707
Epoch #147: loss=0.2626315818021172
Epoch #148: loss=0.1674425049047721
Epoch #149: loss=0.2504098195778696
Epoch #150: loss=0.2459147588202828
Epoch #151: loss=0.2643390925307023
Epoch #152: loss=0.25166644469687816
Epoch #153: loss=0.25644553256662267
Epoch #154: loss=0.23044235573003166
Epoch #155: loss=0.24719260319283134
Epoch #156: loss=0.2480216865476809
Epoch #157: loss=0.19898783180274462
Epoch #158: loss=0.27553897703948776
Epoch #159: loss=0.44091385838232544
Epoch #160: loss=0.3079482207172795
Epoch #161: loss=0.2868284943856691
Epoch #162: loss=0.23261104994698575
Epoch #163: loss=0.23211840697025
Epoch #164: loss=0.25695964576382385
Epoch #165: loss=0.236129661139689
Epoch #166: loss=0.17505206677474475
Epoch #167: loss=0.17449413886980006
Epoch #168: loss=0.23246965871045464
Epoch #169: loss=0.14773606195261604
Epoch #170: loss=0.23836930880421087
Epoch #171: loss=0.12812726983898565
Epoch #172: loss=0.18519206619576403
Epoch #173: loss=0.20377956173921885
Epoch #174: loss=0.16367285894720177
Epoch #175: loss=0.2403360769936913
Epoch #176: loss=0.19889728960238004
Epoch #177: loss=0.15161224140932686
Epoch #178: loss=0.2045456123979468
Epoch #179: loss=0.185859588808135
Epoch #180: loss=0.1542920374164456
Epoch #181: loss=0.18727795524816765
Epoch #182: loss=0.21541126327295052
Epoch #183: loss=0.14026633257928647
Epoch #184: loss=0.1418486261054089
Epoch #185: loss=0.31278501882364873
Epoch #186: loss=0.2456163748314506
Epoch #187: loss=0.20325423032045364
Epoch #188: loss=0.20996782654210142
Epoch #189: loss=0.4045640810539848
Epoch #190: loss=0.29513986310676527
Epoch #191: loss=0.17698242652573085
Epoch #192: loss=0.18945522841654325
Epoch #193: loss=0.0946342458850459
Epoch #194: loss=0.14988986932133375
Epoch #195: loss=0.24471355543324821
Epoch #196: loss=0.21338109926957832
Epoch #197: loss=0.18380005814527212
Epoch #198: loss=0.1519027977789703
Epoch #199: loss=0.22453171955911735
Epoch #200: loss=0.25938046331468384
Epoch #201: loss=0.15639512907517583
Epoch #202: loss=0.1460543429773105
Epoch #203: loss=0.20513142253223218
Epoch #204: loss=0.16709745812572932
Epoch #205: loss=0.17862236460572795
Epoch #206: loss=0.13669621140549057
Epoch #207: loss=0.15997157971325673
Epoch #208: loss=0.16216837653988286
Epoch #209: loss=0.12785437742346212
Epoch #210: loss=0.1752716169545525
Epoch #211: loss=0.16964074910471313
Epoch #212: loss=0.12069323560909222
Epoch #213: loss=0.1284761071989411
Epoch #214: loss=0.19245487588800883
Epoch #215: loss=0.2180738264792844
Epoch #216: loss=0.249531613564805
Epoch #217: loss=0.29872661907421916
Epoch #218: loss=0.22888648470765666
Epoch #219: loss=0.29350197158361735
Epoch #220: loss=0.26144363495864364
Epoch #221: loss=0.1866801941865369
Epoch #222: loss=0.1495062891197832
Epoch #223: loss=0.14441628852172902
Epoch #224: loss=0.1857079777278398
Epoch #225: loss=0.14762734170807035
Epoch #226: loss=0.13449741762719655
Epoch #227: loss=0.12580784959228417
Epoch #228: loss=0.15283250906749776
Epoch #229: loss=0.11894685814255163
Epoch #230: loss=0.11106292138758458
Epoch #231: loss=0.10758770316054947
Epoch #232: loss=0.11036654797039534
Epoch #233: loss=0.11062836156863916
Epoch #234: loss=0.0998541726485679
Epoch #235: loss=0.17459026390784665
Epoch #236: loss=0.1521693877875805
Epoch #237: loss=0.13665602258161494
Epoch #238: loss=0.140703628918058
Epoch #239: loss=0.1315785787607494
Epoch #240: loss=0.11322765366027229
Epoch #241: loss=0.10599134529107496
Epoch #242: loss=0.10700430564190212
Epoch #243: loss=0.1086421875577224
Epoch #244: loss=0.09410846096120383
Epoch #245: loss=0.1047528258671886
Epoch #246: loss=0.11587405900814031
Epoch #247: loss=0.1834926201324714
Epoch #248: loss=0.15691166645602175
Epoch #249: loss=0.15095456160212817

Training time: 0:16:42.470384

Finished.
n2one setting ettm2_exchange -> electricity
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/ettm2_exchange_epochs_250_seed_2023/model.pkl', muti_dataset='ettm2_exchange', random_seed=2023, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='electricity')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.49294e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.97115e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.49294e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.37054799965369134, 'MAE': 0.4298110091633332}
Finished.
------------------------- record done -------------------------
n2one setting ettm2_exchange -> weather
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/ettm2_exchange_epochs_250_seed_2023/model.pkl', muti_dataset='ettm2_exchange', random_seed=2023, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='weather')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.35016e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.50826e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=5.88119e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.35016e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.652598038147052, 'MAE': 0.5983231474359869}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='electricity_traffic', random_seed=2023, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/electricity_traffic_epochs_250_seed_2023
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=0.8534717410424959
Epoch #1: loss=0.29794358348730554
Epoch #2: loss=0.19960029774667445
Epoch #3: loss=0.14165939992436996
Epoch #4: loss=0.10939327071303548
Epoch #5: loss=0.08990357814959722
Epoch #6: loss=0.06438196956676794
Epoch #7: loss=0.06115151731106858
Epoch #8: loss=0.05117384909903308
Epoch #9: loss=0.048733512041594325
Epoch #10: loss=0.04175974587974432
Epoch #11: loss=0.03919688052799323
Epoch #12: loss=0.03777047047804449
Epoch #13: loss=0.03320735316483079
Epoch #14: loss=0.03364257538251875
Epoch #15: loss=0.032810035348056626
Epoch #16: loss=0.030211616502145804
Epoch #17: loss=0.02943291673409207
Epoch #18: loss=0.020422336464650956
Epoch #19: loss=0.02587765959310538
Epoch #20: loss=0.028152792317464488
Epoch #21: loss=0.028004882207226137
Epoch #22: loss=0.023739765803591408
Epoch #23: loss=0.021952183089907488
Epoch #24: loss=0.02233656571592682
Epoch #25: loss=0.02409147149886163
Epoch #26: loss=0.02220723029639964
Epoch #27: loss=0.023024197438253926
Epoch #28: loss=0.02110796466211742
Epoch #29: loss=0.021792061270924326
Epoch #30: loss=0.018542295257710093
Epoch #31: loss=0.017103843350914628
Epoch #32: loss=0.017797235607615154
Epoch #33: loss=0.02413831204876606
Epoch #34: loss=0.020041632952738555
Epoch #35: loss=0.019608829174844574
Epoch #36: loss=0.016399379045120344
Epoch #37: loss=0.01892738719697135
Epoch #38: loss=0.015577346437420958
Epoch #39: loss=0.018643013812514573
Epoch #40: loss=0.01819100669374184
Epoch #41: loss=0.01833145477600783
Epoch #42: loss=0.01479632872596087
Epoch #43: loss=0.017803857467366173
Epoch #44: loss=0.018849900462042733
Epoch #45: loss=0.022754712170299927
Epoch #46: loss=0.01221903974244183
Epoch #47: loss=0.018040048440022625
Epoch #48: loss=0.015144882000792245
Epoch #49: loss=0.01612260148499656
Epoch #50: loss=0.019183193798788443
Epoch #51: loss=0.014724859322180353
Epoch #52: loss=0.01593918933740082
Epoch #53: loss=0.01856837732178519
Epoch #54: loss=0.013919775384295825
Epoch #55: loss=0.018482882871891534
Epoch #56: loss=0.016238857800198448
Epoch #57: loss=0.02086770826165038
Epoch #58: loss=0.016468612996943802
Epoch #59: loss=0.015291394056514277
Epoch #60: loss=0.012368263852320429
Epoch #61: loss=0.01496559052508374
Epoch #62: loss=0.014560434171610724
Epoch #63: loss=0.026755198513168783
Epoch #64: loss=0.016896622701338335
Epoch #65: loss=0.013809637572143593
Epoch #66: loss=0.013073891560537804
Epoch #67: loss=0.01358401033831164
Epoch #68: loss=0.01332115811657059
Epoch #69: loss=0.017779548117678002
Epoch #70: loss=0.01930621383707858
Epoch #71: loss=0.01597040346670118
Epoch #72: loss=0.012345525591094158
Epoch #73: loss=0.012217952198230261
Epoch #74: loss=0.013450790187529944
Epoch #75: loss=0.013862023088937473
Epoch #76: loss=0.012171652166313386
Epoch #77: loss=0.016220739357173837
Epoch #78: loss=0.015721846416590433
Epoch #79: loss=0.01488950597127724
Epoch #80: loss=0.013616905809932767
Epoch #81: loss=0.016606887934888088
Epoch #82: loss=0.010407176618363916
Epoch #83: loss=0.011727541997398557
Epoch #84: loss=0.012280605436132084
Epoch #85: loss=0.012925951992130475
Epoch #86: loss=0.009428302074965783
Epoch #87: loss=0.012191114810234379
Epoch #88: loss=0.012241950491959912
Epoch #89: loss=0.011012762663757145
Epoch #90: loss=0.01275514839606266
Epoch #91: loss=0.012705067906368111
Epoch #92: loss=0.01212965044861054
Epoch #93: loss=0.009945531932885868
Epoch #94: loss=0.011728756447620446
Epoch #95: loss=0.010563472556116591
Epoch #96: loss=0.014744899318425972
Epoch #97: loss=0.011593025705104202
Epoch #98: loss=0.012921166147302406
Epoch #99: loss=0.008324030246075014
Epoch #100: loss=0.014740632479770867
Epoch #101: loss=0.0147427926726051
Epoch #102: loss=0.01253873004138372
Epoch #103: loss=0.007468444529638831
Epoch #104: loss=0.011944115268058308
Epoch #105: loss=0.009652772202495103
Epoch #106: loss=0.00925216197429539
Epoch #107: loss=0.022319314979472996
Epoch #108: loss=0.009127699852350702
Epoch #109: loss=0.014146196624666457
Epoch #110: loss=0.016348261425912457
Epoch #111: loss=0.012222309104728435
Epoch #112: loss=0.014111858682608742
Epoch #113: loss=0.014497809987622474
Epoch #114: loss=0.00713043757554912
Epoch #115: loss=0.012869502102127787
Epoch #116: loss=0.009029840634945746
Epoch #117: loss=0.021839222354500098
Epoch #118: loss=0.011350908845153619
Epoch #119: loss=0.010106722977335073
Epoch #120: loss=0.012876967899058492
Epoch #121: loss=0.011598613397782159
Epoch #122: loss=0.010418053799796344
Epoch #123: loss=0.008024109668977753
Epoch #124: loss=0.012553739841013933
Epoch #125: loss=0.011855432086090642
Epoch #126: loss=0.009535256702454276
Epoch #127: loss=0.010118130029156817
Epoch #128: loss=0.023352116190895625
Epoch #129: loss=0.00891157950483072
Epoch #130: loss=0.013958872668484473
Epoch #131: loss=0.008427317809555223
Epoch #132: loss=0.008309738405276856
Epoch #133: loss=0.009494409092682978
Epoch #134: loss=0.009069559831180936
Epoch #135: loss=0.020793650775304692
Epoch #136: loss=0.012972512598009642
Epoch #137: loss=0.0087313450905889
Epoch #138: loss=0.00844410778507612
Epoch #139: loss=0.015658253005400517
Epoch #140: loss=0.008047741550396148
Epoch #141: loss=0.009415860316659894
Epoch #142: loss=0.010242096216252083
Epoch #143: loss=0.010973829541323685
Epoch #144: loss=0.007239740446151552
Epoch #145: loss=0.01162641104519435
Epoch #146: loss=0.007160034483150656
Epoch #147: loss=0.007779983917302106
Epoch #148: loss=0.01927852214618662
Epoch #149: loss=0.008599177557675503
Epoch #150: loss=0.008385679080692958
Epoch #151: loss=0.011770324947893863
Epoch #152: loss=0.008704338838043307
Epoch #153: loss=0.00958256796303759
Epoch #154: loss=0.008990870053348126
Epoch #155: loss=0.00894458801441315
Epoch #156: loss=0.00724570656866713
Epoch #157: loss=0.009043780932328306
Epoch #158: loss=0.01032154758548205
Epoch #159: loss=0.011364674424627715
Epoch #160: loss=0.011107356776375109
Epoch #161: loss=0.006313097993801619
Epoch #162: loss=0.011050926287310546
Epoch #163: loss=0.010797906231353053
Epoch #164: loss=0.009611762729507362
Epoch #165: loss=0.006795331793554822
Epoch #166: loss=0.012105551308322617
Epoch #167: loss=0.00957738139338627
Epoch #168: loss=0.01389788761971877
Epoch #169: loss=0.006729310216140902
Epoch #170: loss=0.014272429880749872
Epoch #171: loss=0.007359398873529766
Epoch #172: loss=0.009861519099981248
Epoch #173: loss=0.011154485211591236
Epoch #174: loss=0.012379742682995494
Epoch #175: loss=0.008324564567031295
Epoch #176: loss=0.008073000582790696
Epoch #177: loss=0.00965744619312573
Epoch #178: loss=0.007060476393614938
Epoch #179: loss=0.012402697928690797
Epoch #180: loss=0.009806080329314973
Epoch #181: loss=0.009865233516234972
Epoch #182: loss=0.009768837441169374
Epoch #183: loss=0.00919045632203734
Epoch #184: loss=0.009656620609278504
Epoch #185: loss=0.010010943574842354
Epoch #186: loss=0.011302852840945149
Epoch #187: loss=0.008909839006686593
Epoch #188: loss=0.008640566160862527
Epoch #189: loss=0.008545861479033592
Epoch #190: loss=0.009009754618077554
Epoch #191: loss=0.007039527121879601
Epoch #192: loss=0.009698431351975796
Epoch #193: loss=0.009131487683920867
Epoch #194: loss=0.006736465237849425
Epoch #195: loss=0.010240986466653806
Epoch #196: loss=0.006121388703113991
Epoch #197: loss=0.009309475490532744
Epoch #198: loss=0.009840171230254878
Epoch #199: loss=0.007581773890577086
Epoch #200: loss=0.01869942379417882
Epoch #201: loss=0.009553331600416323
Epoch #202: loss=0.006999202660539909
Epoch #203: loss=0.00835992177660614
Epoch #204: loss=0.01159354389430337
Epoch #205: loss=0.008438035853087842
Epoch #206: loss=0.013735841896734496
Epoch #207: loss=0.008967652521973837
Epoch #208: loss=0.005512303945471701
Epoch #209: loss=0.007384985221427795
Epoch #210: loss=0.008980002355914259
Epoch #211: loss=0.012051029382393611
Epoch #212: loss=0.009395839083472949
Epoch #213: loss=0.008171230347311228
Epoch #214: loss=0.007316160844741287
Epoch #215: loss=0.008178663350246279
Epoch #216: loss=0.007343977704737788
Epoch #217: loss=0.008924343771889092
Epoch #218: loss=0.007175911604108762
Epoch #219: loss=0.010518908682121346
Epoch #220: loss=0.007124714739714172
Epoch #221: loss=0.008140186086269088
Epoch #222: loss=0.011885563105626511
Epoch #223: loss=0.012880778157707053
Epoch #224: loss=0.010294085585900977
Epoch #225: loss=0.006478514372981654
Epoch #226: loss=0.010169207624582706
Epoch #227: loss=0.009426086884416485
Epoch #228: loss=0.007120095863266213
Epoch #229: loss=0.009871985548895127
Epoch #230: loss=0.008688310644410658
Epoch #231: loss=0.009345692851927958
Epoch #232: loss=0.005947978831665516
Epoch #233: loss=0.0131287917601533
Epoch #234: loss=0.007603164923079949
Epoch #235: loss=0.005798897475020507
Epoch #236: loss=0.00863908246890323
Epoch #237: loss=0.010092625156171315
Epoch #238: loss=0.006998155896776103
Epoch #239: loss=0.007472476017087583
Epoch #240: loss=0.00877499049855898
Epoch #241: loss=0.007361798422748226
Epoch #242: loss=0.007185779715274045
Epoch #243: loss=0.009624990188242803
Epoch #244: loss=0.013482895050146325
Epoch #245: loss=0.006738989749757877
Epoch #246: loss=0.0060241091077864765
Epoch #247: loss=0.010377787368216403
Epoch #248: loss=0.006524768898052048
Epoch #249: loss=0.008599627463593123

Training time: 14:46:24.995651

Finished.
n2one setting electricity_traffic -> weather
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/electricity_traffic_epochs_250_seed_2023/model.pkl', muti_dataset='electricity_traffic', random_seed=2023, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='weather')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.04537e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.11743e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=5.40585e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.04537e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.4290742997324554, 'MAE': 0.48983426941967695}
Finished.
------------------------- record done -------------------------
n2one setting electricity_traffic -> exchange
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/electricity_traffic_epochs_250_seed_2023/model.pkl', muti_dataset='electricity_traffic', random_seed=2023, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='exchange')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=4.51933e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.18745165784939447, 'MAE': 0.30062979207406415}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='electricity_weather', random_seed=2023, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/electricity_weather_epochs_250_seed_2023
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=1.807907581501117
Epoch #1: loss=0.8037009654024493
Epoch #2: loss=0.5902455223225036
Epoch #3: loss=0.4576475176240937
Epoch #4: loss=0.3805775677608138
Epoch #5: loss=0.33947988511506005
Epoch #6: loss=0.3023939081588465
Epoch #7: loss=0.28731530279951756
Epoch #8: loss=0.24736493344952806
Epoch #9: loss=0.23004820867728776
Epoch #10: loss=0.21312482697991197
Epoch #11: loss=0.1713318541452761
Epoch #12: loss=0.1653137242888511
Epoch #13: loss=0.16046441124177804
Epoch #14: loss=0.14641129688201446
Epoch #15: loss=0.13883787200864006
Epoch #16: loss=0.14099905114910788
Epoch #17: loss=0.12541704047310248
Epoch #18: loss=0.10861608514331432
Epoch #19: loss=0.11149435130723616
Epoch #20: loss=0.08523234991244005
Epoch #21: loss=0.08218262201545476
Epoch #22: loss=0.07480974156699366
Epoch #23: loss=0.09999380022641648
Epoch #24: loss=0.08596389683135262
Epoch #25: loss=0.07903810533472157
Epoch #26: loss=0.0569789185542989
Epoch #27: loss=0.06277085700198493
Epoch #28: loss=0.06176839910597898
Epoch #29: loss=0.05136691896856819
Epoch #30: loss=0.054689460063637674
Epoch #31: loss=0.07283102183653745
Epoch #32: loss=0.058356055454418104
Epoch #33: loss=0.04630035190421669
Epoch #34: loss=0.04409385104032947
Epoch #35: loss=0.0442646106943939
Epoch #36: loss=0.06002921819007856
Epoch #37: loss=0.07129565007871913
Epoch #38: loss=0.06251125694091515
Epoch #39: loss=0.03679905148888656
Epoch #40: loss=0.0517707531459855
Epoch #41: loss=0.05281005739650396
Epoch #42: loss=0.05042424033293957
Epoch #43: loss=0.03263325226004705
Epoch #44: loss=0.04956627713069481
Epoch #45: loss=0.04750855776255881
Epoch #46: loss=0.07178177020810438
Epoch #47: loss=0.0369812227833898
Epoch #48: loss=0.03322525619883466
Epoch #49: loss=0.029655072598527548
Epoch #50: loss=0.029734365975548665
Epoch #51: loss=0.058681547563249295
Epoch #52: loss=0.048779349406759764
Epoch #53: loss=0.036106143696287306
Epoch #54: loss=0.03719555760964601
Epoch #55: loss=0.02645448034478926
Epoch #56: loss=0.03185374543046638
Epoch #57: loss=0.02467760791490794
Epoch #58: loss=0.023667213736807857
Epoch #59: loss=0.027631163401851264
Epoch #60: loss=0.03091705381240889
Epoch #61: loss=0.036386049511608926
Epoch #62: loss=0.031607357099362424
Epoch #63: loss=0.02577942998587323
Epoch #64: loss=0.03024611674263934
Epoch #65: loss=0.04345999067073543
Epoch #66: loss=0.022672111635842502
Epoch #67: loss=0.02348830919747673
Epoch #68: loss=0.03433475065379207
Epoch #69: loss=0.03211846693636318
Epoch #70: loss=0.0491798137997677
Epoch #71: loss=0.03941592318441998
Epoch #72: loss=0.033137377033477126
Epoch #73: loss=0.02373291640133149
Epoch #74: loss=0.020729241110330785
Epoch #75: loss=0.02390603719061772
Epoch #76: loss=0.03440581431232597
Epoch #77: loss=0.03989064526040998
Epoch #78: loss=0.09794438734277545
Epoch #79: loss=0.023799708959182205
Epoch #80: loss=0.01776619419715198
Epoch #81: loss=0.019731099275477573
Epoch #82: loss=0.01605044442003523
Epoch #83: loss=0.016750339581698224
Epoch #84: loss=0.019670187039045828
Epoch #85: loss=0.022297693930615963
Epoch #86: loss=0.02026641116409109
Epoch #87: loss=0.023357474416566836
Epoch #88: loss=0.024117243338996272
Epoch #89: loss=0.012489232919729108
Epoch #90: loss=0.02159930347707874
Epoch #91: loss=0.030792860629275894
Epoch #92: loss=0.027373020761473216
Epoch #93: loss=0.01655059450179183
Epoch #94: loss=0.09128844733880054
Epoch #95: loss=0.0353669145262615
Epoch #96: loss=0.018964102523608042
Epoch #97: loss=0.024460926866836534
Epoch #98: loss=0.033704320135726315
Epoch #99: loss=0.017190850197917298
Epoch #100: loss=0.01788986082992621
Epoch #101: loss=0.033784126357466616
Epoch #102: loss=0.02022718850832556
Epoch #103: loss=0.020856213930864103
Epoch #104: loss=0.027706458086351884
Epoch #105: loss=0.019500036432873458
Epoch #106: loss=0.017570679117113578
Epoch #107: loss=0.01808140641772819
Epoch #108: loss=0.019913555020000116
Epoch #109: loss=0.034934746418034214
Epoch #110: loss=0.019806857485363063
Epoch #111: loss=0.0205980073700871
Epoch #112: loss=0.015671677754920846
Epoch #113: loss=0.02206505410106831
Epoch #114: loss=0.01736129657382156
Epoch #115: loss=0.014315153874043559
Epoch #116: loss=0.03760995245817109
Epoch #117: loss=0.02044156237179279
Epoch #118: loss=0.020369640436769604
Epoch #119: loss=0.017562526007456655
Epoch #120: loss=0.01557519790576905
Epoch #121: loss=0.013232744685193035
Epoch #122: loss=0.018994630659545937
Epoch #123: loss=0.046577447862632744
Epoch #124: loss=0.027224827403476412
Epoch #125: loss=0.01594814355451592
Epoch #126: loss=0.013031150728346035
Epoch #127: loss=0.016554188915855125
Epoch #128: loss=0.018919895544999763
Epoch #129: loss=0.016850255512275467
Epoch #130: loss=0.020770208994148068
Epoch #131: loss=0.023799802514493616
Epoch #132: loss=0.026930716549823333
Epoch #133: loss=0.022168147615029582
Epoch #134: loss=0.020007013426727942
Epoch #135: loss=0.01832159723793304
Epoch #136: loss=0.021887253878654252
Epoch #137: loss=0.013205060353053074
Epoch #138: loss=0.013905811384226321
Epoch #139: loss=0.02550301092283623
Epoch #140: loss=0.01921414029405396
Epoch #141: loss=0.01125300723890927
Epoch #142: loss=0.019959873050989407
Epoch #143: loss=0.02091233929876179
Epoch #144: loss=0.016598177209874932
Epoch #145: loss=0.010826096348065254
Epoch #146: loss=0.01774765330094036
Epoch #147: loss=0.014278608670703777
Epoch #148: loss=0.014163506833429821
Epoch #149: loss=0.017361953067079976
Epoch #150: loss=0.013808404382147829
Epoch #151: loss=0.0202262934589368
Epoch #152: loss=0.025331979693297393
Epoch #153: loss=0.02371981001431552
Epoch #154: loss=0.015975309128659007
Epoch #155: loss=0.01373014881282975
Epoch #156: loss=0.012112044296005972
Epoch #157: loss=0.01621114172531904
Epoch #158: loss=0.009407271848208477
Epoch #159: loss=0.013998513279404889
Epoch #160: loss=0.014468116397393722
Epoch #161: loss=0.024396606901394183
Epoch #162: loss=0.015023059544142776
Epoch #163: loss=0.013177636011972653
Epoch #164: loss=0.016755340357431175
Epoch #165: loss=0.01553139501388496
Epoch #166: loss=0.00910783672777591
Epoch #167: loss=0.06281561590798242
Epoch #168: loss=0.012985397970487741
Epoch #169: loss=0.01252518048429773
Epoch #170: loss=0.008764617301316353
Epoch #171: loss=0.01514800238533426
Epoch #172: loss=0.009799746946914469
Epoch #173: loss=0.016822386096296726
Epoch #174: loss=0.02989521536168237
Epoch #175: loss=0.016534668549436336
Epoch #176: loss=0.016230316012638142
Epoch #177: loss=0.01404549441282216
Epoch #178: loss=0.015244126267548683
Epoch #179: loss=0.01022318562208232
Epoch #180: loss=0.012562088736786537
Epoch #181: loss=0.013133808541526315
Epoch #182: loss=0.01833588834241387
Epoch #183: loss=0.025283567650848864
Epoch #184: loss=0.013087836502241346
Epoch #185: loss=0.0164020526054636
Epoch #186: loss=0.012110455350659154
Epoch #187: loss=0.016373206093043133
Epoch #188: loss=0.010956242157267782
Epoch #189: loss=0.00829393932191918
Epoch #190: loss=0.009506033681066684
Epoch #191: loss=0.054470087784196194
Epoch #192: loss=0.030475154496435484
Epoch #193: loss=0.018113129698102408
Epoch #194: loss=0.013166526762932345
Epoch #195: loss=0.013071550128636587
Epoch #196: loss=0.012461181234511995
Epoch #197: loss=0.013178727695730485
Epoch #198: loss=0.012101960703442062
Epoch #199: loss=0.013582744641512665
Epoch #200: loss=0.014801321504905991
Epoch #201: loss=0.010786732295910513
Epoch #202: loss=0.01761721130557793
Epoch #203: loss=0.017061477962489472
Epoch #204: loss=0.02317603298071074
Epoch #205: loss=0.011556545305683114
Epoch #206: loss=0.008757104352888213
Epoch #207: loss=0.008411542665711886
Epoch #208: loss=0.01530215010665604
Epoch #209: loss=0.013456925986383664
Epoch #210: loss=0.016875776485321903
Epoch #211: loss=0.01477933253514881
Epoch #212: loss=0.008252910134699249
Epoch #213: loss=0.00997245564277791
Epoch #214: loss=0.009015776208261033
Epoch #215: loss=0.011658246519066854
Epoch #216: loss=0.01620316117151804
Epoch #217: loss=0.011963130239268458
Epoch #218: loss=0.018200959847828706
Epoch #219: loss=0.013069998423779592
Epoch #220: loss=0.01204150969830881
Epoch #221: loss=0.015201641212875743
Epoch #222: loss=0.04309783763234082
Epoch #223: loss=0.018801169427848482
Epoch #224: loss=0.01711535893742301
Epoch #225: loss=0.013241335911248873
Epoch #226: loss=0.014588181462601197
Epoch #227: loss=0.012534742150824753
Epoch #228: loss=0.009390974624997665
Epoch #229: loss=0.016489120178888834
Epoch #230: loss=0.014043962625972524
Epoch #231: loss=0.015158483093961408
Epoch #232: loss=0.025400172692746387
Epoch #233: loss=0.021987810226523664
Epoch #234: loss=0.014646882408279093
Epoch #235: loss=0.015126719156752195
Epoch #236: loss=0.009863769354500677
Epoch #237: loss=0.006994961782409582
Epoch #238: loss=0.011526108407569775
Epoch #239: loss=0.010108423038095274
Epoch #240: loss=0.008996807919363612
Epoch #241: loss=0.014324291530303438
Epoch #242: loss=0.010801786802670856
Epoch #243: loss=0.017587270001580224
Epoch #244: loss=0.01390711302796556
Epoch #245: loss=0.007952756903544257
Epoch #246: loss=0.016822401870572833
Epoch #247: loss=0.008354868823045097
Epoch #248: loss=0.01867502230797929
Epoch #249: loss=0.009638854246019582

Training time: 4:48:26.625988

Finished.
n2one setting electricity_weather -> exchange
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/electricity_weather_epochs_250_seed_2023/model.pkl', muti_dataset='electricity_weather', random_seed=2023, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='exchange')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=5.20577e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.29458177769919464, 'MAE': 0.3708900846694365}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='electricity_exchange', random_seed=2023, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/electricity_exchange_epochs_250_seed_2023
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=1.747655659642263
Epoch #1: loss=0.7069594268559685
Epoch #2: loss=0.49950577151086917
Epoch #3: loss=0.40206207208176875
Epoch #4: loss=0.35357923402612335
Epoch #5: loss=0.3198809000496444
Epoch #6: loss=0.26894761189410754
Epoch #7: loss=0.243407902361593
Epoch #8: loss=0.2320020318846572
Epoch #9: loss=0.19188550157659323
Epoch #10: loss=0.17631029142887758
Epoch #11: loss=0.1519104537517285
Epoch #12: loss=0.15424801017466286
Epoch #13: loss=0.1358846772835791
Epoch #14: loss=0.12422919845757695
Epoch #15: loss=0.1378455266616899
Epoch #16: loss=0.11595883497849424
Epoch #17: loss=0.11301897291807418
Epoch #18: loss=0.08871368815029736
Epoch #19: loss=0.08881673844967117
Epoch #20: loss=0.08359042636336798
Epoch #21: loss=0.08726238399336403
Epoch #22: loss=0.08152509443914818
Epoch #23: loss=0.08514857308876822
Epoch #24: loss=0.0830574916112364
Epoch #25: loss=0.07096585747715096
Epoch #26: loss=0.07585135480604137
Epoch #27: loss=0.0654001375150762
Epoch #28: loss=0.05826894330472252
Epoch #29: loss=0.07052234660885028
Epoch #30: loss=0.06096798262918385
Epoch #31: loss=0.0545075282986675
Epoch #32: loss=0.04827938929177526
Epoch #33: loss=0.05010901579752247
Epoch #34: loss=0.057874559426232944
Epoch #35: loss=0.04584721644128755
Epoch #36: loss=0.04704476579794473
Epoch #37: loss=0.051297590629081596
Epoch #38: loss=0.04457880077688278
Epoch #39: loss=0.049716922234443774
Epoch #40: loss=0.041934269798160916
Epoch #41: loss=0.037774438969083245
Epoch #42: loss=0.03712734670199929
Epoch #43: loss=0.03458618182946551
Epoch #44: loss=0.0443031016410269
Epoch #45: loss=0.04837637700460353
Epoch #46: loss=0.04312403437024758
Epoch #47: loss=0.043429271032818594
Epoch #48: loss=0.037372900057714264
Epoch #49: loss=0.03913939049775998
Epoch #50: loss=0.034148934113617566
Epoch #51: loss=0.03775880580886881
Epoch #52: loss=0.041115235258459955
Epoch #53: loss=0.03844141987956336
Epoch #54: loss=0.027009842133554306
Epoch #55: loss=0.031065277755260468
Epoch #56: loss=0.029302783594905516
Epoch #57: loss=0.03813036019548683
Epoch #58: loss=0.031126447889060743
Epoch #59: loss=0.02668980088744505
Epoch #60: loss=0.05573931965470246
Epoch #61: loss=0.04854360608862413
Epoch #62: loss=0.03202566943955141
Epoch #63: loss=0.043850112403176815
Epoch #64: loss=0.02628659311325309
Epoch #65: loss=0.03048194561643373
Epoch #66: loss=0.029500481919569492
Epoch #67: loss=0.030061585387416004
Epoch #68: loss=0.03409029856518103
Epoch #69: loss=0.022145108053549887
Epoch #70: loss=0.03042810837317847
Epoch #71: loss=0.05031595326674179
Epoch #72: loss=0.030074462086340202
Epoch #73: loss=0.036167842590591186
Epoch #74: loss=0.030073526222329575
Epoch #75: loss=0.035387657353467675
Epoch #76: loss=0.02187361160475806
Epoch #77: loss=0.0341296326975893
Epoch #78: loss=0.022982935751660934
Epoch #79: loss=0.019298092970422166
Epoch #80: loss=0.030232642448895064
Epoch #81: loss=0.02751540145971232
Epoch #82: loss=0.022017436469501692
Epoch #83: loss=0.028626692935677194
Epoch #84: loss=0.0288698666719833
Epoch #85: loss=0.029446023182326357
Epoch #86: loss=0.02937429991379482
Epoch #87: loss=0.03206733401226429
Epoch #88: loss=0.02495078379528931
Epoch #89: loss=0.0188123376094906
Epoch #90: loss=0.02431463945790061
Epoch #91: loss=0.021130968362417096
Epoch #92: loss=0.029234744741094488
Epoch #93: loss=0.02147926002072799
Epoch #94: loss=0.017107323564356902
Epoch #95: loss=0.018907946382654903
Epoch #96: loss=0.03223722874955486
Epoch #97: loss=0.029822957634750336
Epoch #98: loss=0.031022959409138656
Epoch #99: loss=0.02206556274984485
Epoch #100: loss=0.023917890050856108
Epoch #101: loss=0.027831761198476423
Epoch #102: loss=0.03281481846095569
Epoch #103: loss=0.03867341102260847
Epoch #104: loss=0.03531235774751119
Epoch #105: loss=0.021593192659670878
Epoch #106: loss=0.021197775699053876
Epoch #107: loss=0.024713725512946382
Epoch #108: loss=0.047122509577906035
Epoch #109: loss=0.028366483458159293
Epoch #110: loss=0.02013708542983577
Epoch #111: loss=0.02009222362902423
Epoch #112: loss=0.02108252138422383
Epoch #113: loss=0.016266975386420655
Epoch #114: loss=0.024355235820428093
Epoch #115: loss=0.028921766961021715
Epoch #116: loss=0.01855240601640744
Epoch #117: loss=0.02322934486632204
Epoch #118: loss=0.020712304904938063
Epoch #119: loss=0.023611013989861613
Epoch #120: loss=0.020184754484579376
Epoch #121: loss=0.015739053503523854
Epoch #122: loss=0.019423033726312457
Epoch #123: loss=0.02214423242544523
Epoch #124: loss=0.018561503311900825
Epoch #125: loss=0.02169231839349216
Epoch #126: loss=0.022733116454492948
Epoch #127: loss=0.02214133903337501
Epoch #128: loss=0.021826321585210022
Epoch #129: loss=0.016455534283152433
Epoch #130: loss=0.03135359903944707
Epoch #131: loss=0.024107283047091862
Epoch #132: loss=0.015000327841285334
Epoch #133: loss=0.016423016784098396
Epoch #134: loss=0.019800916950340103
Epoch #135: loss=0.02029791090454861
Epoch #136: loss=0.018502742255539675
Epoch #137: loss=0.017626041824763815
Epoch #138: loss=0.019374699617904473
Epoch #139: loss=0.032800102160481376
Epoch #140: loss=0.025459769405007056
Epoch #141: loss=0.016433783279099192
Epoch #142: loss=0.015847284482341926
Epoch #143: loss=0.044589429954173025
Epoch #144: loss=0.014665856511589694
Epoch #145: loss=0.02073181808202904
Epoch #146: loss=0.018814541109059838
Epoch #147: loss=0.022027782742344522
Epoch #148: loss=0.01939739126589653
Epoch #149: loss=0.02045575859369059
Epoch #150: loss=0.017439040831661017
Epoch #151: loss=0.01721012840184771
Epoch #152: loss=0.019122088674772927
Epoch #153: loss=0.015216446528264312
Epoch #154: loss=0.03663296010741528
Epoch #155: loss=0.022150812644403894
Epoch #156: loss=0.02122781021091634
Epoch #157: loss=0.023661880487488372
Epoch #158: loss=0.019871593661936096
Epoch #159: loss=0.01829066318739347
Epoch #160: loss=0.014052549708460522
Epoch #161: loss=0.020969640489952146
Epoch #162: loss=0.02106133940464061
Epoch #163: loss=0.01842163454498501
Epoch #164: loss=0.010790195693187614
Epoch #165: loss=0.02087177750640365
Epoch #166: loss=0.019844629048825578
Epoch #167: loss=0.01960432497001788
Epoch #168: loss=0.022578905272158854
Epoch #169: loss=0.01761282312380869
Epoch #170: loss=0.018488863847444206
Epoch #171: loss=0.016480866275139658
Epoch #172: loss=0.013997641017152377
Epoch #173: loss=0.013530693537719112
Epoch #174: loss=0.014307844102783214
Epoch #175: loss=0.016175227763596922
Epoch #176: loss=0.010394744614035169
Epoch #177: loss=0.016674533239027945
Epoch #178: loss=0.014308617105562449
Epoch #179: loss=0.013372468848880466
Epoch #180: loss=0.0135625891669124
Epoch #181: loss=0.01723547809497923
Epoch #182: loss=0.020624081852051817
Epoch #183: loss=0.016550582407734507
Epoch #184: loss=0.017193941318247422
Epoch #185: loss=0.018928724206102258
Epoch #186: loss=0.020954392476383834
Epoch #187: loss=0.01281546932176473
Epoch #188: loss=0.01409344312148933
Epoch #189: loss=0.01740110707233977
Epoch #190: loss=0.02514046629289969
Epoch #191: loss=0.022168698182080428
Epoch #192: loss=0.025625509415161556
Epoch #193: loss=0.017062178519582742
Epoch #194: loss=0.017054744724907537
Epoch #195: loss=0.009237041361548579
Epoch #196: loss=0.013397671112622273
Epoch #197: loss=0.014900412665885989
Epoch #198: loss=0.01672136034985642
Epoch #199: loss=0.017913598503997426
Epoch #200: loss=0.019493482632054394
Epoch #201: loss=0.014752026292598752
Epoch #202: loss=0.021584831243472904
Epoch #203: loss=0.024167769605337747
Epoch #204: loss=0.015260120439074232
Epoch #205: loss=0.037823411397650795
Epoch #206: loss=0.02287981249951109
Epoch #207: loss=0.020203043943614978
Epoch #208: loss=0.016549009303236804
Epoch #209: loss=0.015840116122385096
Epoch #210: loss=0.011951361613179695
Epoch #211: loss=0.012989688693867052
Epoch #212: loss=0.009727389327095437
Epoch #213: loss=0.016237874717842986
Epoch #214: loss=0.038727366208473196
Epoch #215: loss=0.02592961083479101
Epoch #216: loss=0.016018520172340033
Epoch #217: loss=0.01551526342837089
Epoch #218: loss=0.011897999084449166
Epoch #219: loss=0.016197841758323173
Epoch #220: loss=0.014939714170246468
Epoch #221: loss=0.01920104096254396
Epoch #222: loss=0.02834405455371014
Epoch #223: loss=0.02125008085528162
Epoch #224: loss=0.019731365739952454
Epoch #225: loss=0.011024469005379728
Epoch #226: loss=0.02075372821043492
Epoch #227: loss=0.023675925400986345
Epoch #228: loss=0.01780216046768215
Epoch #229: loss=0.012925040906123212
Epoch #230: loss=0.016098495815455274
Epoch #231: loss=0.009876800230407018
Epoch #232: loss=0.011878957786692143
Epoch #233: loss=0.012274165925998694
Epoch #234: loss=0.015263277936817393
Epoch #235: loss=0.016220173484109354
Epoch #236: loss=0.02691042640315097
Epoch #237: loss=0.01663600150930998
Epoch #238: loss=0.022472492083560357
Epoch #239: loss=0.01814464582707082
Epoch #240: loss=0.016030457140601844
Epoch #241: loss=0.010747243248553959
Epoch #242: loss=0.012666218019568992
Epoch #243: loss=0.01077630644387107
Epoch #244: loss=0.011618248239109338
Epoch #245: loss=0.010346302704287248
Epoch #246: loss=0.015602043301831665
Epoch #247: loss=0.022068736866671355
Epoch #248: loss=0.021029890324718015
Epoch #249: loss=0.012659534607450606

Training time: 4:30:59.911079

Finished.
n2one setting electricity_exchange -> weather
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/electricity_exchange_epochs_250_seed_2023/model.pkl', muti_dataset='electricity_exchange', random_seed=2023, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='weather')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.36177e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.52019e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.36177e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.43307371773283393, 'MAE': 0.49525665856587936}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='traffic_weather', random_seed=2023, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/traffic_weather_epochs_250_seed_2023
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=1.2019227627027143
Epoch #1: loss=0.4271997545451463
Epoch #2: loss=0.30051657840240376
Epoch #3: loss=0.24748658144695534
Epoch #4: loss=0.19528530253651175
Epoch #5: loss=0.15270689611851768
Epoch #6: loss=0.1385551367903145
Epoch #7: loss=0.11159141760042592
Epoch #8: loss=0.11094853371541175
Epoch #9: loss=0.09018268351241746
Epoch #10: loss=0.07290521632410116
Epoch #11: loss=0.07900858673252384
Epoch #12: loss=0.06941846278621941
Epoch #13: loss=0.07009936026391901
Epoch #14: loss=0.06558611122303987
Epoch #15: loss=0.053496632409839984
Epoch #16: loss=0.052207415786609555
Epoch #17: loss=0.041879991348675524
Epoch #18: loss=0.05174103893312366
Epoch #19: loss=0.04559692286761469
Epoch #20: loss=0.0342695739890841
Epoch #21: loss=0.04113499131349969
Epoch #22: loss=0.04074565298409046
Epoch #23: loss=0.041842531357254194
Epoch #24: loss=0.03229509253792417
Epoch #25: loss=0.03919829897711532
Epoch #26: loss=0.0450717183231137
Epoch #27: loss=0.03826109302921854
Epoch #28: loss=0.031557917227441036
Epoch #29: loss=0.030353574981852142
Epoch #30: loss=0.02914800567799961
Epoch #31: loss=0.028394662171391444
Epoch #32: loss=0.02507386586584543
Epoch #33: loss=0.02384465312895151
Epoch #34: loss=0.03329068656419802
Epoch #35: loss=0.029201674206273606
Epoch #36: loss=0.03718007290248155
Epoch #37: loss=0.020520989805919345
Epoch #38: loss=0.022299912189204782
Epoch #39: loss=0.022193119215132954
Epoch #40: loss=0.022822552636949562
Epoch #41: loss=0.02315731725907467
Epoch #42: loss=0.028952705145284936
Epoch #43: loss=0.023605469479416323
Epoch #44: loss=0.02441792286504301
Epoch #45: loss=0.023226052073319757
Epoch #46: loss=0.02007453307043499
Epoch #47: loss=0.019073566734929243
Epoch #48: loss=0.03884608579614946
Epoch #49: loss=0.01918273125651926
Epoch #50: loss=0.021946866054579232
Epoch #51: loss=0.022743658038503322
Epoch #52: loss=0.027458463175322436
Epoch #53: loss=0.022574057871122374
Epoch #54: loss=0.023729872071763147
Epoch #55: loss=0.018232564290441143
Epoch #56: loss=0.016418704417119116
Epoch #57: loss=0.019306537141305813
Epoch #58: loss=0.028925339135680628
Epoch #59: loss=0.018735595973519772
Epoch #60: loss=0.021991533538492328
Epoch #61: loss=0.014891341533658706
Epoch #62: loss=0.017566095344411045
Epoch #63: loss=0.029531393888720305
Epoch #64: loss=0.016380776588735275
Epoch #65: loss=0.018001179161745328
Epoch #66: loss=0.022558306706504244
Epoch #67: loss=0.030451076206870993
Epoch #68: loss=0.022833666815459182
Epoch #69: loss=0.02296392903896573
Epoch #70: loss=0.016389234935936123
Epoch #71: loss=0.016175313000457135
Epoch #72: loss=0.020236520550522098
Epoch #73: loss=0.01792184708330325
Epoch #74: loss=0.01690988304543934
Epoch #75: loss=0.018505863803921285
Epoch #76: loss=0.024449105383495293
Epoch #77: loss=0.016647055684819584
Epoch #78: loss=0.0213380182287585
Epoch #79: loss=0.016203146924371966
Epoch #80: loss=0.01937872912769619
Epoch #81: loss=0.023199268490226042
Epoch #82: loss=0.0185434108891452
Epoch #83: loss=0.013632038676692278
Epoch #84: loss=0.018552383181904098
Epoch #85: loss=0.012866212285426865
Epoch #86: loss=0.019159494908406585
Epoch #87: loss=0.020953317034934468
Epoch #88: loss=0.016298469405670847
Epoch #89: loss=0.012121855578952163
Epoch #90: loss=0.015918557905368634
Epoch #91: loss=0.013513191987684721
Epoch #92: loss=0.017253388370459883
Epoch #93: loss=0.019084634459661402
Epoch #94: loss=0.016457833593219902
Epoch #95: loss=0.013035015140262616
Epoch #96: loss=0.0228160122569534
Epoch #97: loss=0.03472798664065399
Epoch #98: loss=0.019477715564347374
Epoch #99: loss=0.013949585212357887
Epoch #100: loss=0.014757728385997293
Epoch #101: loss=0.01094199816746753
Epoch #102: loss=0.016107052599017898
Epoch #103: loss=0.01623297698959275
Epoch #104: loss=0.02501612539174505
Epoch #105: loss=0.014018711371521407
Epoch #106: loss=0.007123618546789691
Epoch #107: loss=0.018405155962878977
Epoch #108: loss=0.014979393521292598
Epoch #109: loss=0.013712031134124478
Epoch #110: loss=0.027646379294670984
Epoch #111: loss=0.010598124483671577
Epoch #112: loss=0.014059200256507448
Epoch #113: loss=0.013068192825009753
Epoch #114: loss=0.013259204230675718
Epoch #115: loss=0.01394411819365724
Epoch #116: loss=0.01233143268948445
Epoch #117: loss=0.015647273935190914
Epoch #118: loss=0.01724697415048869
Epoch #119: loss=0.009886309217452779
Epoch #120: loss=0.013741268335211275
Epoch #121: loss=0.01238581004354847
Epoch #122: loss=0.020537545095148827
Epoch #123: loss=0.013878851204526244
Epoch #124: loss=0.019697235361122323
Epoch #125: loss=0.020836009413009428
Epoch #126: loss=0.013061067805907229
Epoch #127: loss=0.0182941707616018
Epoch #128: loss=0.011715026508321538
Epoch #129: loss=0.01646704429813408
Epoch #130: loss=0.011825060239528751
Epoch #131: loss=0.017507362434542727
Epoch #132: loss=0.014918482477909705
Epoch #133: loss=0.012484683137216478
Epoch #134: loss=0.013707835258223035
Epoch #135: loss=0.024860473828068264
Epoch #136: loss=0.015769663924754592
Epoch #137: loss=0.013934883009013091
Epoch #138: loss=0.016870345629023824
Epoch #139: loss=0.012056055723299217
Epoch #140: loss=0.011003047057448048
Epoch #141: loss=0.010881475575408806
Epoch #142: loss=0.01776874672046065
Epoch #143: loss=0.010700384508406083
Epoch #144: loss=0.012695034114880694
Epoch #145: loss=0.013420582118432084
Epoch #146: loss=0.013847804919514765
Epoch #147: loss=0.00681984854433009
Epoch #148: loss=0.016584318238840153
Epoch #149: loss=0.009806163057316906
Epoch #150: loss=0.016122466653689264
Epoch #151: loss=0.012669788380843055
Epoch #152: loss=0.010727231217724448
Epoch #153: loss=0.011255418761392902
Epoch #154: loss=0.009889216083670894
Epoch #155: loss=0.01931334108214926
Epoch #156: loss=0.013393771640514862
Epoch #157: loss=0.011291345141528238
Epoch #158: loss=0.008682631824338895
Epoch #159: loss=0.008850748397185476
Epoch #160: loss=0.008083182158852733
Epoch #161: loss=0.01949798814741764
Epoch #162: loss=0.01709041882590886
Epoch #163: loss=0.0083827989462595
Epoch #164: loss=0.008881898572596853
Epoch #165: loss=0.019136230997884907
Epoch #166: loss=0.01584715821399686
Epoch #167: loss=0.02026687519342509
Epoch #168: loss=0.011737357890934118
Epoch #169: loss=0.010868560380290833
Epoch #170: loss=0.01065332844617031
Epoch #171: loss=0.008408341042177362
Epoch #172: loss=0.010540496829586724
Epoch #173: loss=0.012963794236998798
Epoch #174: loss=0.008931872966812833
Epoch #175: loss=0.012340126816599042
Epoch #176: loss=0.01718068153477689
Epoch #177: loss=0.016587508867552708
Epoch #178: loss=0.008563934258475186
Epoch #179: loss=0.008348660833845471
Epoch #180: loss=0.015207478185140382
Epoch #181: loss=0.015651012835528614
Epoch #182: loss=0.00814197123920396
Epoch #183: loss=0.022589541907314218
Epoch #184: loss=0.019432525632231495
Epoch #185: loss=0.008211590447719392
Epoch #186: loss=0.012991903458732686
Epoch #187: loss=0.009374890022094783
Epoch #188: loss=0.009064134619784617
Epoch #189: loss=0.013742571916818715
Epoch #190: loss=0.012178378947315683
Epoch #191: loss=0.010938852272407562
Epoch #192: loss=0.016184832215118263
Epoch #193: loss=0.011565677708891631
Epoch #194: loss=0.008375939156197804
Epoch #195: loss=0.009807053912539289
Epoch #196: loss=0.009158852698591718
Epoch #197: loss=0.011803625229125222
Epoch #198: loss=0.010530928592948135
Epoch #199: loss=0.01411466774684946
Epoch #200: loss=0.012446066636085062
Epoch #201: loss=0.012965144803577763
Epoch #202: loss=0.00876743561966086
Epoch #203: loss=0.0086198281829753
Epoch #204: loss=0.016455655084919762
Epoch #205: loss=0.012807411107362216
Epoch #206: loss=0.014647059907236998
Epoch #207: loss=0.011434943498193411
Epoch #208: loss=0.007372880643906847
Epoch #209: loss=0.009472028906800604
Epoch #210: loss=0.01055076866079846
Epoch #211: loss=0.011334498241528825
Epoch #212: loss=0.010536727594913205
Epoch #213: loss=0.015270622045775511
Epoch #214: loss=0.010099465656854623
Epoch #215: loss=0.01391995678391509
Epoch #216: loss=0.010539525402605618
Epoch #217: loss=0.007719365249998576
Epoch #218: loss=0.011036850630876576
Epoch #219: loss=0.012599625601315228
Epoch #220: loss=0.00841633858766858
Epoch #221: loss=0.00971659324922429
Epoch #222: loss=0.01025940508842977
Epoch #223: loss=0.012369915526309144
Epoch #224: loss=0.013816434545366097
Epoch #225: loss=0.011373170908825102
Epoch #226: loss=0.008378992622555915
Epoch #227: loss=0.005635731765987411
Epoch #228: loss=0.0124639903125245
Epoch #229: loss=0.011027558764793752
Epoch #230: loss=0.010782053701470498
Epoch #231: loss=0.012195682743916771
Epoch #232: loss=0.01157465214625542
Epoch #233: loss=0.009019709976274572
Epoch #234: loss=0.015306105114939635
Epoch #235: loss=0.010193416893958489
Epoch #236: loss=0.0072362784796152135
Epoch #237: loss=0.013587409907084473
Epoch #238: loss=0.00910936829952973
Epoch #239: loss=0.008709685118620544
Epoch #240: loss=0.012534749916986576
Epoch #241: loss=0.009526911738748412
Epoch #242: loss=0.0077847764762466526
Epoch #243: loss=0.015195051896994669
Epoch #244: loss=0.007035444137858707
Epoch #245: loss=0.008945369758019477
Epoch #246: loss=0.018340120918658505
Epoch #247: loss=0.010182634457131834
Epoch #248: loss=0.0076885656113632765
Epoch #249: loss=0.00814634058050623

Training time: 10:45:58.381266

Finished.
n2one setting traffic_weather -> electricity
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/traffic_weather_epochs_250_seed_2023/model.pkl', muti_dataset='traffic_weather', random_seed=2023, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='electricity')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=8.54545e-09): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.72649e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=4.42062e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=8.54545e-09): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.40835894503590675, 'MAE': 0.45384029304496665}
Finished.
------------------------- record done -------------------------
n2one setting traffic_weather -> exchange
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/traffic_weather_epochs_250_seed_2023/model.pkl', muti_dataset='traffic_weather', random_seed=2023, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='exchange')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=5.07457e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.4364919612160379, 'MAE': 0.4327142337574588}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='traffic_exchange', random_seed=2023, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/traffic_exchange_epochs_250_seed_2023
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=1.183098923303615
Epoch #1: loss=0.399579421148218
Epoch #2: loss=0.2706125671322318
Epoch #3: loss=0.23041993633083913
Epoch #4: loss=0.17221488850298283
Epoch #5: loss=0.13445870971585486
Epoch #6: loss=0.1157549261147606
Epoch #7: loss=0.10082914751750985
Epoch #8: loss=0.10005709292024545
Epoch #9: loss=0.07361305994302805
Epoch #10: loss=0.07720212575808254
Epoch #11: loss=0.06812322466398707
Epoch #12: loss=0.05676667367739753
Epoch #13: loss=0.06000906355818198
Epoch #14: loss=0.05004335106537817
Epoch #15: loss=0.04958112709031537
Epoch #16: loss=0.049719967846169895
Epoch #17: loss=0.04597218716945285
Epoch #18: loss=0.043381338912040934
Epoch #19: loss=0.04388182467871998
Epoch #20: loss=0.03729294107975331
Epoch #21: loss=0.036748399014247514
Epoch #22: loss=0.038776107351215366
Epoch #23: loss=0.03799091735868126
Epoch #24: loss=0.034868925197007156
Epoch #25: loss=0.029576225367230202
Epoch #26: loss=0.03491268417605295
Epoch #27: loss=0.032580534546320254
Epoch #28: loss=0.041035730759987885
Epoch #29: loss=0.033991670737915466
Epoch #30: loss=0.03017186897963382
Epoch #31: loss=0.027535756630956826
Epoch #32: loss=0.03410823478553465
Epoch #33: loss=0.03055647552058893
Epoch #34: loss=0.02989506660304942
Epoch #35: loss=0.03343300714810518
Epoch #36: loss=0.03505799338186759
Epoch #37: loss=0.026507531146490827
Epoch #38: loss=0.023409882255217018
Epoch #39: loss=0.029919001026997
Epoch #40: loss=0.025859181581924122
Epoch #41: loss=0.02384732156448687
Epoch #42: loss=0.027973791313673446
Epoch #43: loss=0.022678561939994388
Epoch #44: loss=0.02413621435969554
Epoch #45: loss=0.02472112684483217
Epoch #46: loss=0.02761438475833435
Epoch #47: loss=0.041900208811596805
Epoch #48: loss=0.02740305135636751
Epoch #49: loss=0.017666955288009847
Epoch #50: loss=0.019761112401055856
Epoch #51: loss=0.024168682238823017
Epoch #52: loss=0.027986714380911028
Epoch #53: loss=0.02438442771879678
Epoch #54: loss=0.02230426371766618
Epoch #55: loss=0.020756881820551795
Epoch #56: loss=0.02687736676889472
Epoch #57: loss=0.02116044853023125
Epoch #58: loss=0.019338633425014587
Epoch #59: loss=0.02306871599994129
Epoch #60: loss=0.023583999355444044
Epoch #61: loss=0.021307040493595466
Epoch #62: loss=0.014786470718226856
Epoch #63: loss=0.015707504060273273
Epoch #64: loss=0.02823151347144432
Epoch #65: loss=0.01973205832399872
Epoch #66: loss=0.019552271246666558
Epoch #67: loss=0.016043687527517562
Epoch #68: loss=0.02764671978657134
Epoch #69: loss=0.01874168966308065
Epoch #70: loss=0.014600499668800882
Epoch #71: loss=0.017567713792876062
Epoch #72: loss=0.029435326677516915
Epoch #73: loss=0.019900262766536433
Epoch #74: loss=0.01881666513345199
Epoch #75: loss=0.019029891679364766
Epoch #76: loss=0.023295477832381176
Epoch #77: loss=0.018530246895642942
Epoch #78: loss=0.01753740208283677
Epoch #79: loss=0.015829207070155775
Epoch #80: loss=0.017185634632772853
Epoch #81: loss=0.025960961368503052
Epoch #82: loss=0.02107909145897479
Epoch #83: loss=0.023706157338650276
Epoch #84: loss=0.013742325230783933
Epoch #85: loss=0.020257668432762214
Epoch #86: loss=0.017092648119700474
Epoch #87: loss=0.018444934284619602
Epoch #88: loss=0.018391168912793725
Epoch #89: loss=0.016191063584356853
Epoch #90: loss=0.019546872774746548
Epoch #91: loss=0.017837247713910546
Epoch #92: loss=0.014793111516788988
Epoch #93: loss=0.017950947977678933
Epoch #94: loss=0.015213592437125379
Epoch #95: loss=0.02059819101808919
Epoch #96: loss=0.018586147372275003
Epoch #97: loss=0.016873253164237538
Epoch #98: loss=0.022275279454108135
Epoch #99: loss=0.01650943386095145
Epoch #100: loss=0.015915352412253277
Epoch #101: loss=0.01867478478702599
Epoch #102: loss=0.03022800603079952
Epoch #103: loss=0.014895364068229094
Epoch #104: loss=0.014264012371616749
Epoch #105: loss=0.014817288513569664
Epoch #106: loss=0.015532632283973453
Epoch #107: loss=0.01561934605679721
Epoch #108: loss=0.01429789636960526
Epoch #109: loss=0.015755864314235823
Epoch #110: loss=0.018000835806349473
Epoch #111: loss=0.022796817307478224
Epoch #112: loss=0.01410462398445983
Epoch #113: loss=0.01226537139258674
Epoch #114: loss=0.01590925189706593
Epoch #115: loss=0.013551282715644479
Epoch #116: loss=0.013884597603799027
Epoch #117: loss=0.020993903210258177
Epoch #118: loss=0.0161948957390837
Epoch #119: loss=0.020943020459133384
Epoch #120: loss=0.013846772845972318
Epoch #121: loss=0.01271845580573092
Epoch #122: loss=0.011785236901861893
Epoch #123: loss=0.015456045745425702
Epoch #124: loss=0.017941816898776036
Epoch #125: loss=0.016103600436686433
Epoch #126: loss=0.013245895400646026
Epoch #127: loss=0.01418315482562697
Epoch #128: loss=0.016327670826871032
Epoch #129: loss=0.01272257130038416
Epoch #130: loss=0.012011753501110957
Epoch #131: loss=0.01592880596092972
Epoch #132: loss=0.019669230873214788
Epoch #133: loss=0.011849468135045461
Epoch #134: loss=0.012375948690988616
Epoch #135: loss=0.01700078787666146
Epoch #136: loss=0.009865303655876243
Epoch #137: loss=0.018166065834081785
Epoch #138: loss=0.013538314216708678
Epoch #139: loss=0.012157791387189405
Epoch #140: loss=0.018540393833529963
Epoch #141: loss=0.01471351582966591
Epoch #142: loss=0.011015025552060461
Epoch #143: loss=0.0183788812744375
Epoch #144: loss=0.01608412181597267
Epoch #145: loss=0.01616517461972825
Epoch #146: loss=0.013691719890914686
Epoch #147: loss=0.014985430469581236
Epoch #148: loss=0.013804637155818068
Epoch #149: loss=0.01882772632985277
Epoch #150: loss=0.010776407409498827
Epoch #151: loss=0.016014492532958266
Epoch #152: loss=0.01413183116063679
Epoch #153: loss=0.011040762130986637
Epoch #154: loss=0.017550901928481688
Epoch #155: loss=0.015911707865046178
Epoch #156: loss=0.013499269446447738
Epoch #157: loss=0.016546993272247908
Epoch #158: loss=0.013480957824035666
Epoch #159: loss=0.01860235298738879
Epoch #160: loss=0.012840742369961185
Epoch #161: loss=0.01321777954780237
Epoch #162: loss=0.014338808000487832
Epoch #163: loss=0.012064107800813809
Epoch #164: loss=0.010304593997716438
Epoch #165: loss=0.0187995727111093
Epoch #166: loss=0.014495834566892966
Epoch #167: loss=0.01854146859289452
Epoch #168: loss=0.011218293124057845
Epoch #169: loss=0.0128589492015562
Epoch #170: loss=0.012541845234545167
Epoch #171: loss=0.014380118310359178
Epoch #172: loss=0.011780086224771575
Epoch #173: loss=0.012791393422080076
Epoch #174: loss=0.015680605899234805
Epoch #175: loss=0.01287908661552242
Epoch #176: loss=0.008911918445497661
Epoch #177: loss=0.012023127688066204
Epoch #178: loss=0.017249187650680863
Epoch #179: loss=0.016310472352946734
Epoch #180: loss=0.013707398292360173
Epoch #181: loss=0.015538716925831308
Epoch #182: loss=0.01378363369537207
Epoch #183: loss=0.01631102492747127
Epoch #184: loss=0.015007530943150686
Epoch #185: loss=0.014735664544130632
Epoch #186: loss=0.014350325410227871
Epoch #187: loss=0.01308721435402156
Epoch #188: loss=0.00930177062970985
Epoch #189: loss=0.01146145412000398
Epoch #190: loss=0.01137675901389167
Epoch #191: loss=0.014297319265904076
Epoch #192: loss=0.010707996241131466
Epoch #193: loss=0.009985778446495935
Epoch #194: loss=0.0176623636039928
Epoch #195: loss=0.011985522271069563
Epoch #196: loss=0.011883193296052506
Epoch #197: loss=0.012306037291281605
Epoch #198: loss=0.015873474844422957
Epoch #199: loss=0.015215087498922232
Epoch #200: loss=0.012967954908334382
Epoch #201: loss=0.011545572582038184
Epoch #202: loss=0.012958393212148235
Epoch #203: loss=0.012775930548026547
Epoch #204: loss=0.01626030402667834
Epoch #205: loss=0.01291692880812421
Epoch #206: loss=0.013622770819782236
Epoch #207: loss=0.016164144558327584
Epoch #208: loss=0.014407606311436828
Epoch #209: loss=0.008539129439538985
Epoch #210: loss=0.010903856979845956
Epoch #211: loss=0.016444474034782115
Epoch #212: loss=0.016796753812887727
Epoch #213: loss=0.010409284979411088
Epoch #214: loss=0.011709379318107359
Epoch #215: loss=0.02425313557035841
Epoch #216: loss=0.009209425599667533
Epoch #217: loss=0.012233614399808395
Epoch #218: loss=0.008518059438378175
Epoch #219: loss=0.024084070348983695
Epoch #220: loss=0.0108365107675273
Epoch #221: loss=0.010705257567206408
Epoch #222: loss=0.015126206749419939
Epoch #223: loss=0.012922321608418535
Epoch #224: loss=0.013240859549691307
Epoch #225: loss=0.007477811592665624
Epoch #226: loss=0.01318325848962935
Epoch #227: loss=0.008538722694147613
Epoch #228: loss=0.009677288107822875
Epoch #229: loss=0.015671163975246434
Epoch #230: loss=0.010948308972908945
Epoch #231: loss=0.01585188989043129
Epoch #232: loss=0.008613690973999482
Epoch #233: loss=0.011600430952634185
Epoch #234: loss=0.01378799007456862
Epoch #235: loss=0.013347654917910321
Epoch #236: loss=0.009511744478970349
Epoch #237: loss=0.010550155231913808
Epoch #238: loss=0.011798707150784588
Epoch #239: loss=0.011675911617529547
Epoch #240: loss=0.013027852330180561
Epoch #241: loss=0.011650006995595263
Epoch #242: loss=0.01109280650606675
Epoch #243: loss=0.008365844481593887
Epoch #244: loss=0.011206374193030548
Epoch #245: loss=0.014255683465015101
Epoch #246: loss=0.007209329948367166
Epoch #247: loss=0.015246584958493195
Epoch #248: loss=0.00949316445518638
Epoch #249: loss=0.012493042812325413

Training time: 10:28:35.863254

Finished.
n2one setting traffic_exchange -> electricity
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/traffic_exchange_epochs_250_seed_2023/model.pkl', muti_dataset='traffic_exchange', random_seed=2023, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='electricity')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.08052e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.33482e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=5.81261e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.08052e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.39692498573386076, 'MAE': 0.4462879109553708}
Finished.
------------------------- record done -------------------------
n2one setting traffic_exchange -> weather
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/traffic_exchange_epochs_250_seed_2023/model.pkl', muti_dataset='traffic_exchange', random_seed=2023, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='weather')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.29313e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.64302e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.29313e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.6825614969206453, 'MAE': 0.6391041518451325}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='weather_exchange', random_seed=2023, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/weather_exchange_epochs_250_seed_2023
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=7.239412896773395
Epoch #1: loss=2.9563688250149
Epoch #2: loss=2.5161509303485645
Epoch #3: loss=2.6658810166751636
Epoch #4: loss=2.2598821555866913
Epoch #5: loss=2.2187009383650387
Epoch #6: loss=2.016229675096624
Epoch #7: loss=1.8029566845473122
Epoch #8: loss=1.8028987085118013
Epoch #9: loss=1.8091299989644218
Epoch #10: loss=1.671142758692012
Epoch #11: loss=1.6755940072676714
Epoch #12: loss=1.5687162350205814
Epoch #13: loss=1.453278678305009
Epoch #14: loss=1.3523271399385788
Epoch #15: loss=1.2659851330168106
Epoch #16: loss=1.1938760666286243
Epoch #17: loss=1.1872433336342083
Epoch #18: loss=1.1438942656797522
Epoch #19: loss=1.1448607760317184
Epoch #20: loss=1.1164112985134125
Epoch #21: loss=1.1359138979631311
Epoch #22: loss=1.0449136425467098
Epoch #23: loss=1.028080501977135
Epoch #24: loss=1.0537675864556257
Epoch #25: loss=0.9210276673821842
Epoch #26: loss=0.920154902864905
Epoch #27: loss=0.9251040588406956
Epoch #28: loss=0.9137992701109718
Epoch #29: loss=0.835345711778192
Epoch #30: loss=0.9299620267222909
Epoch #31: loss=0.8590044554542092
Epoch #32: loss=0.8188829001258401
Epoch #33: loss=0.8832797285388497
Epoch #34: loss=0.9107436976012062
Epoch #35: loss=0.8414147583877339
Epoch #36: loss=0.9129312283852521
Epoch #37: loss=0.8389018703909481
Epoch #38: loss=0.7154877694214091
Epoch #39: loss=0.7898681865018957
Epoch #40: loss=0.6993216118391823
Epoch #41: loss=0.6817338221213397
Epoch #42: loss=0.6947282841976952
Epoch #43: loss=0.6518252071212319
Epoch #44: loss=0.6208271971520256
Epoch #45: loss=0.6715512959396138
Epoch #46: loss=0.606244733228403
Epoch #47: loss=0.5964717838694068
Epoch #48: loss=0.6158959918162402
Epoch #49: loss=0.5497221736346974
Epoch #50: loss=0.5756064434261883
Epoch #51: loss=0.6526459613267113
Epoch #52: loss=0.5928436850800234
Epoch #53: loss=0.5643770238932442
Epoch #54: loss=0.5758669814642738
Epoch #55: loss=0.6782964932567933
Epoch #56: loss=0.5576043409459731
Epoch #57: loss=0.5430673536132363
Epoch #58: loss=0.5079750392366859
Epoch #59: loss=0.5523319910554325
Epoch #60: loss=0.547271679429447
Epoch #61: loss=0.4894241883474238
Epoch #62: loss=0.516938228817547
Epoch #63: loss=0.5898215884671492
Epoch #64: loss=0.5790851177538142
Epoch #65: loss=0.47581728328676787
Epoch #66: loss=0.4229522702448508
Epoch #67: loss=0.4365184609504307
Epoch #68: loss=0.42990390868747935
Epoch #69: loss=0.49631476139321046
Epoch #70: loss=0.4695817595895599
Epoch #71: loss=0.5130392681149876
Epoch #72: loss=0.5257468315608361
Epoch #73: loss=0.39626755214789333
Epoch #74: loss=0.4217496014693204
Epoch #75: loss=0.4245448642793824
Epoch #76: loss=0.38399644152206536
Epoch #77: loss=0.47246453911066055
Epoch #78: loss=0.4935305096647319
Epoch #79: loss=0.5042981685084456
Epoch #80: loss=0.45271029630128073
Epoch #81: loss=0.39118313219617395
Epoch #82: loss=0.45235967592281456
Epoch #83: loss=0.35273350819068794
Epoch #84: loss=0.2897680748034926
Epoch #85: loss=0.3466915558366215
Epoch #86: loss=0.40034836136242924
Epoch #87: loss=0.3517458649242626
Epoch #88: loss=0.38263100210358114
Epoch #89: loss=0.3307509961373666
Epoch #90: loss=0.34837690477862077
Epoch #91: loss=0.3845333969768356
Epoch #92: loss=0.360510705586742
Epoch #93: loss=0.30229196040069356
Epoch #94: loss=0.2781541075776605
Epoch #95: loss=0.29287924135432525
Epoch #96: loss=0.252235284403843
Epoch #97: loss=0.34002694115042686
Epoch #98: loss=0.27078939996221485
Epoch #99: loss=0.34388412841979193
Epoch #100: loss=0.3076932469711584
Epoch #101: loss=0.2866261840304908
Epoch #102: loss=0.2594786126385717
Epoch #103: loss=0.3146362469038543
Epoch #104: loss=0.29421763648005095
Epoch #105: loss=0.3732120438533671
Epoch #106: loss=0.23466666327679858
Epoch #107: loss=0.28889430117081194
Epoch #108: loss=0.32955703205045533
Epoch #109: loss=0.4037155495846973
Epoch #110: loss=0.310652224237428
Epoch #111: loss=0.26475709523348245
Epoch #112: loss=0.26072339165736647
Epoch #113: loss=0.24680986364974694
Epoch #114: loss=0.2973797476905234
Epoch #115: loss=0.24782323661972494
Epoch #116: loss=0.22944050873903668
Epoch #117: loss=0.2520023389335941
Epoch #118: loss=0.2646657167112126
Epoch #119: loss=0.20775074358372128
Epoch #120: loss=0.32329664028742733
Epoch #121: loss=0.3192695313516785
Epoch #122: loss=0.26509959688958
Epoch #123: loss=0.18719155801569715
Epoch #124: loss=0.29445804984254
Epoch #125: loss=0.2447943586636992
Epoch #126: loss=0.2367752317120047
Epoch #127: loss=0.21405467259533265
Epoch #128: loss=0.27954760774531784
Epoch #129: loss=0.21791408583521843
Epoch #130: loss=0.15751245880828185
Epoch #131: loss=0.24366994717103593
Epoch #132: loss=0.1930343396961689
Epoch #133: loss=0.17682582600151792
Epoch #134: loss=0.20713103518766515
Epoch #135: loss=0.21338028625092087
Epoch #136: loss=0.2657001174109824
Epoch #137: loss=0.2570640350527623
Epoch #138: loss=0.1818593073198024
Epoch #139: loss=0.2217232771217823
Epoch #140: loss=0.186679635744761
Epoch #141: loss=0.17429817029658487
Epoch #142: loss=0.29344503954052925
Epoch #143: loss=0.20027357580907204
Epoch #144: loss=0.16844682932338295
Epoch #145: loss=0.17494927357663126
Epoch #146: loss=0.17119381313814835
Epoch #147: loss=0.18703942866448095
Epoch #148: loss=0.18185054302653847
Epoch #149: loss=0.13652353941955986
Epoch #150: loss=0.17489010742043748
Epoch #151: loss=0.13958939427838607
Epoch #152: loss=0.13147098998374798
Epoch #153: loss=0.17598937846281948
Epoch #154: loss=0.16753259706584847
Epoch #155: loss=0.18886729833834312
Epoch #156: loss=0.16026242504663327
Epoch #157: loss=0.16629963970797904
Epoch #158: loss=0.2526624190675862
Epoch #159: loss=0.21211023793062744
Epoch #160: loss=0.18451272411381497
Epoch #161: loss=0.12324537709355354
Epoch #162: loss=0.15714818978791728
Epoch #163: loss=0.1608209001886494
Epoch #164: loss=0.21243691433440237
Epoch #165: loss=0.20615152116207516
Epoch #166: loss=0.19917202204027595
Epoch #167: loss=0.2130211804281263
Epoch #168: loss=0.23662256427547512
Epoch #169: loss=0.20306920304017909
Epoch #170: loss=0.18098883254124837
Epoch #171: loss=0.18067661015426412
Epoch #172: loss=0.13215113496955702
Epoch #173: loss=0.1912928899640546
Epoch #174: loss=0.12806537385810823
Epoch #175: loss=0.15074405074119568
Epoch #176: loss=0.1642938212984625
Epoch #177: loss=0.13730737818952868
Epoch #178: loss=0.14534804860458655
Epoch #179: loss=0.12647935507052085
Epoch #180: loss=0.13819807176204288
Epoch #181: loss=0.10019054444616332
Epoch #182: loss=0.19059372550862677
Epoch #183: loss=0.11727930419147015
Epoch #184: loss=0.1087099708178464
Epoch #185: loss=0.1288455872632125
Epoch #186: loss=0.12275297797339804
Epoch #187: loss=0.0896455772111521
Epoch #188: loss=0.13661368607598193
Epoch #189: loss=0.19833998152000062
Epoch #190: loss=0.2367220906650319
Epoch #191: loss=0.17043436986996846
Epoch #192: loss=0.13325309534283244
Epoch #193: loss=0.12960804763304837
Epoch #194: loss=0.16759773946421988
Epoch #195: loss=0.16629943180391016
Epoch #196: loss=0.1185314332080238
Epoch #197: loss=0.1169340236879447
Epoch #198: loss=0.09852431851493962
Epoch #199: loss=0.0966461087741396
Epoch #200: loss=0.1632730168673922
Epoch #201: loss=0.14382090622230487
Epoch #202: loss=0.1255164973656921
Epoch #203: loss=0.12602185222375042
Epoch #204: loss=0.1473867160761181
Epoch #205: loss=0.14840608758523183
Epoch #206: loss=0.12191304170033511
Epoch #207: loss=0.15326463765300372
Epoch #208: loss=0.1386490842765745
Epoch #209: loss=0.09832562418544993
Epoch #210: loss=0.07838938308551031
Epoch #211: loss=0.15500061865895987
Epoch #212: loss=0.0845479996436659
Epoch #213: loss=0.09861977705184151
Epoch #214: loss=0.12208431838628124
Epoch #215: loss=0.07832257279797512
Epoch #216: loss=0.10402751505813178
Epoch #217: loss=0.09843069199910935
Epoch #218: loss=0.07923985541085987
Epoch #219: loss=0.14748886172824047
Epoch #220: loss=0.11800178666325177
Epoch #221: loss=0.1456851216361803
Epoch #222: loss=0.13017634198288708
Epoch #223: loss=0.11801014385898323
Epoch #224: loss=0.06973378184963674
Epoch #225: loss=0.09351850542075493
Epoch #226: loss=0.16076873308595488
Epoch #227: loss=0.14615428507985437
Epoch #228: loss=0.15231275158550808
Epoch #229: loss=0.13077325445106802
Epoch #230: loss=0.08235255952047951
Epoch #231: loss=0.11798513636869543
Epoch #232: loss=0.1619302531813874
Epoch #233: loss=0.08255039319834288
Epoch #234: loss=0.0815951195819413
Epoch #235: loss=0.11505639191497774
Epoch #236: loss=0.1356685327311211
Epoch #237: loss=0.15582100553985903
Epoch #238: loss=0.24141351517070742
Epoch #239: loss=0.15963202397174694
Epoch #240: loss=0.10166743830503787
Epoch #241: loss=0.09519362915307283
Epoch #242: loss=0.09094155878376435
Epoch #243: loss=0.06555237484109752
Epoch #244: loss=0.07210024616078418
Epoch #245: loss=0.05690567362505723
Epoch #246: loss=0.067473518607371
Epoch #247: loss=0.07512249688015264
Epoch #248: loss=0.08108796058770489
Epoch #249: loss=0.06509146370979793

Training time: 0:32:57.994845

Finished.
n2one setting weather_exchange -> electricity
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/weather_exchange_epochs_250_seed_2023/model.pkl', muti_dataset='weather_exchange', random_seed=2023, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='electricity')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.75255e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=3.57176e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.75255e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.3813700229410008, 'MAE': 0.42550701302104876}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='etth1_etth2', random_seed=2024, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/etth1_etth2_epochs_250_seed_2024
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=4.95864771093641
Epoch #1: loss=2.75612587588174
Epoch #2: loss=2.2043770807129994
Epoch #3: loss=2.0603668860026767
Epoch #4: loss=1.974581743989672
Epoch #5: loss=1.766412649835859
Epoch #6: loss=1.6997767175946916
Epoch #7: loss=1.5856041312217712
Epoch #8: loss=1.5184860825538635
Epoch #9: loss=1.3910782762936182
Epoch #10: loss=1.3022757428033012
Epoch #11: loss=1.2841890624591283
Epoch #12: loss=1.2424468653542655
Epoch #13: loss=1.162439967904772
Epoch #14: loss=1.0600204978670393
Epoch #15: loss=1.1387157993657249
Epoch #16: loss=1.1064020182405199
Epoch #17: loss=0.9965989461966923
Epoch #18: loss=0.9896442294120789
Epoch #19: loss=0.9204047109399524
Epoch #20: loss=0.9150404334068298
Epoch #21: loss=0.8972808165209634
Epoch #22: loss=0.8271681581224714
Epoch #23: loss=0.7550204055649894
Epoch #24: loss=0.7644776616777692
Epoch #25: loss=0.7265597837311881
Epoch #26: loss=0.7335290568215507
Epoch #27: loss=0.8508107023579734
Epoch #28: loss=0.7048883949007306
Epoch #29: loss=0.6274112335273198
Epoch #30: loss=0.6475114439214978
Epoch #31: loss=0.7506275985922132
Epoch #32: loss=0.6827853194304875
Epoch #33: loss=0.5986394158431462
Epoch #34: loss=0.6270178151982171
Epoch #35: loss=0.6019547241074699
Epoch #36: loss=0.7126050421169826
Epoch #37: loss=0.6055093088320324
Epoch #38: loss=0.5332740098237991
Epoch #39: loss=0.6339889892510006
Epoch #40: loss=0.5511571743658611
Epoch #41: loss=0.5602689917598452
Epoch #42: loss=0.6202706822327205
Epoch #43: loss=0.515317269733974
Epoch #44: loss=0.4146512619086674
Epoch #45: loss=0.5678223350218364
Epoch #46: loss=0.4997384399175644
Epoch #47: loss=0.4696810798985617
Epoch #48: loss=0.41154189833572935
Epoch #49: loss=0.4862815908023289
Epoch #50: loss=0.5454129363809314
Epoch #51: loss=0.5037226911102023
Epoch #52: loss=0.4514323558126177
Epoch #53: loss=0.5105493664741516
Epoch #54: loss=0.4073249569961003
Epoch #55: loss=0.4508964887687138
Epoch #56: loss=0.35249524669987814
Epoch #57: loss=0.38157314700739725
Epoch #58: loss=0.3693284903253828
Epoch #59: loss=0.48028937195028576
Epoch #60: loss=0.38221610869680134
Epoch #61: loss=0.4094878648008619
Epoch #62: loss=0.378738525722708
Epoch #63: loss=0.3856399187019893
Epoch #64: loss=0.37673515507153105
Epoch #65: loss=0.331287610743727
Epoch #66: loss=0.3207221861396517
Epoch #67: loss=0.28494224910225185
Epoch #68: loss=0.3065755122474262
Epoch #69: loss=0.2912517711520195
Epoch #70: loss=0.29738188641411917
Epoch #71: loss=0.3848277820008142
Epoch #72: loss=0.425986080297402
Epoch #73: loss=0.471542979989733
Epoch #74: loss=0.34790355605738504
Epoch #75: loss=0.3573740412081991
Epoch #76: loss=0.434259831905365
Epoch #77: loss=0.3632846793958119
Epoch #78: loss=0.33430627094847815
Epoch #79: loss=0.3261681869626045
Epoch #80: loss=0.29774272654737743
Epoch #81: loss=0.316936011825289
Epoch #82: loss=0.26005825826099943
Epoch #83: loss=0.4436956431184496
Epoch #84: loss=0.3465309111135347
Epoch #85: loss=0.3545713254383632
Epoch #86: loss=0.2762350482600076
Epoch #87: loss=0.2750551647373608
Epoch #88: loss=0.28825234941073824
Epoch #89: loss=0.3734681595649038
Epoch #90: loss=0.36362884300095694
Epoch #91: loss=0.2971790113619396
Epoch #92: loss=0.28949584705489023
Epoch #93: loss=0.3527154730898993
Epoch #94: loss=0.3180735068661826
Epoch #95: loss=0.30464059646640507
Epoch #96: loss=0.2970756430711065
Epoch #97: loss=0.31793008744716644
Epoch #98: loss=0.30769752711057663
Epoch #99: loss=0.2944777597274099
Epoch #100: loss=0.285370660679681
Epoch #101: loss=0.2763341377888407
Epoch #102: loss=0.22924425772258214
Epoch #103: loss=0.28672852580036434
Epoch #104: loss=0.2731405624321529
Epoch #105: loss=0.2727626881429127
Epoch #106: loss=0.25377037482602255
Epoch #107: loss=0.2710397190281323
Epoch #108: loss=0.24241618705647333
Epoch #109: loss=0.29554387820618494
Epoch #110: loss=0.3232436946460179
Epoch #111: loss=0.2873365730047226
Epoch #112: loss=0.3389917718512671
Epoch #113: loss=0.2836509293743542
Epoch #114: loss=0.24623371767146246
Epoch #115: loss=0.3704278607453619
Epoch #116: loss=0.2377395938549723
Epoch #117: loss=0.2599676677158901
Epoch #118: loss=0.23759773905788148
Epoch #119: loss=0.23893098533153534
Epoch #120: loss=0.22269925262246812
Epoch #121: loss=0.2045571729540825
Epoch #122: loss=0.2593528596418245
Epoch #123: loss=0.2857649496623448
Epoch #124: loss=0.29028145543166567
Epoch #125: loss=0.19321374595165253
Epoch #126: loss=0.2492240826998438
Epoch #127: loss=0.2426201498934201
Epoch #128: loss=0.3187604770064354
Epoch #129: loss=0.32958449104002546
Epoch #130: loss=0.30703274479934145
Epoch #131: loss=0.22354513832501002
Epoch #132: loss=0.2887821985142572
Epoch #133: loss=0.19611634846244538
Epoch #134: loss=0.25489950605801176
Epoch #135: loss=0.22650246109281266
Epoch #136: loss=0.2871233895421028
Epoch #137: loss=0.22701397431748255
Epoch #138: loss=0.24135440587997437
Epoch #139: loss=0.21654497406312398
Epoch #140: loss=0.20251675375870296
Epoch #141: loss=0.21787358075380325
Epoch #142: loss=0.24719480159027235
Epoch #143: loss=0.20705665754420416
Epoch #144: loss=0.19936256376760347
Epoch #145: loss=0.2361045777797699
Epoch #146: loss=0.2532683568341391
Epoch #147: loss=0.20567128487995692
Epoch #148: loss=0.21678170348916734
Epoch #149: loss=0.17468937379973276
Epoch #150: loss=0.1967748190675463
Epoch #151: loss=0.2943615402494158
Epoch #152: loss=0.26648272946476936
Epoch #153: loss=0.21029787404196604
Epoch #154: loss=0.27324287593364716
Epoch #155: loss=0.21226712316274643
Epoch #156: loss=0.25366041490009855
Epoch #157: loss=0.20796157951865876
Epoch #158: loss=0.19387819139020784
Epoch #159: loss=0.19371964090636798
Epoch #160: loss=0.19698194627250945
Epoch #161: loss=0.175736597606114
Epoch #162: loss=0.18948587668793543
Epoch #163: loss=0.2269526001598154
Epoch #164: loss=0.23347374158246176
Epoch #165: loss=0.20065678390009062
Epoch #166: loss=0.21240907747830665
Epoch #167: loss=0.20247236107076919
Epoch #168: loss=0.18123340447034156
Epoch #169: loss=0.207537051822458
Epoch #170: loss=0.18526762298175267
Epoch #171: loss=0.19213553358401572
Epoch #172: loss=0.18185178030814445
Epoch #173: loss=0.18966049062354223
Epoch #174: loss=0.22044518004570687
Epoch #175: loss=0.17893415316939354
Epoch #176: loss=0.17688641750386783
Epoch #177: loss=0.23867852560111455
Epoch #178: loss=0.17385084980300494
Epoch #179: loss=0.18902756114091193
Epoch #180: loss=0.13277042816792214
Epoch #181: loss=0.16913566791585513
Epoch #182: loss=0.19588073183383262
Epoch #183: loss=0.17638512328267097
Epoch #184: loss=0.19216097093054227
Epoch #185: loss=0.2200534279857363
Epoch #186: loss=0.1800722564969744
Epoch #187: loss=0.15580440099750245
Epoch #188: loss=0.18692354751484735
Epoch #189: loss=0.17415213638118335
Epoch #190: loss=0.13872481563261577
Epoch #191: loss=0.17836485004850797
Epoch #192: loss=0.1575290166905948
Epoch #193: loss=0.19251537801963942
Epoch #194: loss=0.17122073311890876
Epoch #195: loss=0.1481391383068902
Epoch #196: loss=0.17758780504976
Epoch #197: loss=0.21092928839581354
Epoch #198: loss=0.2203082756272384
Epoch #199: loss=0.1665817563022886
Epoch #200: loss=0.15819965675473213
Epoch #201: loss=0.14504750285829818
Epoch #202: loss=0.20819185727408954
Epoch #203: loss=0.16790493258408137
Epoch #204: loss=0.2091420968728406
Epoch #205: loss=0.1514668640281473
Epoch #206: loss=0.23601779554571425
Epoch #207: loss=0.20695930400065013
Epoch #208: loss=0.1868679496858801
Epoch #209: loss=0.15404919907450676
Epoch #210: loss=0.16837530263832637
Epoch #211: loss=0.1183956850852285
Epoch #212: loss=0.12694783828088216
Epoch #213: loss=0.10842570928590638
Epoch #214: loss=0.13637271310601914
Epoch #215: loss=0.11976130359939166
Epoch #216: loss=0.21804031091076986
Epoch #217: loss=0.20432379416057042
Epoch #218: loss=0.15687040771756852
Epoch #219: loss=0.14288025402597018
Epoch #220: loss=0.13215949067047664
Epoch #221: loss=0.16270894237927028
Epoch #222: loss=0.14513062579291208
Epoch #223: loss=0.188182921814067
Epoch #224: loss=0.16965485310980252
Epoch #225: loss=0.14949280555759156
Epoch #226: loss=0.151804153408323
Epoch #227: loss=0.16444166323968343
Epoch #228: loss=0.15518960409930774
Epoch #229: loss=0.14718449860811234
Epoch #230: loss=0.14967814460396767
Epoch #231: loss=0.10363416533385005
Epoch #232: loss=0.15506560302206449
Epoch #233: loss=0.11747850477695465
Epoch #234: loss=0.10418764555028506
Epoch #235: loss=0.13211057548012053
Epoch #236: loss=0.26050722652247976
Epoch #237: loss=0.15550132308687484
Epoch #238: loss=0.29352021110909327
Epoch #239: loss=0.16044097925935472
Epoch #240: loss=0.16733470665557043
Epoch #241: loss=0.34365285454051836
Epoch #242: loss=0.19861954769917897
Epoch #243: loss=0.181015350988933
Epoch #244: loss=0.19214192192469323
Epoch #245: loss=0.22928343127880776
Epoch #246: loss=0.2154631604041372
Epoch #247: loss=0.1620303818157741
Epoch #248: loss=0.1494190421487604
Epoch #249: loss=0.12101996849690165

Training time: 0:10:17.739285

Finished.
n2one setting etth1_etth2 -> electricity
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth1_etth2_epochs_250_seed_2024/model.pkl', muti_dataset='etth1_etth2', random_seed=2024, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='electricity')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=8.64183e-09): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.68851e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=4.19051e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=8.64183e-09): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.36000475597107895, 'MAE': 0.4231053336418882}
Finished.
------------------------- record done -------------------------
n2one setting etth1_etth2 -> weather
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth1_etth2_epochs_250_seed_2024/model.pkl', muti_dataset='etth1_etth2', random_seed=2024, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='weather')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.37304e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.64397e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.37304e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.6694544374953602, 'MAE': 0.6386410258616877}
Finished.
------------------------- record done -------------------------
n2one setting etth1_etth2 -> exchange
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth1_etth2_epochs_250_seed_2024/model.pkl', muti_dataset='etth1_etth2', random_seed=2024, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='exchange')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
Evaluation result: {'MSE': 0.18863055850862773, 'MAE': 0.3001526532976145}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='etth1_ettm1', random_seed=2024, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/etth1_ettm1_epochs_250_seed_2024
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=5.412467612160577
Epoch #1: loss=2.7648038069407144
Epoch #2: loss=2.463404907120599
Epoch #3: loss=2.18719791703754
Epoch #4: loss=2.0190831820170083
Epoch #5: loss=1.8952824274698894
Epoch #6: loss=1.751132779651218
Epoch #7: loss=1.7000507513682048
Epoch #8: loss=1.5066866013738844
Epoch #9: loss=1.467731191052331
Epoch #10: loss=1.4966502123408847
Epoch #11: loss=1.4356868200831943
Epoch #12: loss=1.2947101129425898
Epoch #13: loss=1.2690441012382507
Epoch #14: loss=1.3255552583270602
Epoch #15: loss=1.130094564623303
Epoch #16: loss=1.1948987709151373
Epoch #17: loss=1.1070416801505618
Epoch #18: loss=1.0616555743747287
Epoch #19: loss=1.0673533976078033
Epoch #20: loss=0.9934795929325951
Epoch #21: loss=1.0725053946177165
Epoch #22: loss=0.8927856816185845
Epoch #23: loss=0.9107917845249176
Epoch #24: loss=0.8731178012159135
Epoch #25: loss=0.8811243010891808
Epoch #26: loss=0.8198128475083245
Epoch #27: loss=0.8275879687733121
Epoch #28: loss=0.715046183930503
Epoch #29: loss=0.9334133234288957
Epoch #30: loss=0.8077940344810486
Epoch #31: loss=0.6987099448839823
Epoch #32: loss=0.7361414200729794
Epoch #33: loss=0.780770308441586
Epoch #34: loss=0.7308402591281467
Epoch #35: loss=0.6988406048880683
Epoch #36: loss=0.7016066544585757
Epoch #37: loss=0.692562546994951
Epoch #38: loss=0.6011683030260934
Epoch #39: loss=0.590626448392868
Epoch #40: loss=0.6520986838473214
Epoch #41: loss=0.6900188823541006
Epoch #42: loss=0.6043808692031436
Epoch #43: loss=0.6051066948307885
Epoch #44: loss=0.6627043568425708
Epoch #45: loss=0.6259051677253511
Epoch #46: loss=0.860023996896214
Epoch #47: loss=0.5993578450547324
Epoch #48: loss=0.6037126729885737
Epoch #49: loss=0.6435132010115517
Epoch #50: loss=0.64878877500693
Epoch #51: loss=0.5226717789967855
Epoch #52: loss=0.5225323620769713
Epoch #53: loss=0.5839423570368025
Epoch #54: loss=0.5090834663973914
Epoch #55: loss=0.49479593998856014
Epoch #56: loss=0.734641777144538
Epoch #57: loss=0.5100579228666093
Epoch #58: loss=0.5936614556445016
Epoch #59: loss=0.45486814280351
Epoch #60: loss=0.47016123433907825
Epoch #61: loss=0.4721725897656547
Epoch #62: loss=0.4285705288251241
Epoch #63: loss=0.44860081374645233
Epoch #64: loss=0.39090896646181744
Epoch #65: loss=0.5642208043071959
Epoch #66: loss=0.44286152058177525
Epoch #67: loss=0.5378122594621446
Epoch #68: loss=0.48741304046577877
Epoch #69: loss=0.47316037118434906
Epoch #70: loss=0.4704723507165909
Epoch #71: loss=0.4252442833450105
Epoch #72: loss=0.3957659850517909
Epoch #73: loss=0.36477356486850315
Epoch #74: loss=0.3794676760832469
Epoch #75: loss=0.3658165890309546
Epoch #76: loss=0.4195147487852309
Epoch #77: loss=0.4718625131580565
Epoch #78: loss=0.4274502694606781
Epoch #79: loss=0.3833211577600903
Epoch #80: loss=0.4234602351983388
Epoch #81: loss=0.33509400818083024
Epoch #82: loss=0.33176756815777886
Epoch #83: loss=0.39191736032565433
Epoch #84: loss=0.41751887318160796
Epoch #85: loss=0.37616100245051914
Epoch #86: loss=0.3497266256146961
Epoch #87: loss=0.3748058204849561
Epoch #88: loss=0.44924073417981464
Epoch #89: loss=0.4085709999005
Epoch #90: loss=0.30082984848154914
Epoch #91: loss=0.34262626038657296
Epoch #92: loss=0.2826523582140605
Epoch #93: loss=0.36177284518877667
Epoch #94: loss=0.3393091708421707
Epoch #95: loss=0.3390674773189757
Epoch #96: loss=0.3474196717143059
Epoch #97: loss=0.3029085679186715
Epoch #98: loss=0.3254131202896436
Epoch #99: loss=0.2977460308207406
Epoch #100: loss=0.328151179684533
Epoch #101: loss=0.2842014523016082
Epoch #102: loss=0.3098836963375409
Epoch #103: loss=0.36166229016251034
Epoch #104: loss=0.3829844387041198
Epoch #105: loss=0.5096448982755343
Epoch #106: loss=0.3607638205091159
Epoch #107: loss=0.3595646884706285
Epoch #108: loss=0.2925555167926682
Epoch #109: loss=0.2868756925066312
Epoch #110: loss=0.27308716624975204
Epoch #111: loss=0.2871408156222767
Epoch #112: loss=0.25669744114081067
Epoch #113: loss=0.233280877272288
Epoch #114: loss=0.2628273665904999
Epoch #115: loss=0.2807580969399876
Epoch #116: loss=0.31742294132709503
Epoch #117: loss=0.32989547153313953
Epoch #118: loss=0.3258134913113382
Epoch #119: loss=0.2638069772058063
Epoch #120: loss=0.24943041760060522
Epoch #121: loss=0.3346662513083882
Epoch #122: loss=0.24463890492916107
Epoch #123: loss=0.32386767615874607
Epoch #124: loss=0.3247308002577888
Epoch #125: loss=0.22430789801809523
Epoch #126: loss=0.2091094652811686
Epoch #127: loss=0.26750047836038804
Epoch #128: loss=0.22984137427475718
Epoch #129: loss=0.25258493464854026
Epoch #130: loss=0.24256925036509833
Epoch #131: loss=0.22871076895131004
Epoch #132: loss=0.21539786747760242
Epoch #133: loss=0.18119446229603556
Epoch #134: loss=0.33788641004098785
Epoch #135: loss=0.28787266545825535
Epoch #136: loss=0.23007503441638416
Epoch #137: loss=0.3120110341244274
Epoch #138: loss=0.27700072071618503
Epoch #139: loss=0.27281972020864487
Epoch #140: loss=0.25676993115080726
Epoch #141: loss=0.19763673717776933
Epoch #142: loss=0.2218572534620762
Epoch #143: loss=0.23163557632101905
Epoch #144: loss=0.21907180257969433
Epoch #145: loss=0.25403928260008496
Epoch #146: loss=0.23591063419977823
Epoch #147: loss=0.2196417036983702
Epoch #148: loss=0.25133268866274094
Epoch #149: loss=0.25073789556821185
Epoch #150: loss=0.22461824284659493
Epoch #151: loss=0.23522487241360876
Epoch #152: loss=0.19976420824726424
Epoch #153: loss=0.21237240607539812
Epoch #154: loss=0.22928887771235573
Epoch #155: loss=0.24613775478469002
Epoch #156: loss=0.3398201647731993
Epoch #157: loss=0.20698180670539537
Epoch #158: loss=0.19337009597155783
Epoch #159: loss=0.16612735307878918
Epoch #160: loss=0.20500806801848942
Epoch #161: loss=0.18414639929930368
Epoch #162: loss=0.14154847214619318
Epoch #163: loss=0.17578264926042822
Epoch #164: loss=0.26759428365363014
Epoch #165: loss=0.22242839344673687
Epoch #166: loss=0.2630098375181357
Epoch #167: loss=0.2706503706673781
Epoch #168: loss=0.15208878450923496
Epoch #169: loss=0.17905540805723932
Epoch #170: loss=0.20750103518366814
Epoch #171: loss=0.18283438144458664
Epoch #172: loss=0.18705936831732592
Epoch #173: loss=0.25965281824270886
Epoch #174: loss=0.24872956093814638
Epoch #175: loss=0.19540459269450772
Epoch #176: loss=0.1758003586696254
Epoch #177: loss=0.19245907643602955
Epoch #178: loss=0.14255936236845124
Epoch #179: loss=0.13473551099499068
Epoch #180: loss=0.1347026659382714
Epoch #181: loss=0.13733462492624918
Epoch #182: loss=0.1700711159242524
Epoch #183: loss=0.12755368442998993
Epoch #184: loss=0.13660430618458325
Epoch #185: loss=0.26883014664053917
Epoch #186: loss=0.16249455035560662
Epoch #187: loss=0.1370669116990434
Epoch #188: loss=0.13240678484241167
Epoch #189: loss=0.1427663585378064
Epoch #190: loss=0.14514014000693956
Epoch #191: loss=0.14362609303659862
Epoch #192: loss=0.12169003797074159
Epoch #193: loss=0.15497290301654074
Epoch #194: loss=0.14241251701282132
Epoch #195: loss=0.14471475469569364
Epoch #196: loss=0.13672948214742872
Epoch #197: loss=0.1263446362896098
Epoch #198: loss=0.14928307839565808
Epoch #199: loss=0.16454031794435447
Epoch #200: loss=0.12707483478718334
Epoch #201: loss=0.17399419885542658
Epoch #202: loss=0.15107171403037178
Epoch #203: loss=0.1296369381662872
Epoch #204: loss=0.14075254400571188
Epoch #205: loss=0.0973080268336667
Epoch #206: loss=0.15885295905172825
Epoch #207: loss=0.09633879611889522
Epoch #208: loss=0.18249534256756306
Epoch #209: loss=0.15632623020145628
Epoch #210: loss=0.15945792156789038
Epoch #211: loss=0.14476482259730497
Epoch #212: loss=0.15027701295912266
Epoch #213: loss=0.16685545899801785
Epoch #214: loss=0.1606414740284284
Epoch #215: loss=0.16552672824925846
Epoch #216: loss=0.13018091788722408
Epoch #217: loss=0.14739656945069632
Epoch #218: loss=0.11247069016098976
Epoch #219: loss=0.10146689456370142
Epoch #220: loss=0.15720104136400753
Epoch #221: loss=0.1535239066514704
Epoch #222: loss=0.13747197969092262
Epoch #223: loss=0.20251239753431743
Epoch #224: loss=0.16216487313310304
Epoch #225: loss=0.15116516769760185
Epoch #226: loss=0.09572152710623211
Epoch #227: loss=0.14770697998917764
Epoch #228: loss=0.1511464969565471
Epoch #229: loss=0.1663593246291081
Epoch #230: loss=0.13399785177575219
Epoch #231: loss=0.11002130289044645
Epoch #232: loss=0.09391146815485424
Epoch #233: loss=0.13910727782381904
Epoch #234: loss=0.09940269237591161
Epoch #235: loss=0.13174735258022943
Epoch #236: loss=0.1112085317985879
Epoch #237: loss=0.1344166871988111
Epoch #238: loss=0.09117068371011151
Epoch #239: loss=0.1634649590899547
Epoch #240: loss=0.08343823347240686
Epoch #241: loss=0.12318901717662811
Epoch #242: loss=0.17773300947414505
Epoch #243: loss=0.13042468991544512
Epoch #244: loss=0.1457931221359306
Epoch #245: loss=0.13211470966537794
Epoch #246: loss=0.09967349283397198
Epoch #247: loss=0.13197505474090576
Epoch #248: loss=0.1569866579439905
Epoch #249: loss=0.13179402922590575

Training time: 0:16:58.339557

Finished.
n2one setting etth1_ettm1 -> electricity
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth1_ettm1_epochs_250_seed_2024/model.pkl', muti_dataset='etth1_ettm1', random_seed=2024, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='electricity')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.17518e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.38109e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=5.86226e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.17518e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.3693869672672114, 'MAE': 0.42508390345147035}
Finished.
------------------------- record done -------------------------
n2one setting etth1_ettm1 -> weather
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth1_ettm1_epochs_250_seed_2024/model.pkl', muti_dataset='etth1_ettm1', random_seed=2024, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='weather')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.52741e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.91617e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.52741e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.8427382580481192, 'MAE': 0.7154774000055594}
Finished.
------------------------- record done -------------------------
n2one setting etth1_ettm1 -> exchange
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth1_ettm1_epochs_250_seed_2024/model.pkl', muti_dataset='etth1_ettm1', random_seed=2024, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='exchange')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=4.73063e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.29765402628524784, 'MAE': 0.36815849851727334}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='etth1_ettm2', random_seed=2024, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/etth1_ettm2_epochs_250_seed_2024
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=5.5964345004823475
Epoch #1: loss=2.8091491063435874
Epoch #2: loss=2.4899570875697665
Epoch #3: loss=2.2445838385158114
Epoch #4: loss=2.0846469865904913
Epoch #5: loss=1.9537273049354553
Epoch #6: loss=1.8134645819664001
Epoch #7: loss=1.7450781133439806
Epoch #8: loss=1.5372185044818454
Epoch #9: loss=1.5051696764098272
Epoch #10: loss=1.5269499156210158
Epoch #11: loss=1.4683634771241083
Epoch #12: loss=1.3311224182446797
Epoch #13: loss=1.2966335482067533
Epoch #14: loss=1.3477996521525912
Epoch #15: loss=1.1821778582202063
Epoch #16: loss=1.2116847435633342
Epoch #17: loss=1.116672224468655
Epoch #18: loss=1.105914877520667
Epoch #19: loss=1.1091689533657498
Epoch #20: loss=1.0207400189505682
Epoch #21: loss=1.075162000126309
Epoch #22: loss=0.8968741728199853
Epoch #23: loss=0.9239468342728085
Epoch #24: loss=0.8750221497482724
Epoch #25: loss=0.8347412910726335
Epoch #26: loss=0.7914068367746141
Epoch #27: loss=0.8429101606210073
Epoch #28: loss=0.7332547207673391
Epoch #29: loss=1.0003410081068675
Epoch #30: loss=0.8537178602483537
Epoch #31: loss=0.7399394975768195
Epoch #32: loss=0.8108421166737875
Epoch #33: loss=0.8078239692582024
Epoch #34: loss=0.7731369667583041
Epoch #35: loss=0.81113534172376
Epoch #36: loss=0.7547686960962083
Epoch #37: loss=0.7144069804085625
Epoch #38: loss=0.6208794828918245
Epoch #39: loss=0.6010828581121233
Epoch #40: loss=0.6379656427436404
Epoch #41: loss=0.7070584065384335
Epoch #42: loss=0.6012850602467855
Epoch #43: loss=0.5681323723660575
Epoch #44: loss=0.6345088730255762
Epoch #45: loss=0.601800435119205
Epoch #46: loss=0.8451248937182956
Epoch #47: loss=0.5931404332319895
Epoch #48: loss=0.5886715402205785
Epoch #49: loss=0.6574807415405909
Epoch #50: loss=0.7039241608646181
Epoch #51: loss=0.5538075847758187
Epoch #52: loss=0.5470015787416034
Epoch #53: loss=0.5984379450480143
Epoch #54: loss=0.5147740046183268
Epoch #55: loss=0.4678848468595081
Epoch #56: loss=0.7689063350359598
Epoch #57: loss=0.5051895446247525
Epoch #58: loss=0.5879643741581175
Epoch #59: loss=0.46404533253775704
Epoch #60: loss=0.4893142299519645
Epoch #61: loss=0.514998295240932
Epoch #62: loss=0.4604083134068383
Epoch #63: loss=0.46111974947982365
Epoch #64: loss=0.39880113965935177
Epoch #65: loss=0.5845574405458238
Epoch #66: loss=0.4626780715253618
Epoch #67: loss=0.5552055024438434
Epoch #68: loss=0.501981183886528
Epoch #69: loss=0.5180798553758197
Epoch #70: loss=0.4823891090022193
Epoch #71: loss=0.4340575502978431
Epoch #72: loss=0.40925199952390456
Epoch #73: loss=0.39283311035897994
Epoch #74: loss=0.41107309526867336
Epoch #75: loss=0.38020148542192245
Epoch #76: loss=0.4022055086162355
Epoch #77: loss=0.46587638225820327
Epoch #78: loss=0.4272993753353755
Epoch #79: loss=0.3811326192484962
Epoch #80: loss=0.4328126079506344
Epoch #81: loss=0.34125958383083344
Epoch #82: loss=0.35291151536835563
Epoch #83: loss=0.400400772690773
Epoch #84: loss=0.4244437747531467
Epoch #85: loss=0.38844483925236595
Epoch #86: loss=0.3549191844132211
Epoch #87: loss=0.39133241607083213
Epoch #88: loss=0.36618127922217053
Epoch #89: loss=0.42445989780955845
Epoch #90: loss=0.2923048535982768
Epoch #91: loss=0.3452935905920135
Epoch #92: loss=0.29813314725955326
Epoch #93: loss=0.4247427350944943
Epoch #94: loss=0.43323226521412533
Epoch #95: loss=0.4039057261413998
Epoch #96: loss=0.3735906183719635
Epoch #97: loss=0.3158690142962668
Epoch #98: loss=0.3285720224181811
Epoch #99: loss=0.33333370751804775
Epoch #100: loss=0.398130370510949
Epoch #101: loss=0.32686953412161934
Epoch #102: loss=0.29714688658714294
Epoch #103: loss=0.2810140285227034
Epoch #104: loss=0.33838976588514114
Epoch #105: loss=0.39848988586001927
Epoch #106: loss=0.31333719856209225
Epoch #107: loss=0.33982477751043105
Epoch #108: loss=0.2563541829586029
Epoch #109: loss=0.2819120271338357
Epoch #110: loss=0.26705118185944027
Epoch #111: loss=0.29460416899787056
Epoch #112: loss=0.23887542635202408
Epoch #113: loss=0.23113745037052366
Epoch #114: loss=0.2633899135722054
Epoch #115: loss=0.2917028135723538
Epoch #116: loss=0.28212699708011413
Epoch #117: loss=0.23672396110163796
Epoch #118: loss=0.2889588433835242
Epoch #119: loss=0.24570007705026203
Epoch #120: loss=0.2341599232620663
Epoch #121: loss=0.33314572109116447
Epoch #122: loss=0.2405475378036499
Epoch #123: loss=0.31967242600189316
Epoch #124: loss=0.30438607848352855
Epoch #125: loss=0.21916677388879988
Epoch #126: loss=0.1891320724454191
Epoch #127: loss=0.24577504355046484
Epoch #128: loss=0.20380871204866302
Epoch #129: loss=0.23331408864921993
Epoch #130: loss=0.27201316671239006
Epoch #131: loss=0.290615057779683
Epoch #132: loss=0.34102290206485325
Epoch #133: loss=0.2687559955649906
Epoch #134: loss=0.37284893376959694
Epoch #135: loss=0.2942418100105392
Epoch #136: loss=0.24306012565890947
Epoch #137: loss=0.32451225651635063
Epoch #138: loss=0.25562183807293576
Epoch #139: loss=0.27087535253829426
Epoch #140: loss=0.2490996312763956
Epoch #141: loss=0.1911777469019095
Epoch #142: loss=0.24253356953461966
Epoch #143: loss=0.26396630414658123
Epoch #144: loss=0.23151087595356834
Epoch #145: loss=0.2374205891456869
Epoch #146: loss=0.22648934771617255
Epoch #147: loss=0.19944217883878285
Epoch #148: loss=0.2276468724012375
Epoch #149: loss=0.22685333589712778
Epoch #150: loss=0.20963918955789673
Epoch #151: loss=0.21749774490793547
Epoch #152: loss=0.19079663190576765
Epoch #153: loss=0.21183429120315445
Epoch #154: loss=0.24381540384557512
Epoch #155: loss=0.2515656281676557
Epoch #156: loss=0.34046292884482277
Epoch #157: loss=0.20675551891326904
Epoch #158: loss=0.191529995865292
Epoch #159: loss=0.16312900744378567
Epoch #160: loss=0.21436276328232554
Epoch #161: loss=0.18361431939734352
Epoch #162: loss=0.15594479627907276
Epoch #163: loss=0.18585747687353027
Epoch #164: loss=0.250877914743291
Epoch #165: loss=0.19827880130873787
Epoch #166: loss=0.28338248158494633
Epoch #167: loss=0.2807465328110589
Epoch #168: loss=0.1315127369016409
Epoch #169: loss=0.16566111333668232
Epoch #170: loss=0.1543936294813951
Epoch #171: loss=0.15169258415699005
Epoch #172: loss=0.1940640782316526
Epoch #173: loss=0.2609099867857165
Epoch #174: loss=0.27812757674190736
Epoch #175: loss=0.21019775047898293
Epoch #176: loss=0.17519350080854362
Epoch #177: loss=0.19799301048947704
Epoch #178: loss=0.14888204551405376
Epoch #179: loss=0.13802356335024038
Epoch #180: loss=0.14901381068759495
Epoch #181: loss=0.1398250059121185
Epoch #182: loss=0.15660500153899193
Epoch #183: loss=0.1185327439258496
Epoch #184: loss=0.12291376065048906
Epoch #185: loss=0.2679972979757521
Epoch #186: loss=0.16236849894954097
Epoch #187: loss=0.13183615915477276
Epoch #188: loss=0.17126122034258312
Epoch #189: loss=0.18467921805050638
Epoch #190: loss=0.1717760759509272
Epoch #191: loss=0.16417900721232095
Epoch #192: loss=0.1416028949121634
Epoch #193: loss=0.15597043310602507
Epoch #194: loss=0.14943468881150088
Epoch #195: loss=0.14412824561198553
Epoch #196: loss=0.13821576452917522
Epoch #197: loss=0.1384725034650829
Epoch #198: loss=0.11963269166234466
Epoch #199: loss=0.15686225270231566
Epoch #200: loss=0.12000782063437833
Epoch #201: loss=0.16965964208874437
Epoch #202: loss=0.13863035270737278
Epoch #203: loss=0.13698059382538
Epoch #204: loss=0.1309305996530586
Epoch #205: loss=0.090681627082328
Epoch #206: loss=0.16002901312377718
Epoch #207: loss=0.10329431254002783
Epoch #208: loss=0.19459976773295137
Epoch #209: loss=0.17993228303061592
Epoch #210: loss=0.18086515222158697
Epoch #211: loss=0.14888359958098996
Epoch #212: loss=0.16882964099446932
Epoch #213: loss=0.20439050905406475
Epoch #214: loss=0.1999038284023603
Epoch #215: loss=0.18546695758899054
Epoch #216: loss=0.15888701586259735
Epoch #217: loss=0.14773783356779152
Epoch #218: loss=0.1122943510611852
Epoch #219: loss=0.0993333939048979
Epoch #220: loss=0.1328738590495454
Epoch #221: loss=0.13595912729700407
Epoch #222: loss=0.11544931369523208
Epoch #223: loss=0.12838447176747853
Epoch #224: loss=0.09534958067039649
Epoch #225: loss=0.11629095621820953
Epoch #226: loss=0.09561954769823286
Epoch #227: loss=0.12940352461818191
Epoch #228: loss=0.11639928196867307
Epoch #229: loss=0.15715865211354363
Epoch #230: loss=0.12195545041726695
Epoch #231: loss=0.10025566392060783
Epoch #232: loss=0.10596098367952639
Epoch #233: loss=0.13660621073924833
Epoch #234: loss=0.10537038350270854
Epoch #235: loss=0.1369089771889978
Epoch #236: loss=0.11135488086276585
Epoch #237: loss=0.16524710971862078
Epoch #238: loss=0.11262086913403538
Epoch #239: loss=0.15498915345718464
Epoch #240: loss=0.09283330974479516
Epoch #241: loss=0.15450607074631584
Epoch #242: loss=0.20389600843191147
Epoch #243: loss=0.15282888969199526
Epoch #244: loss=0.11884075972355074
Epoch #245: loss=0.11940199188474152
Epoch #246: loss=0.096341863895456
Epoch #247: loss=0.10195619302491347
Epoch #248: loss=0.14597486839112309
Epoch #249: loss=0.11457568365666601

Training time: 0:16:48.375684

Finished.
n2one setting etth1_ettm2 -> electricity
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth1_ettm2_epochs_250_seed_2024/model.pkl', muti_dataset='etth1_ettm2', random_seed=2024, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='electricity')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.19459e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.37875e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=5.82386e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.19459e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.4065068276407475, 'MAE': 0.4345859086981315}
Finished.
------------------------- record done -------------------------
n2one setting etth1_ettm2 -> weather
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth1_ettm2_epochs_250_seed_2024/model.pkl', muti_dataset='etth1_ettm2', random_seed=2024, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='weather')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.57498e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=3.14138e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.57498e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.5371983666392692, 'MAE': 0.5523129261583654}
Finished.
------------------------- record done -------------------------
n2one setting etth1_ettm2 -> exchange
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth1_ettm2_epochs_250_seed_2024/model.pkl', muti_dataset='etth1_ettm2', random_seed=2024, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='exchange')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=4.46236e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.280114482289188, 'MAE': 0.35885569186882615}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='etth1_electricity', random_seed=2024, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/etth1_electricity_epochs_250_seed_2024
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=1.649215858883974
Epoch #1: loss=0.6522319054276478
Epoch #2: loss=0.45244239043535256
Epoch #3: loss=0.33580431510217307
Epoch #4: loss=0.2885294007818873
Epoch #5: loss=0.2671785316452747
Epoch #6: loss=0.21346707053755115
Epoch #7: loss=0.19417543559357886
Epoch #8: loss=0.18264210519449012
Epoch #9: loss=0.16298552427622603
Epoch #10: loss=0.14732989183876935
Epoch #11: loss=0.1242202620885176
Epoch #12: loss=0.11445053168231757
Epoch #13: loss=0.10829683030364899
Epoch #14: loss=0.10714938988477536
Epoch #15: loss=0.08583393124522777
Epoch #16: loss=0.08951420817955784
Epoch #17: loss=0.07547278108824862
Epoch #18: loss=0.08865777621749879
Epoch #19: loss=0.06746533969928306
Epoch #20: loss=0.05681888049286676
Epoch #21: loss=0.0587209921625511
Epoch #22: loss=0.06319170027244382
Epoch #23: loss=0.04993407712488367
Epoch #24: loss=0.05669482808032005
Epoch #25: loss=0.05211650378002626
Epoch #26: loss=0.04483684003580271
Epoch #27: loss=0.051205051196852654
Epoch #28: loss=0.037354638297451524
Epoch #29: loss=0.04064503162900531
Epoch #30: loss=0.03921738449234243
Epoch #31: loss=0.04288606057880537
Epoch #32: loss=0.035381100743884085
Epoch #33: loss=0.032365562709434546
Epoch #34: loss=0.06052418343904542
Epoch #35: loss=0.05011503161612626
Epoch #36: loss=0.03873793817715856
Epoch #37: loss=0.02558608850174606
Epoch #38: loss=0.033416633492068776
Epoch #39: loss=0.03134093805863636
Epoch #40: loss=0.026721969016163225
Epoch #41: loss=0.05305155785754323
Epoch #42: loss=0.031389659203918334
Epoch #43: loss=0.025263646188782664
Epoch #44: loss=0.03787479800507199
Epoch #45: loss=0.029944212506695583
Epoch #46: loss=0.03716031664261231
Epoch #47: loss=0.022033940738055674
Epoch #48: loss=0.031093993804860498
Epoch #49: loss=0.023032327611340074
Epoch #50: loss=0.029895065914521495
Epoch #51: loss=0.021036722525534016
Epoch #52: loss=0.023725026327599884
Epoch #53: loss=0.016250997800564552
Epoch #54: loss=0.033961716497021656
Epoch #55: loss=0.03485546611027974
Epoch #56: loss=0.030943176652327542
Epoch #57: loss=0.023251891173791457
Epoch #58: loss=0.021535724212654025
Epoch #59: loss=0.028545080335589307
Epoch #60: loss=0.015301789225238125
Epoch #61: loss=0.022395106009598084
Epoch #62: loss=0.01838225473205434
Epoch #63: loss=0.025193783562494125
Epoch #64: loss=0.02695350362862483
Epoch #65: loss=0.016189763643324556
Epoch #66: loss=0.02613183151236338
Epoch #67: loss=0.026637128209652628
Epoch #68: loss=0.027382116487232697
Epoch #69: loss=0.020710125331633453
Epoch #70: loss=0.013433316091456019
Epoch #71: loss=0.015711915212195005
Epoch #72: loss=0.02013362005740552
Epoch #73: loss=0.026306368687428625
Epoch #74: loss=0.030568810661817422
Epoch #75: loss=0.016378423492580358
Epoch #76: loss=0.014383751582411095
Epoch #77: loss=0.017024912884197677
Epoch #78: loss=0.020893407124875498
Epoch #79: loss=0.017025602811439212
Epoch #80: loss=0.014185787678842444
Epoch #81: loss=0.014483714711542877
Epoch #82: loss=0.01671709933246503
Epoch #83: loss=0.019791748623502803
Epoch #84: loss=0.017700911913757077
Epoch #85: loss=0.028106821235269308
Epoch #86: loss=0.01675135385856975
Epoch #87: loss=0.014522462701900555
Epoch #88: loss=0.01576538456507345
Epoch #89: loss=0.015888765059083574
Epoch #90: loss=0.012782770530221326
Epoch #91: loss=0.014115869214903086
Epoch #92: loss=0.015722518035412265
Epoch #93: loss=0.04260378313093253
Epoch #94: loss=0.018693109170715438
Epoch #95: loss=0.012752438003833338
Epoch #96: loss=0.018825976352673024
Epoch #97: loss=0.015351147967477318
Epoch #98: loss=0.02059291885482863
Epoch #99: loss=0.016368288628788235
Epoch #100: loss=0.014567976271259463
Epoch #101: loss=0.011837767667978968
Epoch #102: loss=0.014758406894987097
Epoch #103: loss=0.013268165912183865
Epoch #104: loss=0.0117105957370524
Epoch #105: loss=0.010560907392972716
Epoch #106: loss=0.026936001202734698
Epoch #107: loss=0.011378335890540478
Epoch #108: loss=0.018407396497467546
Epoch #109: loss=0.011663699550653535
Epoch #110: loss=0.014428234421513213
Epoch #111: loss=0.01923069388856941
Epoch #112: loss=0.012090215885730619
Epoch #113: loss=0.012969211926868344
Epoch #114: loss=0.015729181286765308
Epoch #115: loss=0.0166287652654119
Epoch #116: loss=0.013886306933361594
Epoch #117: loss=0.010576570150300451
Epoch #118: loss=0.011951102778988481
Epoch #119: loss=0.014250076877410388
Epoch #120: loss=0.024687517732495397
Epoch #121: loss=0.016241713381931755
Epoch #122: loss=0.012669966180976367
Epoch #123: loss=0.00907030303003522
Epoch #124: loss=0.028895963031107207
Epoch #125: loss=0.019690782966435796
Epoch #126: loss=0.011462467239897063
Epoch #127: loss=0.012277054482671324
Epoch #128: loss=0.01166029213150401
Epoch #129: loss=0.01800137741289601
Epoch #130: loss=0.010157818943986147
Epoch #131: loss=0.008590507867643836
Epoch #132: loss=0.008181691666367242
Epoch #133: loss=0.009607416756696737
Epoch #134: loss=0.011463948250510418
Epoch #135: loss=0.01569197753056736
Epoch #136: loss=0.01509916914198207
Epoch #137: loss=0.022218909079284213
Epoch #138: loss=0.015187183794850859
Epoch #139: loss=0.010688233000997215
Epoch #140: loss=0.008218238088951826
Epoch #141: loss=0.008157324134982645
Epoch #142: loss=0.02463595446471761
Epoch #143: loss=0.009542663409885747
Epoch #144: loss=0.01899121441220348
Epoch #145: loss=0.022522581508205693
Epoch #146: loss=0.01418374053421913
Epoch #147: loss=0.009560034519647782
Epoch #148: loss=0.00688048929494959
Epoch #149: loss=0.018134204229792842
Epoch #150: loss=0.009521424870000352
Epoch #151: loss=0.008987279946024234
Epoch #152: loss=0.010665087853650642
Epoch #153: loss=0.011140906760396675
Epoch #154: loss=0.012244153575100027
Epoch #155: loss=0.018863710910742022
Epoch #156: loss=0.01818310879646494
Epoch #157: loss=0.012897612636507114
Epoch #158: loss=0.008203294118507851
Epoch #159: loss=0.008422157151227754
Epoch #160: loss=0.01195943807876419
Epoch #161: loss=0.013435978108978683
Epoch #162: loss=0.016548249176305877
Epoch #163: loss=0.008700375106311584
Epoch #164: loss=0.009039258385799746
Epoch #165: loss=0.01228288465009849
Epoch #166: loss=0.010607599998964849
Epoch #167: loss=0.00837438987227648
Epoch #168: loss=0.007636173987365754
Epoch #169: loss=0.01380318142693751
Epoch #170: loss=0.010802535297244037
Epoch #171: loss=0.007863511400057355
Epoch #172: loss=0.008833891404010058
Epoch #173: loss=0.015933901236499744
Epoch #174: loss=0.013642901731367512
Epoch #175: loss=0.012200809728244898
Epoch #176: loss=0.010671235553007403
Epoch #177: loss=0.008783033338060216
Epoch #178: loss=0.011044488441346003
Epoch #179: loss=0.0060319601744129436
Epoch #180: loss=0.01564480920560749
Epoch #181: loss=0.014176689799258872
Epoch #182: loss=0.009189618375896273
Epoch #183: loss=0.01294677908920609
Epoch #184: loss=0.019642172088468926
Epoch #185: loss=0.01568524791680208
Epoch #186: loss=0.010251259038341232
Epoch #187: loss=0.008350931062567525
Epoch #188: loss=0.009145321107519899
Epoch #189: loss=0.007492820107805395
Epoch #190: loss=0.005956581843148763
Epoch #191: loss=0.011434680457502353
Epoch #192: loss=0.009562626539441337
Epoch #193: loss=0.012958988824077717
Epoch #194: loss=0.010668223328383004
Epoch #195: loss=0.007915206152375037
Epoch #196: loss=0.026795493276816757
Epoch #197: loss=0.014072739653340428
Epoch #198: loss=0.010687684036916258
Epoch #199: loss=0.009737629016610032
Epoch #200: loss=0.00558707753371869
Epoch #201: loss=0.00950501619179083
Epoch #202: loss=0.004580551899136215
Epoch #203: loss=0.00996195718515792
Epoch #204: loss=0.012718115670595137
Epoch #205: loss=0.011327699643028077
Epoch #206: loss=0.012016430210035362
Epoch #207: loss=0.010624198368842793
Epoch #208: loss=0.013645955977042875
Epoch #209: loss=0.007925452337076422
Epoch #210: loss=0.008717550111546965
Epoch #211: loss=0.010966923425989879
Epoch #212: loss=0.008771482841594418
Epoch #213: loss=0.008049088489426096
Epoch #214: loss=0.009842149321740159
Epoch #215: loss=0.007691195470903129
Epoch #216: loss=0.00561974496455017
Epoch #217: loss=0.019498529596514744
Epoch #218: loss=0.009164859518045333
Epoch #219: loss=0.0074087241735070145
Epoch #220: loss=0.006343606178433685
Epoch #221: loss=0.008816708585693892
Epoch #222: loss=0.012152190045659464
Epoch #223: loss=0.008199529839369394
Epoch #224: loss=0.008872609348167672
Epoch #225: loss=0.008641598467209672
Epoch #226: loss=0.0071406846774795954
Epoch #227: loss=0.01321837988720423
Epoch #228: loss=0.007602045342426253
Epoch #229: loss=0.00907229665981282
Epoch #230: loss=0.009597898108839261
Epoch #231: loss=0.012832853312585673
Epoch #232: loss=0.00913619512707888
Epoch #233: loss=0.009279724601893998
Epoch #234: loss=0.01481698912202108
Epoch #235: loss=0.007689199874975446
Epoch #236: loss=0.01585454353441558
Epoch #237: loss=0.006316231282231989
Epoch #238: loss=0.01322697615907556
Epoch #239: loss=0.004102832327478653
Epoch #240: loss=0.0069149483814526765
Epoch #241: loss=0.008353985629871271
Epoch #242: loss=0.015474994127033846
Epoch #243: loss=0.010434302300936685
Epoch #244: loss=0.007468206239825188
Epoch #245: loss=0.01385002703158889
Epoch #246: loss=0.00716525519033539
Epoch #247: loss=0.010446637812803113
Epoch #248: loss=0.00657606019651611
Epoch #249: loss=0.005183371050029897

Training time: 4:30:01.009032

Finished.
n2one setting etth1_electricity -> weather
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth1_electricity_epochs_250_seed_2024/model.pkl', muti_dataset='etth1_electricity', random_seed=2024, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='weather')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.15296e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.296e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=5.66987e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.15296e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.6270614465602695, 'MAE': 0.5968266285493095}
Finished.
------------------------- record done -------------------------
n2one setting etth1_electricity -> exchange
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth1_electricity_epochs_250_seed_2024/model.pkl', muti_dataset='etth1_electricity', random_seed=2024, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='exchange')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=4.82168e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=4.82168e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.27826453876892143, 'MAE': 0.3574414431830388}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='etth1_traffic', random_seed=2024, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/etth1_traffic_epochs_250_seed_2024
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=1.0356604708969523
Epoch #1: loss=0.3474818819788712
Epoch #2: loss=0.25212823965675674
Epoch #3: loss=0.19884718801950557
Epoch #4: loss=0.14793889704658741
Epoch #5: loss=0.11790481406736635
Epoch #6: loss=0.09866271934271613
Epoch #7: loss=0.09218875946899312
Epoch #8: loss=0.07936425211917614
Epoch #9: loss=0.06856318805216714
Epoch #10: loss=0.0631249827065937
Epoch #11: loss=0.06298374037673082
Epoch #12: loss=0.04893925101405183
Epoch #13: loss=0.05668518543473827
Epoch #14: loss=0.04031426622184
Epoch #15: loss=0.04106799122058627
Epoch #16: loss=0.05444871179809873
Epoch #17: loss=0.04306426619086307
Epoch #18: loss=0.03101071820209088
Epoch #19: loss=0.03352541662913386
Epoch #20: loss=0.03519369136130542
Epoch #21: loss=0.030555614503502408
Epoch #22: loss=0.03228214925133074
Epoch #23: loss=0.02825562205107752
Epoch #24: loss=0.029984553929437618
Epoch #25: loss=0.03458698410355948
Epoch #26: loss=0.025963300191883053
Epoch #27: loss=0.02493443296913066
Epoch #28: loss=0.027153848569011756
Epoch #29: loss=0.02434270365966936
Epoch #30: loss=0.025155108036356576
Epoch #31: loss=0.025767645587093307
Epoch #32: loss=0.022803620699475873
Epoch #33: loss=0.021166062240461497
Epoch #34: loss=0.027221798313105366
Epoch #35: loss=0.027215717738894564
Epoch #36: loss=0.02222948014821645
Epoch #37: loss=0.015239917475137253
Epoch #38: loss=0.027275097439059145
Epoch #39: loss=0.03233512364699258
Epoch #40: loss=0.025081042939678083
Epoch #41: loss=0.020344454898810822
Epoch #42: loss=0.021880509835261727
Epoch #43: loss=0.014797292780428503
Epoch #44: loss=0.029321665718272154
Epoch #45: loss=0.015126064591355176
Epoch #46: loss=0.01617904875047159
Epoch #47: loss=0.023575239256988907
Epoch #48: loss=0.021248622103090538
Epoch #49: loss=0.02980619183311751
Epoch #50: loss=0.022117764808390807
Epoch #51: loss=0.016862247270634838
Epoch #52: loss=0.01592040555426391
Epoch #53: loss=0.013375261238997457
Epoch #54: loss=0.025562680928305572
Epoch #55: loss=0.023119287215501726
Epoch #56: loss=0.014029264718739443
Epoch #57: loss=0.01729839558429669
Epoch #58: loss=0.022686174567196717
Epoch #59: loss=0.012372328884528803
Epoch #60: loss=0.014501062968134078
Epoch #61: loss=0.01995163244537964
Epoch #62: loss=0.023926303971071712
Epoch #63: loss=0.013768816632242772
Epoch #64: loss=0.01666753261430608
Epoch #65: loss=0.01627148511665884
Epoch #66: loss=0.020708621226846465
Epoch #67: loss=0.03364757930774722
Epoch #68: loss=0.015544731974330065
Epoch #69: loss=0.019172957810863714
Epoch #70: loss=0.021002472563787664
Epoch #71: loss=0.01138591664343305
Epoch #72: loss=0.015327830107565658
Epoch #73: loss=0.01610557450445356
Epoch #74: loss=0.013200057822356528
Epoch #75: loss=0.015478350627353957
Epoch #76: loss=0.015673298104848365
Epoch #77: loss=0.01702861608123882
Epoch #78: loss=0.012908137295586973
Epoch #79: loss=0.021530823637605445
Epoch #80: loss=0.012425848139564896
Epoch #81: loss=0.015195453390521956
Epoch #82: loss=0.013481914842170657
Epoch #83: loss=0.01811211718708787
Epoch #84: loss=0.012291555328003127
Epoch #85: loss=0.01474668842288092
Epoch #86: loss=0.014935643147056659
Epoch #87: loss=0.01616466833287144
Epoch #88: loss=0.027821336128098957
Epoch #89: loss=0.01635042306040312
Epoch #90: loss=0.013635860294263308
Epoch #91: loss=0.01498695211121984
Epoch #92: loss=0.012664536797959718
Epoch #93: loss=0.011167089525914711
Epoch #94: loss=0.016041549042857792
Epoch #95: loss=0.01775812186108917
Epoch #96: loss=0.011624190260441031
Epoch #97: loss=0.01347555953210396
Epoch #98: loss=0.040277094135939176
Epoch #99: loss=0.015600558025057784
Epoch #100: loss=0.015216199006851531
Epoch #101: loss=0.013005007736712279
Epoch #102: loss=0.012877819991033681
Epoch #103: loss=0.016713270312818187
Epoch #104: loss=0.013339068512163009
Epoch #105: loss=0.01490701707389465
Epoch #106: loss=0.011552072536626025
Epoch #107: loss=0.010728049698486437
Epoch #108: loss=0.015706080109097617
Epoch #109: loss=0.008151844952863554
Epoch #110: loss=0.015853266262477322
Epoch #111: loss=0.015954438446622
Epoch #112: loss=0.011008382274397361
Epoch #113: loss=0.015784886187388294
Epoch #114: loss=0.011907560410035906
Epoch #115: loss=0.01458729525307441
Epoch #116: loss=0.015998511426863492
Epoch #117: loss=0.01338356740777279
Epoch #118: loss=0.009508063141441443
Epoch #119: loss=0.023623591927969315
Epoch #120: loss=0.011141006164378684
Epoch #121: loss=0.010666388981750999
Epoch #122: loss=0.017161142534224832
Epoch #123: loss=0.012812597212254126
Epoch #124: loss=0.012296483065127593
Epoch #125: loss=0.012192376666530252
Epoch #126: loss=0.011630059605279736
Epoch #127: loss=0.011339826543450515
Epoch #128: loss=0.010442822050624133
Epoch #129: loss=0.009575324636021535
Epoch #130: loss=0.014361647161003658
Epoch #131: loss=0.012142718307699652
Epoch #132: loss=0.01187047279771831
Epoch #133: loss=0.015132261224206798
Epoch #134: loss=0.010371061830492505
Epoch #135: loss=0.01129684426545935
Epoch #136: loss=0.011744441475193537
Epoch #137: loss=0.013316197073051194
Epoch #138: loss=0.011200480382899959
Epoch #139: loss=0.01445783348740652
Epoch #140: loss=0.01354155612686007
Epoch #141: loss=0.010867926583092732
Epoch #142: loss=0.011899145012474924
Epoch #143: loss=0.013804781193101068
Epoch #144: loss=0.012777574395067492
Epoch #145: loss=0.00877658669484559
Epoch #146: loss=0.012075256225218529
Epoch #147: loss=0.01200327764903782
Epoch #148: loss=0.01039572622080042
Epoch #149: loss=0.00989992538351438
Epoch #150: loss=0.010974442786797671
Epoch #151: loss=0.010139048101527293
Epoch #152: loss=0.014109324488768882
Epoch #153: loss=0.008073112823169436
Epoch #154: loss=0.00853833185202181
Epoch #155: loss=0.0121177151668151
Epoch #156: loss=0.008970620005803613
Epoch #157: loss=0.01004726277689573
Epoch #158: loss=0.014123398622438168
Epoch #159: loss=0.010866585207641687
Epoch #160: loss=0.010577642457722204
Epoch #161: loss=0.011170818059548728
Epoch #162: loss=0.01025469161418558
Epoch #163: loss=0.008753984014428168
Epoch #164: loss=0.015967942458507526
Epoch #165: loss=0.014838137000574654
Epoch #166: loss=0.009015789284235267
Epoch #167: loss=0.008371954394110705
Epoch #168: loss=0.012429367515624349
Epoch #169: loss=0.01380685883003766
Epoch #170: loss=0.008989617233640518
Epoch #171: loss=0.010323250943149002
Epoch #172: loss=0.007963994830537394
Epoch #173: loss=0.010113934062137852
Epoch #174: loss=0.012465125566368663
Epoch #175: loss=0.010237689328912305
Epoch #176: loss=0.009657735432937829
Epoch #177: loss=0.012697183185643371
Epoch #178: loss=0.011457433006805915
Epoch #179: loss=0.010329251654468866
Epoch #180: loss=0.03094795185093842
Epoch #181: loss=0.010335700630579547
Epoch #182: loss=0.011776526844245783
Epoch #183: loss=0.008304200135866356
Epoch #184: loss=0.0059813372176836495
Epoch #185: loss=0.010812494363272064
Epoch #186: loss=0.013541355545186306
Epoch #187: loss=0.006898974816756156
Epoch #188: loss=0.007248177487358416
Epoch #189: loss=0.012366585127658467
Epoch #190: loss=0.010928561400882825
Epoch #191: loss=0.010435464463183265
Epoch #192: loss=0.00906305537221233
Epoch #193: loss=0.0151928757568615
Epoch #194: loss=0.007761628150863112
Epoch #195: loss=0.009635508233060463
Epoch #196: loss=0.009553687219290199
Epoch #197: loss=0.006005425132027778
Epoch #198: loss=0.01557306494870999
Epoch #199: loss=0.011944640074680149
Epoch #200: loss=0.011434925921930196
Epoch #201: loss=0.009353147494221257
Epoch #202: loss=0.00955834337850439
Epoch #203: loss=0.007769929543704891
Epoch #204: loss=0.0165992353580181
Epoch #205: loss=0.010441236187482463
Epoch #206: loss=0.008594783717154587
Epoch #207: loss=0.009142925788777234
Epoch #208: loss=0.008339349009135874
Epoch #209: loss=0.008751813728503387
Epoch #210: loss=0.009317931125158342
Epoch #211: loss=0.009493166661091332
Epoch #212: loss=0.011823114048302713
Epoch #213: loss=0.016081292385465046
Epoch #214: loss=0.009733696309191853
Epoch #215: loss=0.008305676316684895
Epoch #216: loss=0.0063013059845806706
Epoch #217: loss=0.006960098831217812
Epoch #218: loss=0.010187471036288042
Epoch #219: loss=0.016831582834380406
Epoch #220: loss=0.02199410756971415
Epoch #221: loss=0.005947917596376351
Epoch #222: loss=0.006502874335016218
Epoch #223: loss=0.0238079923448458
Epoch #224: loss=0.010284545297332009
Epoch #225: loss=0.015320210867642181
Epoch #226: loss=0.01164591479187797
Epoch #227: loss=0.01281139353074249
Epoch #228: loss=0.011401079664281725
Epoch #229: loss=0.008655949365519099
Epoch #230: loss=0.009755993469130662
Epoch #231: loss=0.012313026522571652
Epoch #232: loss=0.012176619152579605
Epoch #233: loss=0.006926523546359509
Epoch #234: loss=0.008773290043091174
Epoch #235: loss=0.008797530980134475
Epoch #236: loss=0.011659717948506324
Epoch #237: loss=0.011267436001166087
Epoch #238: loss=0.006274732287469592
Epoch #239: loss=0.010630629312619995
Epoch #240: loss=0.01134462739063721
Epoch #241: loss=0.008511558725254867
Epoch #242: loss=0.006834392912578834
Epoch #243: loss=0.007821535424057929
Epoch #244: loss=0.009553448817971611
Epoch #245: loss=0.01082006639018478
Epoch #246: loss=0.00960936017868277
Epoch #247: loss=0.007922801191770386
Epoch #248: loss=0.012739032382555481
Epoch #249: loss=0.007036181017665366

Training time: 10:32:29.885073

Finished.
n2one setting etth1_traffic -> electricity
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth1_traffic_epochs_250_seed_2024/model.pkl', muti_dataset='etth1_traffic', random_seed=2024, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='electricity')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=9.12652e-09): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.144e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=5.25536e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=9.12652e-09): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.41482386280392436, 'MAE': 0.45936632639392816}
Finished.
------------------------- record done -------------------------
n2one setting etth1_traffic -> weather
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth1_traffic_epochs_250_seed_2024/model.pkl', muti_dataset='etth1_traffic', random_seed=2024, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='weather')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.24036e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.26669e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=5.8302e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.8294158937657781, 'MAE': 0.7893124783705318}
Finished.
------------------------- record done -------------------------
n2one setting etth1_traffic -> exchange
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth1_traffic_epochs_250_seed_2024/model.pkl', muti_dataset='etth1_traffic', random_seed=2024, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='exchange')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=3.54825e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.22529769353460072, 'MAE': 0.3235487023539953}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='etth1_weather', random_seed=2024, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/etth1_weather_epochs_250_seed_2024
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=9.430077639493076
Epoch #1: loss=3.8076510140390107
Epoch #2: loss=2.702462832132975
Epoch #3: loss=2.7483939402031177
Epoch #4: loss=2.3894008217435894
Epoch #5: loss=2.3204521988377427
Epoch #6: loss=2.096181757522352
Epoch #7: loss=2.072389432878205
Epoch #8: loss=1.9474087194962935
Epoch #9: loss=1.8326959790605488
Epoch #10: loss=1.8183883934309988
Epoch #11: loss=1.6790651480356853
Epoch #12: loss=1.6189157131946448
Epoch #13: loss=1.5533389539429636
Epoch #14: loss=1.4505326133785825
Epoch #15: loss=1.2968642928383567
Epoch #16: loss=1.287146604422367
Epoch #17: loss=1.21753538016117
Epoch #18: loss=1.2203413826046567
Epoch #19: loss=1.0906995715516987
Epoch #20: loss=1.1509181134628528
Epoch #21: loss=1.2666642665863037
Epoch #22: loss=1.125700258847439
Epoch #23: loss=1.0331797310800264
Epoch #24: loss=0.9546773614305438
Epoch #25: loss=1.0877870538017966
Epoch #26: loss=1.0272632963729627
Epoch #27: loss=0.9063159516363433
Epoch #28: loss=0.9688792987303301
Epoch #29: loss=0.9341205755869547
Epoch #30: loss=0.8784536014903676
Epoch #31: loss=0.8495370474728671
Epoch #32: loss=0.7736780715711189
Epoch #33: loss=0.7372876152847753
Epoch #34: loss=0.7883020892287745
Epoch #35: loss=0.7284214677232684
Epoch #36: loss=0.6995435068101594
Epoch #37: loss=0.7426399949825171
Epoch #38: loss=0.7578952402779551
Epoch #39: loss=0.6829974542964589
Epoch #40: loss=0.7323879581509214
Epoch #41: loss=0.7139412878137646
Epoch #42: loss=0.7366640730337664
Epoch #43: loss=0.736501749717828
Epoch #44: loss=0.6112797883423892
Epoch #45: loss=0.7802123093243801
Epoch #46: loss=0.7524391322424917
Epoch #47: loss=0.7494890572446765
Epoch #48: loss=0.7562226887905237
Epoch #49: loss=0.5610560847051216
Epoch #50: loss=0.5810394847031796
Epoch #51: loss=0.5904665670611642
Epoch #52: loss=0.6352864612232555
Epoch #53: loss=0.528959901043863
Epoch #54: loss=0.5314543301408942
Epoch #55: loss=0.613339749249545
Epoch #56: loss=0.6207361112941395
Epoch #57: loss=0.6322282769463279
Epoch #58: loss=0.6166848341623942
Epoch #59: loss=0.6301277036016638
Epoch #60: loss=0.6744178623864145
Epoch #61: loss=0.5528174412972999
Epoch #62: loss=0.4414784267093196
Epoch #63: loss=0.445576857436787
Epoch #64: loss=0.4481015042825179
Epoch #65: loss=0.4250981392282428
Epoch #66: loss=0.4681990923303546
Epoch #67: loss=0.4500878119107449
Epoch #68: loss=0.43815346017028345
Epoch #69: loss=0.38104135565685504
Epoch #70: loss=0.4793812582890193
Epoch #71: loss=0.4411505704576319
Epoch #72: loss=0.36581076049443445
Epoch #73: loss=0.4665458717129447
Epoch #74: loss=0.4168918200514533
Epoch #75: loss=0.49661639152151166
Epoch #76: loss=0.43660466598741937
Epoch #77: loss=0.33959165983127826
Epoch #78: loss=0.42680674836491095
Epoch #79: loss=0.379287474083178
Epoch #80: loss=0.35233019292354584
Epoch #81: loss=0.40305171952103125
Epoch #82: loss=0.34132057067119714
Epoch #83: loss=0.35566283672144916
Epoch #84: loss=0.3319248055869883
Epoch #85: loss=0.3150813692446911
Epoch #86: loss=0.3692307404496453
Epoch #87: loss=0.32541788482304773
Epoch #88: loss=0.3872755802038944
Epoch #89: loss=0.4262408681891181
Epoch #90: loss=0.4022490540237138
Epoch #91: loss=0.37625449986168835
Epoch #92: loss=0.3021783022717996
Epoch #93: loss=0.3196171422799428
Epoch #94: loss=0.30066764083775604
Epoch #95: loss=0.3183118791291208
Epoch #96: loss=0.25745392884268903
Epoch #97: loss=0.30506322690934845
Epoch #98: loss=0.30130680763360224
Epoch #99: loss=0.3049473123568477
Epoch #100: loss=0.2523645083561088
Epoch #101: loss=0.30536873977292667
Epoch #102: loss=0.2865151573311199
Epoch #103: loss=0.2703730028235551
Epoch #104: loss=0.2607123689218001
Epoch #105: loss=0.2368924841284752
Epoch #106: loss=0.25841225841731735
Epoch #107: loss=0.2517552414175236
Epoch #108: loss=0.23815239085392517
Epoch #109: loss=0.3505657163984848
Epoch #110: loss=0.2154494056647474
Epoch #111: loss=0.22235462521061752
Epoch #112: loss=0.21092835181590283
Epoch #113: loss=0.25742894049846765
Epoch #114: loss=0.3019314126083345
Epoch #115: loss=0.21710724451325156
Epoch #116: loss=0.23794705533620084
Epoch #117: loss=0.21365782212127338
Epoch #118: loss=0.1620150450052637
Epoch #119: loss=0.19939476838617615
Epoch #120: loss=0.2094000606148532
Epoch #121: loss=0.20038215138695456
Epoch #122: loss=0.15151367257490303
Epoch #123: loss=0.15702872079881755
Epoch #124: loss=0.30339733161257976
Epoch #125: loss=0.25871854094844876
Epoch #126: loss=0.17338146269321442
Epoch #127: loss=0.15302011381947633
Epoch #128: loss=0.15933758020401
Epoch #129: loss=0.19540537126136548
Epoch #130: loss=0.1687916898817727
Epoch #131: loss=0.15606618029150096
Epoch #132: loss=0.14542595838958566
Epoch #133: loss=0.15863212830189502
Epoch #134: loss=0.26140173547195666
Epoch #135: loss=0.1806647085556478
Epoch #136: loss=0.19595905540115904
Epoch #137: loss=0.23254240078456473
Epoch #138: loss=0.1615484356880188
Epoch #139: loss=0.16010453032724786
Epoch #140: loss=0.19344922121275554
Epoch #141: loss=0.23307791781244855
Epoch #142: loss=0.14322360367937523
Epoch #143: loss=0.17837375571781938
Epoch #144: loss=0.2099280653126312
Epoch #145: loss=0.22722531301957188
Epoch #146: loss=0.18210508629228128
Epoch #147: loss=0.17198561549638258
Epoch #148: loss=0.15821771097905707
Epoch #149: loss=0.2093563929877498
Epoch #150: loss=0.15257194017370543
Epoch #151: loss=0.18320436640219254
Epoch #152: loss=0.1571594105299675
Epoch #153: loss=0.11618962522709009
Epoch #154: loss=0.11920028727388743
Epoch #155: loss=0.14617931696050096
Epoch #156: loss=0.11258182866555272
Epoch #157: loss=0.09909002008763226
Epoch #158: loss=0.16990946955753095
Epoch #159: loss=0.12829642550963344
Epoch #160: loss=0.1317248958529848
Epoch #161: loss=0.12692456568280855
Epoch #162: loss=0.12298857781923178
Epoch #163: loss=0.13263165612112393
Epoch #164: loss=0.13067940221817204
Epoch #165: loss=0.18133169751275668
Epoch #166: loss=0.14357983462060941
Epoch #167: loss=0.10816162649654981
Epoch #168: loss=0.1059912983328104
Epoch #169: loss=0.21480562255689592
Epoch #170: loss=0.2878281112873193
Epoch #171: loss=0.1522505862469023
Epoch #172: loss=0.16627744901360889
Epoch #173: loss=0.17116930189006258
Epoch #174: loss=0.18007205799221992
Epoch #175: loss=0.2667960599064827
Epoch #176: loss=0.2122809158807451
Epoch #177: loss=0.27592512344320613
Epoch #178: loss=0.13981640412274635
Epoch #179: loss=0.23556267041148563
Epoch #180: loss=0.24549864503470334
Epoch #181: loss=0.36658047941146477
Epoch #182: loss=0.2080502122866385
Epoch #183: loss=0.1740887158296325
Epoch #184: loss=0.11997523971579292
Epoch #185: loss=0.12635776536031204
Epoch #186: loss=0.11870537438627446
Epoch #187: loss=0.08894623736992027
Epoch #188: loss=0.1193149204394131
Epoch #189: loss=0.14036383242769676
Epoch #190: loss=0.12758658995682542
Epoch #191: loss=0.15264430021246275
Epoch #192: loss=0.21549151837825775
Epoch #193: loss=0.14262175797061485
Epoch #194: loss=0.14735064813584992
Epoch #195: loss=0.1614333837095535
Epoch #196: loss=0.13975277678533035
Epoch #197: loss=0.1726910602865797
Epoch #198: loss=0.12671821660390406
Epoch #199: loss=0.10312846365074317
Epoch #200: loss=0.09333201695346471
Epoch #201: loss=0.10033591272252979
Epoch #202: loss=0.11011318151246417
Epoch #203: loss=0.1421360925517299
Epoch #204: loss=0.1222195103200096
Epoch #205: loss=0.09680403086046378
Epoch #206: loss=0.12737595656830253
Epoch #207: loss=0.15928191211864803
Epoch #208: loss=0.09758174396825559
Epoch #209: loss=0.08399153399196538
Epoch #210: loss=0.10259211317382076
Epoch #211: loss=0.0811284871941263
Epoch #212: loss=0.09730464998971332
Epoch #213: loss=0.07682425432810278
Epoch #214: loss=0.08892700812694701
Epoch #215: loss=0.0776928375390443
Epoch #216: loss=0.14476605668438203
Epoch #217: loss=0.15280164343615374
Epoch #218: loss=0.08579129793427208
Epoch #219: loss=0.11246252709040136
Epoch #220: loss=0.11691132780503143
Epoch #221: loss=0.13669740273193878
Epoch #222: loss=0.10501117967633587
Epoch #223: loss=0.09658778958361257
Epoch #224: loss=0.0927818835797635
Epoch #225: loss=0.07134335132485087
Epoch #226: loss=0.09763714947709531
Epoch #227: loss=0.0969097845143441
Epoch #228: loss=0.12178099708575191
Epoch #229: loss=0.11961207922660944
Epoch #230: loss=0.12436356107619675
Epoch #231: loss=0.11389125646515326
Epoch #232: loss=0.07859614186666229
Epoch #233: loss=0.06644391968394771
Epoch #234: loss=0.064263159239834
Epoch #235: loss=0.09049673982416138
Epoch #236: loss=0.08626545491543683
Epoch #237: loss=0.09194481846961108
Epoch #238: loss=0.08890722483170754
Epoch #239: loss=0.15247152723146207
Epoch #240: loss=0.14027806269851598
Epoch #241: loss=0.14291067872986649
Epoch #242: loss=0.0894389497962865
Epoch #243: loss=0.07910691845146092
Epoch #244: loss=0.06406284154703219
Epoch #245: loss=0.10011373302250197
Epoch #246: loss=0.08377313780400789
Epoch #247: loss=0.0580734292214567
Epoch #248: loss=0.08403071574866772
Epoch #249: loss=0.06602947205079324

Training time: 0:32:00.041563

Finished.
n2one setting etth1_weather -> electricity
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth1_weather_epochs_250_seed_2024/model.pkl', muti_dataset='etth1_weather', random_seed=2024, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='electricity')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.36898e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.91869e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.36898e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.35335601180819154, 'MAE': 0.4164685588379161}
Finished.
------------------------- record done -------------------------
n2one setting etth1_weather -> exchange
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth1_weather_epochs_250_seed_2024/model.pkl', muti_dataset='etth1_weather', random_seed=2024, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='exchange')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=3.4929e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.20088433590002866, 'MAE': 0.3115705562009138}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='etth1_exchange', random_seed=2024, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/etth1_exchange_epochs_250_seed_2024
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=4.810892534255982
Epoch #1: loss=2.4257285912831623
Epoch #2: loss=2.1643560965855917
Epoch #3: loss=1.885823090871175
Epoch #4: loss=1.9002298593521119
Epoch #5: loss=1.7106313149134318
Epoch #6: loss=1.6518998463948569
Epoch #7: loss=1.6736436525980631
Epoch #8: loss=1.6220899184544881
Epoch #9: loss=1.5268728574117025
Epoch #10: loss=1.5171470165252685
Epoch #11: loss=1.450224773089091
Epoch #12: loss=1.3526640812555948
Epoch #13: loss=1.3517828305562338
Epoch #14: loss=1.361512509981791
Epoch #15: loss=1.2401121139526368
Epoch #16: loss=1.2512666622797648
Epoch #17: loss=1.2228292981783548
Epoch #18: loss=1.1816548347473144
Epoch #19: loss=1.1907960534095765
Epoch #20: loss=1.1107363820075988
Epoch #21: loss=1.0374529520670572
Epoch #22: loss=1.0533566753069559
Epoch #23: loss=0.9889528473218282
Epoch #24: loss=0.978818929195404
Epoch #25: loss=1.1013699690500895
Epoch #26: loss=1.0766686916351318
Epoch #27: loss=0.9533519148826599
Epoch #28: loss=0.9743429978688558
Epoch #29: loss=1.225733717282613
Epoch #30: loss=1.0418773452440897
Epoch #31: loss=0.9881506284077962
Epoch #32: loss=0.9047911922136943
Epoch #33: loss=0.819347083568573
Epoch #34: loss=0.8462192138036092
Epoch #35: loss=0.9062105774879455
Epoch #36: loss=0.9262548208236694
Epoch #37: loss=0.972715417544047
Epoch #38: loss=0.881760315100352
Epoch #39: loss=0.8583970745404561
Epoch #40: loss=0.8610139966011048
Epoch #41: loss=0.8265590310096741
Epoch #42: loss=0.6994704087575276
Epoch #43: loss=0.6957899490992229
Epoch #44: loss=0.7116308430830638
Epoch #45: loss=0.7129007418950398
Epoch #46: loss=0.6832876205444336
Epoch #47: loss=0.8500706295172373
Epoch #48: loss=0.6068636635939281
Epoch #49: loss=0.6320647835731507
Epoch #50: loss=0.5967451055844625
Epoch #51: loss=0.6616548240184784
Epoch #52: loss=0.6009632666905721
Epoch #53: loss=0.5693064252535502
Epoch #54: loss=0.6226216077804565
Epoch #55: loss=0.6334442635377248
Epoch #56: loss=0.6215425034364065
Epoch #57: loss=0.5888376533985138
Epoch #58: loss=0.6851715803146362
Epoch #59: loss=0.5558432400226593
Epoch #60: loss=0.6025702039400737
Epoch #61: loss=0.5527762571970621
Epoch #62: loss=0.5477827191352844
Epoch #63: loss=0.5288349946339925
Epoch #64: loss=0.5601331015427907
Epoch #65: loss=0.5138273368279139
Epoch #66: loss=0.6212409456570943
Epoch #67: loss=0.6159474601348242
Epoch #68: loss=0.6576561272144318
Epoch #69: loss=0.4677569389343262
Epoch #70: loss=0.45994791587193806
Epoch #71: loss=0.38865928053855897
Epoch #72: loss=0.43565627733866374
Epoch #73: loss=0.4443532337745031
Epoch #74: loss=0.5666114985942841
Epoch #75: loss=0.5230408996343613
Epoch #76: loss=0.4033389856417974
Epoch #77: loss=0.4799652765194575
Epoch #78: loss=0.5434565991163254
Epoch #79: loss=0.6540201743443806
Epoch #80: loss=0.5361648261547088
Epoch #81: loss=0.530594089627266
Epoch #82: loss=0.5248176018397014
Epoch #83: loss=0.4255188137292862
Epoch #84: loss=0.4313434461752574
Epoch #85: loss=0.4033563514550527
Epoch #86: loss=0.4145993322134018
Epoch #87: loss=0.3544759372870127
Epoch #88: loss=0.33947909077008565
Epoch #89: loss=0.3230908264716466
Epoch #90: loss=0.36649817128976186
Epoch #91: loss=0.447285266717275
Epoch #92: loss=0.4449933747450511
Epoch #93: loss=0.47570805847644804
Epoch #94: loss=0.46222266952196756
Epoch #95: loss=0.35998950004577634
Epoch #96: loss=0.34228841960430145
Epoch #97: loss=0.34027622640132904
Epoch #98: loss=0.37360178331534066
Epoch #99: loss=0.33429398089647294
Epoch #100: loss=0.3585162177681923
Epoch #101: loss=0.36856882919867834
Epoch #102: loss=0.3920395274957021
Epoch #103: loss=0.29240821351607643
Epoch #104: loss=0.38427313019831977
Epoch #105: loss=0.3078353673219681
Epoch #106: loss=0.34950777093569435
Epoch #107: loss=0.4990041881799698
Epoch #108: loss=0.527083483338356
Epoch #109: loss=0.4923511266708374
Epoch #110: loss=0.46520386735598246
Epoch #111: loss=0.3942574421564738
Epoch #112: loss=0.3903576781352361
Epoch #113: loss=0.39640175153811774
Epoch #114: loss=0.40627835045258204
Epoch #115: loss=0.28842603713274
Epoch #116: loss=0.2779156242807706
Epoch #117: loss=0.36341643581787747
Epoch #118: loss=0.4264835029840469
Epoch #119: loss=0.3454315816362699
Epoch #120: loss=0.3795245409011841
Epoch #121: loss=0.4363738149404526
Epoch #122: loss=0.31579141368468605
Epoch #123: loss=0.3342566167314847
Epoch #124: loss=0.22111706535021464
Epoch #125: loss=0.28445810079574585
Epoch #126: loss=0.32983346382776896
Epoch #127: loss=0.3268273830413818
Epoch #128: loss=0.42786255379517873
Epoch #129: loss=0.3030010988314947
Epoch #130: loss=0.3158894201119741
Epoch #131: loss=0.42862643152475355
Epoch #132: loss=0.29095681607723234
Epoch #133: loss=0.31949745963017145
Epoch #134: loss=0.30780421594778695
Epoch #135: loss=0.2852665737271309
Epoch #136: loss=0.46160284876823426
Epoch #137: loss=0.3346721629301707
Epoch #138: loss=0.2924489368995031
Epoch #139: loss=0.30487284511327745
Epoch #140: loss=0.3237334410349528
Epoch #141: loss=0.35358159442742665
Epoch #142: loss=0.4246777812639872
Epoch #143: loss=0.2911997159322103
Epoch #144: loss=0.3028045430779457
Epoch #145: loss=0.2275759796301524
Epoch #146: loss=0.29088453004757564
Epoch #147: loss=0.3326475530862808
Epoch #148: loss=0.24752211968104046
Epoch #149: loss=0.2730368286371231
Epoch #150: loss=0.2559621753791968
Epoch #151: loss=0.3422046338518461
Epoch #152: loss=0.2575183093547821
Epoch #153: loss=0.24897327721118928
Epoch #154: loss=0.2374666010340055
Epoch #155: loss=0.2878195265928904
Epoch #156: loss=0.24320565412441889
Epoch #157: loss=0.2657881513237953
Epoch #158: loss=0.2539371651907762
Epoch #159: loss=0.32746988236904145
Epoch #160: loss=0.30060279568036397
Epoch #161: loss=0.23555339872837067
Epoch #162: loss=0.23327223906914393
Epoch #163: loss=0.31847915053367615
Epoch #164: loss=0.3220481604337692
Epoch #165: loss=0.3060033197204272
Epoch #166: loss=0.29786591430505116
Epoch #167: loss=0.3184322868784269
Epoch #168: loss=0.25797094653050107
Epoch #169: loss=0.2797379473845164
Epoch #170: loss=0.233243461449941
Epoch #171: loss=0.21503110527992247
Epoch #172: loss=0.3102533047397931
Epoch #173: loss=0.291174353659153
Epoch #174: loss=0.25874869947632156
Epoch #175: loss=0.23280628422896069
Epoch #176: loss=0.28282425850629805
Epoch #177: loss=0.23392483790715535
Epoch #178: loss=0.26886234879493714
Epoch #179: loss=0.2648281027873357
Epoch #180: loss=0.36544322272141777
Epoch #181: loss=0.35369168519973754
Epoch #182: loss=0.2993634045124054
Epoch #183: loss=0.26670932918787005
Epoch #184: loss=0.2236227681239446
Epoch #185: loss=0.23467510988314946
Epoch #186: loss=0.23535743405421575
Epoch #187: loss=0.2698296055197716
Epoch #188: loss=0.3251429210106532
Epoch #189: loss=0.2549970140059789
Epoch #190: loss=0.33341160863637925
Epoch #191: loss=0.33507176836331687
Epoch #192: loss=0.27059708833694457
Epoch #193: loss=0.209025872250398
Epoch #194: loss=0.27196840991576515
Epoch #195: loss=0.24246743271748225
Epoch #196: loss=0.19289724975824357
Epoch #197: loss=0.20909271786610287
Epoch #198: loss=0.2049430916706721
Epoch #199: loss=0.16872583255171775
Epoch #200: loss=0.1526416912674904
Epoch #201: loss=0.22201549311478933
Epoch #202: loss=0.3319802239537239
Epoch #203: loss=0.3471481159329414
Epoch #204: loss=0.39022916903098426
Epoch #205: loss=0.2859940687815348
Epoch #206: loss=0.2550626436869303
Epoch #207: loss=0.26262930979331334
Epoch #208: loss=0.19019636362791062
Epoch #209: loss=0.22308643261591593
Epoch #210: loss=0.1924345667163531
Epoch #211: loss=0.25642103056112925
Epoch #212: loss=0.22279511988162995
Epoch #213: loss=0.19106920311848322
Epoch #214: loss=0.18151984413464864
Epoch #215: loss=0.17875779022773106
Epoch #216: loss=0.2804927003880342
Epoch #217: loss=0.22141783634821574
Epoch #218: loss=0.16687932883699735
Epoch #219: loss=0.2243804045021534
Epoch #220: loss=0.19171950941284496
Epoch #221: loss=0.18309560144941012
Epoch #222: loss=0.21561483889818192
Epoch #223: loss=0.2613060787320137
Epoch #224: loss=0.18406009748578073
Epoch #225: loss=0.20730330074826878
Epoch #226: loss=0.2271483470996221
Epoch #227: loss=0.1992943172653516
Epoch #228: loss=0.20301641797026
Epoch #229: loss=0.2469589871664842
Epoch #230: loss=0.25081372012694675
Epoch #231: loss=0.22017943958441416
Epoch #232: loss=0.20700567364692687
Epoch #233: loss=0.18336101000507674
Epoch #234: loss=0.16694450974464417
Epoch #235: loss=0.21526931921641032
Epoch #236: loss=0.16842544227838516
Epoch #237: loss=0.3062829554080963
Epoch #238: loss=0.2487047019104163
Epoch #239: loss=0.2774368479847908
Epoch #240: loss=0.21378742257754008
Epoch #241: loss=0.2749237135052681
Epoch #242: loss=0.29024739091595014
Epoch #243: loss=0.2270535260438919
Epoch #244: loss=0.20302102863788604
Epoch #245: loss=0.15976481487353641
Epoch #246: loss=0.2023957168062528
Epoch #247: loss=0.16272297749916712
Epoch #248: loss=0.18596206630269688
Epoch #249: loss=0.22720657785733542

Training time: 0:10:28.015510

Finished.
n2one setting etth1_exchange -> electricity
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth1_exchange_epochs_250_seed_2024/model.pkl', muti_dataset='etth1_exchange', random_seed=2024, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='electricity')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.29716e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.62004e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.29716e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.35499558324153824, 'MAE': 0.42085115695740366}
Finished.
------------------------- record done -------------------------
n2one setting etth1_exchange -> weather
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth1_exchange_epochs_250_seed_2024/model.pkl', muti_dataset='etth1_exchange', random_seed=2024, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='weather')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.16354e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.31128e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=5.58134e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.16354e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.8872050290083169, 'MAE': 0.7002777564535974}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='etth2_ettm1', random_seed=2024, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/etth2_ettm1_epochs_250_seed_2024
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=5.328301535712348
Epoch #1: loss=2.7545653184254966
Epoch #2: loss=2.4592144356833563
Epoch #3: loss=2.1889289948675366
Epoch #4: loss=2.0387725167804294
Epoch #5: loss=1.8976143267419603
Epoch #6: loss=1.7553131845262315
Epoch #7: loss=1.7000476916631062
Epoch #8: loss=1.5242366592089336
Epoch #9: loss=1.482463585005866
Epoch #10: loss=1.5005147920714483
Epoch #11: loss=1.432964152759976
Epoch #12: loss=1.292051202721066
Epoch #13: loss=1.2705721722708807
Epoch #14: loss=1.326593816280365
Epoch #15: loss=1.1269556780656178
Epoch #16: loss=1.1972538597053952
Epoch #17: loss=1.1095449990696378
Epoch #18: loss=1.0595950583616893
Epoch #19: loss=1.062275340159734
Epoch #20: loss=1.015851292345259
Epoch #21: loss=1.0479503671328227
Epoch #22: loss=0.8649394280380673
Epoch #23: loss=0.9320286379920112
Epoch #24: loss=0.8792141775290171
Epoch #25: loss=0.8341358866956499
Epoch #26: loss=0.7636908259656694
Epoch #27: loss=0.8372816046079
Epoch #28: loss=0.746440589427948
Epoch #29: loss=0.9892420868078867
Epoch #30: loss=0.8451227611965604
Epoch #31: loss=0.732837094200982
Epoch #32: loss=0.7436734106805589
Epoch #33: loss=0.7937937213314904
Epoch #34: loss=0.7481860087977515
Epoch #35: loss=0.8175287743409475
Epoch #36: loss=0.7565965784920586
Epoch #37: loss=0.719975021150377
Epoch #38: loss=0.6085994376076592
Epoch #39: loss=0.5983521358834373
Epoch #40: loss=0.6584209005037943
Epoch #41: loss=0.6842344734403822
Epoch #42: loss=0.5972293830580182
Epoch #43: loss=0.5898226234647963
Epoch #44: loss=0.639589430557357
Epoch #45: loss=0.6066535115242004
Epoch #46: loss=0.8175878127415975
Epoch #47: loss=0.5867270661724938
Epoch #48: loss=0.5884631425142288
Epoch #49: loss=0.625628693236245
Epoch #50: loss=0.692697619398435
Epoch #51: loss=0.557097441620297
Epoch #52: loss=0.5249596999751197
Epoch #53: loss=0.5837593774000803
Epoch #54: loss=0.4895201358530257
Epoch #55: loss=0.45908047093285453
Epoch #56: loss=0.7211720860666699
Epoch #57: loss=0.5012038118309445
Epoch #58: loss=0.5764786104361216
Epoch #59: loss=0.43303245306015015
Epoch #60: loss=0.48485641678174335
Epoch #61: loss=0.45886741909715867
Epoch #62: loss=0.45757566889127094
Epoch #63: loss=0.4657431443532308
Epoch #64: loss=0.41174479325612384
Epoch #65: loss=0.5971692932976617
Epoch #66: loss=0.4846327238612705
Epoch #67: loss=0.5321102539698283
Epoch #68: loss=0.47763599786493516
Epoch #69: loss=0.4960510796970791
Epoch #70: loss=0.4758201572630141
Epoch #71: loss=0.4403771625624763
Epoch #72: loss=0.40047437449296314
Epoch #73: loss=0.4011985378132926
Epoch #74: loss=0.4033323857519362
Epoch #75: loss=0.3824598607089784
Epoch #76: loss=0.3746379431751039
Epoch #77: loss=0.45847826782200074
Epoch #78: loss=0.4596839017338223
Epoch #79: loss=0.40713168018394047
Epoch #80: loss=0.4317837506532669
Epoch #81: loss=0.3537758207983441
Epoch #82: loss=0.33744584934579
Epoch #83: loss=0.4057995511425866
Epoch #84: loss=0.4247839351495107
Epoch #85: loss=0.409539391597112
Epoch #86: loss=0.34837139149506885
Epoch #87: loss=0.3377558771106932
Epoch #88: loss=0.34269490672482383
Epoch #89: loss=0.3785514078206486
Epoch #90: loss=0.28458383099900353
Epoch #91: loss=0.32728047917286557
Epoch #92: loss=0.2842818904254172
Epoch #93: loss=0.3836927463610967
Epoch #94: loss=0.3570360533065266
Epoch #95: loss=0.3349706903100014
Epoch #96: loss=0.33167632006936604
Epoch #97: loss=0.30403383407327866
Epoch #98: loss=0.3313705556922489
Epoch #99: loss=0.3131217079030143
Epoch #100: loss=0.3584290080600315
Epoch #101: loss=0.3170334878895018
Epoch #102: loss=0.307194488743941
Epoch #103: loss=0.25356575349966687
Epoch #104: loss=0.32972631769047844
Epoch #105: loss=0.4009398884243435
Epoch #106: loss=0.31207746432887185
Epoch #107: loss=0.3064761228031582
Epoch #108: loss=0.2657664558953709
Epoch #109: loss=0.27901509569750893
Epoch #110: loss=0.28264785144064164
Epoch #111: loss=0.2951068753997485
Epoch #112: loss=0.25240962124533123
Epoch #113: loss=0.22492453124788073
Epoch #114: loss=0.2656196629007657
Epoch #115: loss=0.30260325057639015
Epoch #116: loss=0.3255065944459703
Epoch #117: loss=0.31319402075476116
Epoch #118: loss=0.3256877197159661
Epoch #119: loss=0.2834133075343238
Epoch #120: loss=0.2715939987036917
Epoch #121: loss=0.3546000238921907
Epoch #122: loss=0.25551000982522964
Epoch #123: loss=0.3520084246993065
Epoch #124: loss=0.33766768044895595
Epoch #125: loss=0.2257501259446144
Epoch #126: loss=0.18999119641052353
Epoch #127: loss=0.22327615403466755
Epoch #128: loss=0.2045384823448128
Epoch #129: loss=0.20525055792596605
Epoch #130: loss=0.22384863098462424
Epoch #131: loss=0.254228407310115
Epoch #132: loss=0.3085891873472267
Epoch #133: loss=0.22472318179077572
Epoch #134: loss=0.38439273585875827
Epoch #135: loss=0.3242694702413347
Epoch #136: loss=0.2493823832935757
Epoch #137: loss=0.3214662762151824
Epoch #138: loss=0.22085787190331352
Epoch #139: loss=0.22674161651068264
Epoch #140: loss=0.21721764446960556
Epoch #141: loss=0.19689832793341744
Epoch #142: loss=0.25025632397996056
Epoch #143: loss=0.23046455863449308
Epoch #144: loss=0.18862821327315438
Epoch #145: loss=0.20964727219608095
Epoch #146: loss=0.1942310126291381
Epoch #147: loss=0.17078246672948202
Epoch #148: loss=0.22158588427636358
Epoch #149: loss=0.2539956851138009
Epoch #150: loss=0.23040925959746042
Epoch #151: loss=0.21626727076040375
Epoch #152: loss=0.19918524142768648
Epoch #153: loss=0.21832732690705192
Epoch #154: loss=0.23918775717417398
Epoch #155: loss=0.2702750373217795
Epoch #156: loss=0.34453055386741954
Epoch #157: loss=0.2063086281220118
Epoch #158: loss=0.2042709911863009
Epoch #159: loss=0.18618904054164886
Epoch #160: loss=0.212112032290962
Epoch #161: loss=0.17399026536279255
Epoch #162: loss=0.1468171781549851
Epoch #163: loss=0.16846742750042015
Epoch #164: loss=0.2219581287354231
Epoch #165: loss=0.18512477767136362
Epoch #166: loss=0.19817193804515731
Epoch #167: loss=0.24543295097019938
Epoch #168: loss=0.13518393826153544
Epoch #169: loss=0.16139742649263805
Epoch #170: loss=0.12273478818436463
Epoch #171: loss=0.16556118201050493
Epoch #172: loss=0.16821901334656608
Epoch #173: loss=0.252435272352563
Epoch #174: loss=0.25331148215466076
Epoch #175: loss=0.19454439895020592
Epoch #176: loss=0.17164586132599247
Epoch #177: loss=0.18012799074252447
Epoch #178: loss=0.12371234285334747
Epoch #179: loss=0.14860233809385034
Epoch #180: loss=0.13645783708327347
Epoch #181: loss=0.14257715249227154
Epoch #182: loss=0.19865188913212883
Epoch #183: loss=0.14459083043038845
Epoch #184: loss=0.17446528789069918
Epoch #185: loss=0.31123806287844974
Epoch #186: loss=0.19383727427985933
Epoch #187: loss=0.18365713126129574
Epoch #188: loss=0.15293906732565826
Epoch #189: loss=0.1403866803480519
Epoch #190: loss=0.15354509382612175
Epoch #191: loss=0.13103270841141543
Epoch #192: loss=0.1103223351140817
Epoch #193: loss=0.10322734547985925
Epoch #194: loss=0.13029687044521174
Epoch #195: loss=0.13940323640902838
Epoch #196: loss=0.13534468453791407
Epoch #197: loss=0.13003607156376043
Epoch #198: loss=0.13357376721170214
Epoch #199: loss=0.1720485734856791
Epoch #200: loss=0.12714486134548983
Epoch #201: loss=0.16740907501015398
Epoch #202: loss=0.14485420286655426
Epoch #203: loss=0.1267098155286577
Epoch #204: loss=0.10541463125911024
Epoch #205: loss=0.10167842109998067
Epoch #206: loss=0.1584189443124665
Epoch #207: loss=0.10798188774949974
Epoch #208: loss=0.16777984073592556
Epoch #209: loss=0.13519836030900478
Epoch #210: loss=0.16104009540544617
Epoch #211: loss=0.15602210640079445
Epoch #212: loss=0.13728156271908018
Epoch #213: loss=0.16164381926258406
Epoch #214: loss=0.15331646479252312
Epoch #215: loss=0.14027055621975
Epoch #216: loss=0.11145456218057209
Epoch #217: loss=0.1541487638735109
Epoch #218: loss=0.1319974867833985
Epoch #219: loss=0.11804370996024874
Epoch #220: loss=0.15726826609008843
Epoch #221: loss=0.14890560300813782
Epoch #222: loss=0.10706361838512951
Epoch #223: loss=0.12803958087331718
Epoch #224: loss=0.09550263680931595
Epoch #225: loss=0.1360941557213664
Epoch #226: loss=0.10512062720954418
Epoch #227: loss=0.13281727520128092
Epoch #228: loss=0.0989662693399522
Epoch #229: loss=0.14622041396796703
Epoch #230: loss=0.11030329080919425
Epoch #231: loss=0.08873395663168696
Epoch #232: loss=0.08291978968514337
Epoch #233: loss=0.14286449365317822
Epoch #234: loss=0.1009708841641744
Epoch #235: loss=0.13510363621430266
Epoch #236: loss=0.0951775535941124
Epoch #237: loss=0.12197653286986881
Epoch #238: loss=0.09275134777029355
Epoch #239: loss=0.13146187220182684
Epoch #240: loss=0.08813765551894903
Epoch #241: loss=0.1283500271124972
Epoch #242: loss=0.18581351368791527
Epoch #243: loss=0.12326005204684204
Epoch #244: loss=0.08999360290666421
Epoch #245: loss=0.10010613304459387
Epoch #246: loss=0.08644855498439735
Epoch #247: loss=0.13473696116771963
Epoch #248: loss=0.17842027731239796
Epoch #249: loss=0.21959553700354364

Training time: 0:17:03.680419

Finished.
n2one setting etth2_ettm1 -> electricity
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth2_ettm1_epochs_250_seed_2024/model.pkl', muti_dataset='etth2_ettm1', random_seed=2024, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='electricity')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.14822e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.15046e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=5.48882e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.14822e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.366877859975798, 'MAE': 0.4255059690415316}
Finished.
------------------------- record done -------------------------
n2one setting etth2_ettm1 -> weather
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth2_ettm1_epochs_250_seed_2024/model.pkl', muti_dataset='etth2_ettm1', random_seed=2024, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='weather')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.41256e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.69755e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.41256e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.6309276228758268, 'MAE': 0.5920733662392955}
Finished.
------------------------- record done -------------------------
n2one setting etth2_ettm1 -> exchange
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth2_ettm1_epochs_250_seed_2024/model.pkl', muti_dataset='etth2_ettm1', random_seed=2024, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='exchange')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=3.96606e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.31089317392099874, 'MAE': 0.37695251983826866}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='etth2_ettm2', random_seed=2024, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/etth2_ettm2_epochs_250_seed_2024
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=5.618275854322645
Epoch #1: loss=2.8173238966200085
Epoch #2: loss=2.496543904145559
Epoch #3: loss=2.2596400711271496
Epoch #4: loss=2.1109144422743054
Epoch #5: loss=1.9715420206387837
Epoch #6: loss=1.826291607485877
Epoch #7: loss=1.7547112703323364
Epoch #8: loss=1.5648339788119
Epoch #9: loss=1.535480174753401
Epoch #10: loss=1.552307340833876
Epoch #11: loss=1.483252035246955
Epoch #12: loss=1.3351248502731323
Epoch #13: loss=1.2976929479175143
Epoch #14: loss=1.3557915025287204
Epoch #15: loss=1.175451636314392
Epoch #16: loss=1.2090649803479512
Epoch #17: loss=1.1279624137613509
Epoch #18: loss=1.092880414591895
Epoch #19: loss=1.0753102335664961
Epoch #20: loss=1.0483460028966267
Epoch #21: loss=1.0623607602384355
Epoch #22: loss=0.9381025069289737
Epoch #23: loss=0.941629429658254
Epoch #24: loss=0.8991485138734182
Epoch #25: loss=0.8586176501380073
Epoch #26: loss=0.829083287053638
Epoch #27: loss=0.8828630910979377
Epoch #28: loss=0.7950819697644975
Epoch #29: loss=1.0033586190806494
Epoch #30: loss=0.8621593217055002
Epoch #31: loss=0.7337743673059676
Epoch #32: loss=0.7638963792059157
Epoch #33: loss=0.79204370909267
Epoch #34: loss=0.7307113806406657
Epoch #35: loss=0.7411917779180739
Epoch #36: loss=0.7018311139610078
Epoch #37: loss=0.69094936715232
Epoch #38: loss=0.6045535008112589
Epoch #39: loss=0.5902870943148931
Epoch #40: loss=0.643443591064877
Epoch #41: loss=0.7138261596361796
Epoch #42: loss=0.6231458816263411
Epoch #43: loss=0.6073894136481814
Epoch #44: loss=0.6963665650950538
Epoch #45: loss=0.7004356053140428
Epoch #46: loss=0.8789489401711358
Epoch #47: loss=0.6301466027895609
Epoch #48: loss=0.6226478185918596
Epoch #49: loss=0.6380882379081514
Epoch #50: loss=0.6663225326273177
Epoch #51: loss=0.554771578974194
Epoch #52: loss=0.5534928871525658
Epoch #53: loss=0.6056922276814779
Epoch #54: loss=0.5079176740513908
Epoch #55: loss=0.4688399268521203
Epoch #56: loss=0.7291915035910077
Epoch #57: loss=0.5013281454642614
Epoch #58: loss=0.5827868266238106
Epoch #59: loss=0.4482704649368922
Epoch #60: loss=0.5248301757706536
Epoch #61: loss=0.5044054239988327
Epoch #62: loss=0.47351841462983024
Epoch #63: loss=0.4664657579527961
Epoch #64: loss=0.4406152367591858
Epoch #65: loss=0.5862471924887763
Epoch #66: loss=0.4937242815891902
Epoch #67: loss=0.5413990798923705
Epoch #68: loss=0.48547344075308907
Epoch #69: loss=0.4880488697025511
Epoch #70: loss=0.4714378383424547
Epoch #71: loss=0.44107624391714734
Epoch #72: loss=0.39983223378658295
Epoch #73: loss=0.38429318368434906
Epoch #74: loss=0.3882598943180508
Epoch #75: loss=0.37811435262362164
Epoch #76: loss=0.37707551154825425
Epoch #77: loss=0.4408184231983291
Epoch #78: loss=0.4175781442059411
Epoch #79: loss=0.3992573751343621
Epoch #80: loss=0.4388710988892449
Epoch #81: loss=0.34525901410314774
Epoch #82: loss=0.3506203947795762
Epoch #83: loss=0.4230993365248044
Epoch #84: loss=0.41875632603963214
Epoch #85: loss=0.3978368987639745
Epoch #86: loss=0.36203957431846195
Epoch #87: loss=0.4122387750281228
Epoch #88: loss=0.3648593955569797
Epoch #89: loss=0.42569586137930554
Epoch #90: loss=0.3037164757649104
Epoch #91: loss=0.33648517893420327
Epoch #92: loss=0.2933709944287936
Epoch #93: loss=0.37626130547788406
Epoch #94: loss=0.3431198505891694
Epoch #95: loss=0.32113634877734715
Epoch #96: loss=0.33129189494583344
Epoch #97: loss=0.303666895462407
Epoch #98: loss=0.33402540120813584
Epoch #99: loss=0.31491927223073113
Epoch #100: loss=0.3501099662648307
Epoch #101: loss=0.32522539380523896
Epoch #102: loss=0.3089577042394214
Epoch #103: loss=0.28449492156505585
Epoch #104: loss=0.34287278602520627
Epoch #105: loss=0.39634206642707187
Epoch #106: loss=0.2986864762173759
Epoch #107: loss=0.3068649669488271
Epoch #108: loss=0.25808917068772846
Epoch #109: loss=0.28409362998273635
Epoch #110: loss=0.26652074605226517
Epoch #111: loss=0.32801353600290084
Epoch #112: loss=0.28884490993287826
Epoch #113: loss=0.32231369614601135
Epoch #114: loss=0.28140590339899063
Epoch #115: loss=0.3423363235261705
Epoch #116: loss=0.3828442982501454
Epoch #117: loss=0.31482287910249496
Epoch #118: loss=0.3340662635034985
Epoch #119: loss=0.2730650131901105
Epoch #120: loss=0.2504090964794159
Epoch #121: loss=0.3409292731020186
Epoch #122: loss=0.22877628025081423
Epoch #123: loss=0.33479200634691453
Epoch #124: loss=0.31927990085548824
Epoch #125: loss=0.2496541142463684
Epoch #126: loss=0.1942580896947119
Epoch #127: loss=0.239889205329948
Epoch #128: loss=0.21933284319109386
Epoch #129: loss=0.19711991937624085
Epoch #130: loss=0.21566390204760763
Epoch #131: loss=0.211700815293524
Epoch #132: loss=0.24126702547073364
Epoch #133: loss=0.1844666918946637
Epoch #134: loss=0.377487413585186
Epoch #135: loss=0.32346834656265044
Epoch #136: loss=0.26635313861899906
Epoch #137: loss=0.3302820862995254
Epoch #138: loss=0.21907948537005317
Epoch #139: loss=0.228337614900536
Epoch #140: loss=0.21619228728943402
Epoch #141: loss=0.17807792044348186
Epoch #142: loss=0.24992992066674763
Epoch #143: loss=0.23595825872487491
Epoch #144: loss=0.183677745776044
Epoch #145: loss=0.18575885850522253
Epoch #146: loss=0.18530547867218652
Epoch #147: loss=0.17826821986171934
Epoch #148: loss=0.20764919287628597
Epoch #149: loss=0.22839983883831236
Epoch #150: loss=0.20118125196960238
Epoch #151: loss=0.20697075045771068
Epoch #152: loss=0.1948786017795404
Epoch #153: loss=0.2220797799527645
Epoch #154: loss=0.252303344094091
Epoch #155: loss=0.26037776470184326
Epoch #156: loss=0.33210787756575477
Epoch #157: loss=0.2010470438334677
Epoch #158: loss=0.1878064808746179
Epoch #159: loss=0.18327338538236088
Epoch #160: loss=0.2199245678881804
Epoch #161: loss=0.17377489474084643
Epoch #162: loss=0.15754490676853392
Epoch #163: loss=0.21149808168411255
Epoch #164: loss=0.23511717261539566
Epoch #165: loss=0.18884682613942358
Epoch #166: loss=0.19664800436132485
Epoch #167: loss=0.25021299305889344
Epoch #168: loss=0.1292858694990476
Epoch #169: loss=0.1793746935824553
Epoch #170: loss=0.14797551536725628
Epoch #171: loss=0.1683504612495502
Epoch #172: loss=0.1972048177073399
Epoch #173: loss=0.26582705767618287
Epoch #174: loss=0.2470621056854725
Epoch #175: loss=0.1939196458293332
Epoch #176: loss=0.1580569619933764
Epoch #177: loss=0.18042592228286797
Epoch #178: loss=0.11716166738834646
Epoch #179: loss=0.1387328952550888
Epoch #180: loss=0.13767515309154987
Epoch #181: loss=0.1527426667097542
Epoch #182: loss=0.19524219673540857
Epoch #183: loss=0.14063185039493772
Epoch #184: loss=0.1470376764320665
Epoch #185: loss=0.294009269732568
Epoch #186: loss=0.1529832873493433
Epoch #187: loss=0.13365406770673063
Epoch #188: loss=0.1549620973981089
Epoch #189: loss=0.139312953584724
Epoch #190: loss=0.18072116788890627
Epoch #191: loss=0.15284838382568625
Epoch #192: loss=0.12187391498850451
Epoch #193: loss=0.11146281887259749
Epoch #194: loss=0.1271502779175838
Epoch #195: loss=0.141920767724514
Epoch #196: loss=0.12822789802319473
Epoch #197: loss=0.12446961738169193
Epoch #198: loss=0.11256902054366139
Epoch #199: loss=0.1431779149505827
Epoch #200: loss=0.11180598185294205
Epoch #201: loss=0.1634326484054327
Epoch #202: loss=0.13147078909807736
Epoch #203: loss=0.13817689754068851
Epoch #204: loss=0.11124698714249664
Epoch #205: loss=0.09940848985893859
Epoch #206: loss=0.1727015320211649
Epoch #207: loss=0.11000940131230487
Epoch #208: loss=0.16482892218563291
Epoch #209: loss=0.14213229674432012
Epoch #210: loss=0.15053087721268335
Epoch #211: loss=0.17314472649660376
Epoch #212: loss=0.1551030177829994
Epoch #213: loss=0.1693140794005659
Epoch #214: loss=0.16723141281141174
Epoch #215: loss=0.15504315495491028
Epoch #216: loss=0.11364888730976316
Epoch #217: loss=0.1236801173331009
Epoch #218: loss=0.09791757290561993
Epoch #219: loss=0.09576715870449941
Epoch #220: loss=0.12444409603873889
Epoch #221: loss=0.13713544834819105
Epoch #222: loss=0.08935449976060125
Epoch #223: loss=0.11766984106765853
Epoch #224: loss=0.08655253280368116
Epoch #225: loss=0.11296544472376506
Epoch #226: loss=0.09825612563225958
Epoch #227: loss=0.11642051074239942
Epoch #228: loss=0.10376616567373276
Epoch #229: loss=0.1307009756565094
Epoch #230: loss=0.10462193056527111
Epoch #231: loss=0.09919361604584588
Epoch #232: loss=0.1267884232931667
Epoch #233: loss=0.19197293784883288
Epoch #234: loss=0.1529399332486921
Epoch #235: loss=0.1792110577225685
Epoch #236: loss=0.11641945607132381
Epoch #237: loss=0.16510710678994656
Epoch #238: loss=0.12185132420725292
Epoch #239: loss=0.12672104593366385
Epoch #240: loss=0.09280350349015659
Epoch #241: loss=0.13471729846464264
Epoch #242: loss=0.18510576006438997
Epoch #243: loss=0.14527800285981762
Epoch #244: loss=0.12461883450547855
Epoch #245: loss=0.11771473185055786
Epoch #246: loss=0.10258795672820674
Epoch #247: loss=0.11198564349777168
Epoch #248: loss=0.1658883004759749
Epoch #249: loss=0.16419531725760964

Training time: 0:17:09.651127

Finished.
n2one setting etth2_ettm2 -> electricity
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth2_ettm2_epochs_250_seed_2024/model.pkl', muti_dataset='etth2_ettm2', random_seed=2024, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='electricity')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.13521e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.30947e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=5.56921e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.13521e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.38042929242982876, 'MAE': 0.43310902823425684}
Finished.
------------------------- record done -------------------------
n2one setting etth2_ettm2 -> weather
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth2_ettm2_epochs_250_seed_2024/model.pkl', muti_dataset='etth2_ettm2', random_seed=2024, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='weather')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.57141e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.97412e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.57141e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.5529484313079915, 'MAE': 0.5683561664627539}
Finished.
------------------------- record done -------------------------
n2one setting etth2_ettm2 -> exchange
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth2_ettm2_epochs_250_seed_2024/model.pkl', muti_dataset='etth2_ettm2', random_seed=2024, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='exchange')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=4.07156e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.28897804470084837, 'MAE': 0.3567834393148691}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='etth2_electricity', random_seed=2024, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/etth2_electricity_epochs_250_seed_2024
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=1.6468480669143724
Epoch #1: loss=0.6492098204609824
Epoch #2: loss=0.4573863908285048
Epoch #3: loss=0.328697826758754
Epoch #4: loss=0.2950870670014765
Epoch #5: loss=0.2655842269248352
Epoch #6: loss=0.20914865962070664
Epoch #7: loss=0.19482708131758178
Epoch #8: loss=0.1753613011366347
Epoch #9: loss=0.16780087673218874
Epoch #10: loss=0.13800846601313935
Epoch #11: loss=0.1248023937365449
Epoch #12: loss=0.10669402356781973
Epoch #13: loss=0.10557689333175559
Epoch #14: loss=0.09365346421851073
Epoch #15: loss=0.08934347483715634
Epoch #16: loss=0.09042690262743612
Epoch #17: loss=0.0732470981424629
Epoch #18: loss=0.08368175790826904
Epoch #19: loss=0.06139409145703766
Epoch #20: loss=0.05488615547499915
Epoch #21: loss=0.061564673212493155
Epoch #22: loss=0.06944946291368091
Epoch #23: loss=0.05582822300493717
Epoch #24: loss=0.055820040407635996
Epoch #25: loss=0.049899158158862006
Epoch #26: loss=0.04759031482091991
Epoch #27: loss=0.04382869271595576
Epoch #28: loss=0.04203018951996389
Epoch #29: loss=0.042029910488054156
Epoch #30: loss=0.039875062639688756
Epoch #31: loss=0.03798671564223563
Epoch #32: loss=0.04151259083371814
Epoch #33: loss=0.0339355264710853
Epoch #34: loss=0.056790295910237855
Epoch #35: loss=0.047329106651354445
Epoch #36: loss=0.033678369698818864
Epoch #37: loss=0.028284653269532476
Epoch #38: loss=0.03360393558836133
Epoch #39: loss=0.03097280995102554
Epoch #40: loss=0.02807762583848316
Epoch #41: loss=0.059032181231021066
Epoch #42: loss=0.0298185947467573
Epoch #43: loss=0.02916637761944269
Epoch #44: loss=0.03453379746728077
Epoch #45: loss=0.024689625098267796
Epoch #46: loss=0.03786001492734635
Epoch #47: loss=0.025723632354794707
Epoch #48: loss=0.025721882558683296
Epoch #49: loss=0.020048946891087903
Epoch #50: loss=0.027281969236266777
Epoch #51: loss=0.031835437065881975
Epoch #52: loss=0.02188441829923427
Epoch #53: loss=0.015802304989270717
Epoch #54: loss=0.03033155164214294
Epoch #55: loss=0.03817534709076693
Epoch #56: loss=0.02312423316186873
Epoch #57: loss=0.022214137617549746
Epoch #58: loss=0.021947795486506414
Epoch #59: loss=0.02937263652409722
Epoch #60: loss=0.019225226562589452
Epoch #61: loss=0.018188590360606636
Epoch #62: loss=0.02068844310074715
Epoch #63: loss=0.028480049286594207
Epoch #64: loss=0.0268310576740499
Epoch #65: loss=0.01849550356954995
Epoch #66: loss=0.022808369400303403
Epoch #67: loss=0.024932707362828162
Epoch #68: loss=0.030263745907421518
Epoch #69: loss=0.020020766742385517
Epoch #70: loss=0.01568553330151012
Epoch #71: loss=0.022136689355048898
Epoch #72: loss=0.019490118666265228
Epoch #73: loss=0.027358420281926532
Epoch #74: loss=0.03546666996638722
Epoch #75: loss=0.01952963529783585
Epoch #76: loss=0.021858977849549818
Epoch #77: loss=0.017418253718571534
Epoch #78: loss=0.023044837706703864
Epoch #79: loss=0.014259413029521523
Epoch #80: loss=0.014006651515246188
Epoch #81: loss=0.017549246469925714
Epoch #82: loss=0.019356757388932902
Epoch #83: loss=0.015393768878023299
Epoch #84: loss=0.0127563740323062
Epoch #85: loss=0.026553220569195863
Epoch #86: loss=0.021988515012397816
Epoch #87: loss=0.019100989510018446
Epoch #88: loss=0.021477161302932583
Epoch #89: loss=0.01589243382530673
Epoch #90: loss=0.009574996050881654
Epoch #91: loss=0.01315024366925993
Epoch #92: loss=0.011938130972295057
Epoch #93: loss=0.03643723884217651
Epoch #94: loss=0.020568204462306755
Epoch #95: loss=0.016082360101125497
Epoch #96: loss=0.019436692539850964
Epoch #97: loss=0.021771233611549933
Epoch #98: loss=0.013416078222933987
Epoch #99: loss=0.01569435644004687
Epoch #100: loss=0.013589903420879415
Epoch #101: loss=0.012490064602965636
Epoch #102: loss=0.012477683931093652
Epoch #103: loss=0.01544559682419853
Epoch #104: loss=0.013028159838385607
Epoch #105: loss=0.01155437149033632
Epoch #106: loss=0.03000945613432744
Epoch #107: loss=0.010685624163271292
Epoch #108: loss=0.023937251219716172
Epoch #109: loss=0.013167687826974476
Epoch #110: loss=0.009774755265048375
Epoch #111: loss=0.015132625217275502
Epoch #112: loss=0.012659642398541952
Epoch #113: loss=0.012980458663638806
Epoch #114: loss=0.013154125836303049
Epoch #115: loss=0.018570491721464615
Epoch #116: loss=0.017242549631812768
Epoch #117: loss=0.014206900469683361
Epoch #118: loss=0.012166518274880661
Epoch #119: loss=0.01352796035005268
Epoch #120: loss=0.01977945273854236
Epoch #121: loss=0.01470943231195234
Epoch #122: loss=0.017212424675008373
Epoch #123: loss=0.014356955121734134
Epoch #124: loss=0.03713121400452531
Epoch #125: loss=0.021063726004773877
Epoch #126: loss=0.012548901545836466
Epoch #127: loss=0.013203170131707406
Epoch #128: loss=0.011276089949050645
Epoch #129: loss=0.013757358805422682
Epoch #130: loss=0.012357616140390036
Epoch #131: loss=0.02260468725580722
Epoch #132: loss=0.012861556798913914
Epoch #133: loss=0.01098872411274919
Epoch #134: loss=0.010600700931784715
Epoch #135: loss=0.011744247038710987
Epoch #136: loss=0.01645878265099316
Epoch #137: loss=0.015586721285660864
Epoch #138: loss=0.015669802427554516
Epoch #139: loss=0.011121360391570773
Epoch #140: loss=0.012432203185087972
Epoch #141: loss=0.007637523741389642
Epoch #142: loss=0.019573411809333044
Epoch #143: loss=0.009526234842047504
Epoch #144: loss=0.01643391518084128
Epoch #145: loss=0.01931887496743357
Epoch #146: loss=0.02181293117805248
Epoch #147: loss=0.00872178056341931
Epoch #148: loss=0.006420113622791577
Epoch #149: loss=0.013987112696468545
Epoch #150: loss=0.01289063982518955
Epoch #151: loss=0.012600318644062343
Epoch #152: loss=0.013574368061754064
Epoch #153: loss=0.010024876479647213
Epoch #154: loss=0.014778375400055666
Epoch #155: loss=0.012387620372819805
Epoch #156: loss=0.010908798666450155
Epoch #157: loss=0.01545851454514976
Epoch #158: loss=0.010633385497806935
Epoch #159: loss=0.011125304258823191
Epoch #160: loss=0.011077684694588774
Epoch #161: loss=0.016200776836584982
Epoch #162: loss=0.01081021061850706
Epoch #163: loss=0.011578977200991996
Epoch #164: loss=0.010905196641077764
Epoch #165: loss=0.01693183423297497
Epoch #166: loss=0.011693241243891662
Epoch #167: loss=0.015774930423770168
Epoch #168: loss=0.011678386196563667
Epoch #169: loss=0.01200270372504658
Epoch #170: loss=0.009299433723687642
Epoch #171: loss=0.012642816655520951
Epoch #172: loss=0.010454393314065362
Epoch #173: loss=0.011192969882735847
Epoch #174: loss=0.00972630851697986
Epoch #175: loss=0.01107904948571004
Epoch #176: loss=0.00835174146378427
Epoch #177: loss=0.013859224331277002
Epoch #178: loss=0.011713031916836527
Epoch #179: loss=0.012945193029825133
Epoch #180: loss=0.012731781282615507
Epoch #181: loss=0.01673958347592128
Epoch #182: loss=0.008347571489937807
Epoch #183: loss=0.011992338773114437
Epoch #184: loss=0.015516131110993361
Epoch #185: loss=0.016150376602770227
Epoch #186: loss=0.017078337088871815
Epoch #187: loss=0.00936906222838244
Epoch #188: loss=0.00934259057639127
Epoch #189: loss=0.007837273721922496
Epoch #190: loss=0.006585655374138008
Epoch #191: loss=0.0072877893680259416
Epoch #192: loss=0.007304069039602792
Epoch #193: loss=0.016189990059324262
Epoch #194: loss=0.011144227647114681
Epoch #195: loss=0.008531623985085966
Epoch #196: loss=0.031852734263094776
Epoch #197: loss=0.016432283567931933
Epoch #198: loss=0.012430016672779614
Epoch #199: loss=0.009822673735736937
Epoch #200: loss=0.007275629544448926
Epoch #201: loss=0.007541227988285089
Epoch #202: loss=0.003839032573967238
Epoch #203: loss=0.006454433609178523
Epoch #204: loss=0.016467192185678513
Epoch #205: loss=0.011820250408008421
Epoch #206: loss=0.009231454713189145
Epoch #207: loss=0.012069002778866335
Epoch #208: loss=0.012404746098160635
Epoch #209: loss=0.010488059886705653
Epoch #210: loss=0.009946802246104641
Epoch #211: loss=0.016040632747207406
Epoch #212: loss=0.009540384389721311
Epoch #213: loss=0.007853255147027794
Epoch #214: loss=0.008780046971745522
Epoch #215: loss=0.009977020730375222
Epoch #216: loss=0.009732943047909918
Epoch #217: loss=0.014156981769302525
Epoch #218: loss=0.00857928646331768
Epoch #219: loss=0.009557662433774278
Epoch #220: loss=0.006835392040534871
Epoch #221: loss=0.010359970887249092
Epoch #222: loss=0.01427563688687259
Epoch #223: loss=0.007461498618085454
Epoch #224: loss=0.009970312797898858
Epoch #225: loss=0.006615806199899852
Epoch #226: loss=0.00727266634815506
Epoch #227: loss=0.009441099787647723
Epoch #228: loss=0.013356150124575545
Epoch #229: loss=0.009663982890764956
Epoch #230: loss=0.009536724087300216
Epoch #231: loss=0.010405191841905102
Epoch #232: loss=0.016618512709101196
Epoch #233: loss=0.01208640634836118
Epoch #234: loss=0.01220548182879234
Epoch #235: loss=0.007964291246824403
Epoch #236: loss=0.021753902843778546
Epoch #237: loss=0.009685799303086952
Epoch #238: loss=0.014055573900383556
Epoch #239: loss=0.010634146400684032
Epoch #240: loss=0.009091978921787813
Epoch #241: loss=0.005911576001447584
Epoch #242: loss=0.014900605616276152
Epoch #243: loss=0.00554901987277667
Epoch #244: loss=0.008584738233699549
Epoch #245: loss=0.01612034231398067
Epoch #246: loss=0.011963113538602157
Epoch #247: loss=0.006512618119237098
Epoch #248: loss=0.004445704212962372
Epoch #249: loss=0.008355350316445012

Training time: 4:36:22.764433

Finished.
n2one setting etth2_electricity -> weather
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth2_electricity_epochs_250_seed_2024/model.pkl', muti_dataset='etth2_electricity', random_seed=2024, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='weather')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.13304e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.33977e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=5.76736e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.13304e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 1.3137407855767635, 'MAE': 0.9198048419672195}
Finished.
------------------------- record done -------------------------
n2one setting etth2_electricity -> exchange
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth2_electricity_epochs_250_seed_2024/model.pkl', muti_dataset='etth2_electricity', random_seed=2024, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='exchange')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=5.60432e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.28631908357870334, 'MAE': 0.3620685503692353}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='etth2_traffic', random_seed=2024, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/etth2_traffic_epochs_250_seed_2024
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=1.0640969403662532
Epoch #1: loss=0.353426974648299
Epoch #2: loss=0.2581809549597794
Epoch #3: loss=0.20994132228187093
Epoch #4: loss=0.1450815738336332
Epoch #5: loss=0.12631070886934723
Epoch #6: loss=0.09854895978848803
Epoch #7: loss=0.09785174914090214
Epoch #8: loss=0.08272380526397287
Epoch #9: loss=0.06940487898367492
Epoch #10: loss=0.06500724109449135
Epoch #11: loss=0.056958897238097855
Epoch #12: loss=0.05394170106891264
Epoch #13: loss=0.054174873592617086
Epoch #14: loss=0.03973550607168445
Epoch #15: loss=0.04024432058514699
Epoch #16: loss=0.05238555376503672
Epoch #17: loss=0.0383877061536242
Epoch #18: loss=0.033528651503612415
Epoch #19: loss=0.031622425386316697
Epoch #20: loss=0.04281206899354721
Epoch #21: loss=0.026733998303951092
Epoch #22: loss=0.031040341359288813
Epoch #23: loss=0.026853477866273647
Epoch #24: loss=0.03248068953862133
Epoch #25: loss=0.02977459772433164
Epoch #26: loss=0.0280508687722382
Epoch #27: loss=0.02553713864999715
Epoch #28: loss=0.032099170417131555
Epoch #29: loss=0.024574019953392028
Epoch #30: loss=0.02132743670309329
Epoch #31: loss=0.026984174843685718
Epoch #32: loss=0.02218193969953488
Epoch #33: loss=0.02410077992186349
Epoch #34: loss=0.027236937704389987
Epoch #35: loss=0.025798096787184477
Epoch #36: loss=0.024844256318919147
Epoch #37: loss=0.01742631250280261
Epoch #38: loss=0.022407648105911646
Epoch #39: loss=0.025039270214971703
Epoch #40: loss=0.0215750035052311
Epoch #41: loss=0.01829086690195338
Epoch #42: loss=0.0215645265280268
Epoch #43: loss=0.016527895936506647
Epoch #44: loss=0.02195862441834639
Epoch #45: loss=0.021168647583488857
Epoch #46: loss=0.01667093149637128
Epoch #47: loss=0.023028589275832234
Epoch #48: loss=0.021461473915672897
Epoch #49: loss=0.024553659350463372
Epoch #50: loss=0.01974196477129496
Epoch #51: loss=0.01701235968937885
Epoch #52: loss=0.017806936739914448
Epoch #53: loss=0.013828160932705008
Epoch #54: loss=0.022008193709316337
Epoch #55: loss=0.025469132334219508
Epoch #56: loss=0.014118151643187004
Epoch #57: loss=0.014928862632153678
Epoch #58: loss=0.02303643714702475
Epoch #59: loss=0.013151783738550553
Epoch #60: loss=0.015617054094245896
Epoch #61: loss=0.019169284775940234
Epoch #62: loss=0.01718780369372177
Epoch #63: loss=0.010793810577283992
Epoch #64: loss=0.016386371736782597
Epoch #65: loss=0.012088041515023929
Epoch #66: loss=0.025302312891609897
Epoch #67: loss=0.03767089893458941
Epoch #68: loss=0.016706500659504922
Epoch #69: loss=0.015139442211165982
Epoch #70: loss=0.017344453443397453
Epoch #71: loss=0.01167331144010184
Epoch #72: loss=0.01527818587925596
Epoch #73: loss=0.01991658926984591
Epoch #74: loss=0.011271739071691881
Epoch #75: loss=0.015781128795866257
Epoch #76: loss=0.018147726824841493
Epoch #77: loss=0.012022037771308215
Epoch #78: loss=0.014806471305944334
Epoch #79: loss=0.024455051796143216
Epoch #80: loss=0.012340704831685893
Epoch #81: loss=0.01238937647565792
Epoch #82: loss=0.014278974886905934
Epoch #83: loss=0.017818315058025735
Epoch #84: loss=0.011439743787221722
Epoch #85: loss=0.01855712965332021
Epoch #86: loss=0.013281209170346774
Epoch #87: loss=0.01868779275461043
Epoch #88: loss=0.018856533173076763
Epoch #89: loss=0.016727196676878987
Epoch #90: loss=0.013947477269225397
Epoch #91: loss=0.017346929819573635
Epoch #92: loss=0.01224783831073064
Epoch #93: loss=0.01131889100895334
Epoch #94: loss=0.015456611505587413
Epoch #95: loss=0.014920102695644307
Epoch #96: loss=0.018373973362450034
Epoch #97: loss=0.015857422390358707
Epoch #98: loss=0.03530962270518692
Epoch #99: loss=0.021177812560886935
Epoch #100: loss=0.014299471768202534
Epoch #101: loss=0.01586077828358798
Epoch #102: loss=0.015157434030526272
Epoch #103: loss=0.013581837427147832
Epoch #104: loss=0.010516572139239028
Epoch #105: loss=0.012135269679018602
Epoch #106: loss=0.014747376957550854
Epoch #107: loss=0.009202312113270024
Epoch #108: loss=0.01531191312512908
Epoch #109: loss=0.011589120971436813
Epoch #110: loss=0.012439421822671414
Epoch #111: loss=0.014841043443718584
Epoch #112: loss=0.010690106778546002
Epoch #113: loss=0.021349013072555892
Epoch #114: loss=0.011088983971196611
Epoch #115: loss=0.014167537887440566
Epoch #116: loss=0.012942251670230566
Epoch #117: loss=0.016188877740117583
Epoch #118: loss=0.010838113377245585
Epoch #119: loss=0.0230867628251809
Epoch #120: loss=0.012685030778384766
Epoch #121: loss=0.012057997146728519
Epoch #122: loss=0.021541670554603414
Epoch #123: loss=0.013796895317011486
Epoch #124: loss=0.012809929927944724
Epoch #125: loss=0.010859844449039986
Epoch #126: loss=0.011214093172813416
Epoch #127: loss=0.010125571339635256
Epoch #128: loss=0.011646496658263004
Epoch #129: loss=0.00984693791159194
Epoch #130: loss=0.013622749058932432
Epoch #131: loss=0.010263826184343681
Epoch #132: loss=0.01245899273921099
Epoch #133: loss=0.01644976352640482
Epoch #134: loss=0.010014553513865269
Epoch #135: loss=0.009930663969345907
Epoch #136: loss=0.012970052338632625
Epoch #137: loss=0.015488844059073604
Epoch #138: loss=0.008525702198590897
Epoch #139: loss=0.013182503598210615
Epoch #140: loss=0.011886101903785577
Epoch #141: loss=0.019999952547256398
Epoch #142: loss=0.010038045408194552
Epoch #143: loss=0.009843187646991793
Epoch #144: loss=0.01132126837590778
Epoch #145: loss=0.00947188735420851
Epoch #146: loss=0.012571034873456623
Epoch #147: loss=0.010499415913424202
Epoch #148: loss=0.011246182786403127
Epoch #149: loss=0.009450609754309338
Epoch #150: loss=0.012909036501898908
Epoch #151: loss=0.011592768182504737
Epoch #152: loss=0.019633235783181963
Epoch #153: loss=0.010187324204733858
Epoch #154: loss=0.01035488891062033
Epoch #155: loss=0.00956053809889499
Epoch #156: loss=0.009451953211415589
Epoch #157: loss=0.009912574440003742
Epoch #158: loss=0.010278355527659279
Epoch #159: loss=0.012506921095550527
Epoch #160: loss=0.013741052883874916
Epoch #161: loss=0.01026580936312397
Epoch #162: loss=0.009846757517862176
Epoch #163: loss=0.010539709565859885
Epoch #164: loss=0.01817395275353397
Epoch #165: loss=0.012362300148411944
Epoch #166: loss=0.009213386504132283
Epoch #167: loss=0.009583432693143257
Epoch #168: loss=0.009149372564572187
Epoch #169: loss=0.011383207754544931
Epoch #170: loss=0.009779220025302912
Epoch #171: loss=0.008522979365267214
Epoch #172: loss=0.009827542383258201
Epoch #173: loss=0.008870982955007222
Epoch #174: loss=0.009789291755710576
Epoch #175: loss=0.01144470525431255
Epoch #176: loss=0.010679910269027879
Epoch #177: loss=0.01320444218953462
Epoch #178: loss=0.007210384546071093
Epoch #179: loss=0.010664486078074163
Epoch #180: loss=0.02401553696531575
Epoch #181: loss=0.011896157620526407
Epoch #182: loss=0.011116478446604721
Epoch #183: loss=0.00973054388867824
Epoch #184: loss=0.008150705852667974
Epoch #185: loss=0.011675035775208018
Epoch #186: loss=0.01208426171579635
Epoch #187: loss=0.007114563331056051
Epoch #188: loss=0.009444252253051965
Epoch #189: loss=0.01098732534080695
Epoch #190: loss=0.017227001147168144
Epoch #191: loss=0.012307808012473831
Epoch #192: loss=0.010222768631094066
Epoch #193: loss=0.012881489364819207
Epoch #194: loss=0.008499086286390487
Epoch #195: loss=0.00902622201645016
Epoch #196: loss=0.011899572341606155
Epoch #197: loss=0.006359317778239013
Epoch #198: loss=0.012766781340920397
Epoch #199: loss=0.013047169567491313
Epoch #200: loss=0.01008824519463413
Epoch #201: loss=0.012018242388621017
Epoch #202: loss=0.008513541488451546
Epoch #203: loss=0.008415447226717481
Epoch #204: loss=0.014089525849559083
Epoch #205: loss=0.011606758749699252
Epoch #206: loss=0.008351294288299518
Epoch #207: loss=0.007522567163573329
Epoch #208: loss=0.010299361072015085
Epoch #209: loss=0.007463974486236363
Epoch #210: loss=0.01052445379312976
Epoch #211: loss=0.01448264913989142
Epoch #212: loss=0.007934254997782837
Epoch #213: loss=0.02190323078779354
Epoch #214: loss=0.009263565518826454
Epoch #215: loss=0.0070103600511154706
Epoch #216: loss=0.00554616743329409
Epoch #217: loss=0.010671842909686211
Epoch #218: loss=0.008420167652459993
Epoch #219: loss=0.019279009707781656
Epoch #220: loss=0.013548642247455769
Epoch #221: loss=0.007510992192353258
Epoch #222: loss=0.0114409055891921
Epoch #223: loss=0.02578037182484784
Epoch #224: loss=0.009589791900736989
Epoch #225: loss=0.017481611760668626
Epoch #226: loss=0.01361980731143553
Epoch #227: loss=0.012400234870650667
Epoch #228: loss=0.007682325889061578
Epoch #229: loss=0.00983337345829093
Epoch #230: loss=0.0079815342606855
Epoch #231: loss=0.012750014084450508
Epoch #232: loss=0.009362427357315505
Epoch #233: loss=0.011256249432419336
Epoch #234: loss=0.010360585364690067
Epoch #235: loss=0.010775964418140811
Epoch #236: loss=0.014481239670352267
Epoch #237: loss=0.007177222970761859
Epoch #238: loss=0.00973981499960764
Epoch #239: loss=0.008958108854649954
Epoch #240: loss=0.011446052404311865
Epoch #241: loss=0.010082867760619486
Epoch #242: loss=0.006758877397271734
Epoch #243: loss=0.00807870787835516
Epoch #244: loss=0.01188945500195866
Epoch #245: loss=0.009290945372164171
Epoch #246: loss=0.010339530215835962
Epoch #247: loss=0.007925106994833843
Epoch #248: loss=0.011628475627967481
Epoch #249: loss=0.0073097489383519105

Training time: 10:26:43.525005

Finished.
n2one setting etth2_traffic -> electricity
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth2_traffic_epochs_250_seed_2024/model.pkl', muti_dataset='etth2_traffic', random_seed=2024, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='electricity')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=7.48136e-09): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.65681e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=4.39931e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=7.48136e-09): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.3944820644616954, 'MAE': 0.4456536589751519}
Finished.
------------------------- record done -------------------------
n2one setting etth2_traffic -> weather
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth2_traffic_epochs_250_seed_2024/model.pkl', muti_dataset='etth2_traffic', random_seed=2024, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='weather')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.28007e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.60044e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.28007e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.6649045508577752, 'MAE': 0.666119841265118}
Finished.
------------------------- record done -------------------------
n2one setting etth2_traffic -> exchange
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth2_traffic_epochs_250_seed_2024/model.pkl', muti_dataset='etth2_traffic', random_seed=2024, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='exchange')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=4.89717e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.2553259669068872, 'MAE': 0.33471637718489616}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='etth2_weather', random_seed=2024, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/etth2_weather_epochs_250_seed_2024
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=9.946447076219501
Epoch #1: loss=3.9134784106052285
Epoch #2: loss=2.7318565556497285
Epoch #3: loss=2.8040507995721065
Epoch #4: loss=2.4239629326444683
Epoch #5: loss=2.353972518082821
Epoch #6: loss=2.1302737683960884
Epoch #7: loss=2.0995123494755137
Epoch #8: loss=1.979620792649009
Epoch #9: loss=1.876634261824868
Epoch #10: loss=1.8595683719172622
Epoch #11: loss=1.7273204507249775
Epoch #12: loss=1.6533967328794075
Epoch #13: loss=1.5825472889524517
Epoch #14: loss=1.4678979606339426
Epoch #15: loss=1.313970107020754
Epoch #16: loss=1.2962494077104512
Epoch #17: loss=1.2470445253632285
Epoch #18: loss=1.2481322884559631
Epoch #19: loss=1.129426775556622
Epoch #20: loss=1.1653314052206096
Epoch #21: loss=1.225481436108098
Epoch #22: loss=1.1308285666234565
Epoch #23: loss=1.0513428377382683
Epoch #24: loss=0.9722109274430708
Epoch #25: loss=1.1227084904006033
Epoch #26: loss=1.0457166324962268
Epoch #27: loss=0.9224205143523939
Epoch #28: loss=0.9879565455696799
Epoch #29: loss=0.9503738446669145
Epoch #30: loss=0.880721699107777
Epoch #31: loss=0.8761435313658281
Epoch #32: loss=0.7788061806649873
Epoch #33: loss=0.7528092355439158
Epoch #34: loss=0.8064288677591266
Epoch #35: loss=0.7380537246212815
Epoch #36: loss=0.723054006244197
Epoch #37: loss=0.7737449226957379
Epoch #38: loss=0.7647919763218273
Epoch #39: loss=0.7147965684081569
Epoch #40: loss=0.7655271815531182
Epoch #41: loss=0.8205774354212212
Epoch #42: loss=0.902091094941804
Epoch #43: loss=0.7974703113238016
Epoch #44: loss=0.6422546036315687
Epoch #45: loss=0.8655139551018224
Epoch #46: loss=0.8742274226564349
Epoch #47: loss=0.7678825421766802
Epoch #48: loss=0.8137421156420852
Epoch #49: loss=0.5827508785507896
Epoch #50: loss=0.6219549486131379
Epoch #51: loss=0.6173438520142527
Epoch #52: loss=0.6623963668490901
Epoch #53: loss=0.5218256937735009
Epoch #54: loss=0.515902396404382
Epoch #55: loss=0.5242336401433656
Epoch #56: loss=0.5364129154971151
Epoch #57: loss=0.544441925756859
Epoch #58: loss=0.5434895519054297
Epoch #59: loss=0.5205024209889498
Epoch #60: loss=0.5584532663677678
Epoch #61: loss=0.5336273177103563
Epoch #62: loss=0.4907997783386346
Epoch #63: loss=0.5608664019541307
Epoch #64: loss=0.521773838635647
Epoch #65: loss=0.4662829375628269
Epoch #66: loss=0.4728296092062285
Epoch #67: loss=0.4628505968686306
Epoch #68: loss=0.4437174020391522
Epoch #69: loss=0.40828855561487604
Epoch #70: loss=0.5047659142450853
Epoch #71: loss=0.47447789257222955
Epoch #72: loss=0.3990145221804128
Epoch #73: loss=0.5105564151749467
Epoch #74: loss=0.4281257345820918
Epoch #75: loss=0.4396885507034533
Epoch #76: loss=0.3923099826682698
Epoch #77: loss=0.3523508337411014
Epoch #78: loss=0.4461480929996028
Epoch #79: loss=0.37303897738456726
Epoch #80: loss=0.3524262390353463
Epoch #81: loss=0.372285728653272
Epoch #82: loss=0.32740114538958576
Epoch #83: loss=0.35535757785493677
Epoch #84: loss=0.32234419159816974
Epoch #85: loss=0.331851271517349
Epoch #86: loss=0.3851166855205189
Epoch #87: loss=0.3366950045932423
Epoch #88: loss=0.3926802896188967
Epoch #89: loss=0.4341982417937481
Epoch #90: loss=0.33624435283920984
Epoch #91: loss=0.3666802929206328
Epoch #92: loss=0.32056690910548874
Epoch #93: loss=0.3193600435148586
Epoch #94: loss=0.30194661382472876
Epoch #95: loss=0.2877331403168765
Epoch #96: loss=0.28067060763185675
Epoch #97: loss=0.3137189806862311
Epoch #98: loss=0.27690719790530927
Epoch #99: loss=0.3623768093459534
Epoch #100: loss=0.33413042602213944
Epoch #101: loss=0.33799993043596094
Epoch #102: loss=0.33174158452135144
Epoch #103: loss=0.2967807357058381
Epoch #104: loss=0.276061304816694
Epoch #105: loss=0.3558695711421244
Epoch #106: loss=0.34218673633806634
Epoch #107: loss=0.3952836814251813
Epoch #108: loss=0.3262242882540732
Epoch #109: loss=0.6143731118151636
Epoch #110: loss=0.29658655641656934
Epoch #111: loss=0.2890046744635611
Epoch #112: loss=0.24609398593505225
Epoch #113: loss=0.2830318142518853
Epoch #114: loss=0.2376522460218632
Epoch #115: loss=0.2002562756339709
Epoch #116: loss=0.21625811177672763
Epoch #117: loss=0.20444289651332478
Epoch #118: loss=0.1587559659824227
Epoch #119: loss=0.19220249862833458
Epoch #120: loss=0.20962608899130966
Epoch #121: loss=0.19940955530513416
Epoch #122: loss=0.14605896095886375
Epoch #123: loss=0.16353236770991122
Epoch #124: loss=0.2772768179349827
Epoch #125: loss=0.2419429642684532
Epoch #126: loss=0.1671545569869605
Epoch #127: loss=0.15409565141255205
Epoch #128: loss=0.16921065725160367
Epoch #129: loss=0.20489752224900507
Epoch #130: loss=0.18621889467943797
Epoch #131: loss=0.16223608973351392
Epoch #132: loss=0.17289796110355493
Epoch #133: loss=0.17510403941075006
Epoch #134: loss=0.28615105784300604
Epoch #135: loss=0.1995426215457194
Epoch #136: loss=0.20428880428274473
Epoch #137: loss=0.23988651156877028
Epoch #138: loss=0.16736038480744217
Epoch #139: loss=0.1684109131720933
Epoch #140: loss=0.21130595457824794
Epoch #141: loss=0.20851722732186317
Epoch #142: loss=0.1407998665941484
Epoch #143: loss=0.15743490331100696
Epoch #144: loss=0.13075506280769
Epoch #145: loss=0.15156373855742541
Epoch #146: loss=0.1623188235768766
Epoch #147: loss=0.15903628436904965
Epoch #148: loss=0.15296714064298253
Epoch #149: loss=0.207596388955911
Epoch #150: loss=0.1625843531254566
Epoch #151: loss=0.2102032762133714
Epoch #152: loss=0.17659524895928122
Epoch #153: loss=0.12903210441723015
Epoch #154: loss=0.13894138451326976
Epoch #155: loss=0.20202427041349988
Epoch #156: loss=0.18149251095724828
Epoch #157: loss=0.1220148518455751
Epoch #158: loss=0.1889951109434619
Epoch #159: loss=0.12954829768701034
Epoch #160: loss=0.13372435465906607
Epoch #161: loss=0.12527229377266133
Epoch #162: loss=0.12144044944734285
Epoch #163: loss=0.15609161025195412
Epoch #164: loss=0.15267616487813718
Epoch #165: loss=0.18450525109515045
Epoch #166: loss=0.1436970421310627
Epoch #167: loss=0.12128063330821919
Epoch #168: loss=0.11684173830982411
Epoch #169: loss=0.16471036365537933
Epoch #170: loss=0.26879243620417337
Epoch #171: loss=0.13747829808430237
Epoch #172: loss=0.14784752103415402
Epoch #173: loss=0.1702564296623071
Epoch #174: loss=0.11781939113456191
Epoch #175: loss=0.15345070923142362
Epoch #176: loss=0.15096640733606886
Epoch #177: loss=0.14264444464986975
Epoch #178: loss=0.10306160202757879
Epoch #179: loss=0.12115500466615865
Epoch #180: loss=0.12401934351884958
Epoch #181: loss=0.1483685519320495
Epoch #182: loss=0.12346160739208713
Epoch #183: loss=0.13867381107852314
Epoch #184: loss=0.11430680018031236
Epoch #185: loss=0.12261025201190602
Epoch #186: loss=0.1345198540525003
Epoch #187: loss=0.11212275978742224
Epoch #188: loss=0.1474372697147456
Epoch #189: loss=0.12388592089215915
Epoch #190: loss=0.130228283786864
Epoch #191: loss=0.1574746719138189
Epoch #192: loss=0.21117129176855087
Epoch #193: loss=0.1292419137828278
Epoch #194: loss=0.1409338057379831
Epoch #195: loss=0.14367347577530326
Epoch #196: loss=0.11979429367365259
Epoch #197: loss=0.16458000699904832
Epoch #198: loss=0.13480566430724028
Epoch #199: loss=0.10520135955602834
Epoch #200: loss=0.09654526610040304
Epoch #201: loss=0.10188184926907222
Epoch #202: loss=0.10549114994478948
Epoch #203: loss=0.15555515467669023
Epoch #204: loss=0.11840576593848792
Epoch #205: loss=0.10137888986730215
Epoch #206: loss=0.12837924198670822
Epoch #207: loss=0.17108931200522365
Epoch #208: loss=0.11847573992880908
Epoch #209: loss=0.10974721864543178
Epoch #210: loss=0.10755953441063563
Epoch #211: loss=0.09708607907999646
Epoch #212: loss=0.10294184636211756
Epoch #213: loss=0.26953808110997535
Epoch #214: loss=0.31309161696470145
Epoch #215: loss=0.25667364872766263
Epoch #216: loss=0.24066012104352316
Epoch #217: loss=0.19992560616722613
Epoch #218: loss=0.16622574649976962
Epoch #219: loss=0.1535751051975019
Epoch #220: loss=0.14253394237973474
Epoch #221: loss=0.12872855015324824
Epoch #222: loss=0.11583209294600016
Epoch #223: loss=0.08996319821612402
Epoch #224: loss=0.08799999483832807
Epoch #225: loss=0.09345309995114803
Epoch #226: loss=0.11974274852510655
Epoch #227: loss=0.09932341706007719
Epoch #228: loss=0.11182016348748496
Epoch #229: loss=0.10554173916126743
Epoch #230: loss=0.12268796975188183
Epoch #231: loss=0.10828655026853085
Epoch #232: loss=0.0793997539828221
Epoch #233: loss=0.06811787617025954
Epoch #234: loss=0.06751381224867972
Epoch #235: loss=0.09342847832224586
Epoch #236: loss=0.09824721989306537
Epoch #237: loss=0.10576997663487088
Epoch #238: loss=0.08223284510048953
Epoch #239: loss=0.10696168272106936
Epoch #240: loss=0.11558865019204942
Epoch #241: loss=0.12978613794301497
Epoch #242: loss=0.08242604497707251
Epoch #243: loss=0.0778839507732879
Epoch #244: loss=0.07041989961131052
Epoch #245: loss=0.10809568829382911
Epoch #246: loss=0.09515005804485444
Epoch #247: loss=0.0659799969501116
Epoch #248: loss=0.07791018878307307
Epoch #249: loss=0.06538758985698223

Training time: 0:32:18.965581

Finished.
n2one setting etth2_weather -> electricity
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth2_weather_epochs_250_seed_2024/model.pkl', muti_dataset='etth2_weather', random_seed=2024, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='electricity')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.40432e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.7672e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.40432e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.35598757479626225, 'MAE': 0.4184060858256875}
Finished.
------------------------- record done -------------------------
n2one setting etth2_weather -> exchange
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth2_weather_epochs_250_seed_2024/model.pkl', muti_dataset='etth2_weather', random_seed=2024, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='exchange')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=3.79232e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.2168336356606581, 'MAE': 0.3194198015096451}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='etth2_exchange', random_seed=2024, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/etth2_exchange_epochs_250_seed_2024
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=4.642198880513509
Epoch #1: loss=2.4107879241307577
Epoch #2: loss=2.1988581736882526
Epoch #3: loss=1.8939332962036133
Epoch #4: loss=1.9146194060643513
Epoch #5: loss=1.7129884243011475
Epoch #6: loss=1.6529401222864786
Epoch #7: loss=1.6660158236821492
Epoch #8: loss=1.6293955326080323
Epoch #9: loss=1.5342830896377564
Epoch #10: loss=1.4839500506718954
Epoch #11: loss=1.4521188338597615
Epoch #12: loss=1.375536855061849
Epoch #13: loss=1.3690825541814169
Epoch #14: loss=1.344885230064392
Epoch #15: loss=1.2429496367772421
Epoch #16: loss=1.2400654554367065
Epoch #17: loss=1.2201265176137288
Epoch #18: loss=1.1687627196311952
Epoch #19: loss=1.1717154701550803
Epoch #20: loss=1.1058608531951903
Epoch #21: loss=1.0284752011299134
Epoch #22: loss=1.0551567514737448
Epoch #23: loss=1.016061496734619
Epoch #24: loss=0.9879302382469177
Epoch #25: loss=1.1207147717475892
Epoch #26: loss=1.1115533828735351
Epoch #27: loss=0.9571991840998332
Epoch #28: loss=0.9488158067067464
Epoch #29: loss=1.2172977566719054
Epoch #30: loss=1.0698830684026082
Epoch #31: loss=1.0376436630884807
Epoch #32: loss=0.9342577656110128
Epoch #33: loss=0.826270314057668
Epoch #34: loss=0.8622581283251445
Epoch #35: loss=0.8873335440953573
Epoch #36: loss=0.8671046098073324
Epoch #37: loss=0.8619192997614543
Epoch #38: loss=0.7784912983576456
Epoch #39: loss=0.8344760855038961
Epoch #40: loss=0.8044716437657674
Epoch #41: loss=0.8071205457051595
Epoch #42: loss=0.7348260700702667
Epoch #43: loss=0.7106129924456278
Epoch #44: loss=0.7670405666033427
Epoch #45: loss=0.7394556681315104
Epoch #46: loss=0.7009587327639262
Epoch #47: loss=0.8628446797529856
Epoch #48: loss=0.6323110938072205
Epoch #49: loss=0.6482790768146515
Epoch #50: loss=0.6738723536332448
Epoch #51: loss=0.7521907806396484
Epoch #52: loss=0.6204955478509268
Epoch #53: loss=0.5848799745241801
Epoch #54: loss=0.6532794634501139
Epoch #55: loss=0.6422127803166707
Epoch #56: loss=0.6852014362812042
Epoch #57: loss=0.6518940468629201
Epoch #58: loss=0.7473388731479644
Epoch #59: loss=0.6368996997674307
Epoch #60: loss=0.6907127539316813
Epoch #61: loss=0.5397984981536865
Epoch #62: loss=0.5261101841926574
Epoch #63: loss=0.4848343094189962
Epoch #64: loss=0.5522660026947658
Epoch #65: loss=0.49279196560382843
Epoch #66: loss=0.566234028339386
Epoch #67: loss=0.6067749838034312
Epoch #68: loss=0.6571238239606222
Epoch #69: loss=0.45747059881687163
Epoch #70: loss=0.4726057529449463
Epoch #71: loss=0.4038989295562108
Epoch #72: loss=0.45331961909929913
Epoch #73: loss=0.4552918632825216
Epoch #74: loss=0.6322369108597438
Epoch #75: loss=0.550165730714798
Epoch #76: loss=0.46406564911206566
Epoch #77: loss=0.5126313745975495
Epoch #78: loss=0.5097179740667344
Epoch #79: loss=0.5710565268993377
Epoch #80: loss=0.496320112546285
Epoch #81: loss=0.5128475646177928
Epoch #82: loss=0.4614992916584015
Epoch #83: loss=0.39607226153214775
Epoch #84: loss=0.43416336675484973
Epoch #85: loss=0.4000757177670797
Epoch #86: loss=0.40814796487490335
Epoch #87: loss=0.33184989194075265
Epoch #88: loss=0.3562471995751063
Epoch #89: loss=0.33839234511057537
Epoch #90: loss=0.39475557108720144
Epoch #91: loss=0.488019722700119
Epoch #92: loss=0.4662288149197896
Epoch #93: loss=0.4787120044231415
Epoch #94: loss=0.4748603363831838
Epoch #95: loss=0.36050537625948587
Epoch #96: loss=0.352621990442276
Epoch #97: loss=0.35840294261773425
Epoch #98: loss=0.38950755099455514
Epoch #99: loss=0.3831992795070012
Epoch #100: loss=0.39290280838807423
Epoch #101: loss=0.3952350248893102
Epoch #102: loss=0.4754403779904048
Epoch #103: loss=0.3318067823847135
Epoch #104: loss=0.44189145068327584
Epoch #105: loss=0.3216005166371663
Epoch #106: loss=0.3201009194056193
Epoch #107: loss=0.37709684173266095
Epoch #108: loss=0.30893990993499754
Epoch #109: loss=0.350249050060908
Epoch #110: loss=0.32037649552027386
Epoch #111: loss=0.32866949737071993
Epoch #112: loss=0.35511734187602995
Epoch #113: loss=0.4361238360404968
Epoch #114: loss=0.4965215961138407
Epoch #115: loss=0.3776559094587962
Epoch #116: loss=0.333537479241689
Epoch #117: loss=0.37521603852510454
Epoch #118: loss=0.40427252848943074
Epoch #119: loss=0.3114825566609701
Epoch #120: loss=0.38133760591348015
Epoch #121: loss=0.4215990970532099
Epoch #122: loss=0.3049291968345642
Epoch #123: loss=0.319295634329319
Epoch #124: loss=0.1955209051569303
Epoch #125: loss=0.2717101022601128
Epoch #126: loss=0.29789604942003883
Epoch #127: loss=0.3199104900161425
Epoch #128: loss=0.3725736290216446
Epoch #129: loss=0.27407506803671516
Epoch #130: loss=0.30284762730201087
Epoch #131: loss=0.43070614486932757
Epoch #132: loss=0.3167581190665563
Epoch #133: loss=0.32798702518145245
Epoch #134: loss=0.3685093735655149
Epoch #135: loss=0.28899654050668083
Epoch #136: loss=0.4157464096943537
Epoch #137: loss=0.33589436213175455
Epoch #138: loss=0.24952905972798664
Epoch #139: loss=0.28417581965525945
Epoch #140: loss=0.31134897222121555
Epoch #141: loss=0.32186227639516196
Epoch #142: loss=0.4220185299714406
Epoch #143: loss=0.33354881405830383
Epoch #144: loss=0.3811916599671046
Epoch #145: loss=0.2896443918347359
Epoch #146: loss=0.31084756503502525
Epoch #147: loss=0.43152335782845813
Epoch #148: loss=0.37336269319057463
Epoch #149: loss=0.3784341881672541
Epoch #150: loss=0.3409077207247416
Epoch #151: loss=0.3573191990454992
Epoch #152: loss=0.27924938102563224
Epoch #153: loss=0.2372716744740804
Epoch #154: loss=0.2362197905778885
Epoch #155: loss=0.26816653509934746
Epoch #156: loss=0.22406674275795618
Epoch #157: loss=0.25602954129378
Epoch #158: loss=0.2514500096440315
Epoch #159: loss=0.29707283626000086
Epoch #160: loss=0.2801093945900599
Epoch #161: loss=0.22399505029122035
Epoch #162: loss=0.22769675304492315
Epoch #163: loss=0.35306472281614937
Epoch #164: loss=0.3253644540905952
Epoch #165: loss=0.30331428597370785
Epoch #166: loss=0.3057061736782392
Epoch #167: loss=0.3428951109449069
Epoch #168: loss=0.33116655697425207
Epoch #169: loss=0.4476783047119776
Epoch #170: loss=0.35372637659311296
Epoch #171: loss=0.333752429485321
Epoch #172: loss=0.37273426751295724
Epoch #173: loss=0.3661862606803576
Epoch #174: loss=0.2699560989936193
Epoch #175: loss=0.24769192039966584
Epoch #176: loss=0.29178317288557687
Epoch #177: loss=0.24698868344227473
Epoch #178: loss=0.23219595849514008
Epoch #179: loss=0.22504131893316906
Epoch #180: loss=0.3001967278619607
Epoch #181: loss=0.2835186089078585
Epoch #182: loss=0.2821313485503197
Epoch #183: loss=0.27188873042662937
Epoch #184: loss=0.20485568592945735
Epoch #185: loss=0.22533979614575703
Epoch #186: loss=0.21128212188680967
Epoch #187: loss=0.22077231779694556
Epoch #188: loss=0.2264023979504903
Epoch #189: loss=0.2186095744371414
Epoch #190: loss=0.29006913950045904
Epoch #191: loss=0.29197506606578827
Epoch #192: loss=0.22604926725228627
Epoch #193: loss=0.18881530066331229
Epoch #194: loss=0.2664185881614685
Epoch #195: loss=0.23750756780306498
Epoch #196: loss=0.1859565022091071
Epoch #197: loss=0.21103337158759436
Epoch #198: loss=0.22840222467978796
Epoch #199: loss=0.19079486032327017
Epoch #200: loss=0.15577694028615952
Epoch #201: loss=0.25980160012841225
Epoch #202: loss=0.35871044248342515
Epoch #203: loss=0.38434736529986063
Epoch #204: loss=0.4170049697160721
Epoch #205: loss=0.29128102560838065
Epoch #206: loss=0.2638711924354235
Epoch #207: loss=0.26842682659626005
Epoch #208: loss=0.20391512562831243
Epoch #209: loss=0.24054851159453391
Epoch #210: loss=0.2135769342382749
Epoch #211: loss=0.2674289653698603
Epoch #212: loss=0.2141586830218633
Epoch #213: loss=0.18168088247378666
Epoch #214: loss=0.16915924822290737
Epoch #215: loss=0.20290543437004088
Epoch #216: loss=0.341155373553435
Epoch #217: loss=0.23886299679676692
Epoch #218: loss=0.18712551246086756
Epoch #219: loss=0.26065478622913363
Epoch #220: loss=0.19896907210350037
Epoch #221: loss=0.19407521883646647
Epoch #222: loss=0.22694751545786856
Epoch #223: loss=0.25294024099906287
Epoch #224: loss=0.17220524350802105
Epoch #225: loss=0.19927183364828427
Epoch #226: loss=0.2365399310986201
Epoch #227: loss=0.18475376665592194
Epoch #228: loss=0.18857902362942697
Epoch #229: loss=0.18611236910025278
Epoch #230: loss=0.20557672902941704
Epoch #231: loss=0.18098047723372776
Epoch #232: loss=0.17453334629535674
Epoch #233: loss=0.17031418457627295
Epoch #234: loss=0.1505895676712195
Epoch #235: loss=0.20437884430090586
Epoch #236: loss=0.15150931378205618
Epoch #237: loss=0.2984943504134814
Epoch #238: loss=0.23607725352048875
Epoch #239: loss=0.24317208975553511
Epoch #240: loss=0.25469371577103933
Epoch #241: loss=0.35434168179829917
Epoch #242: loss=0.3827702964345614
Epoch #243: loss=0.2923924873272578
Epoch #244: loss=0.2268971453110377
Epoch #245: loss=0.17011895552277564
Epoch #246: loss=0.21916256075104076
Epoch #247: loss=0.16479701399803162
Epoch #248: loss=0.21609765986601512
Epoch #249: loss=0.2208164267241955

Training time: 0:10:09.351963

Finished.
n2one setting etth2_exchange -> electricity
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth2_exchange_epochs_250_seed_2024/model.pkl', muti_dataset='etth2_exchange', random_seed=2024, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='electricity')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.18342e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.43027e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=5.88139e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.18342e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.3631799728983448, 'MAE': 0.4277878543957728}
Finished.
------------------------- record done -------------------------
n2one setting etth2_exchange -> weather
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/etth2_exchange_epochs_250_seed_2024/model.pkl', muti_dataset='etth2_exchange', random_seed=2024, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='weather')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.07287e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.11282e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=5.12483e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.11282e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.4102445729664794, 'MAE': 0.4627864149190188}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='ettm1_ettm2', random_seed=2024, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/ettm1_ettm2_epochs_250_seed_2024
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=5.2984255010431465
Epoch #1: loss=2.6910452192479912
Epoch #2: loss=2.3014667359265415
Epoch #3: loss=2.0195070288398047
Epoch #4: loss=1.8212192762981763
Epoch #5: loss=1.663049199364402
Epoch #6: loss=1.6040214679457925
Epoch #7: loss=1.4395965879613704
Epoch #8: loss=1.321139227260243
Epoch #9: loss=1.2914463010701267
Epoch #10: loss=1.1842726198109714
Epoch #11: loss=1.1062699502164668
Epoch #12: loss=1.0403108921918003
Epoch #13: loss=1.1363502551208844
Epoch #14: loss=0.9810404885898937
Epoch #15: loss=0.9923170452768152
Epoch #16: loss=0.85204909064553
Epoch #17: loss=0.8390347009355371
Epoch #18: loss=0.7194185554981232
Epoch #19: loss=0.8305727270516482
Epoch #20: loss=0.678548661145297
Epoch #21: loss=0.6643384695053101
Epoch #22: loss=0.7072568888014014
Epoch #23: loss=0.6881679147481918
Epoch #24: loss=0.6267376474358819
Epoch #25: loss=0.5979905941269614
Epoch #26: loss=0.5698856128887697
Epoch #27: loss=0.6059199571609497
Epoch #28: loss=0.6407179588621313
Epoch #29: loss=0.6064871183850549
Epoch #30: loss=0.5937891941178929
Epoch #31: loss=0.6319371800531041
Epoch #32: loss=0.5439913327043707
Epoch #33: loss=0.5398915762251074
Epoch #34: loss=0.5523991666056893
Epoch #35: loss=0.45983481813560834
Epoch #36: loss=0.4778393534096805
Epoch #37: loss=0.4473289779641412
Epoch #38: loss=0.43249681998382916
Epoch #39: loss=0.40009714933958923
Epoch #40: loss=0.4053867350925099
Epoch #41: loss=0.404433937235312
Epoch #42: loss=0.6194534383036874
Epoch #43: loss=0.5658662251450799
Epoch #44: loss=0.4607725007967515
Epoch #45: loss=0.38788825544444
Epoch #46: loss=0.45259344984184613
Epoch #47: loss=0.4023772092028098
Epoch #48: loss=0.5224054577675733
Epoch #49: loss=0.411307383667339
Epoch #50: loss=0.3968884430148385
Epoch #51: loss=0.37095476212826645
Epoch #52: loss=0.40852117809382354
Epoch #53: loss=0.3242009017955173
Epoch #54: loss=0.2910095764832063
Epoch #55: loss=0.4147280976176262
Epoch #56: loss=0.43505124612288043
Epoch #57: loss=0.3503852608529004
Epoch #58: loss=0.30871607295491477
Epoch #59: loss=0.27131169831210916
Epoch #60: loss=0.24288939820094543
Epoch #61: loss=0.324877684766596
Epoch #62: loss=0.23589895326982846
Epoch #63: loss=0.20874254270033402
Epoch #64: loss=0.22115359929474918
Epoch #65: loss=0.18512026457624
Epoch #66: loss=0.2889700267802585
Epoch #67: loss=0.35011276602745056
Epoch #68: loss=0.27147128839384427
Epoch #69: loss=0.22267453846606342
Epoch #70: loss=0.2189012826843695
Epoch #71: loss=0.21148148314519363
Epoch #72: loss=0.24435303258624944
Epoch #73: loss=0.1807012530890378
Epoch #74: loss=0.22958532043478705
Epoch #75: loss=0.2396102479913018
Epoch #76: loss=0.2443827637894587
Epoch #77: loss=0.24770045585253023
Epoch #78: loss=0.19891216334971515
Epoch #79: loss=0.2487803694199432
Epoch #80: loss=0.15023881061510605
Epoch #81: loss=0.2098594792187214
Epoch #82: loss=0.19675316018137065
Epoch #83: loss=0.18562265011397275
Epoch #84: loss=0.13598726046356288
Epoch #85: loss=0.20414774424650453
Epoch #86: loss=0.16758014972914348
Epoch #87: loss=0.2185379605401646
Epoch #88: loss=0.1331326954744079
Epoch #89: loss=0.1808925951746377
Epoch #90: loss=0.26943290419876575
Epoch #91: loss=0.1948956630446694
Epoch #92: loss=0.16115525195544417
Epoch #93: loss=0.12790459868582812
Epoch #94: loss=0.13677102818407796
Epoch #95: loss=0.12185587920248508
Epoch #96: loss=0.2081951918927106
Epoch #97: loss=0.157273530960083
Epoch #98: loss=0.18327291546897453
Epoch #99: loss=0.17086055874824524
Epoch #100: loss=0.1452835351228714
Epoch #101: loss=0.12299034185707569
Epoch #102: loss=0.11316543563523075
Epoch #103: loss=0.105427018112757
Epoch #104: loss=0.09711187519133091
Epoch #105: loss=0.1002781040627848
Epoch #106: loss=0.38565661978315224
Epoch #107: loss=0.21759181740609082
Epoch #108: loss=0.15563033724373038
Epoch #109: loss=0.17279209958558733
Epoch #110: loss=0.0910896933214231
Epoch #111: loss=0.11308095130053433
Epoch #112: loss=0.14964734102514657
Epoch #113: loss=0.101457168771462
Epoch #114: loss=0.10026807270266792
Epoch #115: loss=0.09040470709177581
Epoch #116: loss=0.085531883449717
Epoch #117: loss=0.102469754320654
Epoch #118: loss=0.10713485814630985
Epoch #119: loss=0.11787198585542766
Epoch #120: loss=0.1435059141367674
Epoch #121: loss=0.09303822554647923
Epoch #122: loss=0.10670204960148442
Epoch #123: loss=0.18768476085229355
Epoch #124: loss=0.09481614049185406
Epoch #125: loss=0.20156713253395123
Epoch #126: loss=0.09532413326881149
Epoch #127: loss=0.0745913079855117
Epoch #128: loss=0.06793587714095008
Epoch #129: loss=0.1012271468273618
Epoch #130: loss=0.08207388577813451
Epoch #131: loss=0.1111943440681154
Epoch #132: loss=0.07630114807662638
Epoch #133: loss=0.090611868114634
Epoch #134: loss=0.0804853131994605
Epoch #135: loss=0.0634987335652113
Epoch #136: loss=0.09399166432293979
Epoch #137: loss=0.08246809083291075
Epoch #138: loss=0.08877470725300637
Epoch #139: loss=0.10497556931593201
Epoch #140: loss=0.1325954801656983
Epoch #141: loss=0.10369447076862509
Epoch #142: loss=0.07596984082324938
Epoch #143: loss=0.07113543889400634
Epoch #144: loss=0.07135423518378627
Epoch #145: loss=0.05491187347268516
Epoch #146: loss=0.06268694505772808
Epoch #147: loss=0.0712202690880407
Epoch #148: loss=0.12412736166945913
Epoch #149: loss=0.11322783848101442
Epoch #150: loss=0.121821415187283
Epoch #151: loss=0.11008758216419003
Epoch #152: loss=0.11211783709851178
Epoch #153: loss=0.06490568435666236
Epoch #154: loss=0.06115881434049119
Epoch #155: loss=0.06879111633382061
Epoch #156: loss=0.06338111683726311
Epoch #157: loss=0.10499046912247484
Epoch #158: loss=0.08718082105571573
Epoch #159: loss=0.08531170592389324
Epoch #160: loss=0.06655781817707149
Epoch #161: loss=0.05383434345607053
Epoch #162: loss=0.048522102646529675
Epoch #163: loss=0.05041151540353894
Epoch #164: loss=0.08700100057335063
Epoch #165: loss=0.0661312665112994
Epoch #166: loss=0.04265693096782674
Epoch #167: loss=0.05164407193660736
Epoch #168: loss=0.04872225834564729
Epoch #169: loss=0.04980673200704835
Epoch #170: loss=0.06119723706929521
Epoch #171: loss=0.08796350522474809
Epoch #172: loss=0.07754510243169287
Epoch #173: loss=0.08310952854596755
Epoch #174: loss=0.0799918548965996
Epoch #175: loss=0.060734315860000526
Epoch #176: loss=0.07688971507278355
Epoch #177: loss=0.05297754069959575
Epoch #178: loss=0.08380032550882209
Epoch #179: loss=0.08762413119389252
Epoch #180: loss=0.07614625575528904
Epoch #181: loss=0.07705250187692317
Epoch #182: loss=0.05154382166537372
Epoch #183: loss=0.06238949933851307
Epoch #184: loss=0.07822790962051261
Epoch #185: loss=0.0938764416020025
Epoch #186: loss=0.06985492555593903
Epoch #187: loss=0.14071585136381062
Epoch #188: loss=0.10817192630334334
Epoch #189: loss=0.0497140283273025
Epoch #190: loss=0.05966307277875868
Epoch #191: loss=0.05803955930539153
Epoch #192: loss=0.05926793483509259
Epoch #193: loss=0.06474064840850505
Epoch #194: loss=0.13127247150987387
Epoch #195: loss=0.11701714450662787
Epoch #196: loss=0.07199728344990448
Epoch #197: loss=0.20730601835318588
Epoch #198: loss=0.10554291231727059
Epoch #199: loss=0.07291007617657835
Epoch #200: loss=0.06807450950145721
Epoch #201: loss=0.0562311400744048
Epoch #202: loss=0.03261087407273325
Epoch #203: loss=0.04234479930759831
Epoch #204: loss=0.04473118289289149
Epoch #205: loss=0.051760745658115906
Epoch #206: loss=0.045254659838974476
Epoch #207: loss=0.06001102924346924
Epoch #208: loss=0.06004131365228783
Epoch #209: loss=0.06024898122996092
Epoch #210: loss=0.05425583075901324
Epoch #211: loss=0.04782612240788611
Epoch #212: loss=0.05701531643386592
Epoch #213: loss=0.04332457338883118
Epoch #214: loss=0.04274231555279006
Epoch #215: loss=0.036761041984639385
Epoch #216: loss=0.031709878049282866
Epoch #217: loss=0.3261733047494834
Epoch #218: loss=0.09375087087127296
Epoch #219: loss=0.04342184791510755
Epoch #220: loss=0.04404206641695716
Epoch #221: loss=0.04256565288894556
Epoch #222: loss=0.04158882064406167
Epoch #223: loss=0.045336877376857126
Epoch #224: loss=0.07050768798217177
Epoch #225: loss=0.03371890905228528
Epoch #226: loss=0.04656631584194573
Epoch #227: loss=0.05443732343106107
Epoch #228: loss=0.03678529222749851
Epoch #229: loss=0.05657682999629866
Epoch #230: loss=0.04703195608982986
Epoch #231: loss=0.12834028256210414
Epoch #232: loss=0.03933998646045273
Epoch #233: loss=0.030958619773049246
Epoch #234: loss=0.048280387693508106
Epoch #235: loss=0.0416077626445754
Epoch #236: loss=0.03371024809100411
Epoch #237: loss=0.12898358126932924
Epoch #238: loss=0.050121243027123535
Epoch #239: loss=0.05586177318102934
Epoch #240: loss=0.11708497507920997
Epoch #241: loss=0.09573595666072586
Epoch #242: loss=0.060310058846053755
Epoch #243: loss=0.04165992767296054
Epoch #244: loss=0.05077582419934598
Epoch #245: loss=0.06385565748099577
Epoch #246: loss=0.038077141835608265
Epoch #247: loss=0.023989901031282814
Epoch #248: loss=0.038114350670101965
Epoch #249: loss=0.19693095133301208

Training time: 0:23:09.223653

Finished.
n2one setting ettm1_ettm2 -> electricity
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/ettm1_ettm2_epochs_250_seed_2024/model.pkl', muti_dataset='ettm1_ettm2', random_seed=2024, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='electricity')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.31117e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.57749e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.31117e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.3626118603762886, 'MAE': 0.4246979609016768}
Finished.
------------------------- record done -------------------------
n2one setting ettm1_ettm2 -> weather
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/ettm1_ettm2_epochs_250_seed_2024/model.pkl', muti_dataset='ettm1_ettm2', random_seed=2024, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='weather')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.41283e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.57575e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.41283e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.7431026260767349, 'MAE': 0.6741334926338667}
Finished.
------------------------- record done -------------------------
n2one setting ettm1_ettm2 -> exchange
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/ettm1_ettm2_epochs_250_seed_2024/model.pkl', muti_dataset='ettm1_ettm2', random_seed=2024, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='exchange')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=4.29214e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.2381001216605814, 'MAE': 0.3333156132754996}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='ettm1_electricity', random_seed=2024, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/ettm1_electricity_epochs_250_seed_2024
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=1.7250861654798668
Epoch #1: loss=0.6849175877420299
Epoch #2: loss=0.47778807333614454
Epoch #3: loss=0.396938440580684
Epoch #4: loss=0.32122401345566093
Epoch #5: loss=0.26686312805816353
Epoch #6: loss=0.24598351558558193
Epoch #7: loss=0.24282752520528184
Epoch #8: loss=0.21929777204900622
Epoch #9: loss=0.16231793776737996
Epoch #10: loss=0.1636152016111167
Epoch #11: loss=0.13477662855377756
Epoch #12: loss=0.1357915375202176
Epoch #13: loss=0.13577992239046888
Epoch #14: loss=0.13046612817210607
Epoch #15: loss=0.10469864174746067
Epoch #16: loss=0.11357754517167089
Epoch #17: loss=0.09483125034989004
Epoch #18: loss=0.0748005096677197
Epoch #19: loss=0.08492459535475326
Epoch #20: loss=0.07149594852207117
Epoch #21: loss=0.08767775925022202
Epoch #22: loss=0.07569379209125617
Epoch #23: loss=0.09148055837624314
Epoch #24: loss=0.07217454164939742
Epoch #25: loss=0.05156016872399094
Epoch #26: loss=0.05395154552450067
Epoch #27: loss=0.060438064699824226
Epoch #28: loss=0.04497476049042475
Epoch #29: loss=0.054909782017109894
Epoch #30: loss=0.0463870951375666
Epoch #31: loss=0.04803853088440037
Epoch #32: loss=0.044576948096958856
Epoch #33: loss=0.038246875640049756
Epoch #34: loss=0.05784706674875265
Epoch #35: loss=0.05131481316064047
Epoch #36: loss=0.033905979426835764
Epoch #37: loss=0.04026994937455775
Epoch #38: loss=0.03741861659478037
Epoch #39: loss=0.02747579260816214
Epoch #40: loss=0.03544002254030679
Epoch #41: loss=0.029108966429330724
Epoch #42: loss=0.03740941267250195
Epoch #43: loss=0.03046511618367183
Epoch #44: loss=0.03641121907451426
Epoch #45: loss=0.032465942680880606
Epoch #46: loss=0.03609458536557651
Epoch #47: loss=0.02994228122722223
Epoch #48: loss=0.03031923030559198
Epoch #49: loss=0.028724932333409607
Epoch #50: loss=0.025480656684039676
Epoch #51: loss=0.029628411954386633
Epoch #52: loss=0.044791842754355195
Epoch #53: loss=0.033579626403950964
Epoch #54: loss=0.02960681625857882
Epoch #55: loss=0.033892064417027926
Epoch #56: loss=0.03126243207076317
Epoch #57: loss=0.028560015838593245
Epoch #58: loss=0.026202054110465077
Epoch #59: loss=0.03422252023073631
Epoch #60: loss=0.08459168508037342
Epoch #61: loss=0.033592562427174816
Epoch #62: loss=0.019628109202821212
Epoch #63: loss=0.02238583961797665
Epoch #64: loss=0.02311707961868147
Epoch #65: loss=0.023746097047325134
Epoch #66: loss=0.021408850743353972
Epoch #67: loss=0.015563390511564955
Epoch #68: loss=0.023839027219359394
Epoch #69: loss=0.026892976136878133
Epoch #70: loss=0.01893168205836457
Epoch #71: loss=0.02677056721358347
Epoch #72: loss=0.02509476825228825
Epoch #73: loss=0.019239999108782307
Epoch #74: loss=0.015195191301004288
Epoch #75: loss=0.029023655617843855
Epoch #76: loss=0.018487771680923627
Epoch #77: loss=0.021066097427280172
Epoch #78: loss=0.02729316073201922
Epoch #79: loss=0.01602264559573174
Epoch #80: loss=0.01828403315729187
Epoch #81: loss=0.025185694137843415
Epoch #82: loss=0.026163942654826116
Epoch #83: loss=0.016745759032039724
Epoch #84: loss=0.013713461158626029
Epoch #85: loss=0.023405690950619236
Epoch #86: loss=0.016602338167224416
Epoch #87: loss=0.020284594862416096
Epoch #88: loss=0.016165796362711482
Epoch #89: loss=0.02183116702867829
Epoch #90: loss=0.038174017773259496
Epoch #91: loss=0.020849288819064612
Epoch #92: loss=0.019019425248079203
Epoch #93: loss=0.029161767504270557
Epoch #94: loss=0.01713924794647203
Epoch #95: loss=0.015538621118785926
Epoch #96: loss=0.012022345002315636
Epoch #97: loss=0.017729994848108267
Epoch #98: loss=0.022073882746794934
Epoch #99: loss=0.01621449489920703
Epoch #100: loss=0.013321211938946143
Epoch #101: loss=0.02253727758294869
Epoch #102: loss=0.04246497709417704
Epoch #103: loss=0.01588718572029064
Epoch #104: loss=0.013682512154610525
Epoch #105: loss=0.015791627915059947
Epoch #106: loss=0.01365633152945301
Epoch #107: loss=0.023839172292486707
Epoch #108: loss=0.019834085342723944
Epoch #109: loss=0.018653480544179982
Epoch #110: loss=0.012685438790057046
Epoch #111: loss=0.015532098190875792
Epoch #112: loss=0.007773369895392491
Epoch #113: loss=0.027072577749193836
Epoch #114: loss=0.023686451967160432
Epoch #115: loss=0.018571078582352633
Epoch #116: loss=0.011354274247949719
Epoch #117: loss=0.01727587673851676
Epoch #118: loss=0.02069275294527333
Epoch #119: loss=0.020853183731396056
Epoch #120: loss=0.026318305750420958
Epoch #121: loss=0.02568371369957879
Epoch #122: loss=0.01979173300068977
Epoch #123: loss=0.016647596140139657
Epoch #124: loss=0.01747244078888984
Epoch #125: loss=0.013548907855419016
Epoch #126: loss=0.012887455338923574
Epoch #127: loss=0.011375925706170712
Epoch #128: loss=0.014076513353088613
Epoch #129: loss=0.011081708941422122
Epoch #130: loss=0.014100965612998558
Epoch #131: loss=0.020270392747528582
Epoch #132: loss=0.015563179218106488
Epoch #133: loss=0.01742076988853388
Epoch #134: loss=0.011516706899147907
Epoch #135: loss=0.0126648198703447
Epoch #136: loss=0.048588397053241486
Epoch #137: loss=0.012901862318128213
Epoch #138: loss=0.010449899953571906
Epoch #139: loss=0.013044158822025104
Epoch #140: loss=0.019071268395335603
Epoch #141: loss=0.014469556955947451
Epoch #142: loss=0.012790323022426196
Epoch #143: loss=0.013938852416576554
Epoch #144: loss=0.01608032682708588
Epoch #145: loss=0.013727307621286537
Epoch #146: loss=0.010726988854353787
Epoch #147: loss=0.014386742840049765
Epoch #148: loss=0.01072548677059906
Epoch #149: loss=0.013161226430822287
Epoch #150: loss=0.010933722511210187
Epoch #151: loss=0.01097929809571746
Epoch #152: loss=0.013199915317678263
Epoch #153: loss=0.037861090517253615
Epoch #154: loss=0.01592928618979879
Epoch #155: loss=0.012117077596134966
Epoch #156: loss=0.011263566522516102
Epoch #157: loss=0.015349515759335628
Epoch #158: loss=0.01542107630044177
Epoch #159: loss=0.014424100624473137
Epoch #160: loss=0.023316964087639627
Epoch #161: loss=0.012014554199693916
Epoch #162: loss=0.008599032233090487
Epoch #163: loss=0.010892500084542938
Epoch #164: loss=0.01101052671250486
Epoch #165: loss=0.011722074389767675
Epoch #166: loss=0.01129277285114099
Epoch #167: loss=0.01237256668265404
Epoch #168: loss=0.014964230632632466
Epoch #169: loss=0.01233383534233863
Epoch #170: loss=0.015354779668514201
Epoch #171: loss=0.02061950174600015
Epoch #172: loss=0.02199803560611747
Epoch #173: loss=0.017513773552561458
Epoch #174: loss=0.013760908085331097
Epoch #175: loss=0.018596757954079095
Epoch #176: loss=0.010508416940953235
Epoch #177: loss=0.008914845290275414
Epoch #178: loss=0.0058968460112978445
Epoch #179: loss=0.01450582598220327
Epoch #180: loss=0.008582532163204468
Epoch #181: loss=0.010745134433950242
Epoch #182: loss=0.012982854307369687
Epoch #183: loss=0.013206916963099502
Epoch #184: loss=0.015301700816447678
Epoch #185: loss=0.00849500592339445
Epoch #186: loss=0.02418364259868114
Epoch #187: loss=0.016249699232481273
Epoch #188: loss=0.02286901624090633
Epoch #189: loss=0.014525734040944371
Epoch #190: loss=0.01550331050673842
Epoch #191: loss=0.01202658336404122
Epoch #192: loss=0.010973270391047433
Epoch #193: loss=0.010167525240524493
Epoch #194: loss=0.01253705638483081
Epoch #195: loss=0.009618062076105523
Epoch #196: loss=0.008538524838513695
Epoch #197: loss=0.009120539349629347
Epoch #198: loss=0.009891698126669226
Epoch #199: loss=0.010797533456303448
Epoch #200: loss=0.009289325751940697
Epoch #201: loss=0.011195450700701675
Epoch #202: loss=0.010235798033585503
Epoch #203: loss=0.009541821528870629
Epoch #204: loss=0.01778561761103085
Epoch #205: loss=0.01381842467175791
Epoch #206: loss=0.02023416824715285
Epoch #207: loss=0.010195685817063394
Epoch #208: loss=0.008072509577007633
Epoch #209: loss=0.012191715217866944
Epoch #210: loss=0.016687317372559914
Epoch #211: loss=0.015904334382363033
Epoch #212: loss=0.009181376189135115
Epoch #213: loss=0.00764616666049963
Epoch #214: loss=0.012051026416358296
Epoch #215: loss=0.017826812591821697
Epoch #216: loss=0.018563557168292107
Epoch #217: loss=0.008352979566026703
Epoch #218: loss=0.010583697488171706
Epoch #219: loss=0.009423194333899721
Epoch #220: loss=0.008838236694247322
Epoch #221: loss=0.014627987963353642
Epoch #222: loss=0.01151173613505811
Epoch #223: loss=0.010851567073510216
Epoch #224: loss=0.007821234048737623
Epoch #225: loss=0.010665410072651428
Epoch #226: loss=0.010473028790340367
Epoch #227: loss=0.010028607101728074
Epoch #228: loss=0.007392283419728509
Epoch #229: loss=0.01242209193089848
Epoch #230: loss=0.013096404891564224
Epoch #231: loss=0.01337128377720119
Epoch #232: loss=0.012195569728690491
Epoch #233: loss=0.009172674213256968
Epoch #234: loss=0.010567590405905748
Epoch #235: loss=0.006575194697436829
Epoch #236: loss=0.016734500358642795
Epoch #237: loss=0.01090951625931677
Epoch #238: loss=0.007056765518361963
Epoch #239: loss=0.008754614433135167
Epoch #240: loss=0.009599311688055105
Epoch #241: loss=0.007440944836392202
Epoch #242: loss=0.010597108380626382
Epoch #243: loss=0.013155642761014323
Epoch #244: loss=0.010780426290507586
Epoch #245: loss=0.00737546083767385
Epoch #246: loss=0.01161132722573021
Epoch #247: loss=0.01237818105349665
Epoch #248: loss=0.010910072869811268
Epoch #249: loss=0.01035382139972686

Training time: 4:34:11.760161

Finished.
n2one setting ettm1_electricity -> weather
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/ettm1_electricity_epochs_250_seed_2024/model.pkl', muti_dataset='ettm1_electricity', random_seed=2024, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='weather')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.11806e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.19523e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=5.43382e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=5.43382e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.5088561433314536, 'MAE': 0.539271051776975}
Finished.
------------------------- record done -------------------------
n2one setting ettm1_electricity -> exchange
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/ettm1_electricity_epochs_250_seed_2024/model.pkl', muti_dataset='ettm1_electricity', random_seed=2024, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='exchange')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=5.54071e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=5.54071e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.35615077540549617, 'MAE': 0.38854725438576265}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='ettm1_traffic', random_seed=2024, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/ettm1_traffic_epochs_250_seed_2024
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=1.0836163619607857
Epoch #1: loss=0.36989515715994514
Epoch #2: loss=0.2659247617780275
Epoch #3: loss=0.2095024048138457
Epoch #4: loss=0.1598833104700089
Epoch #5: loss=0.13263643431218014
Epoch #6: loss=0.09825693516173792
Epoch #7: loss=0.0948414296884717
Epoch #8: loss=0.08485281349982285
Epoch #9: loss=0.09015260926871331
Epoch #10: loss=0.07414171708436237
Epoch #11: loss=0.05828858804403626
Epoch #12: loss=0.0600866754389958
Epoch #13: loss=0.06283957209706921
Epoch #14: loss=0.05437654557766954
Epoch #15: loss=0.048281504282939064
Epoch #16: loss=0.04632040130519623
Epoch #17: loss=0.03755534513835355
Epoch #18: loss=0.03928642109103654
Epoch #19: loss=0.05294469619684032
Epoch #20: loss=0.0433478226850795
Epoch #21: loss=0.03785478058969869
Epoch #22: loss=0.0352863725688189
Epoch #23: loss=0.03264117843878454
Epoch #24: loss=0.03560270067253166
Epoch #25: loss=0.03704192840218446
Epoch #26: loss=0.03129738696823099
Epoch #27: loss=0.03182484384840578
Epoch #28: loss=0.02968865808233926
Epoch #29: loss=0.02363591664257371
Epoch #30: loss=0.031411428166391225
Epoch #31: loss=0.023686396847249682
Epoch #32: loss=0.030595647706476842
Epoch #33: loss=0.024993650199560596
Epoch #34: loss=0.02116510701189212
Epoch #35: loss=0.03199820177904826
Epoch #36: loss=0.020568765602850664
Epoch #37: loss=0.02212131689587994
Epoch #38: loss=0.02133639114119437
Epoch #39: loss=0.03894007911756027
Epoch #40: loss=0.01766105210512582
Epoch #41: loss=0.02173472614292128
Epoch #42: loss=0.025062797006346574
Epoch #43: loss=0.030897580980102427
Epoch #44: loss=0.024219449066613264
Epoch #45: loss=0.017928852138456484
Epoch #46: loss=0.017115218071002972
Epoch #47: loss=0.02431137456934961
Epoch #48: loss=0.023562422458414216
Epoch #49: loss=0.02377064170907673
Epoch #50: loss=0.01910963744668326
Epoch #51: loss=0.020246489908954796
Epoch #52: loss=0.02045405977374937
Epoch #53: loss=0.019217804152897762
Epoch #54: loss=0.018915965856839172
Epoch #55: loss=0.01562821128898229
Epoch #56: loss=0.022950678715817865
Epoch #57: loss=0.018435381556528258
Epoch #58: loss=0.020086775518819648
Epoch #59: loss=0.017922141430270086
Epoch #60: loss=0.018145261097828488
Epoch #61: loss=0.018221538456757995
Epoch #62: loss=0.02294162002217157
Epoch #63: loss=0.013084378128852386
Epoch #64: loss=0.019947401843030264
Epoch #65: loss=0.016698241843639276
Epoch #66: loss=0.015238323692647892
Epoch #67: loss=0.019639408776420834
Epoch #68: loss=0.01474301216003452
Epoch #69: loss=0.025306761209193708
Epoch #70: loss=0.020081535662483864
Epoch #71: loss=0.019488573930020105
Epoch #72: loss=0.014424546573066792
Epoch #73: loss=0.016570659980928246
Epoch #74: loss=0.018849969750494543
Epoch #75: loss=0.017782717977796084
Epoch #76: loss=0.015420853451126653
Epoch #77: loss=0.01741873154960485
Epoch #78: loss=0.012218199736290139
Epoch #79: loss=0.011390725349163838
Epoch #80: loss=0.01804709483885776
Epoch #81: loss=0.016612950646700413
Epoch #82: loss=0.012567033196046292
Epoch #83: loss=0.021422618108093185
Epoch #84: loss=0.014083633873989605
Epoch #85: loss=0.010973537525151166
Epoch #86: loss=0.015604786962658254
Epoch #87: loss=0.013217008430806131
Epoch #88: loss=0.020152693549696826
Epoch #89: loss=0.017359133654301116
Epoch #90: loss=0.013681558916033999
Epoch #91: loss=0.008933944372425708
Epoch #92: loss=0.016325933869335934
Epoch #93: loss=0.0167558405653748
Epoch #94: loss=0.014555648886816675
Epoch #95: loss=0.020282691138658358
Epoch #96: loss=0.011760911390074407
Epoch #97: loss=0.015581042265135715
Epoch #98: loss=0.01438015278801242
Epoch #99: loss=0.016777680223501786
Epoch #100: loss=0.015761773113703664
Epoch #101: loss=0.015001444139202869
Epoch #102: loss=0.01312970360339633
Epoch #103: loss=0.012925222785962065
Epoch #104: loss=0.017830225267096284
Epoch #105: loss=0.01253521177918519
Epoch #106: loss=0.011301436864094593
Epoch #107: loss=0.015622977350725402
Epoch #108: loss=0.013958504279980218
Epoch #109: loss=0.012545304375707054
Epoch #110: loss=0.03342857699899135
Epoch #111: loss=0.010141073573297498
Epoch #112: loss=0.012518762690896586
Epoch #113: loss=0.012518240316345047
Epoch #114: loss=0.01799419572058679
Epoch #115: loss=0.014697981406546509
Epoch #116: loss=0.012107767334354511
Epoch #117: loss=0.014912469150060585
Epoch #118: loss=0.01189697152915766
Epoch #119: loss=0.012906748336141893
Epoch #120: loss=0.011334164966887796
Epoch #121: loss=0.014749935310088426
Epoch #122: loss=0.018373914748200354
Epoch #123: loss=0.011133341065302992
Epoch #124: loss=0.011876338487270982
Epoch #125: loss=0.01719350486510126
Epoch #126: loss=0.012661893119775362
Epoch #127: loss=0.014057933255608161
Epoch #128: loss=0.015817772233926712
Epoch #129: loss=0.013335485054811577
Epoch #130: loss=0.01261720050037369
Epoch #131: loss=0.014675875624627374
Epoch #132: loss=0.012972987069690368
Epoch #133: loss=0.014702374282074859
Epoch #134: loss=0.010005401192965196
Epoch #135: loss=0.009766198773849714
Epoch #136: loss=0.013708183802044865
Epoch #137: loss=0.013313916241531425
Epoch #138: loss=0.009094461494444534
Epoch #139: loss=0.040455140274881896
Epoch #140: loss=0.01789108598808825
Epoch #141: loss=0.008435396983363305
Epoch #142: loss=0.011090043454069883
Epoch #143: loss=0.009431536905099461
Epoch #144: loss=0.009781691466080359
Epoch #145: loss=0.008159086209587713
Epoch #146: loss=0.01269280941772727
Epoch #147: loss=0.011397071688294354
Epoch #148: loss=0.013895819414106175
Epoch #149: loss=0.00845995334331157
Epoch #150: loss=0.012547652038441142
Epoch #151: loss=0.009034638140834111
Epoch #152: loss=0.017079126408519937
Epoch #153: loss=0.010621031499035732
Epoch #154: loss=0.011924832636405253
Epoch #155: loss=0.012511058584215543
Epoch #156: loss=0.00905893553697737
Epoch #157: loss=0.011648284444698924
Epoch #158: loss=0.011388642195559386
Epoch #159: loss=0.00908461515721705
Epoch #160: loss=0.016353531152161718
Epoch #161: loss=0.020557201250325858
Epoch #162: loss=0.02089228792026901
Epoch #163: loss=0.010219803214280409
Epoch #164: loss=0.015672194879349503
Epoch #165: loss=0.00729532951223656
Epoch #166: loss=0.011503485974116691
Epoch #167: loss=0.012487449564516704
Epoch #168: loss=0.016556255158809315
Epoch #169: loss=0.008356782407774552
Epoch #170: loss=0.011916166000108993
Epoch #171: loss=0.0086741612190246
Epoch #172: loss=0.01154432869690336
Epoch #173: loss=0.011664582677882961
Epoch #174: loss=0.009509596446179852
Epoch #175: loss=0.009243464706827517
Epoch #176: loss=0.010088289942042843
Epoch #177: loss=0.008049665816636745
Epoch #178: loss=0.010701289357452676
Epoch #179: loss=0.008632499520266311
Epoch #180: loss=0.019405867965599385
Epoch #181: loss=0.010001831069588794
Epoch #182: loss=0.008464408678227085
Epoch #183: loss=0.014518110207720064
Epoch #184: loss=0.006562770832264513
Epoch #185: loss=0.011308636576457207
Epoch #186: loss=0.009065929673321945
Epoch #187: loss=0.012831249369559318
Epoch #188: loss=0.015495743253251633
Epoch #189: loss=0.010406089252702954
Epoch #190: loss=0.009949186552860156
Epoch #191: loss=0.010684082264883816
Epoch #192: loss=0.00975226637487471
Epoch #193: loss=0.01884916882208156
Epoch #194: loss=0.007777037002507758
Epoch #195: loss=0.014184624292257798
Epoch #196: loss=0.008917174671981887
Epoch #197: loss=0.00654200129669676
Epoch #198: loss=0.011736353860672228
Epoch #199: loss=0.013960974165683832
Epoch #200: loss=0.014596112623707021
Epoch #201: loss=0.009685315186045007
Epoch #202: loss=0.009057353960325752
Epoch #203: loss=0.010152044905539503
Epoch #204: loss=0.009792802777895565
Epoch #205: loss=0.01871528153547501
Epoch #206: loss=0.022586003096653137
Epoch #207: loss=0.010100398950001655
Epoch #208: loss=0.006381501294364449
Epoch #209: loss=0.009696511506631849
Epoch #210: loss=0.023944402526277848
Epoch #211: loss=0.009895031179665755
Epoch #212: loss=0.00854751557523261
Epoch #213: loss=0.008369023798881773
Epoch #214: loss=0.016691898711164135
Epoch #215: loss=0.007327779312684753
Epoch #216: loss=0.011357218173284106
Epoch #217: loss=0.012236365312316685
Epoch #218: loss=0.008218263674540476
Epoch #219: loss=0.007625573089663398
Epoch #220: loss=0.010507212676476465
Epoch #221: loss=0.012257034996350964
Epoch #222: loss=0.011716694032381923
Epoch #223: loss=0.01021581279532077
Epoch #224: loss=0.007787790628380003
Epoch #225: loss=0.006877649326822486
Epoch #226: loss=0.011310746564943491
Epoch #227: loss=0.008057888994710629
Epoch #228: loss=0.006498059026017771
Epoch #229: loss=0.008980424891839141
Epoch #230: loss=0.016289445938516348
Epoch #231: loss=0.012509397436336072
Epoch #232: loss=0.01653064596866106
Epoch #233: loss=0.006338762735768543
Epoch #234: loss=0.009722018288001727
Epoch #235: loss=0.008658644170321647
Epoch #236: loss=0.008818969756898493
Epoch #237: loss=0.011351877800450242
Epoch #238: loss=0.008780130298097558
Epoch #239: loss=0.01634380382894808
Epoch #240: loss=0.011828321109002856
Epoch #241: loss=0.010557415240311159
Epoch #242: loss=0.013792254354897166
Epoch #243: loss=0.007503846127150805
Epoch #244: loss=0.006627895549672602
Epoch #245: loss=0.007472223772891111
Epoch #246: loss=0.008482044126373317
Epoch #247: loss=0.013980077622273114
Epoch #248: loss=0.007859284510906148
Epoch #249: loss=0.008266202497248796

Training time: 10:33:43.800600

Finished.
n2one setting ettm1_traffic -> electricity
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/ettm1_traffic_epochs_250_seed_2024/model.pkl', muti_dataset='ettm1_traffic', random_seed=2024, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='electricity')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.03263e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.09147e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=5.26158e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.03263e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.39319690366662924, 'MAE': 0.4442875842720956}
Finished.
------------------------- record done -------------------------
n2one setting ettm1_traffic -> weather
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/ettm1_traffic_epochs_250_seed_2024/model.pkl', muti_dataset='ettm1_traffic', random_seed=2024, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='weather')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.06596e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.19366e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=5.36591e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.06596e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.7428753251086047, 'MAE': 0.7254658400359945}
Finished.
------------------------- record done -------------------------
n2one setting ettm1_traffic -> exchange
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/ettm1_traffic_epochs_250_seed_2024/model.pkl', muti_dataset='ettm1_traffic', random_seed=2024, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='exchange')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=4.99282e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.4118718045718254, 'MAE': 0.40117296399091584}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='ettm1_weather', random_seed=2024, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/ettm1_weather_epochs_250_seed_2024
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=5.21036602999713
Epoch #1: loss=2.832659740705748
Epoch #2: loss=2.7880186261357487
Epoch #3: loss=2.1871409351761275
Epoch #4: loss=2.061104229978613
Epoch #5: loss=1.8451736714388873
Epoch #6: loss=1.6054498891572695
Epoch #7: loss=1.6262320892230884
Epoch #8: loss=1.3981470771738
Epoch #9: loss=1.372221645471212
Epoch #10: loss=1.2596123121880196
Epoch #11: loss=1.1762616666587624
Epoch #12: loss=1.2095366490853798
Epoch #13: loss=1.1316379582559741
Epoch #14: loss=1.0290576493417896
Epoch #15: loss=0.9892436958648063
Epoch #16: loss=0.8829872414872453
Epoch #17: loss=0.893320745713002
Epoch #18: loss=0.8554507381207234
Epoch #19: loss=0.8454743672061611
Epoch #20: loss=0.8776869161708934
Epoch #21: loss=0.7346323132514954
Epoch #22: loss=0.770502731606767
Epoch #23: loss=0.6862354471876815
Epoch #24: loss=0.6840598502674619
Epoch #25: loss=0.7256712269138645
Epoch #26: loss=0.6505071472477268
Epoch #27: loss=0.7002889932812871
Epoch #28: loss=0.7313277310616261
Epoch #29: loss=0.6791125519855602
Epoch #30: loss=0.6529675746286238
Epoch #31: loss=0.637027154097686
Epoch #32: loss=0.7030145993103852
Epoch #33: loss=0.5937387274729239
Epoch #34: loss=0.708170356782707
Epoch #35: loss=0.6278559855512671
Epoch #36: loss=0.5494812324240401
Epoch #37: loss=0.5090806057324281
Epoch #38: loss=0.4529369336527747
Epoch #39: loss=0.4249646792540679
Epoch #40: loss=0.4350642900209169
Epoch #41: loss=0.4475426665834478
Epoch #42: loss=0.41341060400009155
Epoch #43: loss=0.5640105484305201
Epoch #44: loss=0.552010668290628
Epoch #45: loss=0.4997821490506868
Epoch #46: loss=0.42664977183213104
Epoch #47: loss=0.48782704488651174
Epoch #48: loss=0.4586319214588887
Epoch #49: loss=0.38222839663157593
Epoch #50: loss=0.36899919687090693
Epoch #51: loss=0.38962752392163147
Epoch #52: loss=0.36129777697292537
Epoch #53: loss=0.325741605581464
Epoch #54: loss=0.3302197947695449
Epoch #55: loss=0.47441002847375097
Epoch #56: loss=0.3545266832854297
Epoch #57: loss=0.3522362040506827
Epoch #58: loss=0.3285550322081592
Epoch #59: loss=0.3692422956228256
Epoch #60: loss=0.3225896273915832
Epoch #61: loss=0.28382500965852997
Epoch #62: loss=0.3329485312506959
Epoch #63: loss=0.2869681611254409
Epoch #64: loss=0.3519941306597478
Epoch #65: loss=0.26426243258489146
Epoch #66: loss=0.3167811048191947
Epoch #67: loss=0.29328106021558914
Epoch #68: loss=0.22817497418538943
Epoch #69: loss=0.2805963055910291
Epoch #70: loss=0.2827631363997588
Epoch #71: loss=0.2868538148499824
Epoch #72: loss=0.22031737380736582
Epoch #73: loss=0.2652663628797273
Epoch #74: loss=0.28771427052246556
Epoch #75: loss=0.22843056977600665
Epoch #76: loss=0.29943360186911916
Epoch #77: loss=0.20880637861586906
Epoch #78: loss=0.21875633843041756
Epoch #79: loss=0.21913382008269028
Epoch #80: loss=0.21503697617633924
Epoch #81: loss=0.23410064626384425
Epoch #82: loss=0.1799502068677464
Epoch #83: loss=0.28468368685728795
Epoch #84: loss=0.2845139286002597
Epoch #85: loss=0.22662717446282105
Epoch #86: loss=0.28691452137521795
Epoch #87: loss=0.6930805666221155
Epoch #88: loss=0.3684765837482504
Epoch #89: loss=0.2633628893542934
Epoch #90: loss=0.2848937602059261
Epoch #91: loss=0.24978541099541895
Epoch #92: loss=0.3137314903575021
Epoch #93: loss=0.21986105474265846
Epoch #94: loss=0.1543357233340676
Epoch #95: loss=0.19589597188137672
Epoch #96: loss=0.16989947986361142
Epoch #97: loss=0.13649794167360743
Epoch #98: loss=0.19493974584179954
Epoch #99: loss=0.16560092729491158
Epoch #100: loss=0.13895585951772896
Epoch #101: loss=0.11318982986582292
Epoch #102: loss=0.185104839101031
Epoch #103: loss=0.18686077854520566
Epoch #104: loss=0.1434691450684457
Epoch #105: loss=0.19648819457034808
Epoch #106: loss=0.13920401439473434
Epoch #107: loss=0.13135586246042638
Epoch #108: loss=0.09740720643989138
Epoch #109: loss=0.14056766143924482
Epoch #110: loss=0.21492218850432215
Epoch #111: loss=0.17809300346148982
Epoch #112: loss=0.10860837271084657
Epoch #113: loss=0.16387605405337102
Epoch #114: loss=0.1567149318351939
Epoch #115: loss=0.22184381019827482
Epoch #116: loss=0.11999521535393354
Epoch #117: loss=0.16454637825891777
Epoch #118: loss=0.14560185131188985
Epoch #119: loss=0.11989243479596602
Epoch #120: loss=0.14649617893470301
Epoch #121: loss=0.12685045670415904
Epoch #122: loss=0.1298587301091568
Epoch #123: loss=0.11740795638714288
Epoch #124: loss=0.10007866743851353
Epoch #125: loss=0.09557598718517535
Epoch #126: loss=0.12265928245678141
Epoch #127: loss=0.1788205325200751
Epoch #128: loss=0.12344642064055882
Epoch #129: loss=0.1346635518444551
Epoch #130: loss=0.1067634459484268
Epoch #131: loss=0.10216090979205596
Epoch #132: loss=0.09191257582121604
Epoch #133: loss=0.08331337168410018
Epoch #134: loss=0.17863345921442314
Epoch #135: loss=0.1461148846995186
Epoch #136: loss=0.12451855214060964
Epoch #137: loss=0.07799922825919615
Epoch #138: loss=0.1076521863405769
Epoch #139: loss=0.07986268290394061
Epoch #140: loss=0.13994217588490732
Epoch #141: loss=0.08047713485319873
Epoch #142: loss=0.09869155373323608
Epoch #143: loss=0.06970069572530888
Epoch #144: loss=0.08323971851653345
Epoch #145: loss=0.09987976432249353
Epoch #146: loss=0.06797932284707958
Epoch #147: loss=0.07384656077703915
Epoch #148: loss=0.15863335545401316
Epoch #149: loss=0.103228504996042
Epoch #150: loss=0.06516701672729608
Epoch #151: loss=0.07664423148978401
Epoch #152: loss=0.08893087662353709
Epoch #153: loss=0.09078745735255447
Epoch #154: loss=0.07708822356889376
Epoch #155: loss=0.06087983006963859
Epoch #156: loss=0.05640360340476036
Epoch #157: loss=0.08768399230934479
Epoch #158: loss=0.07271672241591118
Epoch #159: loss=0.08943413077174006
Epoch #160: loss=0.07567422157405196
Epoch #161: loss=0.11123462816750682
Epoch #162: loss=0.1422847629197546
Epoch #163: loss=0.08746347304534267
Epoch #164: loss=0.0733604849492376
Epoch #165: loss=0.07148393429815769
Epoch #166: loss=0.08050260003152732
Epoch #167: loss=0.11024205643381621
Epoch #168: loss=0.07681988255196326
Epoch #169: loss=0.0737844976420338
Epoch #170: loss=0.06426275048304249
Epoch #171: loss=0.06047104674114569
Epoch #172: loss=0.07152790888338476
Epoch #173: loss=0.06831973494106047
Epoch #174: loss=0.06857686912691272
Epoch #175: loss=0.12287269734047554
Epoch #176: loss=0.3470712943455657
Epoch #177: loss=0.35466970674492215
Epoch #178: loss=0.12242370909331618
Epoch #179: loss=0.08657043603425091
Epoch #180: loss=0.07918516649688417
Epoch #181: loss=0.06268337978100455
Epoch #182: loss=0.1050608728483722
Epoch #183: loss=0.0545170737078061
Epoch #184: loss=0.0665387020672898
Epoch #185: loss=0.05620602593832725
Epoch #186: loss=0.06013259639007014
Epoch #187: loss=0.06280409401232326
Epoch #188: loss=0.042554000644265
Epoch #189: loss=0.0440259446213777
Epoch #190: loss=0.05077965391447415
Epoch #191: loss=0.05076035701141164
Epoch #192: loss=0.12027997538648746
Epoch #193: loss=0.048827476245728696
Epoch #194: loss=0.04967467320730557
Epoch #195: loss=0.08914192929561879
Epoch #196: loss=0.06730433508149676
Epoch #197: loss=0.053735363649556764
Epoch #198: loss=0.08445164045268619
Epoch #199: loss=0.06779315440940696
Epoch #200: loss=0.07902396402346927
Epoch #201: loss=0.04714368276197363
Epoch #202: loss=0.05175496355907337
Epoch #203: loss=0.05220667033682804
Epoch #204: loss=0.054440226355517235
Epoch #205: loss=0.04248598004011689
Epoch #206: loss=0.03959470959631978
Epoch #207: loss=0.04432945421619995
Epoch #208: loss=0.06227654747262194
Epoch #209: loss=0.03678828810115118
Epoch #210: loss=0.06368122811152323
Epoch #211: loss=0.03812420700450201
Epoch #212: loss=0.027982839886602516
Epoch #213: loss=0.06165660540195736
Epoch #214: loss=0.08388603778203597
Epoch #215: loss=0.04249262661245223
Epoch #216: loss=0.043549991414152285
Epoch #217: loss=0.057049263646272386
Epoch #218: loss=0.04330499080085271
Epoch #219: loss=0.11015160814733119
Epoch #220: loss=0.05329722747508738
Epoch #221: loss=0.05042664862766459
Epoch #222: loss=0.053244695419798024
Epoch #223: loss=0.056057127368812625
Epoch #224: loss=0.07948603866168776
Epoch #225: loss=0.05812811317878801
Epoch #226: loss=0.06133663359828092
Epoch #227: loss=0.04970226532502754
Epoch #228: loss=0.06366732162800995
Epoch #229: loss=0.08291494154144784
Epoch #230: loss=0.04687896086456808
Epoch #231: loss=0.05950892809778452
Epoch #232: loss=0.06059310071774431
Epoch #233: loss=0.05127473140286433
Epoch #234: loss=0.03947931641360392
Epoch #235: loss=0.0615348628143201
Epoch #236: loss=0.060073021396591854
Epoch #237: loss=0.04116125832739714
Epoch #238: loss=0.047540755300606426
Epoch #239: loss=0.041780891211552394
Epoch #240: loss=0.03747580038081553
Epoch #241: loss=0.04775892568097727
Epoch #242: loss=0.03250929032376892
Epoch #243: loss=0.044465333449880816
Epoch #244: loss=0.08535535328996342
Epoch #245: loss=0.07613814237049303
Epoch #246: loss=0.20278757266901634
Epoch #247: loss=0.1413948566124246
Epoch #248: loss=0.05593589172270652
Epoch #249: loss=0.04311786902867056

Training time: 0:39:32.422008

Finished.
n2one setting ettm1_weather -> electricity
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/ettm1_weather_epochs_250_seed_2024/model.pkl', muti_dataset='ettm1_weather', random_seed=2024, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='electricity')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.43513e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.85989e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.43513e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.3537555389874603, 'MAE': 0.41586327177641286}
Finished.
------------------------- record done -------------------------
n2one setting ettm1_weather -> exchange
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/ettm1_weather_epochs_250_seed_2024/model.pkl', muti_dataset='ettm1_weather', random_seed=2024, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='exchange')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=3.46832e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.22541770920549792, 'MAE': 0.31753630630000196}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='ettm1_exchange', random_seed=2024, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/ettm1_exchange_epochs_250_seed_2024
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=5.057639052993373
Epoch #1: loss=2.617659016659385
Epoch #2: loss=2.2675923360021493
Epoch #3: loss=2.0721634500905086
Epoch #4: loss=1.9635198179044222
Epoch #5: loss=1.8083749946795011
Epoch #6: loss=1.819339162424991
Epoch #7: loss=1.6742066395910162
Epoch #8: loss=1.6438104729903371
Epoch #9: loss=1.5069292407286794
Epoch #10: loss=1.4811273123088635
Epoch #11: loss=1.4075090445970233
Epoch #12: loss=1.4209037893696834
Epoch #13: loss=1.329383649324116
Epoch #14: loss=1.3362537183259662
Epoch #15: loss=1.3271744690443341
Epoch #16: loss=1.2981033701645701
Epoch #17: loss=1.221088265117846
Epoch #18: loss=1.1903052894692672
Epoch #19: loss=1.192672051881489
Epoch #20: loss=1.0637709529776322
Epoch #21: loss=1.1126964249108966
Epoch #22: loss=1.007917278691342
Epoch #23: loss=1.0425671056697243
Epoch #24: loss=0.9471336540422941
Epoch #25: loss=1.0081368057351363
Epoch #26: loss=1.037758548008768
Epoch #27: loss=1.0124516393008984
Epoch #28: loss=0.9576114416122437
Epoch #29: loss=0.9168537729664853
Epoch #30: loss=0.9788796964444613
Epoch #31: loss=0.8975057225478323
Epoch #32: loss=0.7914742419594213
Epoch #33: loss=0.9004296064376831
Epoch #34: loss=0.7912203795031497
Epoch #35: loss=0.8387775044692191
Epoch #36: loss=0.8595781953711259
Epoch #37: loss=0.8128550554576673
Epoch #38: loss=0.7677770131512692
Epoch #39: loss=0.8011632498941923
Epoch #40: loss=0.7732162067764684
Epoch #41: loss=0.795657731984791
Epoch #42: loss=0.8722116947174072
Epoch #43: loss=0.7835531830787659
Epoch #44: loss=0.8761538423989949
Epoch #45: loss=0.8162871912906045
Epoch #46: loss=0.7472478982649351
Epoch #47: loss=0.6173276007175446
Epoch #48: loss=0.72722769097278
Epoch #49: loss=0.6800414229694166
Epoch #50: loss=0.6564286319833053
Epoch #51: loss=0.5822207582624335
Epoch #52: loss=0.6282525988001573
Epoch #53: loss=0.4784651226118991
Epoch #54: loss=0.5044136831634923
Epoch #55: loss=0.5482579846131174
Epoch #56: loss=0.5415803688137155
Epoch #57: loss=0.5843517827360254
Epoch #58: loss=0.714981988856667
Epoch #59: loss=0.6278076077762403
Epoch #60: loss=0.5900958839215731
Epoch #61: loss=0.5604124351551658
Epoch #62: loss=0.5488990608014559
Epoch #63: loss=0.4932641677166286
Epoch #64: loss=0.4576533499516939
Epoch #65: loss=0.504978397959157
Epoch #66: loss=0.4997288608237317
Epoch #67: loss=0.6532823866919467
Epoch #68: loss=0.46722546611961563
Epoch #69: loss=0.42491897077936874
Epoch #70: loss=0.514221014160859
Epoch #71: loss=0.48517491864530665
Epoch #72: loss=0.5567176843944349
Epoch #73: loss=0.5800636896961614
Epoch #74: loss=0.5375363591470217
Epoch #75: loss=0.4779770107645738
Epoch #76: loss=0.44234327266090795
Epoch #77: loss=0.511425666118923
Epoch #78: loss=0.40863122438129623
Epoch #79: loss=0.44546167787752655
Epoch #80: loss=0.5219364236844214
Epoch #81: loss=0.41662920070321935
Epoch #82: loss=0.5335197966349753
Epoch #83: loss=0.43749352113196727
Epoch #84: loss=0.4304858869627902
Epoch #85: loss=0.42818692718681534
Epoch #86: loss=0.39519735072788437
Epoch #87: loss=0.3788633401456632
Epoch #88: loss=0.4823680739653738
Epoch #89: loss=0.36917526745482493
Epoch #90: loss=0.3779111269273256
Epoch #91: loss=0.4735825171596126
Epoch #92: loss=0.35606212992417186
Epoch #93: loss=0.37352660141493144
Epoch #94: loss=0.41470588900541006
Epoch #95: loss=0.41600774071718516
Epoch #96: loss=0.4166904334959231
Epoch #97: loss=0.376310563401172
Epoch #98: loss=0.4199383572528237
Epoch #99: loss=0.39030734135916356
Epoch #100: loss=0.36445249224963944
Epoch #101: loss=0.3957522966359791
Epoch #102: loss=0.48431485499206345
Epoch #103: loss=0.3880966046923085
Epoch #104: loss=0.3063235957371561
Epoch #105: loss=0.39214134255522176
Epoch #106: loss=0.4354462709866072
Epoch #107: loss=0.3792623250108016
Epoch #108: loss=0.46599667793826055
Epoch #109: loss=0.4503088232718016
Epoch #110: loss=0.46629667046823
Epoch #111: loss=0.585893557259911
Epoch #112: loss=0.4442148412528791
Epoch #113: loss=0.38660185744887904
Epoch #114: loss=0.4398775477158396
Epoch #115: loss=0.4443963232793306
Epoch #116: loss=0.30759767795863907
Epoch #117: loss=0.3614159422485452
Epoch #118: loss=0.38074592932274465
Epoch #119: loss=0.2926958469968093
Epoch #120: loss=0.34188280685951833
Epoch #121: loss=0.2934758404367848
Epoch #122: loss=0.3586321700560419
Epoch #123: loss=0.28397454556665924
Epoch #124: loss=0.2737901226470345
Epoch #125: loss=0.2851159505938229
Epoch #126: loss=0.28161491806569855
Epoch #127: loss=0.2599557053885962
Epoch #128: loss=0.3746842161605233
Epoch #129: loss=0.2698107048084861
Epoch #130: loss=0.29715031816771154
Epoch #131: loss=0.24492352102932177
Epoch #132: loss=0.23622766411618182
Epoch #133: loss=0.5138980503145018
Epoch #134: loss=0.3698359500420721
Epoch #135: loss=0.27440381991235835
Epoch #136: loss=0.24761246380053067
Epoch #137: loss=0.3145689266292672
Epoch #138: loss=0.27135657519102097
Epoch #139: loss=0.20434583684331492
Epoch #140: loss=0.2512013955335868
Epoch #141: loss=0.2913079246094352
Epoch #142: loss=0.3713393642714149
Epoch #143: loss=0.19687388445201673
Epoch #144: loss=0.2516530383574335
Epoch #145: loss=0.20767807725228762
Epoch #146: loss=0.28314657117191117
Epoch #147: loss=0.2564230655369006
Epoch #148: loss=0.2424454128271655
Epoch #149: loss=0.24122226924488419
Epoch #150: loss=0.2149693099291701
Epoch #151: loss=0.24662252633195175
Epoch #152: loss=0.22419857508257815
Epoch #153: loss=0.2687772206569973
Epoch #154: loss=0.20992940938786456
Epoch #155: loss=0.2211097761204368
Epoch #156: loss=0.38725786146364716
Epoch #157: loss=0.2128136703058293
Epoch #158: loss=0.2180018632819778
Epoch #159: loss=0.29296045828806727
Epoch #160: loss=0.16606515724408
Epoch #161: loss=0.24128149920388273
Epoch #162: loss=0.30804101260084854
Epoch #163: loss=0.34705112482372086
Epoch #164: loss=0.2870337374900517
Epoch #165: loss=0.2129431139481695
Epoch #166: loss=0.26779535295147644
Epoch #167: loss=0.16883362476762973
Epoch #168: loss=0.17936620077020243
Epoch #169: loss=0.21884136607772425
Epoch #170: loss=0.20317469066695162
Epoch #171: loss=0.16778265096639333
Epoch #172: loss=0.30244560265227366
Epoch #173: loss=0.22430889661374845
Epoch #174: loss=0.2537058321268935
Epoch #175: loss=0.18309101031014793
Epoch #176: loss=0.21883269595472435
Epoch #177: loss=0.19266332254598015
Epoch #178: loss=0.17103283224921478
Epoch #179: loss=0.15928426032003604
Epoch #180: loss=0.18160446595988775
Epoch #181: loss=0.20991292909571999
Epoch #182: loss=0.2072498606223809
Epoch #183: loss=0.15606657631303134
Epoch #184: loss=0.2035131464271169
Epoch #185: loss=0.15246370160265973
Epoch #186: loss=0.18853116192315755
Epoch #187: loss=0.1533538652093787
Epoch #188: loss=0.17902498496206184
Epoch #189: loss=0.22076785133073204
Epoch #190: loss=0.11350478604435921
Epoch #191: loss=0.13284290581941605
Epoch #192: loss=0.16593922145272555
Epoch #193: loss=0.11326667176265466
Epoch #194: loss=0.1734226928337624
Epoch #195: loss=0.16512565922580266
Epoch #196: loss=0.17474918302736783
Epoch #197: loss=0.19947692300928266
Epoch #198: loss=0.15092094811169723
Epoch #199: loss=0.14894348304522664
Epoch #200: loss=0.16174595391279772
Epoch #201: loss=0.14504253060409897
Epoch #202: loss=0.1417363764424073
Epoch #203: loss=0.13810160207120994
Epoch #204: loss=0.1266882013725607
Epoch #205: loss=0.1470075326138421
Epoch #206: loss=0.10793648013158848
Epoch #207: loss=0.08819873532966564
Epoch #208: loss=0.1315954664820119
Epoch #209: loss=0.24866882731255732
Epoch #210: loss=0.2358214239540853
Epoch #211: loss=0.12684647386011325
Epoch #212: loss=0.21320300039492154
Epoch #213: loss=0.11872144063052378
Epoch #214: loss=0.18970429426745364
Epoch #215: loss=0.1635870464930409
Epoch #216: loss=0.12139246024583515
Epoch #217: loss=0.1738937212840507
Epoch #218: loss=0.1544647328555584
Epoch #219: loss=0.12075111230737284
Epoch #220: loss=0.15638617602618118
Epoch #221: loss=0.17935127608085932
Epoch #222: loss=0.1255827155944548
Epoch #223: loss=0.3873183839023113
Epoch #224: loss=0.37658700660655375
Epoch #225: loss=0.2494494264063082
Epoch #226: loss=0.2583532941184546
Epoch #227: loss=0.3032145296272479
Epoch #228: loss=0.2384693355936753
Epoch #229: loss=0.2006497163521616
Epoch #230: loss=0.17059219236436643
Epoch #231: loss=0.13852993320477636
Epoch #232: loss=0.14048920885512703
Epoch #233: loss=0.09422375477458302
Epoch #234: loss=0.1591490440462765
Epoch #235: loss=0.12673119004619748
Epoch #236: loss=0.10527725302075085
Epoch #237: loss=0.07722788245270126
Epoch #238: loss=0.14901767867176155
Epoch #239: loss=0.1162802983859652
Epoch #240: loss=0.10854568254006536
Epoch #241: loss=0.09021127910206192
Epoch #242: loss=0.10961390325897619
Epoch #243: loss=0.10161370587976355
Epoch #244: loss=0.10262187315445197
Epoch #245: loss=0.09842097327897423
Epoch #246: loss=0.09178574324438446
Epoch #247: loss=0.1228233588565337
Epoch #248: loss=0.09804438476107623
Epoch #249: loss=0.1240851728147582

Training time: 0:17:07.744140

Finished.
n2one setting ettm1_exchange -> electricity
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/ettm1_exchange_epochs_250_seed_2024/model.pkl', muti_dataset='ettm1_exchange', random_seed=2024, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='electricity')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.36009e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.69184e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.36009e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.35759436875425266, 'MAE': 0.4190780072754135}
Finished.
------------------------- record done -------------------------
n2one setting ettm1_exchange -> weather
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/ettm1_exchange_epochs_250_seed_2024/model.pkl', muti_dataset='ettm1_exchange', random_seed=2024, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='weather')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.27834e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.3943e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=5.65762e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.27834e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 1.0352524181074938, 'MAE': 0.7519392292915502}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='ettm2_electricity', random_seed=2024, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/ettm2_electricity_epochs_250_seed_2024
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=1.8039263079324401
Epoch #1: loss=0.7027136297470116
Epoch #2: loss=0.4933090550623026
Epoch #3: loss=0.4085467445652887
Epoch #4: loss=0.32884104253656893
Epoch #5: loss=0.27684464126107206
Epoch #6: loss=0.25801443485879755
Epoch #7: loss=0.25294764330947256
Epoch #8: loss=0.2260818281940308
Epoch #9: loss=0.1697491803262607
Epoch #10: loss=0.16837579081485787
Epoch #11: loss=0.14528004523276924
Epoch #12: loss=0.14499791052910577
Epoch #13: loss=0.14240198818046645
Epoch #14: loss=0.13727782983108458
Epoch #15: loss=0.1066155891124922
Epoch #16: loss=0.11759219493111034
Epoch #17: loss=0.09104626971375511
Epoch #18: loss=0.08573116446250534
Epoch #19: loss=0.08930462957864785
Epoch #20: loss=0.07431824192831135
Epoch #21: loss=0.09359150096291609
Epoch #22: loss=0.07448123144659113
Epoch #23: loss=0.09459655789422791
Epoch #24: loss=0.07219081117592034
Epoch #25: loss=0.05243708460075579
Epoch #26: loss=0.05437833675413381
Epoch #27: loss=0.06857178766415062
Epoch #28: loss=0.04640660023440169
Epoch #29: loss=0.055351442925306596
Epoch #30: loss=0.04355231230147183
Epoch #31: loss=0.04579848822848647
Epoch #32: loss=0.053457713838812544
Epoch #33: loss=0.03732291403148009
Epoch #34: loss=0.048093115495719825
Epoch #35: loss=0.04989719096885388
Epoch #36: loss=0.04428582763192749
Epoch #37: loss=0.04240227487016784
Epoch #38: loss=0.03548741928604712
Epoch #39: loss=0.03178415582262832
Epoch #40: loss=0.03739449221932565
Epoch #41: loss=0.027836221839716457
Epoch #42: loss=0.03741941263854324
Epoch #43: loss=0.032693501014868356
Epoch #44: loss=0.03907373820498855
Epoch #45: loss=0.030611179667351072
Epoch #46: loss=0.03958236722763717
Epoch #47: loss=0.029031004714611244
Epoch #48: loss=0.029584948667072612
Epoch #49: loss=0.030388494773981083
Epoch #50: loss=0.028892317290995718
Epoch #51: loss=0.023782691031172646
Epoch #52: loss=0.050474238866682064
Epoch #53: loss=0.033615868845786226
Epoch #54: loss=0.024325757700156037
Epoch #55: loss=0.035086241905599926
Epoch #56: loss=0.03552960215384668
Epoch #57: loss=0.030193240350569566
Epoch #58: loss=0.03213896064847955
Epoch #59: loss=0.028295776069357834
Epoch #60: loss=0.08550568824810682
Epoch #61: loss=0.02978453269622468
Epoch #62: loss=0.021633680842196884
Epoch #63: loss=0.0217202183750531
Epoch #64: loss=0.02372542430060433
Epoch #65: loss=0.024094740016503732
Epoch #66: loss=0.024919722078814092
Epoch #67: loss=0.0222856724064612
Epoch #68: loss=0.03117335004295806
Epoch #69: loss=0.019039783950443437
Epoch #70: loss=0.01898086346904691
Epoch #71: loss=0.02725560755569892
Epoch #72: loss=0.020709111560023873
Epoch #73: loss=0.02042718863529181
Epoch #74: loss=0.01508219152792772
Epoch #75: loss=0.029888771936165574
Epoch #76: loss=0.016901139074204354
Epoch #77: loss=0.020708373149208925
Epoch #78: loss=0.019107993678835006
Epoch #79: loss=0.01974017728338893
Epoch #80: loss=0.013528140682196918
Epoch #81: loss=0.03012146213174663
Epoch #82: loss=0.029302602074549433
Epoch #83: loss=0.020314152467875258
Epoch #84: loss=0.014220241539801629
Epoch #85: loss=0.018369226671751105
Epoch #86: loss=0.02155448104527005
Epoch #87: loss=0.023594437127775817
Epoch #88: loss=0.018217793339372512
Epoch #89: loss=0.02216318601578283
Epoch #90: loss=0.034770951960312976
Epoch #91: loss=0.022026680014026072
Epoch #92: loss=0.01959279932380452
Epoch #93: loss=0.04215429160927692
Epoch #94: loss=0.017851870358677534
Epoch #95: loss=0.01845518188797925
Epoch #96: loss=0.014419143102889186
Epoch #97: loss=0.01804938910235174
Epoch #98: loss=0.018571879179988347
Epoch #99: loss=0.01620516012610688
Epoch #100: loss=0.012883663631843106
Epoch #101: loss=0.018113934575560424
Epoch #102: loss=0.0393348230341455
Epoch #103: loss=0.016627239414556683
Epoch #104: loss=0.01368678505767378
Epoch #105: loss=0.014883846500909506
Epoch #106: loss=0.0131303204480539
Epoch #107: loss=0.020324891261827953
Epoch #108: loss=0.01561566134259043
Epoch #109: loss=0.016028624907569365
Epoch #110: loss=0.014596799366261686
Epoch #111: loss=0.021716340076976388
Epoch #112: loss=0.01773878954826428
Epoch #113: loss=0.02057481975594805
Epoch #114: loss=0.022585728588473356
Epoch #115: loss=0.023002531922966557
Epoch #116: loss=0.012503458400441373
Epoch #117: loss=0.011555042174902145
Epoch #118: loss=0.018337035929315713
Epoch #119: loss=0.022775516722909544
Epoch #120: loss=0.03653897064722547
Epoch #121: loss=0.02322364502680398
Epoch #122: loss=0.016162048886966308
Epoch #123: loss=0.010102619566738674
Epoch #124: loss=0.015607655752998642
Epoch #125: loss=0.013255615868740323
Epoch #126: loss=0.013986948947001428
Epoch #127: loss=0.014640010911052631
Epoch #128: loss=0.01748814695817699
Epoch #129: loss=0.009570800580160766
Epoch #130: loss=0.013911974134557623
Epoch #131: loss=0.01555139580685421
Epoch #132: loss=0.01285751155066179
Epoch #133: loss=0.021395743047329035
Epoch #134: loss=0.015808693754293182
Epoch #135: loss=0.014997852198093714
Epoch #136: loss=0.04004744123173064
Epoch #137: loss=0.010164763499507175
Epoch #138: loss=0.010922288050764964
Epoch #139: loss=0.010504642007409606
Epoch #140: loss=0.01838874397571451
Epoch #141: loss=0.015137083395836588
Epoch #142: loss=0.011805257188343493
Epoch #143: loss=0.020359037247456958
Epoch #144: loss=0.0202017100733012
Epoch #145: loss=0.014790870190362435
Epoch #146: loss=0.01120693617981104
Epoch #147: loss=0.013101451574573686
Epoch #148: loss=0.010992241097724273
Epoch #149: loss=0.013886208352155939
Epoch #150: loss=0.011520449781133962
Epoch #151: loss=0.007079800913557101
Epoch #152: loss=0.00969771507042895
Epoch #153: loss=0.03553288344715312
Epoch #154: loss=0.013190097874556444
Epoch #155: loss=0.011697183830729483
Epoch #156: loss=0.014307351519054936
Epoch #157: loss=0.014964825597680061
Epoch #158: loss=0.017222747954971952
Epoch #159: loss=0.011556117947589248
Epoch #160: loss=0.017813102334298767
Epoch #161: loss=0.016045925498649714
Epoch #162: loss=0.010633493898645414
Epoch #163: loss=0.011503031841129996
Epoch #164: loss=0.01037325975744626
Epoch #165: loss=0.015698141717500648
Epoch #166: loss=0.013139956012222332
Epoch #167: loss=0.010188584291854458
Epoch #168: loss=0.016921800579132333
Epoch #169: loss=0.012366192077331424
Epoch #170: loss=0.013980962413907365
Epoch #171: loss=0.014688971860569913
Epoch #172: loss=0.020007500535610737
Epoch #173: loss=0.019043987042534287
Epoch #174: loss=0.015258654696075806
Epoch #175: loss=0.020760194179833394
Epoch #176: loss=0.01463827153316107
Epoch #177: loss=0.008008737149399408
Epoch #178: loss=0.0066874669209937565
Epoch #179: loss=0.011563904527876335
Epoch #180: loss=0.011077044319911265
Epoch #181: loss=0.008999033555011854
Epoch #182: loss=0.016413763668040985
Epoch #183: loss=0.016231890978114203
Epoch #184: loss=0.015379865173862669
Epoch #185: loss=0.007961387361297879
Epoch #186: loss=0.031000663578712433
Epoch #187: loss=0.016596098776895506
Epoch #188: loss=0.02820456482688139
Epoch #189: loss=0.01180798804756683
Epoch #190: loss=0.014778285246645382
Epoch #191: loss=0.011531845835656073
Epoch #192: loss=0.009769172180378781
Epoch #193: loss=0.010419071097882796
Epoch #194: loss=0.010414120851029187
Epoch #195: loss=0.008601605581068462
Epoch #196: loss=0.007949171877285208
Epoch #197: loss=0.010655542354678724
Epoch #198: loss=0.013658839315968476
Epoch #199: loss=0.012423871372228893
Epoch #200: loss=0.014581416216356322
Epoch #201: loss=0.009734653833768252
Epoch #202: loss=0.010279438263071835
Epoch #203: loss=0.011530322471829215
Epoch #204: loss=0.012020628729797282
Epoch #205: loss=0.008486169539055372
Epoch #206: loss=0.021744872236319997
Epoch #207: loss=0.01334064328579824
Epoch #208: loss=0.014409915507274556
Epoch #209: loss=0.011881463739148578
Epoch #210: loss=0.015678970290471363
Epoch #211: loss=0.014477097266634186
Epoch #212: loss=0.01128480245246333
Epoch #213: loss=0.013521189392538718
Epoch #214: loss=0.015132486707487436
Epoch #215: loss=0.011465073751982762
Epoch #216: loss=0.0176469829935105
Epoch #217: loss=0.007012674631796362
Epoch #218: loss=0.009056631141168302
Epoch #219: loss=0.009848939291741882
Epoch #220: loss=0.012831388053123794
Epoch #221: loss=0.013235073929711481
Epoch #222: loss=0.010888777934039343
Epoch #223: loss=0.012156351955017743
Epoch #224: loss=0.00931985527061344
Epoch #225: loss=0.010463749349495278
Epoch #226: loss=0.008165182071704283
Epoch #227: loss=0.011977455649049448
Epoch #228: loss=0.010534648608531332
Epoch #229: loss=0.01632815482026858
Epoch #230: loss=0.01540379353317836
Epoch #231: loss=0.016869282741092582
Epoch #232: loss=0.010540627038926139
Epoch #233: loss=0.012746307108774276
Epoch #234: loss=0.010772724444181966
Epoch #235: loss=0.0069097505985122425
Epoch #236: loss=0.010562008438321217
Epoch #237: loss=0.008908306439764832
Epoch #238: loss=0.010142903881135581
Epoch #239: loss=0.01085526180483053
Epoch #240: loss=0.006255410729535065
Epoch #241: loss=0.010255012854776648
Epoch #242: loss=0.012798165936595957
Epoch #243: loss=0.011969632720620321
Epoch #244: loss=0.007282864382607632
Epoch #245: loss=0.009293326548565774
Epoch #246: loss=0.020421267112616045
Epoch #247: loss=0.011070720483065044
Epoch #248: loss=0.011271746570022822
Epoch #249: loss=0.009698276796885934

Training time: 4:31:53.762448

Finished.
n2one setting ettm2_electricity -> weather
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/ettm2_electricity_epochs_250_seed_2024/model.pkl', muti_dataset='ettm2_electricity', random_seed=2024, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='weather')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=9.39437e-09): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.87388e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=4.58676e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.87388e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.7603599583042165, 'MAE': 0.6812805672836825}
Finished.
------------------------- record done -------------------------
n2one setting ettm2_electricity -> exchange
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/ettm2_electricity_epochs_250_seed_2024/model.pkl', muti_dataset='ettm2_electricity', random_seed=2024, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='exchange')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=4.81883e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.28420788255392787, 'MAE': 0.35352659838028655}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='ettm2_traffic', random_seed=2024, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/ettm2_traffic_epochs_250_seed_2024
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=1.1115489758781552
Epoch #1: loss=0.3744525250905158
Epoch #2: loss=0.27295241679010523
Epoch #3: loss=0.21413845208741272
Epoch #4: loss=0.16486436907385496
Epoch #5: loss=0.13196663616090706
Epoch #6: loss=0.09860504294221822
Epoch #7: loss=0.09446512262538537
Epoch #8: loss=0.08102280834320622
Epoch #9: loss=0.09253118662665015
Epoch #10: loss=0.07706625880737536
Epoch #11: loss=0.05554558627172997
Epoch #12: loss=0.05855995090998376
Epoch #13: loss=0.06367268564098462
Epoch #14: loss=0.05460530830566416
Epoch #15: loss=0.04926371575628684
Epoch #16: loss=0.04524349711594286
Epoch #17: loss=0.038426913638393324
Epoch #18: loss=0.03371939853042842
Epoch #19: loss=0.054996560361179794
Epoch #20: loss=0.04424790440384354
Epoch #21: loss=0.03654392216971223
Epoch #22: loss=0.03849165396976102
Epoch #23: loss=0.030251031112217077
Epoch #24: loss=0.03374085225296108
Epoch #25: loss=0.03334455405877218
Epoch #26: loss=0.03234847380330558
Epoch #27: loss=0.029637972955626115
Epoch #28: loss=0.029468300820884103
Epoch #29: loss=0.02769738265909553
Epoch #30: loss=0.02876596152595749
Epoch #31: loss=0.02865708816288741
Epoch #32: loss=0.02792592091848039
Epoch #33: loss=0.026126331071511234
Epoch #34: loss=0.02112065597566939
Epoch #35: loss=0.031367153338826594
Epoch #36: loss=0.02071423743697329
Epoch #37: loss=0.025492485093629204
Epoch #38: loss=0.031703311944500955
Epoch #39: loss=0.03077540120866282
Epoch #40: loss=0.017125077636654832
Epoch #41: loss=0.0246876424544796
Epoch #42: loss=0.0238631610571378
Epoch #43: loss=0.02746282541733139
Epoch #44: loss=0.023898241052205088
Epoch #45: loss=0.021516819352447827
Epoch #46: loss=0.01916942918776638
Epoch #47: loss=0.020620035226558166
Epoch #48: loss=0.020809900119639603
Epoch #49: loss=0.021026277806295693
Epoch #50: loss=0.019800033036473805
Epoch #51: loss=0.01859700008344016
Epoch #52: loss=0.01929357440535188
Epoch #53: loss=0.020235290429652227
Epoch #54: loss=0.01938028488742115
Epoch #55: loss=0.017285118041031184
Epoch #56: loss=0.021627664387016476
Epoch #57: loss=0.019205123007421266
Epoch #58: loss=0.019093917223773172
Epoch #59: loss=0.01896323188881999
Epoch #60: loss=0.01588749579412122
Epoch #61: loss=0.018550983159792186
Epoch #62: loss=0.019062894594109907
Epoch #63: loss=0.0143883602088864
Epoch #64: loss=0.019938348077265876
Epoch #65: loss=0.020097144355331754
Epoch #66: loss=0.019042990666884557
Epoch #67: loss=0.022193948366628045
Epoch #68: loss=0.017156969798293568
Epoch #69: loss=0.018920373663274297
Epoch #70: loss=0.01880754258634068
Epoch #71: loss=0.018042159887518252
Epoch #72: loss=0.015253240879050873
Epoch #73: loss=0.020532354561866445
Epoch #74: loss=0.015334057276679112
Epoch #75: loss=0.0180366614353811
Epoch #76: loss=0.015153964120623436
Epoch #77: loss=0.012557754374661547
Epoch #78: loss=0.018593001049179967
Epoch #79: loss=0.013429835027640894
Epoch #80: loss=0.01542058365758485
Epoch #81: loss=0.01481763420880288
Epoch #82: loss=0.014247970788611338
Epoch #83: loss=0.026118310127480122
Epoch #84: loss=0.013409043516680883
Epoch #85: loss=0.014639494054055189
Epoch #86: loss=0.010908124492846947
Epoch #87: loss=0.014462071206681495
Epoch #88: loss=0.019812535549373178
Epoch #89: loss=0.015061711375950968
Epoch #90: loss=0.015137631229928002
Epoch #91: loss=0.010826046990206943
Epoch #92: loss=0.016127667818712367
Epoch #93: loss=0.013347550012039385
Epoch #94: loss=0.014162758882641568
Epoch #95: loss=0.02367016473801845
Epoch #96: loss=0.011560526664300961
Epoch #97: loss=0.01605105446228189
Epoch #98: loss=0.013478359254700492
Epoch #99: loss=0.013207445242061473
Epoch #100: loss=0.01995126093263852
Epoch #101: loss=0.01276255891553552
Epoch #102: loss=0.016009731645041644
Epoch #103: loss=0.011876947836602909
Epoch #104: loss=0.01565573198754417
Epoch #105: loss=0.01403059428366395
Epoch #106: loss=0.01290665080090494
Epoch #107: loss=0.018900739438770464
Epoch #108: loss=0.02075248595576105
Epoch #109: loss=0.011521024576766748
Epoch #110: loss=0.023243606967927797
Epoch #111: loss=0.01281691931509749
Epoch #112: loss=0.011440660468565901
Epoch #113: loss=0.011460864810846594
Epoch #114: loss=0.020879801669159908
Epoch #115: loss=0.01399358783453265
Epoch #116: loss=0.014822557503874931
Epoch #117: loss=0.016665455031519134
Epoch #118: loss=0.014739802588728302
Epoch #119: loss=0.01037351411835913
Epoch #120: loss=0.013702735454985553
Epoch #121: loss=0.013303067800963622
Epoch #122: loss=0.010608029007782465
Epoch #123: loss=0.01123682335060117
Epoch #124: loss=0.014927998117782242
Epoch #125: loss=0.019492877532132205
Epoch #126: loss=0.013828365523897523
Epoch #127: loss=0.013021580003505201
Epoch #128: loss=0.014325882756944163
Epoch #129: loss=0.012449361096210415
Epoch #130: loss=0.01441465697727923
Epoch #131: loss=0.015053349877214928
Epoch #132: loss=0.014380739263663027
Epoch #133: loss=0.013785288693505485
Epoch #134: loss=0.011084059052608581
Epoch #135: loss=0.011450499780252064
Epoch #136: loss=0.011124145292731926
Epoch #137: loss=0.010994962583859192
Epoch #138: loss=0.010477918382618846
Epoch #139: loss=0.0360477932875413
Epoch #140: loss=0.017764353294255967
Epoch #141: loss=0.009672478011833026
Epoch #142: loss=0.013795631133110069
Epoch #143: loss=0.007930601762368596
Epoch #144: loss=0.010058409093203342
Epoch #145: loss=0.010681321201007227
Epoch #146: loss=0.01490795460215184
Epoch #147: loss=0.010829951633010614
Epoch #148: loss=0.012889623650135514
Epoch #149: loss=0.01056877342236782
Epoch #150: loss=0.015137125323477222
Epoch #151: loss=0.010498020710706426
Epoch #152: loss=0.014138309497505967
Epoch #153: loss=0.0101319760566249
Epoch #154: loss=0.01519640356262656
Epoch #155: loss=0.009220300906200513
Epoch #156: loss=0.012564590089867224
Epoch #157: loss=0.011869253885958537
Epoch #158: loss=0.012933812030089137
Epoch #159: loss=0.012244393657463553
Epoch #160: loss=0.01726142867212397
Epoch #161: loss=0.014508009100211373
Epoch #162: loss=0.021161991613980396
Epoch #163: loss=0.007933093724327742
Epoch #164: loss=0.012854757452240151
Epoch #165: loss=0.00788320113140607
Epoch #166: loss=0.010152632989839332
Epoch #167: loss=0.013076557399172182
Epoch #168: loss=0.017330145262254303
Epoch #169: loss=0.011334842011596014
Epoch #170: loss=0.011428054002251525
Epoch #171: loss=0.013520844986262616
Epoch #172: loss=0.009469534658649798
Epoch #173: loss=0.009845630878286292
Epoch #174: loss=0.011123990459545456
Epoch #175: loss=0.007148191045529267
Epoch #176: loss=0.012610559351358883
Epoch #177: loss=0.009176262154965049
Epoch #178: loss=0.008450615878066836
Epoch #179: loss=0.014398352538095066
Epoch #180: loss=0.015894253611074438
Epoch #181: loss=0.007644430397239482
Epoch #182: loss=0.012184827104601571
Epoch #183: loss=0.013358032882109846
Epoch #184: loss=0.00844034127219842
Epoch #185: loss=0.011622149474900973
Epoch #186: loss=0.010540121171561726
Epoch #187: loss=0.013474191688642573
Epoch #188: loss=0.015556880663497138
Epoch #189: loss=0.009614776826428568
Epoch #190: loss=0.007138013092690027
Epoch #191: loss=0.013329508177673909
Epoch #192: loss=0.012244375634258122
Epoch #193: loss=0.014204713163390928
Epoch #194: loss=0.009890961982062785
Epoch #195: loss=0.012656594679229104
Epoch #196: loss=0.01028294653139282
Epoch #197: loss=0.00852443561948239
Epoch #198: loss=0.00812785007757466
Epoch #199: loss=0.015162179406899276
Epoch #200: loss=0.017206600547436397
Epoch #201: loss=0.011429442053071073
Epoch #202: loss=0.010632172426248726
Epoch #203: loss=0.009063804320963568
Epoch #204: loss=0.0118045363007012
Epoch #205: loss=0.020614777833681003
Epoch #206: loss=0.01754636835183
Epoch #207: loss=0.00914579215630384
Epoch #208: loss=0.007954104159234431
Epoch #209: loss=0.011218624053544814
Epoch #210: loss=0.019913289233483962
Epoch #211: loss=0.009165028387716387
Epoch #212: loss=0.007525324875743894
Epoch #213: loss=0.011252695273026959
Epoch #214: loss=0.01066936600764249
Epoch #215: loss=0.007088708496640009
Epoch #216: loss=0.016380226548519363
Epoch #217: loss=0.011222235489341821
Epoch #218: loss=0.01109008262149151
Epoch #219: loss=0.008644932868612695
Epoch #220: loss=0.01029787040664281
Epoch #221: loss=0.007900083308645021
Epoch #222: loss=0.008666896079134175
Epoch #223: loss=0.01179912501626983
Epoch #224: loss=0.008552914487435627
Epoch #225: loss=0.009075056762253318
Epoch #226: loss=0.00873664872759876
Epoch #227: loss=0.011613345216808495
Epoch #228: loss=0.0077224102853448
Epoch #229: loss=0.010256928602148985
Epoch #230: loss=0.015300101899059984
Epoch #231: loss=0.007907073271601783
Epoch #232: loss=0.020270410307635312
Epoch #233: loss=0.0075243822551260536
Epoch #234: loss=0.010025882790786457
Epoch #235: loss=0.0069051326082181
Epoch #236: loss=0.012405611270043054
Epoch #237: loss=0.013359981610972188
Epoch #238: loss=0.0059058229829812
Epoch #239: loss=0.019023913631622678
Epoch #240: loss=0.01155764904520757
Epoch #241: loss=0.009193130719287356
Epoch #242: loss=0.010410964981547982
Epoch #243: loss=0.008855216663130367
Epoch #244: loss=0.006739329170044582
Epoch #245: loss=0.009576022064229014
Epoch #246: loss=0.010957594077878408
Epoch #247: loss=0.013959195394540219
Epoch #248: loss=0.006195335248151163
Epoch #249: loss=0.010040465607198503

Training time: 10:32:24.935502

Finished.
n2one setting ettm2_traffic -> electricity
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/ettm2_traffic_epochs_250_seed_2024/model.pkl', muti_dataset='ettm2_traffic', random_seed=2024, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='electricity')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=7.98073e-09): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.84259e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=4.68007e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=7.98073e-09): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.4015247617012774, 'MAE': 0.4481136711842022}
Finished.
------------------------- record done -------------------------
n2one setting ettm2_traffic -> weather
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/ettm2_traffic_epochs_250_seed_2024/model.pkl', muti_dataset='ettm2_traffic', random_seed=2024, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='weather')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=9.94894e-09): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.00177e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=5.211e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=9.94894e-09): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.5478502563552389, 'MAE': 0.579193634690408}
Finished.
------------------------- record done -------------------------
n2one setting ettm2_traffic -> exchange
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/ettm2_traffic_epochs_250_seed_2024/model.pkl', muti_dataset='ettm2_traffic', random_seed=2024, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='exchange')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=4.65441e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.3065522668850918, 'MAE': 0.3664095672468654}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='ettm2_weather', random_seed=2024, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/ettm2_weather_epochs_250_seed_2024
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=5.461003213315396
Epoch #1: loss=2.8502678548967517
Epoch #2: loss=2.8294718909907983
Epoch #3: loss=2.214711308479309
Epoch #4: loss=2.078688479758598
Epoch #5: loss=1.893008016251229
Epoch #6: loss=1.6405045470675907
Epoch #7: loss=1.6656566439448177
Epoch #8: loss=1.4233515327041213
Epoch #9: loss=1.394073476662507
Epoch #10: loss=1.2701767634701084
Epoch #11: loss=1.1938703768962138
Epoch #12: loss=1.2199638969189412
Epoch #13: loss=1.175648616777884
Epoch #14: loss=1.0460890418774373
Epoch #15: loss=1.00063492961832
Epoch #16: loss=0.8981824613906242
Epoch #17: loss=0.9004285158337774
Epoch #18: loss=0.8732969712566685
Epoch #19: loss=0.8537418649003312
Epoch #20: loss=0.8775797582961418
Epoch #21: loss=0.7454698971799902
Epoch #22: loss=0.7702470921181344
Epoch #23: loss=0.7051020184078732
Epoch #24: loss=0.7088605246028384
Epoch #25: loss=0.7628699943826005
Epoch #26: loss=0.6496692718686284
Epoch #27: loss=0.7129495973522598
Epoch #28: loss=0.7319154046677254
Epoch #29: loss=0.6947902739048004
Epoch #30: loss=0.6762160288321005
Epoch #31: loss=0.5899239488550134
Epoch #32: loss=0.6407387675465764
Epoch #33: loss=0.5589978630478317
Epoch #34: loss=0.6275513220477749
Epoch #35: loss=0.6230867102339461
Epoch #36: loss=0.5291297339104317
Epoch #37: loss=0.5158427993993502
Epoch #38: loss=0.4582808154660302
Epoch #39: loss=0.4328845537997581
Epoch #40: loss=0.43562081214543935
Epoch #41: loss=0.4540397267083864
Epoch #42: loss=0.41161696733655156
Epoch #43: loss=0.5268822634542311
Epoch #44: loss=0.535874133979952
Epoch #45: loss=0.4665514037415788
Epoch #46: loss=0.39169201335391485
Epoch #47: loss=0.48682692324793014
Epoch #48: loss=0.4356826181347306
Epoch #49: loss=0.3850104957818985
Epoch #50: loss=0.3510065598262323
Epoch #51: loss=0.3746823889983667
Epoch #52: loss=0.3524024965795311
Epoch #53: loss=0.3141110515272295
Epoch #54: loss=0.33926899690885803
Epoch #55: loss=0.4659488849543236
Epoch #56: loss=0.35003747126540624
Epoch #57: loss=0.34842022729886546
Epoch #58: loss=0.3308526756795677
Epoch #59: loss=0.3886854499578476
Epoch #60: loss=0.3539898157925219
Epoch #61: loss=0.3063222242368234
Epoch #62: loss=0.31795889421089274
Epoch #63: loss=0.2721539574700433
Epoch #64: loss=0.34350947471889287
Epoch #65: loss=0.2526341237731882
Epoch #66: loss=0.3196964026302905
Epoch #67: loss=0.2941594117799321
Epoch #68: loss=0.2235750372748117
Epoch #69: loss=0.28803696644467275
Epoch #70: loss=0.30419112822493993
Epoch #71: loss=0.3053493157431886
Epoch #72: loss=0.2211405910752915
Epoch #73: loss=0.2585756816976779
Epoch #74: loss=0.2680383189707189
Epoch #75: loss=0.22807233841032595
Epoch #76: loss=0.3245838687226579
Epoch #77: loss=0.21852091961615794
Epoch #78: loss=0.2140135295890473
Epoch #79: loss=0.20941500989971934
Epoch #80: loss=0.2089411004169567
Epoch #81: loss=0.503309083146018
Epoch #82: loss=0.25252239990073283
Epoch #83: loss=0.2938522972367905
Epoch #84: loss=0.2647663733443698
Epoch #85: loss=0.2074559187969646
Epoch #86: loss=0.1793950000727499
Epoch #87: loss=0.2684483999336088
Epoch #88: loss=0.24627365433686488
Epoch #89: loss=0.22390953229891286
Epoch #90: loss=0.3077222996869603
Epoch #91: loss=0.24487408008929845
Epoch #92: loss=0.23442194268510147
Epoch #93: loss=0.1738054101934304
Epoch #94: loss=0.14125327101430377
Epoch #95: loss=0.20769414788967855
Epoch #96: loss=0.1660410122492829
Epoch #97: loss=0.14789301589936824
Epoch #98: loss=0.19793074004150726
Epoch #99: loss=0.16925278587921247
Epoch #100: loss=0.13423631803409472
Epoch #101: loss=0.11067325531228168
Epoch #102: loss=0.16154729806490847
Epoch #103: loss=0.1711369407539432
Epoch #104: loss=0.15365603082888835
Epoch #105: loss=0.1865235107976037
Epoch #106: loss=0.13798613141517382
Epoch #107: loss=0.1260287115300024
Epoch #108: loss=0.0936165442941962
Epoch #109: loss=0.10798437994074177
Epoch #110: loss=0.1802225453225342
Epoch #111: loss=0.15706018076555148
Epoch #112: loss=0.0861951339285116
Epoch #113: loss=0.16699807003543182
Epoch #114: loss=0.1603180276783737
Epoch #115: loss=0.23387995170983109
Epoch #116: loss=0.12322681226037643
Epoch #117: loss=0.15793639753718633
Epoch #118: loss=0.13141786085592733
Epoch #119: loss=0.10694705231769665
Epoch #120: loss=0.13297646200737437
Epoch #121: loss=0.11410263709321215
Epoch #122: loss=0.12603214906679616
Epoch #123: loss=0.11360933621590202
Epoch #124: loss=0.0996659111130882
Epoch #125: loss=0.09229737751789995
Epoch #126: loss=0.1084467468632234
Epoch #127: loss=0.17249415019476735
Epoch #128: loss=0.13107161123204875
Epoch #129: loss=0.15210502020813324
Epoch #130: loss=0.1155419881279404
Epoch #131: loss=0.10531211347394698
Epoch #132: loss=0.1074287581282693
Epoch #133: loss=0.10034319461398833
Epoch #134: loss=0.19078038023734414
Epoch #135: loss=0.13523798889002284
Epoch #136: loss=0.11523020614844721
Epoch #137: loss=0.08009907240803177
Epoch #138: loss=0.0988738745753024
Epoch #139: loss=0.0730082152562367
Epoch #140: loss=0.09961485953347103
Epoch #141: loss=0.07184661617754279
Epoch #142: loss=0.1055982899222825
Epoch #143: loss=0.069720890070941
Epoch #144: loss=0.07522449733034985
Epoch #145: loss=0.08856547075147564
Epoch #146: loss=0.07639109912152225
Epoch #147: loss=0.08570292400749954
Epoch #148: loss=0.1515093466418015
Epoch #149: loss=0.09446818577880794
Epoch #150: loss=0.06394963938038091
Epoch #151: loss=0.0794325077251808
Epoch #152: loss=0.0858103255363735
Epoch #153: loss=0.15591803198126522
Epoch #154: loss=0.1290603300204148
Epoch #155: loss=0.08852990052184544
Epoch #156: loss=0.07579588361487195
Epoch #157: loss=0.10231883577196985
Epoch #158: loss=0.07807907774238973
Epoch #159: loss=0.11600781680160277
Epoch #160: loss=0.11301105031491937
Epoch #161: loss=0.22101679091920723
Epoch #162: loss=0.1961598800042191
Epoch #163: loss=0.11218785801650705
Epoch #164: loss=0.09151165479341068
Epoch #165: loss=0.1388178626327096
Epoch #166: loss=0.11068682246715636
Epoch #167: loss=0.09088598891488604
Epoch #168: loss=0.07735391368938459
Epoch #169: loss=0.07979313621448504
Epoch #170: loss=0.07587259136945815
Epoch #171: loss=0.04860119083644571
Epoch #172: loss=0.06379349371166648
Epoch #173: loss=0.09087523669507858
Epoch #174: loss=0.11347491550888564
Epoch #175: loss=0.10606547132939906
Epoch #176: loss=0.1716962509562035
Epoch #177: loss=0.12580209708697088
Epoch #178: loss=0.09255254459944931
Epoch #179: loss=0.08970121060170837
Epoch #180: loss=0.08230121917015798
Epoch #181: loss=0.06833785002094668
Epoch #182: loss=0.10266031390307723
Epoch #183: loss=0.06391546905443475
Epoch #184: loss=0.06238821116150231
Epoch #185: loss=0.05234480694540449
Epoch #186: loss=0.057625767346974964
Epoch #187: loss=0.061892277812836946
Epoch #188: loss=0.03978649237369363
Epoch #189: loss=0.044292173272854576
Epoch #190: loss=0.040581473552093315
Epoch #191: loss=0.05369666329509503
Epoch #192: loss=0.11442864645977278
Epoch #193: loss=0.053134159940118726
Epoch #194: loss=0.047186222758043454
Epoch #195: loss=0.07650733457223789
Epoch #196: loss=0.057656495202634786
Epoch #197: loss=0.05441316612366889
Epoch #198: loss=0.09011608201104242
Epoch #199: loss=0.06851165240781533
Epoch #200: loss=0.07760168871263394
Epoch #201: loss=0.04573544419395763
Epoch #202: loss=0.04595793239973687
Epoch #203: loss=0.041914628994827334
Epoch #204: loss=0.04358488223137888
Epoch #205: loss=0.04058994936781961
Epoch #206: loss=0.039817966948691254
Epoch #207: loss=0.050337680212750625
Epoch #208: loss=0.06470441813203129
Epoch #209: loss=0.03929622077408272
Epoch #210: loss=0.06895574063968819
Epoch #211: loss=0.044632375693401775
Epoch #212: loss=0.03170400757241894
Epoch #213: loss=0.05818090927661271
Epoch #214: loss=0.08505789706533826
Epoch #215: loss=0.03918464700817256
Epoch #216: loss=0.044192465022206306
Epoch #217: loss=0.050578756836821906
Epoch #218: loss=0.03442643628128477
Epoch #219: loss=0.09801126631429873
Epoch #220: loss=0.05181504808668349
Epoch #221: loss=0.04416189992145912
Epoch #222: loss=0.041356378625071535
Epoch #223: loss=0.04723587549115355
Epoch #224: loss=0.06385338712936721
Epoch #225: loss=0.062325017551916675
Epoch #226: loss=0.0542569989590226
Epoch #227: loss=0.04271065071225166
Epoch #228: loss=0.05304528901202453
Epoch #229: loss=0.07918439450598247
Epoch #230: loss=0.04095227905624622
Epoch #231: loss=0.05920055253129151
Epoch #232: loss=0.05484254169907119
Epoch #233: loss=0.041477965895791315
Epoch #234: loss=0.035196550932989734
Epoch #235: loss=0.0641601607329338
Epoch #236: loss=0.06091750503794567
Epoch #237: loss=0.055390734338780515
Epoch #238: loss=0.05213968011525434
Epoch #239: loss=0.04378279827132418
Epoch #240: loss=0.03554530836943839
Epoch #241: loss=0.04799257664362321
Epoch #242: loss=0.0341696064994746
Epoch #243: loss=0.04395872523152345
Epoch #244: loss=0.04037533236063413
Epoch #245: loss=0.0432775365566281
Epoch #246: loss=0.041700474641008956
Epoch #247: loss=0.06369412178173661
Epoch #248: loss=0.05576705864655811
Epoch #249: loss=0.06705838137281102

Training time: 0:38:34.873836

Finished.
n2one setting ettm2_weather -> electricity
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/ettm2_weather_epochs_250_seed_2024/model.pkl', muti_dataset='ettm2_weather', random_seed=2024, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='electricity')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.37751e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.78416e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.37751e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.35789457156630783, 'MAE': 0.4186341186812045}
Finished.
------------------------- record done -------------------------
n2one setting ettm2_weather -> exchange
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/ettm2_weather_epochs_250_seed_2024/model.pkl', muti_dataset='ettm2_weather', random_seed=2024, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='exchange')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=3.60147e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.23095538115547148, 'MAE': 0.3236974960227809}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='ettm2_exchange', random_seed=2024, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/ettm2_exchange_epochs_250_seed_2024
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=5.575524499541835
Epoch #1: loss=2.8230146420629403
Epoch #2: loss=2.3227333520588123
Epoch #3: loss=2.1479848121341907
Epoch #4: loss=2.048856917180513
Epoch #5: loss=1.8960826773392527
Epoch #6: loss=1.89071702329736
Epoch #7: loss=1.7444870471954346
Epoch #8: loss=1.7260602210697376
Epoch #9: loss=1.5882261489567004
Epoch #10: loss=1.5698294137653552
Epoch #11: loss=1.4503870826018483
Epoch #12: loss=1.4883375732522262
Epoch #13: loss=1.3909508366333811
Epoch #14: loss=1.3859726755242598
Epoch #15: loss=1.3744300854833502
Epoch #16: loss=1.3624102755596763
Epoch #17: loss=1.2728690473656905
Epoch #18: loss=1.2525122918580707
Epoch #19: loss=1.252383457986932
Epoch #20: loss=1.121780979005914
Epoch #21: loss=1.1718846057590686
Epoch #22: loss=1.059159018491444
Epoch #23: loss=1.097389086296684
Epoch #24: loss=1.0043083209740489
Epoch #25: loss=1.0176349752827694
Epoch #26: loss=1.0596250295639038
Epoch #27: loss=0.9924302728552568
Epoch #28: loss=0.971250810121235
Epoch #29: loss=0.9304130453812448
Epoch #30: loss=1.0426057671245776
Epoch #31: loss=0.9571585216020283
Epoch #32: loss=0.897745320671483
Epoch #33: loss=0.9216795626439547
Epoch #34: loss=0.8301970299921537
Epoch #35: loss=0.8248199287213778
Epoch #36: loss=0.8649789534117046
Epoch #37: loss=0.8370762122304816
Epoch #38: loss=0.8001957316147653
Epoch #39: loss=0.834886306210568
Epoch #40: loss=0.7931593593798185
Epoch #41: loss=0.7825735870160555
Epoch #42: loss=0.8638328407940111
Epoch #43: loss=0.7105088484914679
Epoch #44: loss=0.8334303874718515
Epoch #45: loss=0.8630758555311906
Epoch #46: loss=0.8463261409809715
Epoch #47: loss=0.6681499559628336
Epoch #48: loss=0.7668179430459675
Epoch #49: loss=0.7381012408356917
Epoch #50: loss=0.6976846048706457
Epoch #51: loss=0.6007273840276819
Epoch #52: loss=0.6637098538248163
Epoch #53: loss=0.5057112213812376
Epoch #54: loss=0.5512479073122928
Epoch #55: loss=0.5809088694421869
Epoch #56: loss=0.5799846241348668
Epoch #57: loss=0.5733974638738131
Epoch #58: loss=0.6381832016141791
Epoch #59: loss=0.5953645314040937
Epoch #60: loss=0.6397714614868164
Epoch #61: loss=0.579210987216548
Epoch #62: loss=0.5722999588439339
Epoch #63: loss=0.5658572394596902
Epoch #64: loss=0.5393540106321636
Epoch #65: loss=0.5431844160745019
Epoch #66: loss=0.5463223567134455
Epoch #67: loss=0.7006304452293798
Epoch #68: loss=0.47254802208197744
Epoch #69: loss=0.42863778064125463
Epoch #70: loss=0.5086071820635545
Epoch #71: loss=0.4987734258174896
Epoch #72: loss=0.5393626650697306
Epoch #73: loss=0.5287640996669468
Epoch #74: loss=0.4989326957025026
Epoch #75: loss=0.44352562411835317
Epoch #76: loss=0.44125975590003164
Epoch #77: loss=0.5065077245235443
Epoch #78: loss=0.41330534298168986
Epoch #79: loss=0.45406956892264516
Epoch #80: loss=0.5370489100092336
Epoch #81: loss=0.4538117372675946
Epoch #82: loss=0.5442025010523043
Epoch #83: loss=0.4452181074180101
Epoch #84: loss=0.4381499164982846
Epoch #85: loss=0.4599978845370443
Epoch #86: loss=0.4538817350801669
Epoch #87: loss=0.43030206388548803
Epoch #88: loss=0.557795054034183
Epoch #89: loss=0.44951012495316955
Epoch #90: loss=0.4184766819602565
Epoch #91: loss=0.47648667034349945
Epoch #92: loss=0.3573083014864671
Epoch #93: loss=0.36589999261655304
Epoch #94: loss=0.40431370233234604
Epoch #95: loss=0.42010889908200816
Epoch #96: loss=0.4312057973522889
Epoch #97: loss=0.3202621834842782
Epoch #98: loss=0.4125656767895347
Epoch #99: loss=0.3629401690865818
Epoch #100: loss=0.35244669804447576
Epoch #101: loss=0.39837579821285446
Epoch #102: loss=0.5281569291102258
Epoch #103: loss=0.432416076722898
Epoch #104: loss=0.36086029363305944
Epoch #105: loss=0.3964271372870395
Epoch #106: loss=0.44289546655981166
Epoch #107: loss=0.3548681249744014
Epoch #108: loss=0.4301580018118808
Epoch #109: loss=0.42474091053009033
Epoch #110: loss=0.40372627502993536
Epoch #111: loss=0.3784913624587812
Epoch #112: loss=0.33613159076163646
Epoch #113: loss=0.29819822703537185
Epoch #114: loss=0.38291386828610774
Epoch #115: loss=0.4056373215035388
Epoch #116: loss=0.2932667575384441
Epoch #117: loss=0.35871847366031845
Epoch #118: loss=0.4355447386440478
Epoch #119: loss=0.3525184122355361
Epoch #120: loss=0.36391319412934153
Epoch #121: loss=0.31059976078962026
Epoch #122: loss=0.39479667889444453
Epoch #123: loss=0.29947059170195933
Epoch #124: loss=0.31636624116646617
Epoch #125: loss=0.29389694646785136
Epoch #126: loss=0.28738841336024434
Epoch #127: loss=0.277230566269473
Epoch #128: loss=0.37425612540621506
Epoch #129: loss=0.23315249069740898
Epoch #130: loss=0.2489116536943536
Epoch #131: loss=0.23058951528448807
Epoch #132: loss=0.23501029689061015
Epoch #133: loss=0.49866513162851334
Epoch #134: loss=0.33188485550253016
Epoch #135: loss=0.239724075323657
Epoch #136: loss=0.2307018590600867
Epoch #137: loss=0.2942446019304426
Epoch #138: loss=0.27301622064490066
Epoch #139: loss=0.20875876789030276
Epoch #140: loss=0.26602866657470403
Epoch #141: loss=0.29296802847008957
Epoch #142: loss=0.37086382861200134
Epoch #143: loss=0.21704993789133273
Epoch #144: loss=0.2762003619419901
Epoch #145: loss=0.2127014492687426
Epoch #146: loss=0.2664015065682562
Epoch #147: loss=0.2520787072809119
Epoch #148: loss=0.23803149163722992
Epoch #149: loss=0.2464230311544318
Epoch #150: loss=0.2561588301078269
Epoch #151: loss=0.22220217201270556
Epoch #152: loss=0.18701625458504023
Epoch #153: loss=0.21368396870399775
Epoch #154: loss=0.1716564495704676
Epoch #155: loss=0.20911786469974017
Epoch #156: loss=0.388435046923788
Epoch #157: loss=0.2056744118270121
Epoch #158: loss=0.2134407805769067
Epoch #159: loss=0.2906201803370526
Epoch #160: loss=0.1632077168477209
Epoch #161: loss=0.22174155202351117
Epoch #162: loss=0.2484354251309445
Epoch #163: loss=0.2108946105367259
Epoch #164: loss=0.19584927668696955
Epoch #165: loss=0.14195685363129565
Epoch #166: loss=0.2416606392515333
Epoch #167: loss=0.14326773152539604
Epoch #168: loss=0.14991790644432368
Epoch #169: loss=0.2132416691042875
Epoch #170: loss=0.19236137306219653
Epoch #171: loss=0.2115041362611871
Epoch #172: loss=0.3788516454790768
Epoch #173: loss=0.33674607112219457
Epoch #174: loss=0.3152284986878696
Epoch #175: loss=0.2093817638723474
Epoch #176: loss=0.24301349175603768
Epoch #177: loss=0.23126083181092613
Epoch #178: loss=0.25033531730112274
Epoch #179: loss=0.20638855703567205
Epoch #180: loss=0.2343495409739645
Epoch #181: loss=0.26715717347044693
Epoch #182: loss=0.2872387280589656
Epoch #183: loss=0.18929171562194824
Epoch #184: loss=0.24754012925060173
Epoch #185: loss=0.19664798324045382
Epoch #186: loss=0.187114102275748
Epoch #187: loss=0.14411137213832453
Epoch #188: loss=0.17710617792449498
Epoch #189: loss=0.23392441476646222
Epoch #190: loss=0.11638084955905613
Epoch #191: loss=0.13737434580137856
Epoch #192: loss=0.17987966713936707
Epoch #193: loss=0.13621878545535238
Epoch #194: loss=0.1822599149064014
Epoch #195: loss=0.1559053475135251
Epoch #196: loss=0.1437996933726888
Epoch #197: loss=0.15154666335959183
Epoch #198: loss=0.12593661581999377
Epoch #199: loss=0.13227862531417295
Epoch #200: loss=0.15864137481702
Epoch #201: loss=0.1319802924990654
Epoch #202: loss=0.1300927322161825
Epoch #203: loss=0.15010680356308034
Epoch #204: loss=0.12604191546377383
Epoch #205: loss=0.15661323894011348
Epoch #206: loss=0.12672338673942968
Epoch #207: loss=0.09668554149960217
Epoch #208: loss=0.14543481720121285
Epoch #209: loss=0.26543675186602694
Epoch #210: loss=0.2373480934061502
Epoch #211: loss=0.14512347351563604
Epoch #212: loss=0.21104534087996735
Epoch #213: loss=0.12242119308365018
Epoch #214: loss=0.19310330501512477
Epoch #215: loss=0.14671779247490982
Epoch #216: loss=0.12884596127428508
Epoch #217: loss=0.15044215988171727
Epoch #218: loss=0.1467665906407331
Epoch #219: loss=0.13643742529185196
Epoch #220: loss=0.17589372592537025
Epoch #221: loss=0.18322198975243068
Epoch #222: loss=0.138154035140025
Epoch #223: loss=0.38176184049562406
Epoch #224: loss=0.35483947004142563
Epoch #225: loss=0.23139502108097076
Epoch #226: loss=0.19414359252703817
Epoch #227: loss=0.2361078403497997
Epoch #228: loss=0.22016657300685583
Epoch #229: loss=0.21627364582137057
Epoch #230: loss=0.22743241920282967
Epoch #231: loss=0.19745392156274696
Epoch #232: loss=0.16619452696881795
Epoch #233: loss=0.10106438123866131
Epoch #234: loss=0.1685953646113998
Epoch #235: loss=0.13949275683415563
Epoch #236: loss=0.11153844959641758
Epoch #237: loss=0.06999239737265989
Epoch #238: loss=0.14673314674904472
Epoch #239: loss=0.11776248326427058
Epoch #240: loss=0.09773020230625805
Epoch #241: loss=0.09423405098679818
Epoch #242: loss=0.11657417663618137
Epoch #243: loss=0.11253763676473968
Epoch #244: loss=0.12107204587051743
Epoch #245: loss=0.10757891245578464
Epoch #246: loss=0.09492872939690163
Epoch #247: loss=0.13422856872019015
Epoch #248: loss=0.0972900211222862
Epoch #249: loss=0.12303865053936054

Training time: 0:16:51.174259

Finished.
n2one setting ettm2_exchange -> electricity
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/ettm2_exchange_epochs_250_seed_2024/model.pkl', muti_dataset='ettm2_exchange', random_seed=2024, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='electricity')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.31885e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.64723e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.31885e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.35361397961890123, 'MAE': 0.42395672352379404}
Finished.
------------------------- record done -------------------------
n2one setting ettm2_exchange -> weather
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/ettm2_exchange_epochs_250_seed_2024/model.pkl', muti_dataset='ettm2_exchange', random_seed=2024, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='weather')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.33105e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.53769e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=5.91885e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.33105e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.5462387766643837, 'MAE': 0.5445034830264183}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='electricity_traffic', random_seed=2024, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/electricity_traffic_epochs_250_seed_2024
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=0.7929310415220382
Epoch #1: loss=0.2810903204288507
Epoch #2: loss=0.19695892359743356
Epoch #3: loss=0.13654170766006762
Epoch #4: loss=0.10718645731228091
Epoch #5: loss=0.09378350387380119
Epoch #6: loss=0.06781476039899197
Epoch #7: loss=0.06391489120721439
Epoch #8: loss=0.04983523972600144
Epoch #9: loss=0.054086751503742356
Epoch #10: loss=0.044176347883589824
Epoch #11: loss=0.06709474203923114
Epoch #12: loss=0.041754533818372644
Epoch #13: loss=0.041082181704458126
Epoch #14: loss=0.03270934968328675
Epoch #15: loss=0.03604117989615283
Epoch #16: loss=0.03763259793677
Epoch #17: loss=0.029361099896705312
Epoch #18: loss=0.026444260219138805
Epoch #19: loss=0.031008514697389522
Epoch #20: loss=0.02746181874305706
Epoch #21: loss=0.027321781132260806
Epoch #22: loss=0.02333661127354893
Epoch #23: loss=0.017785737998572998
Epoch #24: loss=0.02898915555380877
Epoch #25: loss=0.01814720965311756
Epoch #26: loss=0.024798547395797083
Epoch #27: loss=0.021460805828006038
Epoch #28: loss=0.027936398124106587
Epoch #29: loss=0.018285706253534942
Epoch #30: loss=0.029224745240294715
Epoch #31: loss=0.02098019423688717
Epoch #32: loss=0.01830624656563598
Epoch #33: loss=0.017771855695074737
Epoch #34: loss=0.019370276286264382
Epoch #35: loss=0.01797818630448222
Epoch #36: loss=0.01723046930732571
Epoch #37: loss=0.017937152174424258
Epoch #38: loss=0.017194179043227717
Epoch #39: loss=0.017846397857608026
Epoch #40: loss=0.01649764748948031
Epoch #41: loss=0.021659484228775824
Epoch #42: loss=0.015843996824381756
Epoch #43: loss=0.012727437668620898
Epoch #44: loss=0.019986506922301205
Epoch #45: loss=0.01636372257585166
Epoch #46: loss=0.037915598494577395
Epoch #47: loss=0.012998017100198076
Epoch #48: loss=0.01374939222206438
Epoch #49: loss=0.01764678459122284
Epoch #50: loss=0.01743686536878772
Epoch #51: loss=0.016999790144337158
Epoch #52: loss=0.016285544260180047
Epoch #53: loss=0.01416347901705284
Epoch #54: loss=0.01531703644153144
Epoch #55: loss=0.013243211440130713
Epoch #56: loss=0.013871271780728254
Epoch #57: loss=0.017783477298243464
Epoch #58: loss=0.014000037511087148
Epoch #59: loss=0.015981766288646986
Epoch #60: loss=0.014759421479452908
Epoch #61: loss=0.015657080648759914
Epoch #62: loss=0.013228407291573844
Epoch #63: loss=0.011206991210936499
Epoch #64: loss=0.015083671174830279
Epoch #65: loss=0.012318363467756688
Epoch #66: loss=0.012443296134318674
Epoch #67: loss=0.015336016384851847
Epoch #68: loss=0.01481616624181465
Epoch #69: loss=0.009779215206926262
Epoch #70: loss=0.01394873897963072
Epoch #71: loss=0.01639933992460266
Epoch #72: loss=0.01365806270656546
Epoch #73: loss=0.02045640792804538
Epoch #74: loss=0.01229520352900348
Epoch #75: loss=0.017626609042669085
Epoch #76: loss=0.012514149176107605
Epoch #77: loss=0.012898247475207496
Epoch #78: loss=0.012280625065300331
Epoch #79: loss=0.01317047142010233
Epoch #80: loss=0.011456327233458605
Epoch #81: loss=0.014913491820548491
Epoch #82: loss=0.012388572576164488
Epoch #83: loss=0.0128300518544329
Epoch #84: loss=0.012250620497540993
Epoch #85: loss=0.012721069182217316
Epoch #86: loss=0.012346063391748157
Epoch #87: loss=0.0133561218959771
Epoch #88: loss=0.012788486046030084
Epoch #89: loss=0.012011920823139947
Epoch #90: loss=0.01239752943441252
Epoch #91: loss=0.01547710571928091
Epoch #92: loss=0.011495220819864082
Epoch #93: loss=0.011753688735474296
Epoch #94: loss=0.018167295891561113
Epoch #95: loss=0.012804059603031872
Epoch #96: loss=0.010283936884778529
Epoch #97: loss=0.009787187928834326
Epoch #98: loss=0.01355682581691965
Epoch #99: loss=0.013044460950427572
Epoch #100: loss=0.016100907164982255
Epoch #101: loss=0.010475489474841474
Epoch #102: loss=0.012572305306315182
Epoch #103: loss=0.012632339897635708
Epoch #104: loss=0.008068718822677982
Epoch #105: loss=0.015332770117893909
Epoch #106: loss=0.010380273370978278
Epoch #107: loss=0.012501021682291218
Epoch #108: loss=0.011877689548814945
Epoch #109: loss=0.01490933256992257
Epoch #110: loss=0.008414680053206792
Epoch #111: loss=0.011631650175942173
Epoch #112: loss=0.010130877854875844
Epoch #113: loss=0.013807479364827782
Epoch #114: loss=0.011629212553451894
Epoch #115: loss=0.012816164275982228
Epoch #116: loss=0.008942569398785889
Epoch #117: loss=0.01298187856500144
Epoch #118: loss=0.013788249019998073
Epoch #119: loss=0.007727562092956943
Epoch #120: loss=0.012037089392274188
Epoch #121: loss=0.011414034207705672
Epoch #122: loss=0.01408012442362423
Epoch #123: loss=0.008820242238281215
Epoch #124: loss=0.011342663397062499
Epoch #125: loss=0.011352888610021018
Epoch #126: loss=0.011907704467943257
Epoch #127: loss=0.01174227187127028
Epoch #128: loss=0.009365771274676839
Epoch #129: loss=0.012009341560062527
Epoch #130: loss=0.009796875357332747
Epoch #131: loss=0.02020674381255635
Epoch #132: loss=0.009801073770311567
Epoch #133: loss=0.008847244503783474
Epoch #134: loss=0.012558130123350175
Epoch #135: loss=0.009358096513535044
Epoch #136: loss=0.010872879048917854
Epoch #137: loss=0.011202935567708247
Epoch #138: loss=0.012674155797824518
Epoch #139: loss=0.012173682147134051
Epoch #140: loss=0.01237058842000953
Epoch #141: loss=0.008897153649843337
Epoch #142: loss=0.009533015092017217
Epoch #143: loss=0.02490888291245916
Epoch #144: loss=0.013203345879618926
Epoch #145: loss=0.009786104325036614
Epoch #146: loss=0.01028157910639605
Epoch #147: loss=0.008013705030141816
Epoch #148: loss=0.01310244251426096
Epoch #149: loss=0.007764525636427368
Epoch #150: loss=0.010732691220500468
Epoch #151: loss=0.010722764455239994
Epoch #152: loss=0.013521308207770574
Epoch #153: loss=0.007949856110236986
Epoch #154: loss=0.010004186893866255
Epoch #155: loss=0.014232884226491854
Epoch #156: loss=0.011362849446908088
Epoch #157: loss=0.007786915111592644
Epoch #158: loss=0.008832536853687713
Epoch #159: loss=0.01628872887023033
Epoch #160: loss=0.016489004602248166
Epoch #161: loss=0.010062506633643113
Epoch #162: loss=0.009609652379602871
Epoch #163: loss=0.010598277303613637
Epoch #164: loss=0.008522824684080915
Epoch #165: loss=0.010240286917249347
Epoch #166: loss=0.01830382548186479
Epoch #167: loss=0.010717316679485278
Epoch #168: loss=0.015011396949811833
Epoch #169: loss=0.006362137538614883
Epoch #170: loss=0.01359097083590335
Epoch #171: loss=0.013036678921411032
Epoch #172: loss=0.007380781156739727
Epoch #173: loss=0.011135345356413324
Epoch #174: loss=0.012394570226465933
Epoch #175: loss=0.012610247098172198
Epoch #176: loss=0.0069209393653988
Epoch #177: loss=0.010825636969455727
Epoch #178: loss=0.012352176995243167
Epoch #179: loss=0.0071183969450672
Epoch #180: loss=0.011957248885081987
Epoch #181: loss=0.00781923381306954
Epoch #182: loss=0.010534631253945596
Epoch #183: loss=0.012894037511808181
Epoch #184: loss=0.00758928183690576
Epoch #185: loss=0.008876094379283595
Epoch #186: loss=0.01948092477671178
Epoch #187: loss=0.008302534123041175
Epoch #188: loss=0.006725652050406981
Epoch #189: loss=0.011865139693193593
Epoch #190: loss=0.01091748310025901
Epoch #191: loss=0.008651159636741063
Epoch #192: loss=0.009340888827563175
Epoch #193: loss=0.008159742452761113
Epoch #194: loss=0.010244232237155787
Epoch #195: loss=0.011433058138144582
Epoch #196: loss=0.008591497100645929
Epoch #197: loss=0.008055537504576899
Epoch #198: loss=0.012710871750144256
Epoch #199: loss=0.00874719676116215
Epoch #200: loss=0.008924627443898544
Epoch #201: loss=0.006638985911464132
Epoch #202: loss=0.010927052372009262
Epoch #203: loss=0.006221991792158317
Epoch #204: loss=0.009428132903732615
Epoch #205: loss=0.008113848865105263
Epoch #206: loss=0.011763063450110682
Epoch #207: loss=0.008346920349506369
Epoch #208: loss=0.00902850147939857
Epoch #209: loss=0.00965046911075206
Epoch #210: loss=0.008327592628739984
Epoch #211: loss=0.009888623602187323
Epoch #212: loss=0.008434217952764852
Epoch #213: loss=0.01480506885608976
Epoch #214: loss=0.007992262960566501
Epoch #215: loss=0.007536295121016071
Epoch #216: loss=0.009493719831881835
Epoch #217: loss=0.006695705762269345
Epoch #218: loss=0.011958796999083449
Epoch #219: loss=0.007734139737420838
Epoch #220: loss=0.011000767252995458
Epoch #221: loss=0.009111771899561501
Epoch #222: loss=0.010160685277531464
Epoch #223: loss=0.0061637573689104245
Epoch #224: loss=0.014561817690104522
Epoch #225: loss=0.014782978321208088
Epoch #226: loss=0.005852681595256503
Epoch #227: loss=0.008219022367919429
Epoch #228: loss=0.007483928939824306
Epoch #229: loss=0.010752922948964025
Epoch #230: loss=0.009036082764842945
Epoch #231: loss=0.007556755370226242
Epoch #232: loss=0.008538075708214387
Epoch #233: loss=0.007849636273240384
Epoch #234: loss=0.007775583118437401
Epoch #235: loss=0.010233642607195913
Epoch #236: loss=0.009052583815777239
Epoch #237: loss=0.007814285228197562
Epoch #238: loss=0.006726407754368164
Epoch #239: loss=0.008221339280861365
Epoch #240: loss=0.009303242475614511
Epoch #241: loss=0.008062055218550128
Epoch #242: loss=0.008959447116481953
Epoch #243: loss=0.006231411690312501
Epoch #244: loss=0.006465919831712964
Epoch #245: loss=0.009344161230486614
Epoch #246: loss=0.00835839605102508
Epoch #247: loss=0.009944800178191998
Epoch #248: loss=0.00927984174493002
Epoch #249: loss=0.008363709944561723

Training time: 15:08:05.278954

Finished.
n2one setting electricity_traffic -> weather
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/electricity_traffic_epochs_250_seed_2024/model.pkl', muti_dataset='electricity_traffic', random_seed=2024, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='weather')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.44503e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.90126e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.44503e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.4158870594525273, 'MAE': 0.4886982517620191}
Finished.
------------------------- record done -------------------------
n2one setting electricity_traffic -> exchange
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/electricity_traffic_epochs_250_seed_2024/model.pkl', muti_dataset='electricity_traffic', random_seed=2024, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='exchange')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=4.14215e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.24364626783836735, 'MAE': 0.3344040360231467}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='electricity_weather', random_seed=2024, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/electricity_weather_epochs_250_seed_2024
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=1.773493005837762
Epoch #1: loss=0.8110241098946728
Epoch #2: loss=0.5636611172032975
Epoch #3: loss=0.4616953991675583
Epoch #4: loss=0.4024525368935093
Epoch #5: loss=0.35499200044516527
Epoch #6: loss=0.3286060368379186
Epoch #7: loss=0.29585113683420233
Epoch #8: loss=0.24115499401041
Epoch #9: loss=0.23215959313451728
Epoch #10: loss=0.23011879689497633
Epoch #11: loss=0.18783360513555214
Epoch #12: loss=0.1665053511580576
Epoch #13: loss=0.15765940659929079
Epoch #14: loss=0.15317078677652893
Epoch #15: loss=0.14435945018610624
Epoch #16: loss=0.14849286653122915
Epoch #17: loss=0.12974516633006955
Epoch #18: loss=0.11799275233096279
Epoch #19: loss=0.1253581900546805
Epoch #20: loss=0.104407499525535
Epoch #21: loss=0.09714435391739998
Epoch #22: loss=0.12553391010320666
Epoch #23: loss=0.08391010178312101
Epoch #24: loss=0.07021957194105177
Epoch #25: loss=0.10426999702888624
Epoch #26: loss=0.07486018535522718
Epoch #27: loss=0.07513741235267532
Epoch #28: loss=0.07886233622535677
Epoch #29: loss=0.08712208276528662
Epoch #30: loss=0.055870912522238594
Epoch #31: loss=0.05386456735405309
Epoch #32: loss=0.058102996159688454
Epoch #33: loss=0.06762041181449373
Epoch #34: loss=0.09800275756845986
Epoch #35: loss=0.048555536947135276
Epoch #36: loss=0.04722313531567088
Epoch #37: loss=0.050765422897504624
Epoch #38: loss=0.05089337410209351
Epoch #39: loss=0.037647910090700135
Epoch #40: loss=0.04652364795922868
Epoch #41: loss=0.04119390175310138
Epoch #42: loss=0.0591975245758977
Epoch #43: loss=0.05200831391660749
Epoch #44: loss=0.052862044450657855
Epoch #45: loss=0.03828111264360827
Epoch #46: loss=0.03287349818931941
Epoch #47: loss=0.055288131954685196
Epoch #48: loss=0.02756741511238261
Epoch #49: loss=0.0450699629620394
Epoch #50: loss=0.04087986911429624
Epoch #51: loss=0.03492022193236926
Epoch #52: loss=0.03182458820731328
Epoch #53: loss=0.031073759645102842
Epoch #54: loss=0.0356188297180475
Epoch #55: loss=0.02921937334771276
Epoch #56: loss=0.029586908040146857
Epoch #57: loss=0.029360782397677452
Epoch #58: loss=0.023340118689642364
Epoch #59: loss=0.030644351870132883
Epoch #60: loss=0.05757145357995243
Epoch #61: loss=0.04124127664490568
Epoch #62: loss=0.027874493238755023
Epoch #63: loss=0.026682004720369832
Epoch #64: loss=0.03299769810843751
Epoch #65: loss=0.03844316246361077
Epoch #66: loss=0.02558674401018543
Epoch #67: loss=0.022602143564083015
Epoch #68: loss=0.023832629237831263
Epoch #69: loss=0.0495498432705356
Epoch #70: loss=0.031316493124558836
Epoch #71: loss=0.018043272054159707
Epoch #72: loss=0.018816912293458297
Epoch #73: loss=0.028157190895795027
Epoch #74: loss=0.02119387296177393
Epoch #75: loss=0.021011346340365975
Epoch #76: loss=0.035144801606539565
Epoch #77: loss=0.03322122380525271
Epoch #78: loss=0.024857185478053816
Epoch #79: loss=0.016811245636375732
Epoch #80: loss=0.018953666421591495
Epoch #81: loss=0.022508955261870005
Epoch #82: loss=0.02466692762470855
Epoch #83: loss=0.01987019831315354
Epoch #84: loss=0.02046555131090059
Epoch #85: loss=0.02306129270996791
Epoch #86: loss=0.019229207471380766
Epoch #87: loss=0.027062333139130625
Epoch #88: loss=0.022729521002003157
Epoch #89: loss=0.020042750625455113
Epoch #90: loss=0.025462301549473536
Epoch #91: loss=0.022330903123577075
Epoch #92: loss=0.024496510997951373
Epoch #93: loss=0.02057606919959821
Epoch #94: loss=0.02908762236094231
Epoch #95: loss=0.03969616909016203
Epoch #96: loss=0.024689638255813537
Epoch #97: loss=0.028952765695868805
Epoch #98: loss=0.016982366360780335
Epoch #99: loss=0.014838240215063674
Epoch #100: loss=0.017745401950483276
Epoch #101: loss=0.02481747054533834
Epoch #102: loss=0.025926209573891627
Epoch #103: loss=0.013277345386626312
Epoch #104: loss=0.014952659507963796
Epoch #105: loss=0.02172577816601145
Epoch #106: loss=0.02235391877523841
Epoch #107: loss=0.014905116642101987
Epoch #108: loss=0.01553306481227284
Epoch #109: loss=0.02122555304703139
Epoch #110: loss=0.017934575910335784
Epoch #111: loss=0.02144307975696872
Epoch #112: loss=0.019999423301818992
Epoch #113: loss=0.021014101904285508
Epoch #114: loss=0.014796051588691154
Epoch #115: loss=0.021257809754991124
Epoch #116: loss=0.014917748808620568
Epoch #117: loss=0.015306553616180415
Epoch #118: loss=0.02743608697810041
Epoch #119: loss=0.016570417393134473
Epoch #120: loss=0.03143713784203475
Epoch #121: loss=0.014000300591092245
Epoch #122: loss=0.017866586218671928
Epoch #123: loss=0.014906506705123352
Epoch #124: loss=0.01342520377194772
Epoch #125: loss=0.015347691492368105
Epoch #126: loss=0.016583586876416145
Epoch #127: loss=0.015310322355729948
Epoch #128: loss=0.023257233619137053
Epoch #129: loss=0.02060293050745223
Epoch #130: loss=0.05670404375090615
Epoch #131: loss=0.026377983700607496
Epoch #132: loss=0.017430939145321806
Epoch #133: loss=0.020393615329714617
Epoch #134: loss=0.012018008121881118
Epoch #135: loss=0.013638103626210844
Epoch #136: loss=0.013990078819412416
Epoch #137: loss=0.012701074641415318
Epoch #138: loss=0.011812556820759892
Epoch #139: loss=0.015600721211483305
Epoch #140: loss=0.020907204526896598
Epoch #141: loss=0.012678287221908554
Epoch #142: loss=0.021917067749978657
Epoch #143: loss=0.01775176227307457
Epoch #144: loss=0.016668411409744042
Epoch #145: loss=0.016691491296997974
Epoch #146: loss=0.013163116482597768
Epoch #147: loss=0.009419336239086003
Epoch #148: loss=0.023319122162600577
Epoch #149: loss=0.012156941677609416
Epoch #150: loss=0.010259028873574097
Epoch #151: loss=0.0193660656538435
Epoch #152: loss=0.018773360628942002
Epoch #153: loss=0.017786221122617435
Epoch #154: loss=0.01882061404613872
Epoch #155: loss=0.01227095927281563
Epoch #156: loss=0.011687324518196604
Epoch #157: loss=0.020654658348465967
Epoch #158: loss=0.013751161375662757
Epoch #159: loss=0.016577859253234094
Epoch #160: loss=0.0402035937087291
Epoch #161: loss=0.013778894815442316
Epoch #162: loss=0.011831880241390237
Epoch #163: loss=0.01043965181200604
Epoch #164: loss=0.014883250659512844
Epoch #165: loss=0.01522711977719595
Epoch #166: loss=0.01358922369682877
Epoch #167: loss=0.016311450068713422
Epoch #168: loss=0.01825872578510904
Epoch #169: loss=0.014739163630485658
Epoch #170: loss=0.01450672659366083
Epoch #171: loss=0.011740393625130819
Epoch #172: loss=0.014903847042099447
Epoch #173: loss=0.007449031244738293
Epoch #174: loss=0.011117942246352793
Epoch #175: loss=0.014546697385614519
Epoch #176: loss=0.023005530479704956
Epoch #177: loss=0.012071534299985465
Epoch #178: loss=0.01276212748595861
Epoch #179: loss=0.02213734282571178
Epoch #180: loss=0.012980553118400651
Epoch #181: loss=0.016544040665567264
Epoch #182: loss=0.016807210438174226
Epoch #183: loss=0.02101076584106703
Epoch #184: loss=0.01845428776957151
Epoch #185: loss=0.020558231256087647
Epoch #186: loss=0.008680266365772906
Epoch #187: loss=0.011376587855186774
Epoch #188: loss=0.015011144074462407
Epoch #189: loss=0.011695310893812101
Epoch #190: loss=0.009083034907821941
Epoch #191: loss=0.013047738963777
Epoch #192: loss=0.010661512086492016
Epoch #193: loss=0.012191545739665934
Epoch #194: loss=0.02231923112144522
Epoch #195: loss=0.015265995439565378
Epoch #196: loss=0.013726695308190708
Epoch #197: loss=0.020004659193475337
Epoch #198: loss=0.01631265551531862
Epoch #199: loss=0.012327433925176372
Epoch #200: loss=0.0112184126584949
Epoch #201: loss=0.013504036852180077
Epoch #202: loss=0.012324555053778921
Epoch #203: loss=0.01387919865613025
Epoch #204: loss=0.008299034384585197
Epoch #205: loss=0.01394145438786105
Epoch #206: loss=0.019846218492770674
Epoch #207: loss=0.009108318560015307
Epoch #208: loss=0.01061959731177345
Epoch #209: loss=0.00991310272130965
Epoch #210: loss=0.008323314722063106
Epoch #211: loss=0.014645156023322043
Epoch #212: loss=0.015637691268598077
Epoch #213: loss=0.01218750195365375
Epoch #214: loss=0.012672438850875954
Epoch #215: loss=0.014186021346066776
Epoch #216: loss=0.014738931723028925
Epoch #217: loss=0.01285694962382848
Epoch #218: loss=0.01327584059177616
Epoch #219: loss=0.012831500295018885
Epoch #220: loss=0.0368358995609025
Epoch #221: loss=0.011208344203613226
Epoch #222: loss=0.009157631386145922
Epoch #223: loss=0.013084700529534712
Epoch #224: loss=0.007795297917019192
Epoch #225: loss=0.00861241920634444
Epoch #226: loss=0.010229543254456432
Epoch #227: loss=0.011548197862704926
Epoch #228: loss=0.014013606745793997
Epoch #229: loss=0.010220542799524415
Epoch #230: loss=0.012419802822462507
Epoch #231: loss=0.01207183165440835
Epoch #232: loss=0.010514875403649015
Epoch #233: loss=0.015301904603684595
Epoch #234: loss=0.011521434639708034
Epoch #235: loss=0.011357987852114915
Epoch #236: loss=0.01701459408518308
Epoch #237: loss=0.014249318608274731
Epoch #238: loss=0.03136264754765229
Epoch #239: loss=0.010611291016788846
Epoch #240: loss=0.022124591882602965
Epoch #241: loss=0.01413000893080607
Epoch #242: loss=0.015596319341922033
Epoch #243: loss=0.011470997833521193
Epoch #244: loss=0.010415502024121026
Epoch #245: loss=0.009731861891950312
Epoch #246: loss=0.016283644591501757
Epoch #247: loss=0.012469757096848642
Epoch #248: loss=0.01093900147476569
Epoch #249: loss=0.008159397333080045

Training time: 4:56:43.684477

Finished.
n2one setting electricity_weather -> exchange
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/electricity_weather_epochs_250_seed_2024/model.pkl', muti_dataset='electricity_weather', random_seed=2024, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='exchange')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=5.32187e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.28906732175124084, 'MAE': 0.3667774891013606}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='electricity_exchange', random_seed=2024, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/electricity_exchange_epochs_250_seed_2024
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=1.5990070811158619
Epoch #1: loss=0.6362758207284933
Epoch #2: loss=0.46748338839022224
Epoch #3: loss=0.36032274890815597
Epoch #4: loss=0.3019709270504108
Epoch #5: loss=0.2779950188376621
Epoch #6: loss=0.24100440443920873
Epoch #7: loss=0.22910396786446266
Epoch #8: loss=0.2167383565321157
Epoch #9: loss=0.1797931610994426
Epoch #10: loss=0.177848866803849
Epoch #11: loss=0.16411910384984002
Epoch #12: loss=0.1432116036175957
Epoch #13: loss=0.12009730546715412
Epoch #14: loss=0.12742802702275452
Epoch #15: loss=0.10830770796419639
Epoch #16: loss=0.10338927033190307
Epoch #17: loss=0.12170567435402095
Epoch #18: loss=0.09814451009850371
Epoch #19: loss=0.08864908873763128
Epoch #20: loss=0.09321468112293892
Epoch #21: loss=0.07702540532094186
Epoch #22: loss=0.07134878165782133
Epoch #23: loss=0.07145339503918284
Epoch #24: loss=0.071393376056354
Epoch #25: loss=0.06267731108824443
Epoch #26: loss=0.07430759421620507
Epoch #27: loss=0.07064292501052417
Epoch #28: loss=0.05661961967889842
Epoch #29: loss=0.07127626119379668
Epoch #30: loss=0.05886748244583969
Epoch #31: loss=0.05271920886621061
Epoch #32: loss=0.05161157986687038
Epoch #33: loss=0.04508650380379229
Epoch #34: loss=0.04857232716744114
Epoch #35: loss=0.04568070097965129
Epoch #36: loss=0.04023158567459365
Epoch #37: loss=0.044491571596452804
Epoch #38: loss=0.060962250446306265
Epoch #39: loss=0.056717839948323454
Epoch #40: loss=0.058256497886881094
Epoch #41: loss=0.05208846395000204
Epoch #42: loss=0.03620832828991681
Epoch #43: loss=0.04034295265978955
Epoch #44: loss=0.04518003018479511
Epoch #45: loss=0.039658048736514656
Epoch #46: loss=0.04174436324157138
Epoch #47: loss=0.031091804102368902
Epoch #48: loss=0.054808914180884655
Epoch #49: loss=0.03677286012706808
Epoch #50: loss=0.040603581950419486
Epoch #51: loss=0.029104897768681127
Epoch #52: loss=0.036681545437610846
Epoch #53: loss=0.044702424389652703
Epoch #54: loss=0.04885715620144975
Epoch #55: loss=0.03145188180726916
Epoch #56: loss=0.026638459234843793
Epoch #57: loss=0.03290986957164385
Epoch #58: loss=0.039858396155519074
Epoch #59: loss=0.03159556930175612
Epoch #60: loss=0.024888458904473387
Epoch #61: loss=0.019765143931665
Epoch #62: loss=0.027824108987426707
Epoch #63: loss=0.03371623619410031
Epoch #64: loss=0.04064691008215821
Epoch #65: loss=0.02784720828118352
Epoch #66: loss=0.03497004094229315
Epoch #67: loss=0.04466524751684246
Epoch #68: loss=0.027599417761613624
Epoch #69: loss=0.027489805078886924
Epoch #70: loss=0.028981748403978498
Epoch #71: loss=0.028152628409347884
Epoch #72: loss=0.03142792728869669
Epoch #73: loss=0.024592420749195022
Epoch #74: loss=0.03551684305076077
Epoch #75: loss=0.026002489901529938
Epoch #76: loss=0.029926343208127926
Epoch #77: loss=0.03131651905762303
Epoch #78: loss=0.02418662265323861
Epoch #79: loss=0.0353937167373534
Epoch #80: loss=0.028047832570500152
Epoch #81: loss=0.028024121907350003
Epoch #82: loss=0.03322906691954672
Epoch #83: loss=0.027854015824234957
Epoch #84: loss=0.03260720069385714
Epoch #85: loss=0.03355804801172659
Epoch #86: loss=0.024796592991651916
Epoch #87: loss=0.01827127601272684
Epoch #88: loss=0.021803027648396655
Epoch #89: loss=0.016376377677114674
Epoch #90: loss=0.03256222470465334
Epoch #91: loss=0.03920231740440505
Epoch #92: loss=0.029508816323343574
Epoch #93: loss=0.029911042706995614
Epoch #94: loss=0.025338432194465504
Epoch #95: loss=0.026835159386230753
Epoch #96: loss=0.02294423318670434
Epoch #97: loss=0.026042885316881183
Epoch #98: loss=0.024472137979630913
Epoch #99: loss=0.014848701883033824
Epoch #100: loss=0.018711556124431577
Epoch #101: loss=0.025076066498364403
Epoch #102: loss=0.028957942410833614
Epoch #103: loss=0.02061133040398314
Epoch #104: loss=0.025924345352446327
Epoch #105: loss=0.024455043581550755
Epoch #106: loss=0.025801540314365677
Epoch #107: loss=0.017650250657914507
Epoch #108: loss=0.023031219656757536
Epoch #109: loss=0.02687714966778335
Epoch #110: loss=0.01801673852287284
Epoch #111: loss=0.02970731289433471
Epoch #112: loss=0.021812418726623897
Epoch #113: loss=0.02100850701085108
Epoch #114: loss=0.013805753964715634
Epoch #115: loss=0.01762089898312991
Epoch #116: loss=0.02723534545708323
Epoch #117: loss=0.017466557000043285
Epoch #118: loss=0.023639135143548352
Epoch #119: loss=0.019005142248201674
Epoch #120: loss=0.022073618724084483
Epoch #121: loss=0.02188146519042993
Epoch #122: loss=0.02934379499468607
Epoch #123: loss=0.025261683588052072
Epoch #124: loss=0.017724616172894028
Epoch #125: loss=0.018134113377997634
Epoch #126: loss=0.0296977851168312
Epoch #127: loss=0.02046592798459805
Epoch #128: loss=0.015398420646909121
Epoch #129: loss=0.023629664509425555
Epoch #130: loss=0.019114997756305097
Epoch #131: loss=0.01648560143235114
Epoch #132: loss=0.018065846682771034
Epoch #133: loss=0.013571054168508352
Epoch #134: loss=0.02434975378789251
Epoch #135: loss=0.034177281812759044
Epoch #136: loss=0.026138623423044963
Epoch #137: loss=0.013967695245434357
Epoch #138: loss=0.01882508786899069
Epoch #139: loss=0.02817420463983481
Epoch #140: loss=0.021632342939909073
Epoch #141: loss=0.021014788648924622
Epoch #142: loss=0.016370858536929878
Epoch #143: loss=0.012082936483191887
Epoch #144: loss=0.017212244493237168
Epoch #145: loss=0.016921576345870187
Epoch #146: loss=0.03200292643064089
Epoch #147: loss=0.018312416800529918
Epoch #148: loss=0.014827211010786436
Epoch #149: loss=0.01706253633109745
Epoch #150: loss=0.01058601164377555
Epoch #151: loss=0.0147502171995591
Epoch #152: loss=0.018549759175358085
Epoch #153: loss=0.016391878404108225
Epoch #154: loss=0.014493180063976355
Epoch #155: loss=0.014040630541304494
Epoch #156: loss=0.010759525842937049
Epoch #157: loss=0.01777805069653957
Epoch #158: loss=0.03656647898738717
Epoch #159: loss=0.030780138539173003
Epoch #160: loss=0.013435781143229217
Epoch #161: loss=0.015712595407993996
Epoch #162: loss=0.018034489927983257
Epoch #163: loss=0.025044666396384623
Epoch #164: loss=0.03150926189766744
Epoch #165: loss=0.022120465365263964
Epoch #166: loss=0.027516168349891006
Epoch #167: loss=0.016453085701111533
Epoch #168: loss=0.016516286229055183
Epoch #169: loss=0.014131453887511351
Epoch #170: loss=0.010849335251586925
Epoch #171: loss=0.010704839941185665
Epoch #172: loss=0.010143876153325355
Epoch #173: loss=0.014127548189556345
Epoch #174: loss=0.016905135144076187
Epoch #175: loss=0.01717448547959475
Epoch #176: loss=0.02267942389142119
Epoch #177: loss=0.021022358602818703
Epoch #178: loss=0.0192426419684219
Epoch #179: loss=0.014283781586353607
Epoch #180: loss=0.019858236789263184
Epoch #181: loss=0.01722155595541504
Epoch #182: loss=0.01572131596988288
Epoch #183: loss=0.019041101385253313
Epoch #184: loss=0.02247806743270715
Epoch #185: loss=0.019583683773042992
Epoch #186: loss=0.022449231159425338
Epoch #187: loss=0.030263315643963055
Epoch #188: loss=0.019721188724372184
Epoch #189: loss=0.013591036547734728
Epoch #190: loss=0.019143269197572317
Epoch #191: loss=0.015098379424997496
Epoch #192: loss=0.013761097659239415
Epoch #193: loss=0.013558538348766434
Epoch #194: loss=0.014163413480643218
Epoch #195: loss=0.0175813825192046
Epoch #196: loss=0.019348435161747096
Epoch #197: loss=0.01802031700460433
Epoch #198: loss=0.013121009186139516
Epoch #199: loss=0.019292790202266822
Epoch #200: loss=0.02810698770546414
Epoch #201: loss=0.014652983141117546
Epoch #202: loss=0.015757299191106144
Epoch #203: loss=0.018207051988165985
Epoch #204: loss=0.0159227300170274
Epoch #205: loss=0.015425573065445269
Epoch #206: loss=0.012988655714178033
Epoch #207: loss=0.014195727770295317
Epoch #208: loss=0.01487040795678822
Epoch #209: loss=0.014783788125998818
Epoch #210: loss=0.014611151815663831
Epoch #211: loss=0.019402904803461272
Epoch #212: loss=0.012550788871040936
Epoch #213: loss=0.010396667370801978
Epoch #214: loss=0.011675508669418308
Epoch #215: loss=0.01324066135000528
Epoch #216: loss=0.01941972427384356
Epoch #217: loss=0.02015942533485985
Epoch #218: loss=0.014913206693920163
Epoch #219: loss=0.009878648167241105
Epoch #220: loss=0.009512958136821693
Epoch #221: loss=0.011746424177058585
Epoch #222: loss=0.019281810915450025
Epoch #223: loss=0.02653131242519482
Epoch #224: loss=0.03441829404596941
Epoch #225: loss=0.011119024287045885
Epoch #226: loss=0.011235881767799726
Epoch #227: loss=0.01759464754027705
Epoch #228: loss=0.013778944656013661
Epoch #229: loss=0.010845642915847612
Epoch #230: loss=0.022644962683850106
Epoch #231: loss=0.014587539924279642
Epoch #232: loss=0.008197065385035169
Epoch #233: loss=0.013293036959706565
Epoch #234: loss=0.015204219253009077
Epoch #235: loss=0.012289878694743264
Epoch #236: loss=0.015894701258758137
Epoch #237: loss=0.03193729867649867
Epoch #238: loss=0.015561759546377885
Epoch #239: loss=0.01607442214907399
Epoch #240: loss=0.015025818599356157
Epoch #241: loss=0.03847234291173121
Epoch #242: loss=0.018986767589781253
Epoch #243: loss=0.011803010233931278
Epoch #244: loss=0.008547271413742395
Epoch #245: loss=0.011441453371167263
Epoch #246: loss=0.019592973970582336
Epoch #247: loss=0.021760212643727333
Epoch #248: loss=0.016738196861637206
Epoch #249: loss=0.015276086581962913

Training time: 4:35:41.793113

Finished.
n2one setting electricity_exchange -> weather
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/electricity_exchange_epochs_250_seed_2024/model.pkl', muti_dataset='electricity_exchange', random_seed=2024, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='weather')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.35184e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.66317e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.45917963873763934, 'MAE': 0.4968793312347462}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='traffic_weather', random_seed=2024, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/traffic_weather_epochs_250_seed_2024
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=1.2908528653552402
Epoch #1: loss=0.4621878617048801
Epoch #2: loss=0.34076903236878886
Epoch #3: loss=0.2592902874053867
Epoch #4: loss=0.2152440184895117
Epoch #5: loss=0.17547652786513707
Epoch #6: loss=0.15398930587495366
Epoch #7: loss=0.12947799922764167
Epoch #8: loss=0.1175709578317699
Epoch #9: loss=0.10997374961467309
Epoch #10: loss=0.08349676738988172
Epoch #11: loss=0.08324592853350951
Epoch #12: loss=0.07688610919914837
Epoch #13: loss=0.08133335420987695
Epoch #14: loss=0.06046453157732439
Epoch #15: loss=0.0536607628575996
Epoch #16: loss=0.05479613597911724
Epoch #17: loss=0.050661740556352995
Epoch #18: loss=0.05787870269593875
Epoch #19: loss=0.04271824258512834
Epoch #20: loss=0.03983170062362275
Epoch #21: loss=0.059936431833586934
Epoch #22: loss=0.04531421286010955
Epoch #23: loss=0.03999342531454015
Epoch #24: loss=0.04020266594844535
Epoch #25: loss=0.03404901963326952
Epoch #26: loss=0.03128433124010707
Epoch #27: loss=0.031654121070132206
Epoch #28: loss=0.03542193461016559
Epoch #29: loss=0.03170706297893409
Epoch #30: loss=0.030663768878324074
Epoch #31: loss=0.038094206064465445
Epoch #32: loss=0.02646774069370958
Epoch #33: loss=0.029341742120720424
Epoch #34: loss=0.030011941546806017
Epoch #35: loss=0.030642225778989937
Epoch #36: loss=0.0301534719238171
Epoch #37: loss=0.025628373070700233
Epoch #38: loss=0.027978075615645318
Epoch #39: loss=0.0213803860247067
Epoch #40: loss=0.02350703473374623
Epoch #41: loss=0.029581588654944604
Epoch #42: loss=0.03511492055743778
Epoch #43: loss=0.027360040759893244
Epoch #44: loss=0.020365636686488737
Epoch #45: loss=0.023775370798240467
Epoch #46: loss=0.04137900285658779
Epoch #47: loss=0.026269054135162413
Epoch #48: loss=0.021618834230000036
Epoch #49: loss=0.024583626581449795
Epoch #50: loss=0.020836875756563008
Epoch #51: loss=0.026647452836022986
Epoch #52: loss=0.021471662083320815
Epoch #53: loss=0.021801407556103932
Epoch #54: loss=0.025859352574591432
Epoch #55: loss=0.03919338146918128
Epoch #56: loss=0.022093741281095973
Epoch #57: loss=0.019272661694189952
Epoch #58: loss=0.02100879845985784
Epoch #59: loss=0.016377349249818804
Epoch #60: loss=0.03733307853224949
Epoch #61: loss=0.020063702818538535
Epoch #62: loss=0.020287056905749726
Epoch #63: loss=0.01987276211333632
Epoch #64: loss=0.031091020366639476
Epoch #65: loss=0.019091711183267895
Epoch #66: loss=0.014199180188812976
Epoch #67: loss=0.0209691571858334
Epoch #68: loss=0.02062016900300797
Epoch #69: loss=0.014393955930269865
Epoch #70: loss=0.017640208655631123
Epoch #71: loss=0.01853929034690294
Epoch #72: loss=0.017610197080832043
Epoch #73: loss=0.018390605152449058
Epoch #74: loss=0.027617146110109438
Epoch #75: loss=0.013637230408389482
Epoch #76: loss=0.014854993838263646
Epoch #77: loss=0.02197920209958259
Epoch #78: loss=0.017645400819592313
Epoch #79: loss=0.018980646276890317
Epoch #80: loss=0.012650926949235811
Epoch #81: loss=0.018792398849794292
Epoch #82: loss=0.019677193685302873
Epoch #83: loss=0.015869800451050117
Epoch #84: loss=0.017256277329120617
Epoch #85: loss=0.017810784995834855
Epoch #86: loss=0.015014648443366651
Epoch #87: loss=0.021500376365791225
Epoch #88: loss=0.02722860498394217
Epoch #89: loss=0.013350460133887618
Epoch #90: loss=0.013295151815999579
Epoch #91: loss=0.015556185678186929
Epoch #92: loss=0.015375187432984423
Epoch #93: loss=0.017650700522207043
Epoch #94: loss=0.01737956260090082
Epoch #95: loss=0.014778908003659904
Epoch #96: loss=0.017670095152851416
Epoch #97: loss=0.019431424017147127
Epoch #98: loss=0.020515693995057028
Epoch #99: loss=0.013762314444164502
Epoch #100: loss=0.011747565779916005
Epoch #101: loss=0.01603409756382855
Epoch #102: loss=0.013128083885627953
Epoch #103: loss=0.023984522027407168
Epoch #104: loss=0.015124304186879722
Epoch #105: loss=0.016104774007610168
Epoch #106: loss=0.025946847042333656
Epoch #107: loss=0.01547736685054727
Epoch #108: loss=0.010089081980818714
Epoch #109: loss=0.016241659654140775
Epoch #110: loss=0.015338558655023482
Epoch #111: loss=0.01206021875134923
Epoch #112: loss=0.012621987280130401
Epoch #113: loss=0.017282734051355168
Epoch #114: loss=0.016174449848525303
Epoch #115: loss=0.013804845380683866
Epoch #116: loss=0.01949745603859491
Epoch #117: loss=0.013198219362419803
Epoch #118: loss=0.014956797658046812
Epoch #119: loss=0.012929395358164052
Epoch #120: loss=0.017835404826942354
Epoch #121: loss=0.01365375551773337
Epoch #122: loss=0.016330586088352483
Epoch #123: loss=0.01494238457471019
Epoch #124: loss=0.015553569678698434
Epoch #125: loss=0.011300848640432279
Epoch #126: loss=0.017877452102454897
Epoch #127: loss=0.010129275205579213
Epoch #128: loss=0.010410297980572298
Epoch #129: loss=0.013474802558063471
Epoch #130: loss=0.013361946305349603
Epoch #131: loss=0.013171715047032162
Epoch #132: loss=0.013764001511722076
Epoch #133: loss=0.010237773819844241
Epoch #134: loss=0.012894805457665472
Epoch #135: loss=0.0138735254850044
Epoch #136: loss=0.014232669150432403
Epoch #137: loss=0.017446606479958014
Epoch #138: loss=0.018078056331891735
Epoch #139: loss=0.01315391022266389
Epoch #140: loss=0.0113499130058109
Epoch #141: loss=0.009177496591160758
Epoch #142: loss=0.015300847723558839
Epoch #143: loss=0.022363986528157635
Epoch #144: loss=0.009754704791801586
Epoch #145: loss=0.011585938897533688
Epoch #146: loss=0.013874635723606171
Epoch #147: loss=0.012863672536941767
Epoch #148: loss=0.011026775866592469
Epoch #149: loss=0.010436041307819145
Epoch #150: loss=0.017085228938224457
Epoch #151: loss=0.011150506848869488
Epoch #152: loss=0.017613721841330476
Epoch #153: loss=0.011650932007640976
Epoch #154: loss=0.013619202033779558
Epoch #155: loss=0.01024730086702458
Epoch #156: loss=0.008450212871016324
Epoch #157: loss=0.015135160377696168
Epoch #158: loss=0.011990200401530135
Epoch #159: loss=0.03204742570396939
Epoch #160: loss=0.012960910680408848
Epoch #161: loss=0.00903603090933718
Epoch #162: loss=0.013224162533784233
Epoch #163: loss=0.0405010862691802
Epoch #164: loss=0.011736752157253455
Epoch #165: loss=0.012542262963006989
Epoch #166: loss=0.013156124869776971
Epoch #167: loss=0.011841717847821676
Epoch #168: loss=0.017359388489198824
Epoch #169: loss=0.012154227969952993
Epoch #170: loss=0.016315939614565927
Epoch #171: loss=0.012496789139445068
Epoch #172: loss=0.01579515780434343
Epoch #173: loss=0.013259711981067216
Epoch #174: loss=0.008430016533685039
Epoch #175: loss=0.029535715996504994
Epoch #176: loss=0.010404177142730873
Epoch #177: loss=0.009223305953855333
Epoch #178: loss=0.009598313555617497
Epoch #179: loss=0.012117051755168333
Epoch #180: loss=0.006746036008251821
Epoch #181: loss=0.009964342351677997
Epoch #182: loss=0.007557127760742128
Epoch #183: loss=0.015450987460074405
Epoch #184: loss=0.011636373129173686
Epoch #185: loss=0.015207175292465484
Epoch #186: loss=0.013396552258546426
Epoch #187: loss=0.013978994971071475
Epoch #188: loss=0.012427897484362679
Epoch #189: loss=0.015239345355505644
Epoch #190: loss=0.01053909004419053
Epoch #191: loss=0.016703216285833634
Epoch #192: loss=0.009935899914043976
Epoch #193: loss=0.009075291036186844
Epoch #194: loss=0.011346561239816266
Epoch #195: loss=0.015945536218390084
Epoch #196: loss=0.013149806892715377
Epoch #197: loss=0.01763118275977815
Epoch #198: loss=0.011618840578185874
Epoch #199: loss=0.011084175546079883
Epoch #200: loss=0.009619394610864047
Epoch #201: loss=0.010768044973759118
Epoch #202: loss=0.011597047268419824
Epoch #203: loss=0.010051422428184631
Epoch #204: loss=0.010137736488153727
Epoch #205: loss=0.008039230406522931
Epoch #206: loss=0.015030241690600722
Epoch #207: loss=0.011908923873544391
Epoch #208: loss=0.008221693276789517
Epoch #209: loss=0.010728137445750673
Epoch #210: loss=0.012683007882597021
Epoch #211: loss=0.009748020982429738
Epoch #212: loss=0.011554652237447937
Epoch #213: loss=0.011715543504750885
Epoch #214: loss=0.009190967544520128
Epoch #215: loss=0.02102521216735064
Epoch #216: loss=0.013979710854933679
Epoch #217: loss=0.009637124860714434
Epoch #218: loss=0.008913451910196166
Epoch #219: loss=0.014302625772665828
Epoch #220: loss=0.007740400262755921
Epoch #221: loss=0.010674317411915556
Epoch #222: loss=0.012260459208789086
Epoch #223: loss=0.008362144456056354
Epoch #224: loss=0.011342536370492625
Epoch #225: loss=0.010062153794425686
Epoch #226: loss=0.008541907706252837
Epoch #227: loss=0.01572284188587874
Epoch #228: loss=0.012057494765063926
Epoch #229: loss=0.009191623317462415
Epoch #230: loss=0.006851381332492018
Epoch #231: loss=0.013774943907772976
Epoch #232: loss=0.006117229158007831
Epoch #233: loss=0.008387016272343579
Epoch #234: loss=0.008885776256065911
Epoch #235: loss=0.007295763433445769
Epoch #236: loss=0.009233842794462878
Epoch #237: loss=0.01089332890865285
Epoch #238: loss=0.011140468857250584
Epoch #239: loss=0.01473141414286337
Epoch #240: loss=0.015045507666129837
Epoch #241: loss=0.012267512523763586
Epoch #242: loss=0.010071871479823614
Epoch #243: loss=0.013593834528119015
Epoch #244: loss=0.009198997564732333
Epoch #245: loss=0.012424652444192427
Epoch #246: loss=0.008711660462001607
Epoch #247: loss=0.005609910932000751
Epoch #248: loss=0.011290284544999054
Epoch #249: loss=0.012275144425222755

Training time: 10:37:04.734495

Finished.
n2one setting traffic_weather -> electricity
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/traffic_weather_epochs_250_seed_2024/model.pkl', muti_dataset='traffic_weather', random_seed=2024, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='electricity')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=9.67943e-09): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.16158e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=5.4401e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=9.67943e-09): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.38824329743576236, 'MAE': 0.4440802151497883}
Finished.
------------------------- record done -------------------------
n2one setting traffic_weather -> exchange
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/traffic_weather_epochs_250_seed_2024/model.pkl', muti_dataset='traffic_weather', random_seed=2024, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='exchange')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=3.31274e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.38199418566604776, 'MAE': 0.39521319600183835}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='traffic_exchange', random_seed=2024, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/traffic_exchange_epochs_250_seed_2024
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=1.193210129285681
Epoch #1: loss=0.4151170279102764
Epoch #2: loss=0.2953292493076845
Epoch #3: loss=0.22561756110396877
Epoch #4: loss=0.17971246230533752
Epoch #5: loss=0.1477359265688507
Epoch #6: loss=0.12279977935055891
Epoch #7: loss=0.09910348074660562
Epoch #8: loss=0.08619606388390236
Epoch #9: loss=0.08922585137616629
Epoch #10: loss=0.07998117020824688
Epoch #11: loss=0.07106834033065229
Epoch #12: loss=0.059264560939273105
Epoch #13: loss=0.06588127686931142
Epoch #14: loss=0.0580958168027687
Epoch #15: loss=0.05527552551852175
Epoch #16: loss=0.05016897387947116
Epoch #17: loss=0.039554019331471756
Epoch #18: loss=0.04738790930152453
Epoch #19: loss=0.04864156389683913
Epoch #20: loss=0.039089785245429165
Epoch #21: loss=0.0401923222763025
Epoch #22: loss=0.04489024604627884
Epoch #23: loss=0.032426127990515065
Epoch #24: loss=0.032600674581521286
Epoch #25: loss=0.04503818784396153
Epoch #26: loss=0.033516185919770264
Epoch #27: loss=0.035322671583398704
Epoch #28: loss=0.029868548604574097
Epoch #29: loss=0.03759677026303078
Epoch #30: loss=0.03540429461107793
Epoch #31: loss=0.04286849735894818
Epoch #32: loss=0.024481148020251944
Epoch #33: loss=0.0427364867069523
Epoch #34: loss=0.027892746977459897
Epoch #35: loss=0.02782839636797428
Epoch #36: loss=0.027243041921535444
Epoch #37: loss=0.031509022346039396
Epoch #38: loss=0.02652988538745491
Epoch #39: loss=0.028677091331571748
Epoch #40: loss=0.028388893153202643
Epoch #41: loss=0.024357684433792087
Epoch #42: loss=0.023807071705332732
Epoch #43: loss=0.02512455278219409
Epoch #44: loss=0.02341329236575081
Epoch #45: loss=0.019242399184357096
Epoch #46: loss=0.02387999986772072
Epoch #47: loss=0.028938454742364093
Epoch #48: loss=0.025039365829501153
Epoch #49: loss=0.03254789921104501
Epoch #50: loss=0.025251844205797232
Epoch #51: loss=0.024317036834197527
Epoch #52: loss=0.019080963706625785
Epoch #53: loss=0.025864885416719528
Epoch #54: loss=0.02107833470806651
Epoch #55: loss=0.02874840306183029
Epoch #56: loss=0.02447324038875268
Epoch #57: loss=0.021940818672562177
Epoch #58: loss=0.016826905401479773
Epoch #59: loss=0.026647166232912984
Epoch #60: loss=0.024209749447141677
Epoch #61: loss=0.017173778825177243
Epoch #62: loss=0.028411980833183845
Epoch #63: loss=0.017129115069428213
Epoch #64: loss=0.024015392070594672
Epoch #65: loss=0.02052862288627035
Epoch #66: loss=0.01699994026332687
Epoch #67: loss=0.020830312433542855
Epoch #68: loss=0.02268510803529436
Epoch #69: loss=0.020210664272649566
Epoch #70: loss=0.017760103544549502
Epoch #71: loss=0.01727133592545194
Epoch #72: loss=0.029366886699786304
Epoch #73: loss=0.02603471855326534
Epoch #74: loss=0.017766857984938777
Epoch #75: loss=0.016617654552736758
Epoch #76: loss=0.01872335075542609
Epoch #77: loss=0.021546463501551347
Epoch #78: loss=0.018547886252982326
Epoch #79: loss=0.01725812584219297
Epoch #80: loss=0.0195939848040252
Epoch #81: loss=0.01631318714594115
Epoch #82: loss=0.025405187779922445
Epoch #83: loss=0.020415301373310057
Epoch #84: loss=0.02297888485013625
Epoch #85: loss=0.016416615275893595
Epoch #86: loss=0.018196911662522616
Epoch #87: loss=0.022675149271735685
Epoch #88: loss=0.014775210275447221
Epoch #89: loss=0.016001165888724505
Epoch #90: loss=0.018563719564746402
Epoch #91: loss=0.02241043724590008
Epoch #92: loss=0.0170058963500907
Epoch #93: loss=0.015393542381115231
Epoch #94: loss=0.016631081992467257
Epoch #95: loss=0.02023496684215524
Epoch #96: loss=0.0202003988680221
Epoch #97: loss=0.01615884735452375
Epoch #98: loss=0.01827090197599402
Epoch #99: loss=0.016852523465326386
Epoch #100: loss=0.01748702351436094
Epoch #101: loss=0.01651171166894706
Epoch #102: loss=0.019306006936902372
Epoch #103: loss=0.017955253432282707
Epoch #104: loss=0.017164284448191555
Epoch #105: loss=0.013418107579365887
Epoch #106: loss=0.012800033064960534
Epoch #107: loss=0.01255582243618261
Epoch #108: loss=0.0150925275041766
Epoch #109: loss=0.01726631256898908
Epoch #110: loss=0.014033039993554321
Epoch #111: loss=0.013414807178999361
Epoch #112: loss=0.013831557609520895
Epoch #113: loss=0.028916063368448923
Epoch #114: loss=0.015695277149317354
Epoch #115: loss=0.013660018675560506
Epoch #116: loss=0.015504726859969729
Epoch #117: loss=0.016016767244055416
Epoch #118: loss=0.020266073046187903
Epoch #119: loss=0.025724483887703778
Epoch #120: loss=0.01499929869779648
Epoch #121: loss=0.017196148860331186
Epoch #122: loss=0.015359862312980949
Epoch #123: loss=0.012752078452660038
Epoch #124: loss=0.01723824362146341
Epoch #125: loss=0.01757197866427723
Epoch #126: loss=0.02067969655122972
Epoch #127: loss=0.013737974488096119
Epoch #128: loss=0.01725174942536911
Epoch #129: loss=0.017866517418537866
Epoch #130: loss=0.01589875238718085
Epoch #131: loss=0.015534140489628838
Epoch #132: loss=0.012694367759405427
Epoch #133: loss=0.01573523379157795
Epoch #134: loss=0.014269942087964315
Epoch #135: loss=0.015349506580601338
Epoch #136: loss=0.014013373478109702
Epoch #137: loss=0.017569653583147254
Epoch #138: loss=0.016325636848274767
Epoch #139: loss=0.014758024598109194
Epoch #140: loss=0.01542968016800915
Epoch #141: loss=0.014436819257874352
Epoch #142: loss=0.01926928182241598
Epoch #143: loss=0.010376123851694343
Epoch #144: loss=0.01745015556851118
Epoch #145: loss=0.013898854883656645
Epoch #146: loss=0.012273305549199998
Epoch #147: loss=0.016852389456437977
Epoch #148: loss=0.02112492345348429
Epoch #149: loss=0.009949213044665053
Epoch #150: loss=0.017813712021071015
Epoch #151: loss=0.01302246426843674
Epoch #152: loss=0.012531862573914936
Epoch #153: loss=0.013603764183514206
Epoch #154: loss=0.013157471454228627
Epoch #155: loss=0.012167258956837575
Epoch #156: loss=0.01361438536206287
Epoch #157: loss=0.012792845987935614
Epoch #158: loss=0.015274835259743281
Epoch #159: loss=0.015200645842183991
Epoch #160: loss=0.015074591818931326
Epoch #161: loss=0.007947752175848642
Epoch #162: loss=0.014368258955608787
Epoch #163: loss=0.014530192129012872
Epoch #164: loss=0.01200566556576998
Epoch #165: loss=0.01027118525255054
Epoch #166: loss=0.01154201656956983
Epoch #167: loss=0.01549332860772613
Epoch #168: loss=0.010857255435424948
Epoch #169: loss=0.013063758135446744
Epoch #170: loss=0.01142230924592225
Epoch #171: loss=0.015340488349507702
Epoch #172: loss=0.01250499591690317
Epoch #173: loss=0.012237615502683978
Epoch #174: loss=0.011843426117252682
Epoch #175: loss=0.011715141944463455
Epoch #176: loss=0.019440731603867004
Epoch #177: loss=0.01996322916803919
Epoch #178: loss=0.008785725528872424
Epoch #179: loss=0.017624920545495117
Epoch #180: loss=0.014023484504261809
Epoch #181: loss=0.018788660111590878
Epoch #182: loss=0.012623038143588594
Epoch #183: loss=0.010105179545476942
Epoch #184: loss=0.010539442651414033
Epoch #185: loss=0.010230752549851877
Epoch #186: loss=0.016270088789858802
Epoch #187: loss=0.014248331718785078
Epoch #188: loss=0.012457321195270406
Epoch #189: loss=0.010507971181396406
Epoch #190: loss=0.01443593764673295
Epoch #191: loss=0.01014120713508241
Epoch #192: loss=0.014515551397156762
Epoch #193: loss=0.014427748901418935
Epoch #194: loss=0.014598788909467563
Epoch #195: loss=0.010434078903107411
Epoch #196: loss=0.018645554291597574
Epoch #197: loss=0.01664684036671018
Epoch #198: loss=0.009247973470513083
Epoch #199: loss=0.009592180596484009
Epoch #200: loss=0.013846066281508558
Epoch #201: loss=0.009030907946249373
Epoch #202: loss=0.012195516821738311
Epoch #203: loss=0.011937400695885232
Epoch #204: loss=0.015526375035693783
Epoch #205: loss=0.016282318298264684
Epoch #206: loss=0.013696212336846928
Epoch #207: loss=0.011298840421993934
Epoch #208: loss=0.00928373006936872
Epoch #209: loss=0.012648649867282827
Epoch #210: loss=0.020375814629469774
Epoch #211: loss=0.011855772940171668
Epoch #212: loss=0.011892983808579158
Epoch #213: loss=0.008077748596096872
Epoch #214: loss=0.02244148707571279
Epoch #215: loss=0.011471043485644991
Epoch #216: loss=0.016291109537896458
Epoch #217: loss=0.01382156982338832
Epoch #218: loss=0.010589974467581052
Epoch #219: loss=0.011294030516285278
Epoch #220: loss=0.013625249183360734
Epoch #221: loss=0.01168136441257706
Epoch #222: loss=0.010199293376855826
Epoch #223: loss=0.015278544241225283
Epoch #224: loss=0.013807164531618614
Epoch #225: loss=0.010257129414654712
Epoch #226: loss=0.014249851815295459
Epoch #227: loss=0.010463961565414787
Epoch #228: loss=0.015025321411700845
Epoch #229: loss=0.018304069018648433
Epoch #230: loss=0.010778351903935873
Epoch #231: loss=0.009242227704524486
Epoch #232: loss=0.010737804789430644
Epoch #233: loss=0.010009291441910418
Epoch #234: loss=0.00931369134321295
Epoch #235: loss=0.013679206074080323
Epoch #236: loss=0.010162788416622286
Epoch #237: loss=0.012070964400833151
Epoch #238: loss=0.010705280402891376
Epoch #239: loss=0.008642103526829582
Epoch #240: loss=0.01037656068828642
Epoch #241: loss=0.010814319679231903
Epoch #242: loss=0.012276371174335384
Epoch #243: loss=0.011564774468298447
Epoch #244: loss=0.009268149703603248
Epoch #245: loss=0.01007105808249466
Epoch #246: loss=0.008431262520895254
Epoch #247: loss=0.008710681450924542
Epoch #248: loss=0.012524310523465452
Epoch #249: loss=0.013721019853206055

Training time: 10:22:09.713904

Finished.
n2one setting traffic_exchange -> electricity
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/traffic_exchange_epochs_250_seed_2024/model.pkl', muti_dataset='traffic_exchange', random_seed=2024, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='electricity')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=8.87949e-09): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.76161e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=4.37716e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=8.87949e-09): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.38977508893624857, 'MAE': 0.4424560393552273}
Finished.
------------------------- record done -------------------------
n2one setting traffic_exchange -> weather
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/traffic_exchange_epochs_250_seed_2024/model.pkl', muti_dataset='traffic_exchange', random_seed=2024, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='weather')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.26282e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.539e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.26282e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.8761726091221034, 'MAE': 0.7617344337032024}
Finished.
------------------------- record done -------------------------
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=250, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.0003, max_threads=None, max_train_length=3000, muti_dataset='weather_exchange', random_seed=2024, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None)
training/weather_exchange_epochs_250_seed_2024
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
done
Epoch #0: loss=15.491628324284273
Epoch #1: loss=3.7070529916707207
Epoch #2: loss=4.6113925295717575
Epoch #3: loss=2.57665134528104
Epoch #4: loss=2.6597339756348553
Epoch #5: loss=2.57803551940357
Epoch #6: loss=2.251366972923279
Epoch #7: loss=2.3310341905145084
Epoch #8: loss=2.07915055401185
Epoch #9: loss=2.0679775371271023
Epoch #10: loss=1.9433398422072916
Epoch #11: loss=1.96491240753847
Epoch #12: loss=1.8646960924653446
Epoch #13: loss=1.8052997063187992
Epoch #14: loss=1.8091281301835005
Epoch #15: loss=1.6822860836982727
Epoch #16: loss=1.6486513264038984
Epoch #17: loss=1.5270341459442587
Epoch #18: loss=1.5258240804952734
Epoch #19: loss=1.4667738430640276
Epoch #20: loss=1.45630443797392
Epoch #21: loss=1.3639496074003332
Epoch #22: loss=1.3512639508527868
Epoch #23: loss=1.2341296848128824
Epoch #24: loss=1.3511331396944382
Epoch #25: loss=1.277538397732903
Epoch #26: loss=1.251739295089946
Epoch #27: loss=1.1829726590829737
Epoch #28: loss=1.1656474961954004
Epoch #29: loss=1.0995396919110243
Epoch #30: loss=1.0763368343605715
Epoch #31: loss=1.1024210382910336
Epoch #32: loss=1.0920233481070574
Epoch #33: loss=1.3672908947748297
Epoch #34: loss=1.0981895906083725
Epoch #35: loss=1.0863708117428947
Epoch #36: loss=1.0640244203455307
Epoch #37: loss=1.027027294916265
Epoch #38: loss=1.0146754226263832
Epoch #39: loss=0.9604970321935766
Epoch #40: loss=0.9344785266062793
Epoch #41: loss=0.9300965842078713
Epoch #42: loss=0.8768340559566722
Epoch #43: loss=0.9433754402048448
Epoch #44: loss=0.8890511270831613
Epoch #45: loss=0.8202780502683976
Epoch #46: loss=0.8013486616751727
Epoch #47: loss=0.7768791019916534
Epoch #48: loss=0.7924240757437313
Epoch #49: loss=0.7627584609915229
Epoch #50: loss=0.8045078901683583
Epoch #51: loss=0.7870716610375572
Epoch #52: loss=0.7647839854745304
Epoch #53: loss=0.7244090639493045
Epoch #54: loss=0.7952345039914636
Epoch #55: loss=0.791047999087502
Epoch #56: loss=0.7062809888054343
Epoch #57: loss=0.7038246401969124
Epoch #58: loss=0.7080641818397185
Epoch #59: loss=0.6467972494223538
Epoch #60: loss=0.7202885633005816
Epoch #61: loss=0.6416915260693606
Epoch #62: loss=0.6249164185103249
Epoch #63: loss=0.8012372903964099
Epoch #64: loss=0.7668096046237385
Epoch #65: loss=0.8135557674309787
Epoch #66: loss=0.6677237950703677
Epoch #67: loss=0.638981068835539
Epoch #68: loss=0.6717474302824806
Epoch #69: loss=0.5601272916092592
Epoch #70: loss=0.6729629627045464
Epoch #71: loss=0.5587050599210402
Epoch #72: loss=0.5826801754095975
Epoch #73: loss=0.5955697692492429
Epoch #74: loss=0.6149214979480294
Epoch #75: loss=0.48489587271914764
Epoch #76: loss=0.5095559945877861
Epoch #77: loss=0.4473057310370838
Epoch #78: loss=0.472946145955254
Epoch #79: loss=0.4966733148869346
Epoch #80: loss=0.5210770219564438
Epoch #81: loss=0.49236131678609285
Epoch #82: loss=0.5249292605063495
Epoch #83: loss=0.4942598360426286
Epoch #84: loss=0.4596065352944767
Epoch #85: loss=0.55191470682621
Epoch #86: loss=0.5039698157240363
Epoch #87: loss=0.5131951544214698
Epoch #88: loss=0.43675943727002425
Epoch #89: loss=0.4144962351111805
Epoch #90: loss=0.41320785003549915
Epoch #91: loss=0.38211071622722287
Epoch #92: loss=0.39160512957502813
Epoch #93: loss=0.6214310899376869
Epoch #94: loss=0.7926870225106969
Epoch #95: loss=0.6454662373837303
Epoch #96: loss=0.539843169205329
Epoch #97: loss=0.39187978208065033
Epoch #98: loss=0.45404906088815017
Epoch #99: loss=0.41009798251530705
Epoch #100: loss=0.3984825606731808
Epoch #101: loss=0.4201343572315048
Epoch #102: loss=0.5055558335255174
Epoch #103: loss=0.3888758214957574
Epoch #104: loss=0.35210027633344426
Epoch #105: loss=0.3423187496031032
Epoch #106: loss=0.3341575212338391
Epoch #107: loss=0.3161887224106228
Epoch #108: loss=0.39646311805528756
Epoch #109: loss=0.36285797594224706
Epoch #110: loss=0.3759591461104505
Epoch #111: loss=0.2804803019937347
Epoch #112: loss=0.330169913085068
Epoch #113: loss=0.31397408772917357
Epoch #114: loss=0.3064885538290529
Epoch #115: loss=0.31663238257169724
Epoch #116: loss=0.3139474247308338
Epoch #117: loss=0.3436783563126536
Epoch #118: loss=0.3370229842908242
Epoch #119: loss=0.36601586946669745
Epoch #120: loss=0.26615555211901665
Epoch #121: loss=0.2965347580611706
Epoch #122: loss=0.2547348227132769
Epoch #123: loss=0.351547903216937
Epoch #124: loss=0.24615441372289376
Epoch #125: loss=0.32306140705066566
Epoch #126: loss=0.23443977600511381
Epoch #127: loss=0.26571754322332497
Epoch #128: loss=0.2768962264937513
Epoch #129: loss=0.3136821858146611
Epoch #130: loss=0.2791837878963527
Epoch #131: loss=0.18270641674890237
Epoch #132: loss=0.22623581373516252
Epoch #133: loss=0.24243601652629235
Epoch #134: loss=0.29796557435218024
Epoch #135: loss=0.2643568329513073
Epoch #136: loss=0.19020417113514507
Epoch #137: loss=0.24620410712326274
Epoch #138: loss=0.31312888794962096
Epoch #139: loss=0.3471147093702765
Epoch #140: loss=0.2792442064074909
Epoch #141: loss=0.2241210902438444
Epoch #142: loss=0.21579534779576695
Epoch #143: loss=0.2029303829459583
Epoch #144: loss=0.20127151994144216
Epoch #145: loss=0.2205750918125405
Epoch #146: loss=0.21119272073402123
Epoch #147: loss=0.19688235069899
Epoch #148: loss=0.22647784332580426
Epoch #149: loss=0.25048404186964035
Epoch #150: loss=0.20426814214271657
Epoch #151: loss=0.2134494748623932
Epoch #152: loss=0.21563775048536413
Epoch #153: loss=0.18293615638771477
Epoch #154: loss=0.21106594943386667
Epoch #155: loss=0.20088490328806288
Epoch #156: loss=0.18729684304665117
Epoch #157: loss=0.24338368940002778
Epoch #158: loss=0.16512486229048057
Epoch #159: loss=0.19743232177022627
Epoch #160: loss=0.25941066312439304
Epoch #161: loss=0.21144983413464882
Epoch #162: loss=0.19699402142535238
Epoch #163: loss=0.15011963631738634
Epoch #164: loss=0.20982787560890703
Epoch #165: loss=0.21841598828049266
Epoch #166: loss=0.19844543517512434
Epoch #167: loss=0.2531539379235576
Epoch #168: loss=0.310826652628534
Epoch #169: loss=0.1560280301115092
Epoch #170: loss=0.12653722011429422
Epoch #171: loss=0.19402793542865446
Epoch #172: loss=0.27049565074198384
Epoch #173: loss=0.43806006364962635
Epoch #174: loss=0.21983159136246233
Epoch #175: loss=0.15902369195485816
Epoch #176: loss=0.20017624624511776
Epoch #177: loss=0.1468935630777303
Epoch #178: loss=0.15863941566032522
Epoch #179: loss=0.1471040098763564
Epoch #180: loss=0.13798821947592146
Epoch #181: loss=0.1470885879414923
Epoch #182: loss=0.19515595743980477
Epoch #183: loss=0.17157064498785665
Epoch #184: loss=0.17148669447530718
Epoch #185: loss=0.151677202214213
Epoch #186: loss=0.16321254116209113
Epoch #187: loss=0.1272540724671939
Epoch #188: loss=0.1284622722031439
Epoch #189: loss=0.15274102711940513
Epoch #190: loss=0.17818753678789911
Epoch #191: loss=0.15671300340224714
Epoch #192: loss=0.19982620054746375
Epoch #193: loss=0.1547344113316606
Epoch #194: loss=0.14851929466514027
Epoch #195: loss=0.18372562670094125
Epoch #196: loss=0.19339912231354153
Epoch #197: loss=0.22646828379262895
Epoch #198: loss=0.23455926074701197
Epoch #199: loss=0.13421871984267936
Epoch #200: loss=0.14533374666729393
Epoch #201: loss=0.13861094732933185
Epoch #202: loss=0.12992811761796474
Epoch #203: loss=0.11655441507258836
Epoch #204: loss=0.13435078193159664
Epoch #205: loss=0.1479537060812992
Epoch #206: loss=0.14199117965557995
Epoch #207: loss=0.10576894649249666
Epoch #208: loss=0.09560668934136629
Epoch #209: loss=0.14823056812233784
Epoch #210: loss=0.11428723517147933
Epoch #211: loss=0.1815929278073942
Epoch #212: loss=0.17762502458165674
Epoch #213: loss=0.11232484345707823
Epoch #214: loss=0.1155084035194972
Epoch #215: loss=0.11597833705737311
Epoch #216: loss=0.09751709545140758
Epoch #217: loss=0.13498278038904948
Epoch #218: loss=0.08765308854772765
Epoch #219: loss=0.08073546759345952
Epoch #220: loss=0.09214508358170004
Epoch #221: loss=0.15435930451049523
Epoch #222: loss=0.13794985062935772
Epoch #223: loss=0.107031610520447
Epoch #224: loss=0.11506335470168029
Epoch #225: loss=0.09391164204434437
Epoch #226: loss=0.1249843794195091
Epoch #227: loss=0.08415981940925121
Epoch #228: loss=0.10108895501231446
Epoch #229: loss=0.13369327235747785
Epoch #230: loss=0.12421484121724087
Epoch #231: loss=0.1446576310759958
Epoch #232: loss=0.282710077953251
Epoch #233: loss=0.18109985057483702
Epoch #234: loss=0.15657217318520827
Epoch #235: loss=0.1539767921409186
Epoch #236: loss=0.12046589861240457
Epoch #237: loss=0.1521690302692792
Epoch #238: loss=0.15224058230352752
Epoch #239: loss=0.21296815837130828
Epoch #240: loss=0.11351581597152878
Epoch #241: loss=0.19455274709445589
Epoch #242: loss=0.12916809879243374
Epoch #243: loss=0.12543233177241156
Epoch #244: loss=0.08776899766834344
Epoch #245: loss=0.08543854288975983
Epoch #246: loss=0.13826659716227474
Epoch #247: loss=0.10896433709079728
Epoch #248: loss=0.19123570144395619
Epoch #249: loss=0.13517616787815795

Training time: 0:32:38.489804

Finished.
n2one setting weather_exchange -> electricity
Dataset: ETTh1
Arguments: Namespace(batch_size=8, dataset='ETTh1', epochs=None, eval=False, gpu=2, irregular=0, iters=None, loader='forecast_csv', lr=0.001, max_threads=None, max_train_length=3000, model_name='TS2vec', model_path='./training/weather_exchange_epochs_250_seed_2024/model.pkl', muti_dataset='weather_exchange', random_seed=2024, record=1, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=None, target='electricity')
Loading data... /home/yupengz/ts2vec/datautils.py:136: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)
  dt.weekofyear.to_numpy(),
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.74623e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=3.40274e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
/home/yupengz/anaconda3/envs/TimesNet/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.74623e-08): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T
Evaluation result: {'MSE': 0.35107463109314, 'MAE': 0.41809515482114684}
Finished.
------------------------- record done -------------------------
